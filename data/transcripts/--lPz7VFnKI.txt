 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So Professor Jerison is relaxing in sunny London, Ontario today. And sent me in as his substitute again. I'm glad to be here and see you all again. So our agenda today, he said that he'd already talked about power series and Taylor's formula, I guess, on last week, right? On Friday. And so I'm going to go a little further with that. Show you some examples. Show you some applications. And then I have this course evaluation survey that I'll hand out in the last 10 minutes or so of the class. I also have this handout that he made that says 18.01, end of term, 2007. If you didn't pick this up coming in, grab it going out. People tend not to pick it up when they walk in, I see. So grab this when you're going out. There's some things missing from it. He has not decided when his office hours will be at the end of term. He will have them, he just hasn't decided when. So check the website for that information. And we're looking forward to the final exam, which is, aren't we? Any questions about this technical stuff? All right, let's talk about power series for a little bit. So I thought I should review for you what the story with power series is. OK, could I have your attention, please? So a power series is a way of writing a function as a sum of integral powers of x. These a0, a1, and so on are numbers. An example of a power series is a polynomial. Not to be forgotten, one type of power series is one which goes on for a finite number of terms and then ends, so that all of the other, all the higher a sub i's are all 0. This is a perfectly good example of a power series. It's a very special kind of power series. And part of what I want to tell you today is that power series behave almost exactly like polynomials. There's just one thing that you have to be careful about when you're using power series that isn't a concern for polynomials, and I'll show you what that is in a minute. So you should think of them as generalized polynomials. The one thing that you have to be careful about is that there is a number, so one caution. There's a number, which I'll call r, where r can be between 0 and it can also be infinity. It's a number between 0 and infinity inclusive. So that when the absolute value of x is less than r, so when x is smaller than r in size, the sum converges. This sum, that sum, converges to a finite value. And when x is bigger than r in absolute value, the sum diverges. This r is called the radius of convergence. So we'll see some examples of what the radius of convergence is in various power series as well, and how you find it also. But let me go on and give you a few more of the properties about power series, which I think that Professor Jarosin talked about earlier. So one of them is there's a radius of convergence. Here's another one. If you're inside of the radius of convergence, then the function has all its derivatives. Has all its derivatives. Just like a polynomial does. You can differentiate it over and over again. And in terms of those derivatives, the number a sub n in the power series can be expressed in terms of the value of the derivative at 0. And this is called Taylor's formula. So I'm saying that inside of this radius of convergence, the function that we're looking at, this f of x, can be written as the value of the function at 0, that's a sub 0, plus the value of the derivative. This bracket n means you take the derivative n times. So when n is 1, you take the derivative once, at 0, divide it by 1 factorial, which is 1, and multiply it by x. That's the linear term in the power series. And then the quadratic term is you take the second derivative, remember to divide by 2 factorial, which is 2, multiply that by x squared, and so on out. So in terms, so the coefficients in the power series just record the values of the derivatives of the function at x equals 0. And they can be computed that way also. Let's see. I think that's the end of my summary of things that he talked about. I think he did one example, and I'll repeat that example, of a power series. This example wasn't due to David Jerison, it was due to Leonard Euler. And it's the example of where the function is the exponential function e to the x. So let's see. Let's compute what I will just repeat for you, the computation of the power series for e to the x, just because it's such an important thing to do. So in order to do that, I have to know what the derivative of e to the x is, and what the second derivative of e to the x is, and so on. Because that comes into the Taylor formula for the coefficients. But we know what the derivative of e to the x is. It's just e to the x again. And it's that way all the way down. All the derivatives are e to the x over and over again. So when I evaluate this at x equals 0, well, the value of e to the x is 1. The value of e to the x is 1 at x equals 0. You get a value of 1 all the way down. So all these derivatives at 0 have the value 1. And now when I plug into this formula, I find e to the x is 1 plus 1 times x plus 1 over 2 factorial times x squared plus 1 over 3 factorial times x cubed plus, and so on. So all of these numbers are 1, and all you wind up with is the factorials in the denominator. That's the power series for e to the x. This was a discovery of Leonard Euler in 1740 or something. Yes, ma'am. STUDENT 1 I'm here writing down the power series. How far do you have to write it out? How far do you have to write the power series before it becomes well-defined? Before it's a satisfactory solution to an exam problem, I suppose, is another way to phrase the question. Until you can see what the pattern is, I can see what the pattern is, is there anyone who's in doubt about what the next term might be? Some people would tell you that you have to write the summation convention thing. Don't believe them. If you write out enough terms to make it clear, that's good enough. OK? Is that an answer for you? Yes, thank you. OK, so that's a basic example. Let's do another basic example of a power series. Oh yes, and by the way, whenever you write out a power series, you should say what the radius of convergence is. And for now, I will just tell you that the radius of convergence of this power series is infinity. That is, this sum always converges for any value of x. And I'll say a little more about that in a few minutes. Yeah. STUDENT 3 So which functions can be written as power series? Which functions can be written as power series? That's an excellent question. Any function that has a reasonable expression can be written as power series. I'm not giving you a very good answer, because the true answer is a little bit complicated. But any of the functions that occur in calculus, like sines and cosines, tangents, they all have power series expansions. OK, we'll see more examples. Let's do another example. Here's another example. I guess this was example 1. So this example, I think, was due to Newton, not Euler. Let's find the power series expansion of this function. 1 over 1 plus x. Well, I think that somewhere along the line you learned about the geometric series, which tells you what the answer to this is. And I'll just write it out. The geometric series tells you that this function can be written as an alternating sum of powers of x. You may wonder where these minuses came from. Well, if you think about the geometric series, as you probably remember, there was a minus sign here. And that gets replaced by these minus signs. I think maybe Gerson talked about this also. Anyway, here's another basic example. Remember what the graph of this function looks like when x is equal to minus 1. Then there's a little problem here because the denominator becomes 0. So the graph has a pole there. It goes up to infinity at x equals minus 1. And that's an indication that the radius of convergence is not infinity. Because if you try to converge to this infinite number by putting in x equals minus 1 here, you'll have a big problem. In fact, you see when you put in x equals minus 1, you keep getting 1 on every term, and it gets bigger and bigger and does not converge. In this example, the radius of convergence is 1. OK, so let's do a new example now. Oh, and by the way, I should say you can calculate these numbers using Taylor's formula. If you haven't seen it, check it out. Calculate the iterated derivatives of this function and plug in x equals 0 and see that you get plus 1, minus 1, plus 1, minus 1, and so on. Yes, sir. STUDENT 2 The radius of convergence, I see that if you do minus 1, it'll go up. If you put in 1, though, it seems like it would be fine. PROFESSOR PATRICK WINSTON. I can see that there's a problem at x equals minus 1. Why is there also a problem at x equals 1, where the graph is perfectly smooth and innocuous and finite? That's another excellent question. The problem is that if you go off to a radius of 1 in any direction and there's a problem, that's it. That's what the radius of convergence is. Here, what does happen if I put in x equals plus 1? So let's look at the partial sums. Put in x equals plus 1 in your mind here. So I'll get the partial sum 1, and then 0, and then 1, and then 0, and then 1. So even though it doesn't blow up to infinity, it still does not converge. And any of these other things will also fail to converge in this example. Well, that's the only two real numbers at the edge. OK, let's do a different example now. How about a trig function? Sine of x. I'm going to compute the power series expansion for the sine of x. And I'm going to do it using Taylor's formula. So Taylor's formula says that I have to start computing derivatives of the sine of x. Now, it sounds like it's going to be a lot of work. Let's see, the derivative of the sine is the cosine. And the derivative of the cosine, that's the second derivative of sine, is what? Remember the minus, it's minus sine of x. OK, now I want to take the third derivative of the sine, which is the derivative of sine prime prime. So it's the derivative of this. And we just decided the derivative of sine is cosine. So I get cosine, but I have this minus sign in front. And now I want to differentiate again. So the cosine becomes a minus sine. And that minus sign cancels with this minus sign to give me sine of x. You follow that? A lot of minus 1's canceling out there. So all of a sudden, I'm right back where I started. These two are the same. And the pattern will now repeat forever and ever. Higher and higher derivatives of sines are just plus or minus sines and cosines. Now, Taylor's formula says I should now substitute x equals 0 into this and see what happens. So let's do that. When x is equal to 0, the sine is 0. The cosine is 1. The sine is 0, so minus 0 is also 0. The cosine is 1, but now there's a minus 1. And now I'm back where I started. And so the pattern will repeat. So the values of the derivatives are all 0's and plus and minus 1's, and they go through that pattern fourfold periodicity over and over again. And so we can write out what the sine of x is using Taylor's formula, using this formula. So I put in the value at 0, which is 0. Then I put in the derivative, which is 1, multiply it by x. Then I have the second derivative divided by 2 factorial, but the second derivative at 0 is 0. So I'm going to drop that term out. Now I have the third derivative, which is minus 1. And remember the 3 factorial in the denominator, and that's the coefficient of x cubed. What's the fourth derivative? Well, here we are. It's on the board at 0. So I drop that term out, go up to the fifth term, the fifth power of x. Its derivative is now 1. We've gone through the pattern. We're back at plus 1 as the value of the iterated derivative. So now I get 1 over 5 factorial times x to the fifth. And now you tell me, have we done enough terms to see what the pattern is? I guess the next term will be a minus 1 over 7 factorial, x to the seventh, and so on. Let me write this out again, just so we have x cubed over 3 factorial. So it's x minus x cubed over 3 factorial plus x to the fifth over 5 factorial. You guessed it. And so on. That's the power series expansion for the sine of x. OK? And so the sines alternate. And these denominators get very big, don't they? Exponentials grow very fast. Let me make a remark. R is infinity here. The radius of convergence of this power series, again, is infinity. And let me just say why. The reason is that the term x to the, well, the general term is going to be like x to the 2n plus 1 divided by 2s n plus 1 factorial. An odd number I can write as 2n plus 1. And what I want to say is that the size of this, what happens to the size of this as x, as n goes to infinity? So let's just think about this for a fixed x. Let's fix a number x. Look at powers of x and think about the size of this expression when n gets to be large. So let's just do that for a second. So x to the 2n plus 1 over 2n plus 1 factorial, I can write out like this. It's x over 1 times x over 2 times, sorry, times x over 3 times x over 2n plus 1. I've multiplied x by itself 2n plus 1 times in the numerator. And I've multiplied the numbers 1, 2, 3, 4, and so on by each other in the denominator. And that gives me the factorial. So I've just written this out like this. Now, x is fixed. So maybe it's a million. OK, big but fixed. What happens to these numbers? Well, at first they're pretty big. This is a million over 2. This is a million over 3. But when n gets to be, maybe if, what is, if n is a million, then this is about 1 half. If n is a billion, then this is about 1 over 2,000. Right? The denominators keep getting bigger and bigger, but the numerators stay the same. They're always x. So when I take the product, if I go far enough out, I'm going to be multiplying by very, very small numbers, more and more of them. And so no matter what x is, these numbers will converge to 0. They'll get smaller and smaller as x gets to be bigger. That's the sign that x is inside of the radius of convergence. This is the sign for you that this series converges for that value of x. And because I could do this for any x, this works. This converges to 0 for any fixed x. That's what tells you that you can take the radius of, that the radius of convergence is infinity. Because in the formula, in the fact, this property that the radius of convergence talks about, if r is equal to infinity, this is no condition on x. Every number is less than infinity in absolute value. So if this convergence to 0 of the general term works for every x, then the radius of convergence is infinity. Well, that was kind of fast, but I think that you heard something about that earlier as well. Anyway, so we've got the sine function, a new function with its own power series. It's a way of computing the sine of x. If you take enough terms, you'll get a good evaluation of the sine of x for any x. This tells you a lot about the function sine of x, but not everything at all. For example, from this formula, it's very hard to see that the sine of x is periodic. Not obvious at all. Somewhere hidden away in this expression is the number pi, the half the period. But that's not clear from the power series at all. So the power series are very good for some things, but they hide other properties of functions. Well, so I want to spend a few minutes telling you about what you can do with a power series once you have one to get new power series. So new power series from old. And this is also called operations on power series. So what are the things that we can do to a power series? Well, one of the things you can do is multiply. So for example, what if I want to compute a power series for x times the sine of x? Well, I have a power series for the sine of x. I just did it. How about a power series for x? Actually, I did that here too. x, the function x, is a very simple polynomial. It's the polynomial where that's 0, a1 is 1, all the other coefficients are 0. So x itself is a power series, a very simple one. And the sine of x is a power series. And what I want to encourage you to do is treat power series just like polynomials and multiply them together. We'll see other operations too. So to compute the power series for x times the sine of x, I just take this one and multiply it by x. So let's see if I can do that right. It distributes through x squared minus x to the fourth over 3 factorial plus x to the sixth over 5 factorial. And so on. And again, the radius of convergence is going to be the smaller of the two radii of convergence here. So it's r equals infinity in this case. OK, you can multiply power series together. It can be a pain if the power series are very long. But if one of them is x, it's pretty simple. OK, that's one thing I can do. Notice something, by the way. This is an odd. Do you know about even and odd functions? Huh? So sine is an odd function. x is an odd function. The product of two odd functions is an even function. And that's reflected in the fact that all the powers that occur in the power series are even. For an odd function like the sine, all the powers that occur are odd powers of x. That's always true. OK, we can multiply. I can also differentiate. So let's just do a case of that. And I'll use the process of differentiation to find out what the power series for the cosine of x is. By writing the cosine of x as the derivative of the sine, and differentiating term by term. So I'll take this expression for the power series of the sine and differentiate it term by term. And I'll get the power series for cosine. So let's see, the derivative of x is 1. Now, the derivative of x cubed is 3x squared. And then there's a 3 factorial in the denominator. And the derivative of x to the fifth is 5x to the fourth. And there's a 5 factorial in the denominator. And so on and so on. And now some cancellation happens. So this is 1 minus, well, the 3 cancels with the last factor in this 3 factorial and leaves you with 2 factorial. And the 5 cancels with the last factor in the 5 factorial and leaves you with a 4 factorial in the denominator. And so there you go. There's the power series expansion for the cosine. It's got all even powers of x. They alternate. You have factorials in the denominator. And of course, you could derive that expression by using Taylor's formula by the same kind of calculation we did here. Taking higher and higher derivatives of the cosine, you get the same periodic pattern of derivatives. And values of derivatives at x equals 0. But here's a cleaner way to do it, simpler way to do it, because we already knew the derivative of the sine. When you differentiate, you keep the same radius of convergence. OK, so we can multiply. I can add 2 and multiply by a constant, things like that. How about integrating? That's what half this course was about, isn't it? So let's integrate something. So the integration I'm going to do is this one. The integral from 0 to x of dt over 1 plus x. What is that integral as a function? So when I find the antiderivative of this, I get the natural log of 1 plus t. And then when I evaluate that at x equals 0, at t equals x, I get the natural log of 1 plus x. And when I evaluate the natural log at 0, I get the natural log of 1, which is 0. So this is what you get. This is really valid, by the way, for x bigger than minus 1. But don't want to think about this quite like this when x is smaller than that. OK, now I'm going to try to apply power series methods here and use this integral to find a power series for the natural log. And I'll do it by plugging in to this expression what the power series for 1 over 1 plus t was. And I know what that is because I wrote it down on the board up here. Changed the variable from x to t there. And so 1 over 1 plus t is 1 minus t plus t squared minus t cubed, and so on. So that's the thing in the inside of the integral. And now it's legal to integrate that term by term. So let's do that. I'm going to get something which I will then evaluate at x and at 0. So when I integrate 1, I get x. When I integrate t, I get t. I'm sorry. When I integrate t, I get t squared over 2. t squared gives me t cubed over 3. And so on and so on. And then when I put in x equals, when I put in t equals x, well I just replace all the t's by x's. And when I put in t equals 0, I get 0. So this equals x. So I've discovered that the natural log of 1 plus x is x minus x squared over 2 plus x cubed over 3 minus x to the fourth over 4, and so on and so on. There's the power series expansion for the natural log of 1 plus x. And because I began with a power series whose radius of convergence was just 1, I began with this power series, the radius of convergence of this is also going to be 1. Also because this function, as I just pointed out, this function goes bad when x becomes less than minus 1. So some problem happens. And that's reflected in the radius of convergence. Cool. So you can integrate. That is the correct power series expansion for the natural log of x. And another victory of Euler's was to use this kind of power series expansion to calculate natural logarithms in a much more efficient way than people had done before. OK, one more property. I think. Where are we at here? 3, 4. Substitute. Substitute. Very appropriate for me as a substitute teacher to tell you about substitution. So I'm going to try to find the power series expansion of e to the minus t squared. OK. And the way I'll do that is by taking the power series expansion for e to the t, which we have, e to the x, which we have up there, and make the substitution x equals minus t squared. In the expansion for e to the x. Do you have a question? STUDENT 2 Well, it was just concerning this radius of convergence. You can't define x so that it's always positive, and if so it wouldn't have a radius of convergence, right? PROFESSOR PATRICK WINSTON, JR, M.D. Like I say, so again, the worry is this natural log of 1 plus x function is perfectly well-behaved for large x. Why does the power series fail to converge for large x? Well, suppose that x is bigger than 1, then here you get bigger and bigger powers of x, which will grow to infinity. And they grow larger, they grow large faster than the numbers 2, 3, 4, 5, and 6 do. They grow exponentially, and these grow just, are they, and these just grow linearly. So again, the general term, when x is bigger than 1, the general term will go off to infinity, even though the function that you're talking about, log of 1 plus x, is perfectly good. So the power series is not good outside of the radius of convergence. It's just the fact of life. Yeah? STUDENT 3 That one, when you were multiplying two functions, you were talking about the smaller radius. PROFESSOR ERIC GRIMSON Yeah. STUDENT 3 I was wondering if you could go through the whole thing. PROFESSOR ERIC GRIMSON I'd rather, talk to me after class. The question is, why is it the smaller of the two radii of convergence? And the basic answer is, well, you can't expect it to be bigger than the smaller one, because the power series only gives you information inside of that range about the function. So. Well, both, in this case, both of the radii of convergence are infinity. x has radius of convergence infinity, for sure. And sine of x does too. So you get infinity in that case. OK? OK, let's just do this. And then I'm going to integrate this, and that'll be the end, I guess, of what I have time for today. So what's the power series expansion for this? The power series expansion of this is going to be a function of t, right? Because the variable here is t. And I get it by taking my expansion for e to the x, and putting in what x is in terms of t. Whoops, and so on and so on. I just put in minus t squared in place of x there in the series expansion for e to the x. So I can work this out a little bit better. Minus t squared is what it is. This is going to give me a t to the fourth. And the minus squared is going to give me a plus. So I get t to the fourth over 2 factorial. Then I get minus t quantity cubed. So there will be a minus sign and a t to the sixth. And the denominator is 3 factorial. So the signs are going to alternate. The powers are all even. And the denominators are these factorials. OK? Several times as this course has gone on, the error function has made an appearance. The error function was, I guess it gets normalized by putting 2 over the square root of pi in front. Whoops. And it's the integral of e to the minus t squared dt from 0 to x. And this normalization is here because that as x gets to be large, the value becomes 1. So this error function is very important in the theory of probability. And I think you calculated this fact at some point in the course. So this is the standard definition of the error function, to put a 2 over the square root of pi in front. Let's calculate its power series expansion. So there's a 2 over the square root of pi that hurts nobody here in the front. And now I want to integrate e to the minus t squared. And I'm going to use this power series expansion for that to see what you get. So I'm just going to write this out, I think. I did it out carefully in another example over there. So I'll do it a little quicker now. Integrate this term by term. You're just integrating powers of t, so it's pretty simple. So I get, and then I'm evaluating at x and at 0. So I get x minus x cubed over 3 plus x to the 5th over 5 times 2 factorial. 5 from integrating the t to the 4th, and the 2 factorial from this denominator that we already had. And then there's a minus x to the 7th over 7 times 3 factorial. And plus, and so on. And you can imagine how they go on from there. I guess to get this exactly in the form that we began talking about, I should multiply through. So the coefficient of x is 2 over the square root of pi, and the coefficient of x cubed is minus 2 over 3 times the square root of pi, and so on. But this is a perfectly good way to write this power series expansion as well. And this is a very good way to compute the value of the error function. It's a new function in our experience. Your calculator probably calculates it, and your calculator probably does it by this method. OK, so that's my sermon on examples of things you can do with power series. So we're going to do the CEG thing in just a minute. Professor Jerison wanted me to make an ad for 18.02, just in case you were thinking of not taking it next term. You really should take it. It will make, it'll put a lot of things in this course into context, for one thing. It's about vector calculus and so on. So you learn about vectors and things like that. But it comes back and explains some things in this course that might have been a little bit strange, like these strange formulas for the product rule, and the quotient rule, and these sort of random formulas. Well, one of the things you learn in 18.02 is that they're all special cases of the chain rule. And just to drive that point home, he wanted me to show you this poem of his that really drives the point home forcefully, I think.