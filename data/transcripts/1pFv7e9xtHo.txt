 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. MIT.edu. OK. So I'd like to pick up again on this neat family of matrices, circulant matrices. But first, let me say here and then put on the web my thought about the projects. So I think the last deadline I can give is the final class. So I think that's not next week, but Wednesday of the following week, I think, is our last class meeting. So it would be great to get them then or earlier. And if anybody or everybody would like to tell the class a little bit about their project, it's a friendly audience. And I'd be happy to make space and time for that. So send me an email and give me the project earlier if you would like to just say a few words in class. Or even if you are willing to say a few words in class, I'll say. Yeah, because I realize. Yeah. OK. So other questions about? So we're finished with all p sets and so on. So it's really just the project. And yeah. How is the project graded? Like, on what basis? How is it graded? Good question. But it's going to be me, I guess. So I'll read all the projects and come up with a grade somehow. I hope you guys have understood that my feeling is that the grades in this course are going to be on the high side, because they should be. Yeah, I think it's that kind of a course. And I've asked you to do a fair amount. And anyway, that's my starting basis. And there's a lot of topics like circulant matrices that I'm not going to be able to give you a p set about. But of course, these are closely connected to the discrete Fourier transform. So let me just write the name of the great man, Fourier. So the discrete Fourier transform is, as you know, a very, very important algorithm in engineering and in mathematics everywhere. Fourier is just a key idea. And so I think it's just good to know about those. So circulant matrices are connected with the finite size matrices, matrices of size n. So our circulant matrices will be n by n. And you remember the special form. So this is a key point about these matrices, C, that they're defined by not n squared entries, only n. If you tell me just the first row of the matrix, and that's all you would tell MATLAB, say, C0, C1, C2, to Cn minus 1, then for a circulant, that's all I need to know. Because these diagonals are constant. This diagonal is constant C1 and then gets completed here. C2 diagonal come to C2 and then gets completed cyclically here. So n numbers and not n squared. And the reason I mention that, or a reason, is that's a big selling point when you go to applications, say, machine learning for images. So you remember the big picture of machine learning, deep learning, was that you had samples, a lot of samples, let's say n samples, maybe. And then each sample in this image part will be an image. So the thing is that an image is described by its pixels. And if I have a 1,000 by 1,000 pixel, so that's a million pixels, the feature vector, the vector that's associated with one sample, is enormous. So I have n samples, but maybe, well, if they're in color, that million suddenly becomes 3 million. So say 3 million features. So our vectors, our vector that the whole computation of deep learning works with, are vectors with 3 million components. And that means that in the ordinary way, if we didn't do anything special, we would be multiplying those by matrices of size like 3 million times 3 million. We would be computing that many weights. That's like impossible. And we would be computing that for each layer in the deep network. So it would go up a norm. So 3 million by 3 million is just we can't compute. We can't use gradient descent to optimize that many weights. So the point is that the matrices in deep learning are special, and they don't depend. They are like circulant matrices. They might not loop around. So the circulant matrices have this cyclic feature that makes the theory extremely nice. But of course, in general, we have matrices, let's say, t, t0, constant diagonals, and maybe a bunch of diagonals. And here, not necessarily symmetric, or they might be symmetric, but they're not cyclic. So what are these matrices called? Well, they have a bunch of names because they're so important. They're linear shift invariant, or linear time invariant, whatever is the right word in your context. So they're convolutions. You could call it a convolution matrix. When you multiply by one of these matrices, I guess I'm going to call it t, you're doing a convolution. And I'll better write down the formula for a convolution. You're not doing a cyclic convolution unless the matrix cycles around. When you multiply by c, this would give you a cyclic convolution. Say if I multiply c by some vector v, the result is the cyclic convolution of the c vector with the v vector. So big C is a matrix, but it's completely defined by its first row or first column. So it's really, I just have a vector operation there. And it's a cyclic one. And over here, t times a vector v will be the convolution of a t vector with v, but not cyclic. And probably these are the ones that would actually come into machine learning. So linear shift invariant, linear time invariant. I would call it, so math people would call it a Tuplitz matrix. So that's why I used the letter T. In engineering, it would be a filter or a convolution or a constant diagonal matrix. These come up at all sorts of places. And they come up in machine learning and with image processing. But basically, because what you're doing at one point in an image is pretty much what you're going to do at the other points. You're not going to figure out special weights for each little pixel in the image. You're going to take, if you have an image, say you have an image with zillions of pixels. Well, you might want to cut down. I mean, you'd be very sensible to do some max pooling, some pooling operation to make it smaller. So that's really like, OK, we don't want this larger system. Let's just reduce it. So max pooling, that operation would be, say, take them three at a time, so nine pixels, and replace that nine pixels by one pixel, the max of those nine numbers. That would be a very simple operation that just reduces the dimension. That's make it smaller, reduce the dimension. OK, so that's a cheap way to make an image four times, or nine times, or 64 times smaller. But the convolution part now, so that's not involving convolution. That's a different operation here. Not even linear, if I take the max in each box, that's not a linear operation. But it's a fast one. OK, so now where do circulants, or convolution, or tuplets, matrices, or filters come into it? So I'll forget about the max pooling, suppose that's happened, and I still have a very big system with n squared pixels, n squared features for each sample. So I want to operate on that by matrices, as usual. I want to choose the weights to bring out the important points. So the whole idea is, on an image like that, I'll use a convolution. I'll use the same operation is happening at each point. So forget the max part. Let me erase, if I can find an eraser here. OK, so I'm not going to, we've done this. So that's done. Now I want to multiply it by weight. So that's already done. OK, so what am I looking to do? What kind of a job would a filter do? A low pass filter would kill, or nearly kill, the high frequencies, the noise. So if I wanted to get a simpler image there, I would use a low pass filter, which might just, it might be this filter here with, let me just put in some numbers that would say a half and a half. So I'm averaging each pixel with its neighbor just to take out some of the high frequencies. The low frequencies, a constant image, all black image would come out not changed. But a very highly speckled image would get largely removed by that averaging. So it's the same idea that comes up in all of signal processing filtering. So just to complete this thought of why do neural nets, so I'm answering this question. How do they come in machine learning? So they come when the samples are images. And then it's natural to use a constant diagonal matrix, a shift invariant matrix, and not an arbitrary matrix. So we only have to compute n weights and not n squared. So that's the point. So that's one reason for talking about convolution and circulant matrices in this course. I guess I feel another reason is that everything to do with the DFT, with Fourier and Fourier transforms and Fourier matrices, that's just like stuff you got to know. I mean, every time you're dealing with vectors that are sort of shift, we're shifting the vectors comes into it, that's Fourier is going to come in. So it's just we should see Fourier. So now I'll go back to this especially nice case where the matrix loops around where I have this cyclic convolution. So this would be cyclic because of the looping around stuff. And let's find the, oh, so what was the point of last time? I started with this permutation matrix. And the permutation matrix has c0 equals 0, c1 equal 1, and the rest of the c's are 0. So it's just the effect of multiplying by this. You can get a box around it here. The effect of multiplying by this permutation matrix is to shift everything and then bring the last one up to the top. So it's a cyclic shift. So cyclic shift, that's a. And I guess at the very end of last time, I was asking about its eigenvalues and eigenvectors. So can we come to that question? So that's the starting question for everything here. I guess we've understood that to get deeper into a matrix, its eigenvalues, eigenvectors, or singular values, singular vectors are the way to go. Actually, what would be the singular values of that matrix? Yeah, let's just think about singular values, and then we'll see why it's eigenvalues we want. What are the singular values of a permutation matrix? They're all 1, all 1. That matrix is a orthogonal matrix. So the SVD of the matrix just has the permutation, and then the identity is there for the sigma. So sigma is I for this matrix. So the singular values don't. And now I guess if we, that's because P transpose P is the identity matrix. Anytime I have, that's an orthogonal matrix, and any time P transpose P is the identity, the singular values will be the eigenvalues of the identity, and they're all just 1's. The eigenvalues of P, that's what we want to find. So let's do that. OK, eigenvalues of P. So one way is to take P minus lambda I. That's just the way we teach in 18.06 and never use again. So puts minus lambda on the diagonal, and of course P is sitting up here, and then the rest is 0. OK, so now following the 18.06 rule, I should take that determinant, right, and set it to 0. This is one of the very few occasions we can actually do it, so allow me to do it. So what is the determinant of this? Well, there's that lambda to the fourth. And I guess I think it's lambda to the fourth minus 1. I think that's the right determinant. That certainly has property, so I would set that to 0. Then I would find that the eigenvalues for that will be 1 and minus 1 and I and minus I. And they're the fourth roots of 1. Lambda to the fourth equal 1. That's our equation. That's our eigenvalue equation. Lambda to the fourth equal 1, or lambda to the nth equal 1. So what would be the eigenvalues for the P 8 by 8? This is the complex plane, of course, real and imaginary. So that's got eight eigenvalues. We want a P to the eighth power would be the identity. And that means that lambda to the eighth power is 1 for the eigenvalues. And what are the eight solutions? Every polynomial equation of degree 8 has got to have eight solutions. That's Gauss's fundamental theorem of algebra. Eight solutions, so what are they? What are the eight numbers whose eighth power gives 1? You all probably know them. So they're 1, of course, the eighth power of 1, the eighth power of minus 1, the eighth power of minus I, and the other guys are just here. The roots of 1 are equally spaced around the circle. So Fourier is coming. You know, Fourier wakes up when he sees that picture. Fourier is going to be here. And it'll be in the eigenvectors. So you OK with the eigenvalues? The eigenvalues of P will be this number. So we better give a name to this number. Let's see. I'm going to call that number W. And it'll be e to the 2 pi i over 8, because the angle is 2 pi. The whole angle is 2 pi divided in eight pieces. So that's 2 pi i over 8, 2 pi i over n for a matrix of, for the n by n permutation. So that's number W. And of course, this guy is W squared. This one is W cubed, W fourth, W fifth, sixth, seventh, and W eighth is the same as 1. Right. Well, the reason I put those numbers up there is that they come into the eigenvectors as well as the eigenvalues. They are the eigenvalues, these eight numbers, 1, 2, 3, 4, 5, 6, 7, 8, are the eight eigenvalues of the matrix. Here's the 4 by 4 case. The matrix is an orthogonal matrix. Oh, what does that tell us about the eigenvectors? The eigenvectors of an orthogonal matrix are orthogonal, just like symmetric matrices. So do you know that little list of matrices with orthogonal eigenvectors? So OK, I'm going to call them Q. So Qi dotted with Qj, the inner product is 1 or 0. 1 if i equals j, 0 if i is not j. Orthogonal eigenvectors. Now, what matrices have orthogonal eigenvectors? We're going back to linear algebra, because this is like a fundamental fact, to know this family of wonderful matrices, matrices with orthogonal eigenvectors. So tell me one bunch of matrices that you know has orthogonal eigenvectors. Symmetric. And what is special about the eigenvalues? They're real. But there are other matrices that have orthogonal eigenvectors. And we really should know the whole story about those guys. They're too important not to know. So what's another bunch of matrices? So these symmetric matrices have orthogonal eigenvectors. And real symmetric, then the eigenvalues will be real. Well, what other kind of matrices have orthogonal eigenvectors? But they might be complex. And the eigenvalues might be complex. And you just have to, you can't know Fourier without saying, OK, I can deal with this complex number. OK, so what's another family of matrices that has orthogonal eigenvectors? Yes. Diagonal, for sure. Right? And then we know that we have the eigenvectors go into the identity matrix. Right? Yeah, so we know everything about diagonal ones. You could say those are included in symmetric. Now let's get some new ones. What else? Orthogonal matrices count. Orthogonal matrices, like permutations, or like rotations, or like reflections, orthogonal matrices. And what's special about their eigenvalues? The eigenvalues of an orthogonal matrix. The magnitude is 1, exactly. There has to be 1, because an orthogonal matrix doesn't change the length of the vector. Q times x has the same length as x for all vectors, and in particular for eigenvectors. So if this was an eigenvector, Qx would equal the lambda x. And now if that equals that, then lambda has to be 1. The magnitude of lambda has to be 1. Of course, complex numbers are expected here. And that's exactly what we're seeing here. All the eigenvalues of permutations are very special orthogonal matrices. I won't add permutations separately to the list, but they count. Now what? So the fact that this is on the list tells us that the eigenvectors that we're going to find are orthogonal. We don't have to do a separate check to see that they are once we compute them. They have to be. They're the eigenvectors of an orthogonal matrix. Now I could ask you, let's keep going with this and get the whole list here. Along with symmetric, there's another bunch of guys, anti-symmetric. Big deal, but those are important. So symmetric means A transpose equals A. Diagonal, you know. A transpose equals A inverse for orthogonal matrices. Now I'm going to put in anti-symmetric matrices where A transpose is minus A. What do you think you know about the eigenvalues for anti-symmetric matrices? Shall we take an example? Anti-symmetric matrix, say 0, 0, 1, and minus 1. What are the eigenvalues of that? Well, if I subtract lambda from the diagonal and take the determinant, I get lambda squared plus 1, equals 0. So lambda is I or minus I. That's a rotation matrix. It's also, yeah, yeah, it's a rotation through 90 degrees. So there could not be a real eigenvalue. Have you thought about that? Or a real eigenvector. If I rotate every vector, how could a vector come out a multiple of itself? How could I have A transpose times a vector equal lambda times a vector? I've rotated it, and yet it's in the same direction. Well, somehow that's possible in imaginary space and not possible in real space. OK, so here's the lambdas are imaginary. And now finally, tell me if you know the name of the whole family of matrices that includes all of those and more of matrices with orthogonal eigenvectors. So what are the special properties then? These would be matrices. Shall I call them M for matrix? So it has orthogonal eigenvectors. So it's a Q times the diagonal times Q transpose. Well, I've really written down somehow. I haven't written a name down for them, but that's the way to get them. I'm allowing any orthogonal eigenvectors. So this is diagonal. I've diagonalized the matrix. And here are any eigenvalues. So the final guy on this list allows any eigenvalues, any complex numbers. But the eigenvectors, I want to be orthogonal. So that's why I have the Q. So how would you recognize such a matrix, and what is the name for them? So we're going beyond 18.06, because probably I don't mention that name, the name for these matrices in 18.06, but I could. And anybody know it? A matrix of that form is a normal matrix, normal. So that's the total list is a normal matrix. So normal matrices look like that. I have to apologize for whoever thought up that name normal. I mean, that's like, OK. In a little more thought, you could have come up with something more meaningful than just saying normal. That's a matrix. That's the absolute opposite of normal. Almost all matrices are not normal. So anyway, but that's what they're called, normal matrices. And finally, how do you recognize a normal matrix? Everybody knows how to recognize a symmetric matrix or a diagonal matrix. And we even know how to recognize an orthogonal matrix or skew or anti-symmetric. But what's the quick test for a normal matrix? Well, I'll just tell you that. A normal matrix has M transpose M equal M M transpose. I'm talking here about real matrices, and I really should move to a complex. But let me just think of them as real. So if, well, the trouble is the matrices might be real, but the eigenvectors are not going to be real. And the eigenvalues are not going to be real. So really, I'm sorry to say really again, I should get out of the limitation to real. And how do I get out of the limitation to real? What do I change here if M is a complex matrix instead of a real matrix? Then whenever you transpose it, you should take its complex conjugate. So now that's the real thing. That's the normal thing. That's the right thing. Right thing is better. So that's a normal matrix. And you can check that if you took that M and you figured out M transpose and did that, it would work. Because in the end, the Q's cancel, and you just have two diagonal matrices there. And that's sort of automatic that diagonal matrices commute. So a normal matrix is one that commutes with its transpose, commutes with its transpose or its conjugate transpose in the complex case. OK. Why did I say all that? Simply because, oh, I guess that so the permutation P is orthogonal. So its eigenvectors, which we're going to write down in a minute, are orthogonal. But actually, this matrix C will be a normal matrix. I didn't see that coming as I started talking about these guys. Yeah, so that's a normal matrix. Because circulant matrices commute. Any two circulant matrices commute. C1, C2 equals C2, C1. And now if C2 is the transpose of, so here's a matrix. Yeah, so these are matrices here. Circulants all commute. It's a little family of matrices. When you multiply them together, you get more of them. You're just staying in that little circulant world with n parameters. And once you know the first row, you know all the other rows. And so in fact, they all have the same eigenvector. So now let me be sure we get the eigenvectors straight. OK. OK. Eigenvectors of P will also be eigenvectors of C because it's a combination of powers of P. So once I find the eigenvectors of P, I've found the eigenvectors of any circulant matrix. And these eigenvectors are very special. And that's the connection to Fourier. That's why we expect a connection to Fourier because we have something periodic. And that's what Fourier is entirely about. OK, so what are these eigenvectors? Let's take P to be 4 by 4. OK, so the eigenvectors are, so we remember the eigenvalues are lambda equal 1, lambda equal minus 1, lambda equal i, and lambda equal minus i. Got four eigenvectors to find. And when we find those, you'll have the picture. OK, what's the eigenvector for lambda equal 1? 1, 1, 1, 1. So let me make it into a vector. And the eigenvector for lambda equal minus 1 is? So I want this shift to change every sign. So I better alternate those signs so that if I shift it, the 1 goes to the minus 1. Minus 1 goes to the 1. So the eigenvalue is minus 1. Now, what about the eigenvalues of i? Sorry, the eigenvector that goes with eigenvalue i? So I'm looking. I want, if I start it with 1 and I do the permutation, I think I just want i, i squared, i cubed there. And I think with this guy, with minus i, I think I want the vector 1 minus i, minus i squared, minus i cubed. So without stopping to check, let's just see the nice point here. All the components of eigenvectors are in this picture. Here, we've got eight eigenvectors. Eight eigenvalues, eight eigenvectors. The eigenvectors have eight components. And every component is a power, is one of these eight numbers. The whole thing is constructed from the same eight numbers, the eigenvalues and the eigenvectors. And really, the key point is, what is the matrix of eigenvectors? So let's just write that down. So the matrix, the eigenvector matrix for all circulants of size n. Now, they all have the same eigenvectors, including p. All circulants c of size n, including p of size n. So what's the eigenvector matrix? What are the eigenvectors? Well, the first vector is all 1's, just as there. So that's an eigenvector of p, right? Because if I multiply by p, I do a shift, a cyclic shift, and I've got all 1's. The next eigenvector is powers of w. And let me remind you, everything is going to be powers of w. e to the 2 pi i over n. It's that complex number that's 1 n-th of the way around. So that's the. So what happens if I multiply that by p? It shifts it, and it multiplies by w, or 1 over w, which is w trans, which is another eigenvector. And then the next one in this list will be going with w squared. So it'll be w fourth, w to the sixth, w to the eighth. Oh, wait a minute. Did I get these lined up right? w goes with w squared. Whoops. w squared, now it's w to the fourth, w to the sixth, w to the eighth, w to the 10th, w to the 12th, and w to the 14th. And it keeps going. So that's the eigenvector with eigenvalue 1. This will have the eigenvalue. It's either w or a conjugate. Might be the conjugate w bar. And you see this matrix. So what would be the last eigenvector? It would be w. So this is 8 by 8. I'm going to call that the Fourier matrix of size 8. And it's the eigenvector matrix. So Fourier matrix equals eigenvector matrix. So I'm just, what I'm saying is that the linear algebra for these circulants is fantastic. They all have the same eigenvector matrix. It happens to be the most important complex matrix in the world. And its properties are golden. And it allows the fast Fourier transform, which we could write in matrix language next time. And all the entries are powers of w. All the entries are on the unit circle at one of those eight points. And the last guy would be w to the 7th, w to the 14th, w to the 21st, 28th, 35th, 42nd, and 49th. So w to the 49th would be the last w to 7 squared. It starts out with w to the 0 times 0. All these are, the entry is, yeah, yeah, you see the picture. w to the 49th. What is actually w to the 49th? If w is the 8th root of 1, so we have w to the 8th is 1, because I'm doing 8 by 8. What is w to the 49th power? w? It's the same as w. OK. Because w to the 48th is 1. 1, right? I take the 6th power of this, and I get the w to the 48th is 1. So w to the 49th is the same as w. Every column, every entry in the matrix is a power of w. And in fact, that power is just the column number times the row number. So those are the good matrices. So that is an orthogonal matrix. Well, almost. It has orthogonal columns, but it doesn't have orthonormal columns. What's the length of that column, of that column vector? The square root of 8, right? I add up 1 squared 8 times, and I take the square root. I get the square root of 8. So this is really, it's the square root of 8 times an orthogonal matrix. Of course, the square root of 8 is just a number to divide out to make the matrix, to make the columns orthonormal instead of just orthogonal. But how do I know that those are orthogonal? Well, I know they have to be, but I'd like to see it clearly. Why is that vector orthogonal to that vector? First of all, they have to be because the matrix is a normal matrix. Normal matrices have orthogonal. Oh, yeah. How do I know it's a normal matrix? Oh, I guess I can do the test. If I have the permutation P, I know that P transpose P equals PP transpose, the permutations commute. So it's a normal matrix. But I'd like to see directly, why is the dot product of the first or the 0th eigenvector and the next eigenvector equal 0? Let me take that dot product. 1 times 1 is 1. 1 times w is w. 1 times w squared is w squared, up to w to the 7th, I guess I'm going to finish at, equals 0. Yes. Well, what's that saying? Those numbers are these points in my picture, those eight points. So those are the eight numbers that go into that column of that eigenvector. Why do they add to 0? How do you see that the sum of those eight numbers is 0? Yeah, the symmetry would do it. When I add that guy to that guy, w to the 0, or w to the 8th, or w to the 0. Yeah, when I add 1 and minus 1, I get 0. When I add these guys, I get 0. When I add these, these, by pairs. But what about a 3 by 3? So 3 by 3, I'll go to w to the, this would be e to the 2 pi i over 3. And then this would be w to the 4 pi, this would be w squared, e to the 4 pi i over 3. And I believe that those three vectors add to 0. And therefore, they're orthogonal to the 1, 1, 1 eigenvector. Because the dot product will just, just want to add those three numbers. So why is that true? 1 plus e to the 2 pi i over 3 plus e to the 4 pi over 3 equals 0. Last minute of class today, we can figure out how to do that. Well, I could get a formula for that. That sum is 1. I could get a closed form and check that I get the answer 0. The quick way to see it is maybe, suppose I multiply by e to the 2 pi i over 3. So I multiply every term. So that's e to the 2 pi i over 3, e to the 4 pi i over 3, and e to the 6 pi i over 3. OK, what do I learn from this? It's the same, because e to the 6 pi over 3 is 1. That's 2 pi i, so that's 1. So I got the same sum, 1 plus this plus this, this plus this plus 1. So I got the same sum when I multiplied by that number. And that sum has to be 0. I can't get the same sum. I can't multiply by this and get the same answer unless I'm multiplying 0. So that shows me that also in the odd, when n is odd, I also have those n numbers adding to 0. OK, those are the basic, the beautiful picture of the eigenvalues, the eigenvectors being orthogonal, and then the actual details here of what those eigenvectors are. OK, good. Hope you have a good weekend. And so we've just got a week and a half left of class. I may probably have one more thing to do about Fourier, and then we'll come back to other topics. But ask any questions, topics that you'd like to see included here. We're closing out 18.065 while you guys do the projects. OK, thank you.