 We've been spending the past few lectures discussing Szemeredi's regularity lemma. And one of the first applications that we discussed of the regularity lemma is the triangle removal lemma. So today, I want to revisit this topic and show you a strengthening of the removal lemma for which new regularity techniques are needed. But first, recall the graph removal lemma. And the graph removal lemma, we have that for every graph h and epsilon bigger than 0. There exists some delta such that if an n-vertex graph has fewer than delta n to the number of vertices of h, many copies of h, then it can be made h-free by removing fewer than epsilon n squared edges. So even in the case when h is a triangle, when this is called the triangle removal lemma, even in that case, basically, the regularity method is more or less the only way that we currently know how to prove this theorem. So we saw this a few lectures ago. What I would like to discuss today is a variant of this result where, instead of considering copies of h, we're now considering induced copies of h. So this is the induced graph removal lemma, where the only difference is that the hypothesis is now going to be changed to induced copies of h. And the conclusion is that you can make the graph induced h free. So let me remind you the difference between an induced graph, subgraph, and the usual subgraph. So we say that h is an induced copy of G, induced subgraph of G, if one can obtain h from G by deleting vertices of G. So you're not allowed to delete edges, but only allowed to delete vertices. So in other words, the four-cycle is not an induced subgraph because, well, if you select four vertices, you don't generate this four-cycle. You get extra edges. It is a subgraph, but not an induced subgraph. So it is a theorem. So the induced graph removal lemma is a theorem, and let's discuss how we might prove that theorem. Question? AUDIENCE 1 Why is this stronger than the graph removal lemma? YUFEI ZHAO Stronger than the graph removal lemma, so it's not stronger, but we'll see the relationship between the two of them. So I claim that it is more difficult to do this theorem. Any more questions? So let's pretend for a second that, ah, OK, what I wrote in here is not quite true. So here's an example. If your H, for example, if your H is three isolated vertices, so what is it saying? We're looking at copies of H, which are three isolated vertices. So really, you are looking at triangles in G complement. So this is exactly the triangle removal lemma in the complement of G, but you can't get rid of these guys by removing edges. So we need to make the modification where, instead of removing these edges, we need to both remove and add by adding or deleting. So maybe at the same time, so you're allowed to add some edges, delete some edges, but in total, you change no more than epsilon n squared edges. So this is sometimes also known as the added distance if you're allowed to change edges. So you can now add edges and delete edges. Any questions about the statement? All right, so let's think about how would you prove this result following the proof that we did for the triangle removal lemma. So let's pretend that we go through this proof and think about what could go wrong. So remember, in the application of the removal lemma, so the recipe has three steps. The first step, we do a partition. So we partition, apply Szemeredi's regularity lemma, do this partition. And the second step is we do a cleaning. And the two key things that happen in the cleaning is we remove low-density pairs of parts and irregular pairs. And the third step, we claim that once we do the cleaning, once we remove those edges, the resulting graph should be H-free, because if we're not H-free, then by considering the vertex parts where H lie and applying a counting lemma, you can generate many more copies of H. So these were the three main steps in the proof of the triangle removal lemma. So let's see what happens if we try to apply this strategy to the induced version. I mean, the partition, you still do the regularity partition. There's nothing really changes there. So let's see in the cleaning step what happens. For low-density pairs, well, so now we need to think about not just low-density pairs, but also high-density pairs, because in the induced, we think about edges and non-edges at the same time. So you might think of a strategy which is like if the edge density is less than n, so less than epsilon, then you remove all those edges. And if the edge density is bigger than 1 plus epsilon, then you add all of those edges in. So this is the natural generalization of our strategy for triangle removal lemma for the induced setting. So so far, everything is still OK. But now, what would you do for the irregular pairs? That's problematic. Previously, for triangle removal lemma, we just said if a pair is irregular, get rid of that pair. And it will never show up in the counting stage. But that strategy no longer works, because, for example, if your graph H being counted is this here, you do the regularity partition, and one of your pairs is irregular, so you, let's say, get rid of all those edges in between, then maybe you have some embedding of H where you are going to use the removed edges. And now, you don't have a counting lemma. You cannot say, I found this copy of H in my removed, my changed graph. And by the counting lemma, I could get many copies of H, because you have no control over this irregular pair anymore. So the fact that you have to add and remove makes it unclear what to do here. And this is a big obstacle in the application of the regularity lemma to the induced removal lemma application. Any questions about this obstacle? So make sure you understand why this is an issue. Otherwise, you won't really appreciate what will happen next. So somehow, we need to find some kind of regularity partition to get no irregular pairs. So the question is, is there a way to partition so that there are no irregular pairs? So for those of you who have started your homework problem on time, you realize that the answer is no. So one of the homework problems is for you to show that for the specific graph known as the half graph. So there was an example in the homework that for the half graph, so you'll see in the homework what this graph is, you cannot partition it so that you get rid of all irregular pairs. So irregular pairs are necessary in the statement of the regularity lemma. So what I want to show you today is a way to do what's called a strong regularity lemma, in which you obtain a somewhat different consequence that will allow you to get rid of irregular pairs in a more restricted setting. So this is the issue, the irregular pairs. Before telling you what this regularity lemma is, I want to give you a small generalization of the induced graph removal lemma, or just a different way to think about the statement. And you can think of it as a colorful version, instead of induced, where you have edges and non-edges, you can also have colored edges. So colorful removal lemma, although this name is not standard. So colorful, let's only talk about graphs. Colorful graph removal lemma. So for every k, r, and epsilon, there exists delta such that if KERDI H is a set of r-edge colorings, of the complete graph on little k vertices, so edge coloring just means using r colors to color the edges, so there are no restrictions about what are allowed, what are not allowed, so just a set of possible r colorings. Then if the complete graph, say slightly differently, so then every r-edge coloring of the complete graph on n vertices with fewer than delta fraction of its k vertex subsets, say k vertex subgraphs belonging to the script H, so every such graph can be made KERDI H-free by recoloring, so using the same r colors, a fewer than epsilon fraction, so less than epsilon fraction of the edges of this KM. So in particular, the version that we just stated, the induced version, so the induced graph removal lemma is the same as having two colors and H having exactly one red-blue coloring of K, complete graph on the same number of vertices as H. So you color red the edges and blue the non-edges, for instance. And you're saying, I want to color a big graph, a big complete graph with red and blue in such a way that there are very few copies of that pattern. So then I can recolor the red and blue in a small number of places to get rid of all such patterns. So having a colored pattern somewhere in your graph, in this complete graph coloring, is the same as having an induced subgraph. Yeah? AUDIENCE 2 So like after then, so like the statement after then is like a really long sentence? YUFEI ZHAO Yeah, so every r-edge coloring of KM with a small number of patterns can be made H-free by recoloring a small fraction of the edges. So like in the triangle removal lemma, every graph with a small number of triangles can be made triangle-free by removing a small number of edges. Any other questions? All right, so this is a restatement of the induced removal lemma with a bit more generality. It's OK if you like this one more or less, but let's talk about the induced version from now on. But the same proofs that I will talk about also applies to this version where you have some more colors. So the variant of the regularity lemma that we'll need is known as a strong regularity lemma. To state the strong regularity lemma, let me recall a notion that came up in the proof of Szemeredi's regularity lemma. And this was the notion of an energy. So recall that if you have a partition denoted P, so if this is a partition of the vertex set of a graph G, and here n is the number of vertices, we defined this notion of energy to be this quantity denoted Q, which is basically a squared mean of the densities between vertex parts, appropriately normalized if the vertex sets do not all have the same size. And in the proof of Szemeredi's regularity lemma, there was an important energy increment step, which says that if you have some partition P that is not epsilon regular, then there exists a refinement Q. And this refinement has the property that Q has a small number of pieces, or not too large as a function of P. So it's bounded, at least in terms of P. But also, if P is not epsilon regular, then the energy of Q is significantly larger than the energy of P. So remember, this was an important step in the proof of regularity lemma. So to state the strong regularity lemma, we need that notion of energy. And the statement of the strong regularity lemma, if you've never seen this kind of thing before, will seem a bit intimidating at first because it involves a whole sequence of parameters. But we'll get used to it. So instead of one epsilon parameter, now you have a sequence of positive epsilons. And part of the strength of this regularity lemma is that, depending on the application you have in mind, you can make the sequence go to 0 pretty quickly, thereby increasing the strength of the regularity lemma. So there exists some M bound, which depends only on your epsilons, such that every graph has not just one, but now we're going to get a pair of vertex partitions, P and Q, with the following properties. So first, Q refines P. So it's a pair of partitions, one refining the other. The number of parts of Q is bounded, just like in the usual regularity lemma. The partition P is epsilon 0 regular. And here is the new part. That's the most important one. Q is very epsilon regular. So it's not just epsilon not regular. It's epsilon sub the number of parts of P regular. So you should think of this as extremely regular, because you get to choose what the sequence of epsilon is. And finally, the energy difference between P and Q is not too big. This is the statement of the strong regularity lemma. It produces for you not just one partition, but a pair of partitions. And in this pair of partitions, you have one partition P, which is similar to the one that we obtained from Szemeredi's regularity lemma. It's some epsilon not regular. But we also get a refinement Q. And this Q is extremely regular. So you can think that is P. Then Q is an extremely regular refinement of P. Any questions about the statement of the strong regularity lemma? So the sequence of epsilons gives you flexibility on how to apply it. But let's see how to prove it. And the proof is, once you understand how this works conceptually, it's pretty short. But let me do it slowly so that we can appreciate this sequence of epsilons. And the idea is that we will repeatedly apply Szemeredi's regularity lemma. Start with the regularity lemma. We'll apply it repeatedly to generate a sequence of partitions. So first, let me remind you a statement of Szemeredi's regularity lemma. This is slightly different from the one that we stated, but comes out of the same proof. So for every epsilon, there exists some M0, which depends on epsilon, such that for every partition P0, so starting with some partition. So actually, let me start with just P. So if you start with some partition of the vertex set of G, there exists a refinement P prime of P into, at most, so the refinement has it such that with each part of P refined into, at most, M0 parts such that P prime, the new partition, is epsilon regular. So this is a statement of Szemeredi's regularity lemma that we will apply repeatedly. So in the version that we've seen before, we would start with a trivial partition and applying refinements repeatedly in the proof to get a partition into a bounded number of parts such that the final partition is epsilon regular. But instead, in the proof of the regularity lemma, if you start with not the trivial partition, but start with a given partition and run the exact same proof, you find this consequence, except now you can guarantee that the final partition is a refinement of the one that you are given. So let's apply this statement. And we obtain a sequence of partitions of G, the vertex set of G, starting with P0 being a trivial partition and so on, such that each partition, each P sub i plus 1, refines the previous one and such that each P sub i plus 1 is epsilon sub P sub i regular. So you apply the regularity lemma with parameter based on the number of parts you currently have. Apply it to the current partition, you get a finer partition that's extremely regular. And you also know that the number of parts of the new partition is bounded in terms of the previous partition. Any questions so far? No. So now we get the sequence of partitions. We can keep on doing this. So G could be arbitrarily large. But eventually, we will be able to obtain the last condition here, which is the only thing that is missing so far. So since the energy is bounded between 0 and 1, there exists some i at most 1 over epsilon 0, such that the energy goes up by less than epsilon 0, because otherwise, your energy would exceed 1. So now let's set P to be this Pi and Q to be the refinement, the next term in the partition. And what we find is that the, OK, so then you have basically all the conditions. So P is epsilon 0 regular, because it is epsilon, the previous term, which is at most epsilon 0 regular. And you have this one as well and this one as well. And we want to show that the number of parts of Q is bounded. And that's basically because each time there was a bound on the number of parts, which depends only on the regularity parameters, and you're repeating that bound a bounded number of times. So Q is, OK, so it's bounded as a function of the sequence of epsilons, this infinite vector of epsilons. But it is a bounded number. You're only iterating this bound a bounded number of times. And that finishes the proof. Any questions? All right. It might be somewhat mysterious to you right now why we do this. So we'll get to the application in a second. But for now, I just want to comment a bit on the bounds. Of course, the bounds depend on what epsilon i's do you use. And typically, you want the epsilon i's to decrease with more parts that you have. And with almost all reasonable applications of this regularity lemma, the strong regularity lemma, so for example, with epsilon i being some epsilon divided by, let's say, i plus 1 or any polynomial of the i's, or you can even let it decay quicker than that as well, you see, basically what happens is that you are applying this M0 bound, M0 applied in succession 1 over epsilon times. And in the regularity lemma, we saw that the M0 that comes out of Szemeredi's graph regularity lemma is the tower function. So the tower function, let's say, tower of i is defined to be the exponential function iterated i times. So of course, I'm being somewhat loose here with the exact dependence. But you get the idea that now we want to apply the tower function i times. Instead of iterating the exponential i times, now you iterate the tower function i times. And some of you are laughing. This is an incredibly large number. It's even larger than the tower function. All right. So in literature, especially around the regularity lemma, this function where you iterate the tower function i times is given the name Wowser, as in wow, this is a huge number. So it's a step up in the Ackermann hierarchy. So if you repeat the Wowser function i times, you move up one ladder in the Ackermann hierarchy and this hierarchy of rapidly growing functions. But in any case, it's bounded. And that's good enough for us. Any questions so far? Yeah. AUDIENCE 2 What do you call the Wowser's function? YUFEI ZHAO So questions, what do you call Wowser iterated? I'm not aware of a standard name for that. Actually, even the name Wowser somehow is very common in the combinatorics community. But I think most people outside this community will not recognize this word. Any more questions? So another way is that it's a step up in Ackermann hierarchy. So it's enumerated 1, 2, 3, 4. You know, if you keep going up. All right. Another remark about this strong regularity lemma is that it will be convenient for us, actually somewhat more essential compared to our previous applications, to make the parts equitable. So P and Q equitable. And basically, the partitions are such that all the parts have basically the same number of vertices. So I won't make it precise, but you can do it. It's not too hard to do it. And you can do this. You can prove it similar to how I described how to modify the proof of the regularity lemma. So I won't belabor that point. But we'll use the equitable version. All right, so how does one use this regularity lemma? Let me state a corollary. And let me call this a corollary start, because you actually need to do some work to get it to follow from the strong regularity lemma. But the corollary is the version that we will apply. That if you start with a decreasing sequence of these epsilons, then there exists a delta such that the following is true. That every n-vertex graph has an equitable vertex partition. We call it Bi through Vk, and a subset Wi of each Vi, such that the following properties hold. First, all the W's are fairly large. They're at least constant proportion of the total vertex set. Between every pair of Wi, Wj, it is epsilon sub k regular. And this is the point I want to emphasize. So here, there are no irregular pairs anymore. So it is every. So no irregular pairs between the Wi's. And also, we need to include the case when i equals to j as well. So each Wi is regular with itself. And furthermore, the edge densities between the V's are similar to the edge densities between the corresponding W's. And here, it is for most pairs, for all but at most epsilon k square pairs, epsilon 0. Yeah, at most epsilon 0. Any questions about the statement? All right. So let me show you how you could deduce the corollary from the strong regularity lemma. So first, let me draw your picture. So here, you have a regularity partition. And so these are your V's. And inside each V, I find a W such that if I look at the edge sets between pairwise blue sets, including the blue sets with themselves, it is always very regular. And also, the edge densities between the blue sets is mostly very similar to the edge density between the ambient white sets. So let me say a few words. I won't go into too many details about how you might deduce this corollary from the strong regularity lemma. So first, let me do something which is slightly simpler, which is to not yet require that the blue sets, WIs, are regular with themselves. So without requiring this is regular, so we can obtain the WIs by picking a uniform random part of the final partition Q inside each part of P in the strong regularity lemma. So you have the strong regularity lemma, which produces for you a pair of partitions like that. So it produces for you a pair of partitions. And what we will do is to pick one of these guys as my W, pick one of these guys at random, and pick one of those guys at random. Because W is so extremely regular, most of these pairs will be regular. So with high probability, you will not encounter any irregular pairs if you pick the Qs at, if you pick the Ws randomly as parts of Q. So that's the key point. Here, we're using that Q is extremely regular. So all the Wi, Wj is regular for all i not equal to j with high probability. But the other thing that we would like is that the edge densities between the Ws are similar to those between the Vs. And for that, we will use this condition about their energies being very similar to each other. So the third consequence, C, is it's a consequence of the energy bound. Because recall that in our proof of the Szemeredi regularity lemma, there was an interpretation of the energy as the second moment of a certain random variable, which we call z. And using that interpretation, I can write down this expression like that, where here, assuming for simplicity that Q is completely equitable, so all the parts have exactly the same size, z of Q is defined to be the edge density between Vi and Vj for random ij. So this is a random variable, z. So you pick a pair of parts uniformly, or maybe with some weights if they're not exactly equal, and you evaluate the edge density. So this energy difference is the difference between the second moments. And because Q is a refinement of P, it is the case that this difference of L2 norms equal to the second moment of the difference of the random variables. So we saw a version of this earlier when we were discussing variance in the context of the proof of the Szemeredi regularity lemma. Here, it's basically the same. You can either look at this inequality part by part of V, or if you like to be a bit more abstract, then this is actually a case of Pythagorean theorem. If you view these as vectors in a certain vector space, then you have some orthogonality. So you have this sum of squares identity. Where does part A come from? So part A, we want the parts, the Wi's, to be not too small. But that comes from a bound on the number of parts of Q. So far, this more or less proves the corollary, except for that we simplified our lives by requiring just that the i not equal to j, the Vi, Vj's are regular. But in the statement up there, we also want the Wi's to be regular with themselves, which will be important for application. So I won't explain how to do that. And part of the reason is that this is also one of the homework problems. So in one of the homework problems on problem set 3, you are asked to prove that every graph has a subset of vertices that is of least constant proportion, such that it is regular with itself. And the methods you use there will be applicable to handle the situation over here as well. So putting all of these ingredients together, we get the corollary, whereby you have this picture, you have this partition. I don't even require the Vi's to be regular. That doesn't matter anymore. All that matters is that between the Wi's, they are very regular, and that there are no irregular parts between these Wi's. And now we'll be able to go back to the induced graph removal lemma, where previously we had an issue with the existence of irregular pairs in the usual Szemeredi regularity partition. And now we have a tool to get around that. So next, we will see how to execute this proof. But at this point, hopefully, you already see an outline, because, well, you no longer need to worry about this thing here. Let's take a quick break. Any questions so far? Yes? AUDIENCE 1 Why are we able to carry with us, like, the expectancy? Like, why did we? Why did we? If you don't use the invariant idea, like, why is that the only thing that seems to carry with us? YUFEI ZHAO OK. So the question was, there was a step where we were looking at some expectations of squares. And so why was that identity true? So if you look back to the proof of Szemeredi's regularity lemma, we already saw an instance of that inequality in the computation of the variance. So you know that the variance of x, on one hand, it is equal to where mu is the mean of x. And on the other hand, it is equal to this quantity. So you agree with this formula. And you can expand it to prove it. And that, the thing that I had, the question that you raised, basically, you can prove by looking at this formula part by part. Any more questions? So let's now prove the induced graph removal lemma. And we'll follow the regularity partition, but with a small twist, instead of using Szemeredi's regularity lemma, we will use that corollary up there. So let's prove the induced graph removal lemma. So the three steps. First, we do partition. So let's suppose you have a, so we suppose G is above. You have a few, very few copies, induced copies of H. Let's apply the corollary to get a partition of the vertex set of G into k parts. And inside each part, I have a W satisfying the following properties, that each Wiwj is regular with the following parameter, which we'll come out of later when we need to use the counting lemma. But it's some number, but don't worry too much about it. So here, I'm going to, OK, so let's say H has little h vertices. So between Wiwj, it is this regular. So we actually have not yet used the full strength of the corollary, where I can make the regularity even depend on k. So we will not need that here, but we'll need it in a later application. So the exponent is little h. So other properties are that the densities between the Vi's and the Wi's do not differ by more than epsilon over 2 for all but a small fraction, so epsilon k squared over 2 pairs. And finally, the sizes of the Wi's are at least delta 0 times n, where delta 0 depends only on epsilon, epsilon on H. OK, great. So this is the partition step. So now let's do the cleaning. In the cleaning step, basically, we're not going to, I mean, there's no longer an issue of irregular pairs if we only look at the Wi's. So we just need to think about the low density pairs or whatever the corresponding analog is. And what happens here is that for every i less than j, crucially including when i equals to j, if the edge densities between the W's is too small, then we remove all edges between Vi and Vj. And if the edge density between the Wi's is too big, then we remove all edges, so we add all edges between Vi and Vj. OK. Yep. How many edges do we end up adding or removing? OK, so the total number of edges added or removed from G is, in this case, so if the edge density in G between the Vi's and Vj's is also very small, then you do not remove very many edges. But most pairs of Vi and Vj have that property. So you tidy up what kind of errors you can get from here and there. And you find that the total number of edges that are added or removed from G is less than, let's say, epsilon n squared. Maybe you need an extra factor of 2, but upon changing some constant factors, it's less than epsilon n squared. So this is some small details you can work out. Here, we're using, so you ask, how is the edge density between Vi and Vj related to Wi and Wj? Well, for most pairs of i and j, they are very similar. And there's a small fraction of them that are not similar, but then you just lump everything in to this bound over here. So maybe I need to, let me just put a 2 here just to be safe. So we deleted a very small number of edges. And now we want to show that the graph that is resulted from this modification does not have any induced edge subgraphs. And the final step is the counting step. So suppose there were any induced edge left after the modification. So I want to show that, in fact, there must be a lot of H's, induced H's, originally in the graph, thereby contradicting the hypothesis. So suppose, so where does this induced edge sit? Well, you have this, the V's. And inside the V's, you have the W's. So suppose my H is that graph for illustration. And in particular, I have a non-edge. So I have an edge, and I also have a non-edge. So between these two, that's the non-edge. So suppose you find a copy of H in the cleaned up graph. Where can that cleaned up, this copy of H sit? Suppose you find it here. So the claim now is that if this copy of H existed here, then I must be able to find many such copies of H in the corresponding yellow parts. Because between the yellow parts, you have regularity. And you also have the right kinds of densities. Because if they didn't have the right kind of density, we would have cleaned it up already. So that's the idea. If you had a copy of this H somewhere, then I zoom into the yellow parts, zoom into these W's, and I find lots of copies of H in between the W's. So suppose, let me write this down. Suppose the little v's in the vertices lies in the, so I'm just indexing where little v lies. So little v lies in big V sub phi v for some phi, which sends the vertices of H to 1 through k. So now we apply counting lemma to embed induced copies of H in G, where the vertex v in H is mapped to a vertex in the corresponding W's. So and we would like to know that there are lots of such copies. And the counting lemma, or rather some variant, but you can actually read the counting lemma that we did last time and view it as a multipartite version. Apply this so far part to part. So we find that the number of such induced copies is within a small error. So within that regularity parameter, multiply by the number of edges of H, which we already canceled out, multiplied by the product of these W i's. So it's within this error of what you would suspect if you naively multiply the edge densities together along with the vertex densities. So these factors are for the edges that you want to embed. And then I also need to multiply the densities for the non-edges, so 1 minus these edge densities. So one way you can think of it is just consider the complement in G. So consider the complement of G to get this version here. And then finally, the product of the vertex set sizes. And the point is that this is not a small number. So the number, hence, the number of induced copies of H in G is at least on the order of 1. So it's at least some number, which is basically this guy over here, epsilon over 4 raised to. All of these are constants. That's the point. All of these guys are constants minus. So here is the main term and then the error term. And then the product of these vertex set sizes. And we saw that each vertex set is not too small. So you have lots of induced copies of H in G. Yep? AUDIENCE 1 How do you deal with the case where the density between the nth case is far from the density of the wj? And you have copies outside of the wi, and you can get rid of copies. YUFEI ZHAO OK, so can you repeat your question? AUDIENCE 1 How are you dealing with the all but epsilon k squared over 2 pairs such that? YUFEI ZHAO OK, so question, how do we deal with the all but epsilon over 2 pairs? And what I wrote in red in dealing with the number of total edges that are added or removed. So think about how many edges are added or removed. In these non-exceptional pairs, the number of edges that are added or removed, so in a. So in, let's just think about added edges. So if the density of v is controlled by that of w, then the number of edges added or removed in that case from all such pairs along with, yeah, so you have epsilon n squared edges changed. On the other hand, if this is not true, then you only have epsilon k squared such pairs ij, for which this cannot be true. So you also only have at most epsilon n squared edges added or removed in such cases. That answers your question? Yes? AUDIENCE 2 Is that number 0? YUFEI ZHAO So your question is, which number is 0? AUDIENCE 2 The number of induced edges, like the top. YUFEI ZHAO The? AUDIENCE 2 Yeah, top board. YUFEI ZHAO Top board, OK. Ah, good. So asking about this number, so that should have been 2. Yes? AUDIENCE 2 Is it k? YUFEI ZHAO OK, it's a question. You don't see k appearing anywhere. So the k in the corollary, do you mean? AUDIENCE 2 Yeah. YUFEI ZHAO So that hasn't come up yet. So it comes up implicitly because we need to lower bound the sizes of these W's. So if you had, so this is partly why we need a bound on the number of parts. But it is true that we do not need epsilon k to depend on k in this application yet. I will mention a different application in a second where you do need that k. OK, so the number of induced H in G is at least this amount. So that's a small lie. You need to maybe consider this is a number of homomorphic. Well, actually, no, we're OK. Never mind. So you can set delta to be this quantity here. And then that finishes the proof. So you have lots of induced copies of H in your graph, which contradicts the hypothesis. So that finishes the proof of the induced removal lemma. And basically, the proof is the same as the usual graph removal lemma, except that now we need some strengthened regularity lemma, which allows us to get rid of irregular parts, but in a more restricted setting. Because we saw you cannot completely get rid of irregular parts. Any questions? Yes. AUDIENCE 2 So I want to address the question of, why did I state this corollary in this more general form of a decreasing sequence of epsilons? Well, so first of all, with strong regularity lemmas, the strength is sometimes always nice to state it with this extra strength, because it's the right way to think about these types of theorems, that the regularity on the parts depends. You can make it depend on the number of parts so that you get much stronger control on the regularity. But there are also some applications. For example, whether I will state next, an application where you do need that kind of strength. So here is what's known as an infinite removal lemma. Here we have not just a single pattern or a finite number of patterns we want to get rid of, but now we have infinitely many patterns. So for every curly H, which is a possibly infinite set of graphs, the graphs themselves are always finite, but this might be an infinite list, and an epsilon parameter. There exists an H0 and a delta positive parameter such that every n-vertex graph with at most delta, so less than delta v to the H induced copies of H for every H in this family with fewer than H0 vertices. So every graph with this property can be made curly H-free, so it means free up, induced curly H-free by adding or removing fewer than epsilon n squared edges. So now instead of a single pattern, you have a possibly infinite set of induced patterns. And I want to make your graph curly H-free, induced curly H-free. And the theorem is that if there exists some finite bound, H0, such that if you have few copies, so for all the patterns up to that point, then you can do what you need to do. OK, so it takes some time to even digest the statement, but it's somehow an infinite version. It's the correct infinite version of the removal lemma if you have infinitely many patterns that you need to remove. And I claim that the proof is actually more or less the same proof as the one that we did here, except now you need to take your epsilon case, as in this corollary, to depend on k. You need to in some way look ahead in this infinite pattern. So here in proof, this epsilon k from corollary depends on k. And also, it depends on your family of patterns, H. All right, finally, I want to mention a perspective, a computer science perspective, on this removal lemma that we've been discussing so far. And that's in the context of something called property testing. And basically, we would like an efficient, efficient meaning fast, a randomized algorithm to distinguish graphs that are triangle-free from those that are epsilon-far from triangle-free, where being epsilon-far from triangle-free means that you need to change more than epsilon n squared edges. Here, n is, as usual, the number of vertices to make triangle-free, to make the graph triangle-free. So the distance, the edit distance, is more than epsilon away from being triangle-free. So somebody gives you a very large graph. Think n is very large. You cannot search through every triple vertices. That's too expensive. But you want some way to test if a graph is triangle-free versus very far away from being triangle-free. So there's a very simple randomized algorithm to do this, which is to just try randomly, sample a random triple of vertices, and check if it's a triangle. So you do this. And just to make our life a bit more secure, let's try it some larger number of times, so some C of epsilon, some constant number of times. And if you find a triangle, if you don't find a triangle, then we return that it's triangle-free. Otherwise, we return that it is epsilon far from triangle-free. So that's the algorithm. So it's a very intuitive algorithm. But why does it work? So we want to know that, indeed, somebody gives you one of these two possibilities. You run the algorithm. You can succeed with high probability. Question? AUDIENCE 1. I think that depends on the epsilon that you got here. YUFEI ZHAO. So let's talk about why this works. So the theorem, for every epsilon, there exists a C such that algorithm succeeds with probability bigger than 2 thirds. And 2 thirds can be any number, so any number that you like, because you can always repeat it to boost that constant probability. So there are two cases. If G is triangle-free, then it always succeeds. You will never find this triangle, and it would return triangle-free. On the other hand, if G is epsilon far from triangle-free, then triangle removal lemma tells us that G has lots of triangles, delta and cubed triangles. So if we sample C being, let's say, 1 over delta times, so delta here is a function of epsilon from the triangle removal lemma, so we find that the probability that the algorithm fails is, at most, so you have lots of triangles. So very likely, you will hit one of these triangles. So the probability that the algorithm fails is, at most, 1 minus delta and cubed divided by total number of triples raised to 1 over delta. And this is, at most, 1 minus 6 delta raised to 1 over delta. And it's, at most, e to the minus 6, so less than 1 third in particular. So this algorithm succeeds with high probability. Now, how big of a C do you need? Well, that depends on the triangle removal lemma. So it's a constant. It's a constant. Does not depend on the size of the graph. But it's a large constant, because we saw in the proof of regularity lemma that it can be very large. But this theorem here is basically the same as the triangle removal lemma. So it's highly non-trivial that it is true, even though the algorithm is extremely naive and simple. I just want to finish off with one more thing. Instead of testing for triangle freeness, you can ask, what other properties can you test? So which graph properties are testable in that sense? So distinguishing something which has the property, so P versus far, epsilon far, from this property P. And you have this tester, which is you sample some number of vertices. So this is called the oblivious tester. So you sample k vertices. And you try to see if it has that property. So there's a class of properties called hereditary. So hereditary properties are properties that are closed under vertex deletion. And these properties are lots of properties that you've seen are of this form. So for example, being H-free is this form being planar. So this form being induced H-free, so this form being three-colorable, being perfect, they're all examples of hereditary properties, properties that if your graph is three-colorable, you take out some vertices, it's still three-colorable. And all the discussions that we've done so far, in particular the infinite removal lemma, if you phrase it in the form of property testing, given the above discussion, it implies that every hereditary property is testable. So in fact, it's testable in the above sense with a one-sided error using an oblivious tester. One-sided error means that if it's up there, if it's triangle-free, then it always succeeds. So here, one of the cases, it always succeeds. And the reason is that you can characterize a hereditary property by a Curly H-induced H-free for some Curly H, namely, you're putting everything into H that do not have this property. This is a possibly infinite set of graphs. And that completely characterizes this hereditary property. And if you read out the infinite removal lemma, it says precisely, using above this interpretation, that you have a property testing algorithm.