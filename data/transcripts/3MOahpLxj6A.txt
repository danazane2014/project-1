 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. OK, so let us start. OK. All right, so today we're starting a new unit in this class. We have covered so far the basics of probability theory, the main concepts and tools as far as just probabilities are concerned. But if that was all that there is in this subject, the subject would not be rich enough. What makes probability theory a lot more interesting and richer is that we can also talk about random variables, which are ways of assigning numerical outcomes, numerical results, to the outcomes of an experiment. So we're going to define what random variables are. And then we're going to describe them using so-called probability mass functions. Basically, some numerical values are more likely to occur than other numerical values. And we capture this by assigning probabilities to them the usual way. And we represent these in a compact way using the so-called probability mass functions. We're going to see a couple of examples of random variables, some of which we have already seen but with different terminology. And so far, it's going to be just a couple of definitions and calculations of the type that you already know how to do. But then we're going to introduce the one new big concept of the day. So up to here, it's going to be mostly an exercise in notation and definitions. But then we got our big concept, which is the concept of the expected value of a random variable, which is some kind of average value of the random variable. And then we're going to also talk very briefly about the close distance of the expectation, which is the concept of the variance of a random variable. OK, so what is a random variable? So it's an assignment of a numerical value to every possible outcome of the experiment. So here's the picture. The sample space is this class, and we've got lots of students in here. This is our sample space, omega. I'm interested in the height of a random student. So I'm going to use a real line where I record height. And let's say this is height in inches. And the experiment happens. I pick a random student, and I go and measure the height of that random student, and that gives me a specific number. So what's a good number in inches? Let's say 60. OK, or I pick another student, and that student has a height of 71 inches and so on. So this is the experiment. These are the outcomes. These are the numerical values of the random variable that we call height. OK, so mathematically, what are we dealing with here? We're basically dealing with a function from the sample space into the real numbers. That function takes as argument an outcome of the experiment, that is a typical student, and produces the value of that function, which is the height of that particular student. So we think of an abstract object that we denote by a capital H, which is the random variable called height. And that random variable is essentially this particular function that we talked about here. OK, so there's a distinction that we're making here. H is height in the abstract. It's the function. These numbers here are particular numerical values that this function takes when you choose one particular outcome of the experiment. Now, when you have a single probability experiment, you can have multiple random variables. So perhaps instead of just height, I'm also interested in the weight of a typical student. And so when the experiment happens, I pick that random student. This is the height of the student, but that student will also have a weight, and I could record it here. And similarly, every student is going to have their own particular weight. So the weight function is a different function from the sample space to the real numbers, and it's a different random variable. So the point I'm making here is that a single probabilistic experiment may involve several interesting random variables. I may be interested in the height of a random student or the weight of the random student. These are different random variables that could be of interest. I can also do other things. Suppose I define an object such as h bar, which is 2.5h. What does that correspond to? Well, this is the height in centimeters. Now, h bar is a function of h itself, but if you were to draw the picture, the picture would go this way. 60 gets mapped to 150, 71 gets mapped to, oh, that's too hard for me, OK, it gets mapped to something, and so on. So h bar is also a random variable. Why? Once I pick a particular student, that particular outcome determines completely the numerical value of h bar, which is the height of that student, but measured in centimeters. What we have here is actually a random variable, which is defined as a function of another random variable. And the point that this example is trying to make is that functions of random variables are also random variables. The experiment happens. The experiment determines a numerical value for this object. And once you have the numerical value for this object, that determines also the numerical value for that object. So given an outcome, the numerical value of this particular object is determined. So h bar is itself a function from the sample space, from outcomes to numerical values. And that makes it a random variable according to the formal definition that we have here. So the formal definition is that the random variable is not random. It's not a variable. It's just a function from the sample space to the real numbers. That's the abstract, the right way of thinking about them. Now random variables can be of different types. They can be discrete or continuous. Suppose that I measure the height in inches, but I round to the nearest inch. Then the numerical values I'm going to get here will be just integers. So that would make it an integer-valued random variable, and this is a discrete random variable. Or maybe I have a scale for measuring height which is infinitely precise and records your height to an infinite number of digits of precision. In that case, your height would be just a general real number. So we would have a random variable that takes values in the entire set of real numbers. Well, I guess not really negative numbers, but the set of non-negative numbers. And that would be a continuous random variable. It takes values in a continuous set. So we will be talking about both discrete and continuous random variables. The first thing we will do will be to devote a few lectures on discrete random variables, because discrete is always easier. And then we're going to repeat everything in the continuous setting. So the discrete is easier, and it's the right place to understand all the concepts, even those who may appear to be elementary. And then you will be set to understand what's going on when we go to the continuous case. So in the continuous case, you get all the complications of calculus and some extra math that comes in there. So it's important to have pinned down all the concepts very well in the easy discrete case, so that you don't have conceptual hurdles when you move on to the continuous case. Now, one important remark that may seem trivial, but it's actually very important so that you don't get tangled up between different types of concepts. There's a fundamental distinction between the random variable itself and the numerical values that it takes. Abstractly speaking, or mathematically speaking, a random variable X, or H in this example, is a function. OK. Maybe if you like programming, the word procedure or subroutine might be better. So what's the subroutine height? Given a student, I take that student, force them on a scale and measure them. That's the subroutine that measures heights. It's really a function that takes students as input and produces numbers as output. The subroutine, we denote it by capital H. That's the random variable. But once you plug in a particular student into that subroutine, you end up getting a particular number. This is the numerical output of that subroutine, or the numerical value of that function. And that numerical value is an element of the real numbers. So the numerical value is a real number, where this capital X is a function from omega to the real numbers. So they're very different types of objects. And the way that we keep track of what we're talking about at any given time is by using capital letters for random variables and lowercase letters for numbers. OK. So now, once we have a random variable at hand, that random variable takes on different numerical values. And we want to describe, to say something about the relative likelihoods of the different numerical values that the random variable can take. So here's our sample space. And here's the real line. And there is a bunch of outcomes that give rise to one particular numerical value. There's another numerical value that arises if we have this outcome. There's another numerical value that arises if we have this outcome. So our sample space is here. The real numbers are here. And what we want to do is to ask the question, how likely is that particular numerical value to occur? So what we're essentially asking is, how likely is it that we obtain an outcome that leads to that particular numerical value? We calculate that overall probability of that numerical value, and we represent that probability using a bar so that we end up generating a bar graph. So that could be a possible bar graph associated with this picture. The size of this bar is the total probability that our random variable took on this numerical value, which is just the sum of the probabilities of the different outcomes that led to that numerical value. So the thing that we're plotting here, the bar graph, we give a name to it. It's a function which we denote by lowercase p capital X. The capital X indicates which random variable we're talking about. And it's a function of little x, which is the range of values that our random variable is taking. So in mathematical notation, the value of the PMF at some particular number, little x, is the probability that our random variable takes on the numerical value, little x. And if you want to be precise about what this means, it's the overall probability of all outcomes for which the random variable ends up taking that value, little x. So this is the overall probability of all omegas that lead to that particular numerical value x of interest. So what do we know about PMFs? Since they're probabilities, all these entries in the bar graph have to be non-negative. Also, if you exhaust all the possible values of little x's, you will have exhausted all the possible outcomes here, because every outcome leads to some particular x. So the sum of these probabilities should be equal to 1. This is the second relation here. So this relation tells us that some little x is going to happen. They happen with different probabilities. But when you consider all the possible little x's together, one of those little x's is going to be realized. Probabilities need to add to 1. OK, so let's get our first example of a non-trivial bar graph. Consider the experiment where I start with a coin, and I start flipping it over and over. And I do this until I obtain heads for the first time. So what are possible outcomes of this experiment? One possible outcome is that I obtain heads at the first toss and then I stop. In this case, my random variable takes the value 1. Or it's possible that I obtain tails and then heads. How many tosses did it take until heads appeared? This would be x equals to 2. Or more generally, I might obtain tails for k minus 1 times, and then obtain heads at the k-th time, in which case our random variable takes the value little k. So that's the experiment. So capital X is a well-defined random variable. It's the number of tosses it takes until I see heads for the first time. These are the possible outcomes. These are elements of our sample space. And these are the values of x depending on the outcome. Clearly, x is a function of the outcome. You tell me the outcome, I'm going to tell you what x is. So what we want to do now is to calculate the PMF of X. So PX of k is, by definition, the probability that our random variable takes the value k. The number for the random variable to take the value k, the first head appears at toss number k. The only way that this event can happen is if we obtain this sequence of events. T's, the first k minus 1 times, tails, and heads at the k-th flip. So this event, that the random variable is equal to k, is the same as this event, k minus 1 tails followed by 1 head. What's the probability of that event? We're assuming that the coin tosses are independent. So to find the probability of this event, we need to multiply the probability of tails times the probability of tails times the probability of tails. We multiply k minus 1 times times the probability of heads, which puts an extra p at the end. And this is the formula for the so-called geometric PMF. And why do we call it geometric? Because if you go and plot the bar graph of this random variable X, we start at 1 with a certain number, which is p. And then at 2, we get p times 1 minus p. At 3, we're going to get something smaller. It's p times 1 minus p squared. And the bars keep going down at the rate of geometric progression. Each bar is smaller than the previous bar, because each time we get an extra factor of 1 minus p involved. So the shape of this PMF is the graph of a geometric sequence. For that reason, we say that it's the geometric PMF. And we call X also a geometric random variable. So the number of coin tosses until the first head is a geometric random variable. So this was an example of how to compute the PMF of a random variable. This was an easy example, because this event could be realized in one and only one way. So to find the probability of this, we just needed to find the probability of this particular outcome. More generally, there's going to be many outcomes that can lead to the same numerical value. And we need to keep track of all of them. For example, in this picture, if I want to find this value of the PMF, I need to add up the probabilities of all the outcomes that lead to that value. So the general procedure is exactly what this picture suggests. To find this probability, you go and identify which outcomes lead to this numerical value and add their probabilities. So let's do a simple example. I take a tetrahedral die. I toss it twice. And there's lots of random variables that you can associate with the same experiment. So the outcome of the first throw, we can call it F. That's a random variable, because it's determined once you tell me what happens in the two, what happens in the experiment. The outcome of the second throw is another random variable. The minimum of the two throws is also a random variable. Once I do the experiment, this random variable takes on a specific numerical value. So suppose I do the experiment and I get a 2 and a 3. So this random variable is going to take the numerical value of 2. This is going to take the numerical value of 3. This is going to take the numerical value of 2. And now suppose that I want to calculate the PMF of this random variable. What I will need to do is to calculate Px of 0, Px of 1, Px of 2, Px of 3, and so on. Let's not do the entire calculation. Then let's just calculate one of the entries of the PMF. So Px of 2, that's the probability that the minimum of the two throws gives us a 2. And this can happen in many ways. There are five ways that it can happen. Those are all the outcomes for which the smallest of the two is equal to 2. That's five outcomes. Assuming that the tetrahedral die is fair and the tosses are independent, each one of these outcomes has probability of 1 over 16. There's five of them, so we get an answer, 5 over 16. So conceptually, this is just the procedure that you use to calculate PMFs the way that you construct this particular bar graph. You consider all the possible values of your random variable, and for each one of those random variables, you find the probability that the random variable takes on that value by adding the probabilities of all the possible outcomes that lead to that particular numerical value. So let's do another more interesting one. So let's revisit the coin tossing problem from last time. Let us fix a number n, and we decide to flip a coin n consecutive times. Each time the coin tosses are independent, at each one of the tosses, we have a probability, p, of obtaining heads. Let's consider the random variable, which is the total number of heads that have been obtained. Well, that's something that we dealt with last time. We know the probabilities for different numbers of heads, but we're just going to do the same now using today's notation. So let's, for concreteness, n equal to 4. px is the PMF of that random variable, x. px of 2 is meant to be, by definition, it's the probability that our random variable takes the value of 2. So this is the probability that we have exactly 2 heads in our 4 tosses. The event of exactly 2 heads can happen in multiple ways. And here I've written down the different ways that it can happen. It turns out that there's exactly 6 ways that it can happen. And each one of these ways, luckily enough, has the same probability, p squared times 1 minus p squared. So that gives us the value for the PMF evaluated at 2. So here we just counted explicitly that we have 6 possible ways that this can happen. And this gave rise to this factor of 6. But this factor of 6 turns out to be the same as this 4 choose 2. If you remember the definition from last time, 4 choose 2 is 4 factorial divided by 2 factorial divided by 2 factorial, which is indeed equal to 6. And this is the more general formula that you would be using. In general, if you have n tosses and you're interested in the probability of obtaining k heads, the probability of that event is given by this formula. So that's the formula that we derived last time, except that last time we didn't use this notation. We just said the probability of k heads is equal to this. Today we introduce the extra notation. And also having that notation, we may be tempted to also plot a bar graph for the PX, in this case, for the coin tossing problem. And if you plot that bar graph as a function of k, when n is a fairly large number, what you will end up obtaining is a bar graph that has a shape of something like this. So certain values of k are more likely than others. And the more likely values are somewhere in the middle of the range. And extreme values, too few heads or too many heads, are unlikely. Now the miraculous thing is that it turns out that this curve gets a pretty definite shape, like a so-called bell curve, when n is big. This is a very deep and central fact from probability theory that we will get to in a couple of months. For now, it's just a curious, could be a curious observation. If you go into MATLAB and put this formula in and ask MATLAB to plot it for you, you're going to get an interesting shape of this form. And later on, we will have to sort of understand where this is coming from and whether there's a nice, simple formula for the asymptotic form that we get. All right, so, so far I've said essentially nothing new, just a little bit of notation and this little conceptual thing that you have to think of random variables as functions on the sample space. So now it's time to introduce something new. This is the big concept of the day. In some sense, it's an easy concept. But it's the most central, most important concept there is that we have to deal with random variables. It's the concept of the expected value of a random variable. So the expected value is meant to be, let's speak loosely, something like an average, where you interpret probabilities as something like frequencies. So you play a certain game, and your rewards are going to be, let me just use my standard numbers. Your rewards are going to be $1 with probability 1 sixth. It's going to be $2 with probability 1 half. And $4 with probability 1 third. So this is a plot of the PMF of some random variable. If you play that game, and you get so many dollars with this probability and so on, how much do you expect to get on the average if you play the game a zillion times? Well, you can think as follows. 1 sixth of the time, I'm going to get $1. 1 half of the time, that outcome is going to happen, and I'm going to get $2. And 1 third of the time, the other outcome happens, and I'm going to get $4. And you evaluate that number, and it turns out to be 2.5. So that's a reasonable way of calculating the average payoff if you think of these probabilities as the frequencies with which you obtain the different payoffs. And loosely speaking, it doesn't hurt to think of probabilities as frequencies when you try to make sense of various things. So what did we do now? What did we do here? We took the probabilities of the different outcomes, of the different numerical values, and multiplied them with the corresponding numerical value. Similar here, we have a probability and the corresponding numerical value, and we added up over all x's. So that's what we did. Looks like an interesting quantity to deal with. So we're going to give a name to it, and we're going to call it the expected value of a random variable. So this formula just captures the calculation that we did. How do we interpret the expected value? So the one interpretation is the one that I used in this example. You can think of it as the average that you get over a large number of repetitions of an experiment, where you interpret the probabilities as the frequencies with which the different numerical values can happen. There's another interpretation that's a little more visual, and that's kind of insightful. If you remember your freshman physics, this kind of formula gives you the center of gravity of an object of this kind. If you take that picture literally, and think of this as a mass of 1 6th sitting here, and the mass of 1 1 2 6th sitting here, and 1 3rd sitting there, and you ask me what's the center of gravity of that structure, this is the formula that gives you the center of gravity. Now what's the center of gravity? It's the place where if you put your pen right underneath, that diagram will stay in place and will not fall on one side and will not fall on the other side. So in this thing, by picture, since the 4 is a little more to the right and a little heavier, the center of gravity should be somewhere around here. And that's what the math gave us. It turns out to be 2 and 1. Once you have this interpretation about centers of gravity, sometimes you can calculate expectations pretty fast. So here's our new random variable. It's the uniform random variable in which each one of the numerical values is equally likely. Here there's a total of n plus 1 possible numerical value, so each one of them has probability 1 over n plus 1. Let's calculate the expected value of this random variable. We can take the formula, literally, and consider all possible outcomes, or all possible numerical values, and weigh them by their corresponding probability, and do this calculation and obtain an answer. But I gave you the intuition of centers of gravity. Can you use that intuition to guess the answer? What's the center of gravity of a structure of this kind? We have symmetry. So it should be in the middle. And what's the middle? It's the average of the two endpoints. So without having to do the algebra, you know that the answer is going to be n over 2. So this is a moral that you should keep whenever you have a PMF which is symmetric around a certain point. That certain point is going to be the expected value associated with this particular PMF. OK, so having defined the expected value, what is there that's left for us to do? Well, we want to investigate how it behaves. What kind of properties does it have? And also, how do you calculate expected values of complicated random variables? So the first complication that we're going to start with is the case where we deal with a function of a random variable. OK, so let me redraw this same picture as before. We have omega. This is our sample space. This is the real line. And we have a random variable that gives rise to various values for X. So the random variable is capital X, and every outcome leads to a particular numerical value for our random variable X. So capital X is really the function that maps these points into the real line. And then I consider a function of this random variable, call it capital Y. And it's a function of my previous random variable. And this new random variable Y takes numerical values that are completely determined once I know the numerical value of capital X. And perhaps you get a diagram of this kind. So X is a random variable. Once you have an outcome, this determines the value of X. Y is also a random variable. Once you have the outcome, that determines the value of Y. Y is completely determined once you know X. We have a formula for how to calculate the expected value of X. Suppose that you're interested in calculating the expected value of Y. How would you go about it? OK, the only thing you have in your hands is the definition. So you could start by just using the definition. And what does this entail? It entails for every particular value of Y, collect all the outcomes that lead to that value of Y, find their probability. Do the same here. For that value, collect those outcomes, find their probability, and weigh by Y. So this formula does the addition over this line. We consider the different outcomes and add things up. There's an alternative way of doing the same accounting where instead of doing the addition over those numbers, we do the addition up here. We consider the different possible values of X. And we think as follows. For each possible value of X, that value is going to occur with this probability. And if that value has occurred, this is how much I'm getting, the g of X. So I'm considering the probability of this outcome. And in that case, Y takes this value. Then I'm considering the probabilities of this outcome. And in that case, g of X takes again that value. Then I consider this particular X. It happens with this much probability. And in that case, g of X takes that value. And similarly here. We end up doing exactly the same arithmetic. It's only a question whether we bundle things together. That is, if we calculate the probability of this, then we're bundling these two cases together. Whereas if we do the addition up here, we do a separate calculation, this probability times this number, and then this probability times that number. So it's just a simple rearrangement of the way that we do the calculations. But it does make a big difference in practice if you actually want to calculate expectations. So the second procedure that I mentioned, where you do the addition by running over the X-axis, corresponds to this formula. Consider all possibilities for X, their probabilities. And when that X happens, how much money are you getting? That gives you the average money that you are getting. All right. So I kind of hand-waved and argued that it's just a different way of accounting. Of course, one needs to prove this formula. And fortunately, it can be proved. You're going to see that in recitation. Most people, once they're a little comfortable with the concepts of probability, actually believe that this is true by definition. In fact, it's not true by definition. It's called the law of the unconscious statistician. It's something that you always do, but it's something that does require justification. All right. So this gives us basically a shortcut for calculating expected values of functions of a random variable without having to find the PMF of that function. We can work with the PMF of the original function. All right. So we're going to use this property over and over. Before we start using it, one general word of caution. The average of a function of a random variable, in general, is not the same as the function of the average. So these two operations of taking averages and taking functions do not commute. What this inequality tells you is that, in general, you cannot reason on the average. So we're going to see instances of where this property is not true. You're going to see lots of them. Let me just throw it here that it's something that's not true in general. But we will be interested in the exceptions where a relation like this is true. But these will be the exceptions. So in general, expectations are average, something like averages. But the function of an average is not the same as the average of the function. OK. So now let's go to properties of expectations. Suppose that alpha is a real number. And I ask you, what's the expected value of that real number? So for example, if I write down this expression, expected value of 2, what is this? Well, we defined random variables. And we defined expectations of random variables. So for this to make syntactic sense, this thing inside here should be a random variable. Is 2, the number 2, is it a random variable? In some sense, yes. It's the random variable that takes always the value of 2. So suppose that you have some experiments. And that experiment always outputs 2 whenever it happens. Then you can say, yes, it's a random experiment. But it always gives me 2. The value of the random variable is always 2, no matter what. It's kind of a degenerate random variable that doesn't have any real randomness in it. But it's still useful to think of it as a special case. So it corresponds to a function from the sample space to the real line that takes only one value. No matter what the outcome is, it always gives me a 2. OK. If you have a random variable that always gives you a 2, what is the expected value going to be? The only entry that shows up in this summation is that number 2. The probability of a 2 is equal to 1. And the value of that random variable is equal to 2. So it's the number itself. So the average value in an experiment that always gives you 2's is 2. All right. So that's nice and simple. Now let's go to our experiment where h was your height in inches. And I know your height in inches, but I'm interested in your height measured in centimeters. How is that going to be related to your height in inches? Well, if you take your height in inches and convert it to centimeters, I have another random variable which is always, no matter what, 2 and 1⁄2 times bigger than the random variable I started with. If you take some quantity and always multiply it by 2 and 1⁄2 what happens to the average of that quantity? It also gets multiplied by 2 and 1⁄2. So you get a relation like this, which says that the average height of a student measured in centimeters is 2 and 1⁄2 times the average height of a student measured in inches. So that makes perfect intuitive sense. If you generalize it, it gives us this relation. That if you have a number, you can pull it outside the expectation and you get the right result. So this is a case where you can reason on the average. If you take a number such as height and multiply it by a certain number, you can reason on the average. I multiply the numbers by 2, the averages will go up by 2. So this is an exception to this cautionary statement that I had up there. How do we prove that this fact is true? Well, we can use the expected value rule here, which tells us that the expected value of alpha X, this is our g of X, essentially, is going to be the sum over all X's of my function g of X times the probability of the X's. In our particular case, g of X is alpha times X. And we have those probabilities. And the alpha goes outside the summation. So we get alpha sum of X's over X's, Xp X of X, which is alpha times the expected value of X. So that's how you prove this relation formally, using this rule up here. And the next formula that I have here also gets proved the same way. What does this formula tell you? If I take everybody's height in centimeters, we already multiplied by alpha, and the gods give everyone a bonus of 10 extra centimeters. What's going to happen to the average height of the class? Well, it will just go up by an extra 10 centimeters. So this expectation is going to be giving you the bonus of beta, just adds a beta to the average height in centimeters, which we also know to be alpha times the expected value of X plus beta. So this is a linearity property of expectations. If you take a linear function of a single random variable, the expected value of that linear function is the linear function of the expected value. So this is our big exception to this cautionary note that we have equal if g is linear. OK. All right. So let's get to the last concept of the day. What kind of functions of random variables may be of interest? One possibility might be the average value of X squared. Why is it interesting? Well, why not? It's the simplest function that you can think of. So if you want to calculate the expected value of X squared, you would use this general rule for how you can calculate expected values of functions of random variables. You consider all the possible X's. For each X, you see what's the probability that it occurs. And if that X occurs, you consider and see how big X squared is. Now, a more interesting quantity, a more interesting expectation that you can calculate has to do not with X squared, but with the distance of X from the mean, and then squared. So let's try to parse what we've got up here. Let's look just at the quantity inside here. What kind of quantity is it? It's a random variable. Why? X is random, a random variable. If expected value of X is a number, subtract the number from a random variable, you get another random variable. Take a random variable and square it, you get another random variable. So the thing inside here is a legitimate random variable. What kind of random variable is it? So suppose that we have our experiment, and we have different X's that can happen. And the mean of X in this picture might be somewhere around here. I do the experiment. I obtain some numerical value of X. Let's say I obtain this numerical value. I look at the distance from the mean, which is this length, and I take the square of that. Each time that I do the experiment, I go and record my distance from the mean and square it. So I give more emphasis to big distances. And then I take the average over all possible outcomes, all possible numerical values. So I'm trying to compute the average squared distance from the mean. This corresponds to this formula here. So the picture that I drew corresponds to that. For every possible numerical value of X, that numerical value corresponds to a certain distance from the mean squared. And I weigh it according to how likely is that particular value of X to arise. So this measures the average squared distance from the mean. Now, because of that expected value rule, of course, this thing is the same as that expectation. It's the average value of the random variable, which is the squared distance from the mean. With this probability, the random variable takes on this numerical value. And the squared distance from the mean ends up taking that particular numerical value. So why is the variance interesting? It tells us how far away from the mean we expect to be on the average. Well, actually, we're not counting distances from the mean, it's distances squared. So it gives more emphasis to the outliers in here. But it's a measure of how spread out a distribution is. A big variance means that those bars go far to the left and the right, typically. Whereas a small variance would mean that all those bars are tightly concentrated around the mean value. It's the average squared deviation. Small variance means that we generally have small deviations. Large variances mean that we generally have large deviations. Now as a practical matter, when you want to calculate the variance, there's a handy formula, which I'm not proving, but you will see it in recitation. It's just two lines of algebra. And it allows us to calculate it in a somewhat simpler way. We need to calculate the expected value of the random variable and the expected value of the squared of the random variable. And these two are going to give us the variance. So to summarize what we did up here, the variance, by definition, is given by this formula. It's the expected value of the squared deviation. But we have the equivalent formula, which comes from application of the expected value rule to the function g of x equals to x minus expected value of x squared. So this is the definition. This comes from the expected value rule. What are some properties of the variance? Of course, variances are always non-negative. Why is it always non-negative? Well, you look at the definition, and you're just adding up non-negative things. We're adding squared deviations. So when you add non-negative things, you get something non-negative. The next question is, how do things scale if you take a linear function of a random variable? Let's think about the effect of beta. If I take a random variable and add a constant to it, how does this affect the amount of spread that we have? It doesn't affect whatever the spread of this thing is. If I add the constant beta, it just moves this diagram here. But the spread doesn't grow or get reduced. The thing is that when I'm adding a constant to a random variable, all of the x's that are going to appear are further to the right. But the expected value also moves to the right. And since we're only interested in distances from the mean, these distances do not get affected. x gets increased by something. The mean gets increased by that same something. The difference stays the same. So adding a constant to a random variable doesn't do anything to its variance. But if I multiply a random variable by a constant alpha, what is that going to do to its variance? Because we have a square here, when I multiply my random variable by a constant, this x gets multiplied by a constant. The mean gets multiplied by a constant. The square gets multiplied by the square of that constant. And because of that reason, we get the square of alpha showing up here. So that's how variances transform under linear transformations. You multiply a random variable by a constant, the variance goes up by the square of that same constant. OK, that's it for today. See you on Wednesday.