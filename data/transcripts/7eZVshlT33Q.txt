 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. OK, so far we've learned about partial derivatives and how to use them to find minima and maxima of functions of two variables or several variables. And now we are going to try to study more in detail how functions of several variables behave, how to compute their variations, how to estimate their variation in arbitrary directions. And so for that we are going to need some more tools actually to study functions. So, more tools to study functions. And, so today's topic is going to be differentials. And, just to motivate that, let me remind you about one trick that you probably know from single variable calculus, namely implicit differentiation. So, let's say that you have a function y equals f of x. Then, you would sometimes write dy equals f prime of x times dx. And then, maybe you would, excuse me, can you please take a seat and sit down? That would be better. Thank you. So, we use implicit differentiation to actually relate infinitesimal changes in y with infinitesimal changes in x. And, one thing we can do with that, for example, is actually figure out the rate of change dy by dx, but also the reciprocal dx by dy. And so, for example, let's say that we have y equals inverse sine of x. Then, we can write x equals sine of y. And, from there, we can actually find out what is the derivative of this function, if we didn't know the answer already, by writing dx equals cosine y over dx. And so, that tells us that dy over dx is going to be one over cosine y. And now, cosine, the relation to sine, is basically one over square root of one minus x squared. And, that's how you find the formula for the derivative of y. OK, formula that probably you already knew, but that's one way to derive it. So, now, we are going to use, also, these kinds of notations, dx, dy, and so on, but use them for functions of several variables. And, of course, we'll have to learn what are the rules of manipulation, and what we can do with them. So, the actual name of that is the total differential, as opposed to the partial derivatives, because, well, it's going to be the total differential because it includes all of the various causes that can change, sorry, all of the contributions that can cause the value of your function, f, to change. So, namely, let's say that you have a function, f, maybe of three variables, x, y, z. Then, you will write df equals f sub x dx plus f sub y dy plus f sub z dz. OK, or maybe, just to remind you of the other notation, partial f partial x dx plus partial f partial y dy plus partial f over partial z dz. Now, what is this object? What are the things on either side of this equality? Well, they're called differentials, and they are not numbers. They are not vectors. They are not matrices. They are a different kind of object. So, these things have their own rules of manipulations, and we have to learn what we can do with them. So, how do we think about them? So, first of all, how do we not think about them? OK, so here's an important thing to pitfall to avoid. So, important, df is not the same thing as delta f. OK, so that thing is meant to be a number. It's going to be a number once you have a small variation of x, a small variation of y, a small variation of z. They are numbers. Delta x, delta y, delta z are actual numbers, and this becomes a number. This guy actually is not a number. You cannot give it a particular value. All you can do with a differential is express it in terms of other differentials. So, in fact, this dx, dy, and dz, well, they're mostly symbols out there. But, if you want to think about them, they are the differentials of x, y, z, and f. So, in fact, you can think of these differentials as placeholders where you will put other things. So, of course, they represent this idea of changes in x, y, z, and f. But, so, one way that one could explain it, I don't really like that, is to say they represent infinitesimal changes. Another way to say it, and I think that's probably closer to the truth, is that these things are somehow placeholders to put values and get a tangent approximation. So, for example, if I do replace these symbols by delta x, delta y, and delta z numbers, then I will actually get a numerical quantity. And, that will be an approximation formula for delta f. It will be the linear approximation, or the tangent plane approximation. So, what we can do, well, so let me start first with maybe something even before that. So, the first thing that it does is it can encode how changes in x, y, z affect the value of f. I would say that's the most general answer to what is this formula, what are these differentials. It's a relation between x, y, z, and f. And, in particular, so this is a placeholder for delta x, delta y, delta z to get an approximation formula, which is delta f is approximately equal to f sub x delta x plus f y delta y plus f y delta z. Sorry, it's getting cramped, but I'm sure you know what's going on here. So, and observe how this one is actually equal while that one is approximately equal. So, they're really not the same. Another thing that the notation suggests we can do, and I claim we can do, is divide everything by some variable that everybody depends on. So, say, for example, that x, y, and z actually depend on some parameter t. Then, they will vary at a certain rate, dx dt, dy dt, dz dt. And, what the differential will tell us, then, is the rate of change of f as a function of t when you plug in these values of x, y, z. And, you will get df dt by dividing everything by dt in here. OK, so the third thing we can do is divide everything by, say, something like dt to get an infinitesimal rate of change. For instance, let me just say a rate of change. OK, so df dt equals f sub x dx dt plus f sub y dy dt plus f sub z dz dt. And, that corresponds to the situation where x is a function of t, y is a function of t, and z is a function of t. Then, that means you can plug in these values into f to get, well, the value of f will depend on t. And then, you can find the rate of change with t of a value of f. OK, so these are the basic rules. And, this is known as the chain rule. It's one instance of the chain rule, which tells you when you have a function that depends on something, and that something in turn depends on something else, how to find the rate of change of a function on the new variable in terms of the derivatives of a function, and also the dependence between the various variables. OK, any questions so far? No? OK, so a word of warning in particular about what I said up here. It's kind of unfortunate, but the textbook actually has a serious mistake on that. I mean, they do have a couple of formulas where they mix a d with a delta. And, I would like you not to do that, please. OK, so, I mean, there's d's and there's deltas, and basically, they don't live in the same world. They don't see each other. The textbook is lying to you. OK, so let's see. I mean, here I made, you know, so the first and the second claims, I don't really need to justify because the first one is just stating some general principle, but I'm not making a precise mathematical claim. The second one, well, I mean, we know that approximation formula already, so I don't need to justify it for you. But, on the other hand, this formula here, I mean, you probably have a right to expect some reason for why this works, why is this valid. After all, I first told you we have these new mysterious objects, and then I'm telling you, well, we can do that. But, you know, I kind of pulled it out of my hat. I mean, I don't have a hat, but I'm going to show you. OK, so why is this valid? How can I get to this? So, here's a first attempt at how to get there. OK, so let's see. Well, we said df is f sub x dx plus f sub y dy plus f sub z dz. But we know, well, if x is a function of t, then dx is x prime of t dt, or dx dt dt, as you prefer. OK, dy is y prime of t dt. dz is z prime of t dt. So, if we plug these into each other, we'll get that df is f sub x times x prime of t dt plus f sub y y prime of t dt plus f sub z z prime of t dt. And now, well, if I have a relation between df and dt, see, I got df equals something times dt. That means the rate of change of f with respect to t should be that coefficient. So, if I divide by dt, then I get the formula that's, well, I get the chain rule. Now, well, that kind of works, but that shouldn't be completely satisfactory. Let's say that you're a true skeptic and you don't believe in differentials yet. Then, it's maybe not very good that I actually used more of these differential notations in deriving the answer. So, that's actually not how it's proved. The way in which you prove the chain rule is not this way because we shouldn't have too much trust in differentials just yet. I mean, at the end of today's lecture, yes, probably we should believe in them. But, so far, we should still be a little bit reluctant to believe this kind of strange object telling us that it's not true. So, here's a better way to think about it. So, one thing that we have trust in so far are approximation formulas. At least, we should have trust in them. So, we should believe that if we change x a little bit, if we change y a little bit, then we are actually going to get a change in f that's approximately given by these guys. So now, well, this is true for any changes in x, y, z. But, in particular, let's look at the changes that we get if we just take these formulas as functions of time and change time a little bit by delta t. OK, so we'll actually use the changes in x, y, z in a small time delta t. And, let's divide everybody by delta t. OK, here I'm just dividing numbers. So, I'm not actually playing any tricks on you. I mean, we don't really know what it means to divide differentials, but dividing numbers is something we know. And now, if I take delta t very small, then this guy tends to the differential, sorry, tends to the derivative, df over dt. Remember, the definition of df over dt is the limit of this ratio when the time interval delta t tends to zero. So, that means if I choose smaller and smaller values of delta t, then these ratios of numbers will actually tend to some value, and that value is the derivative. So, similarly, here, delta x over delta t, when delta t is really small, will tend to the derivative, dx over dt. And, similarly for the others. So, OK, so that means, in particular, so if we take the limit as delta t tends to zero, we get df over dt on one side. On the other side, we get f sub x dx over dt plus f sub y dy over dt plus f sub z dz over dt. And, the approximation becomes better and better. Remember, when we write delta t equal, that means that it's not quite the same. But, if we take smaller and smaller variations, then actually we'll end up with values that are getting closer and closer. So, when we take the limit as delta t tends to zero, eventually we get an equality. I mean, mathematicians have more complicated words to justify this statement, but I will spare them for now. And, you will see about them in the next classes if you go on in that direction. OK, any questions so far? No? OK, so let's check maybe things in an example. Let's say that we really don't have any faith in these things. Let's try to do it. So, let's say I give you a function that's x squared y plus z. And, let's say that maybe x will be t, y will be e to the t, y will be e to the t. OK, so what does the chain rule say? Well, the chain rule tells us that dw over dt is, so we start with partial w over partial x. Well, what is that? That's 2xy times, so maybe I should point out this is w sub x, times dx over dt plus, well, w sub y is x squared times dy over dt plus w sub z, that's going to be just one, dz over dt. OK, and so now let's plug in the actual values of these things. So, x is t, y is e to the t, so that will be 2t e to the t, dx over dt is one, plus x squared is t squared, dy over dt is e to the t, plus dz over dt is cosine t. OK, so at the end of calculation, we get 2t e to the t plus t squared e to the t plus cosine t. That's what the chain rule tells us. How else could we find that? Well, we could just plug in values of x, y and z, express w as a function of t and take its derivative. OK, let's do that just for now. It should be exactly the same answer. And, in fact, in this case, the two calculations are roughly equal in complication. But, say that your function of x, y, z was much more complicated than that, or maybe you actually didn't know a formula for it. You only knew its partial derivatives. Then you would need to use the chain rule. So, sometimes plugging in values is easier, but not always. So, let's just check quickly. So, the other method would be to substitute. So, w as a function of t, well, so remember w was, was it x squared y plus z? So, as a function of t, well, x was t. So, you get t squared, y is e to the t plus z was sine t. So, dw dt, we know how to take a derivative using single variable calculus. Well, we should know. If we don't know, then we should take a look at 18.01 again. So, by the product rule, that will be derivative of t squared is 2t times e to the t plus t squared times the derivative of e to the t is e to the t plus cosine t. OK, and that's the same answer as over there. So, I ended up writing up, you know, maybe I wrote slightly more here, but actually the amount of calculations really was pretty much the same. OK, any questions about that? Yes? What kind of object is w? So, well, I was intending, you know, you can think of w as just another variable that's given as a function of x, y, and z, for example. So, you have a function of x, y, z defined by this formula. And, I call it w. I call its value w so that then I can substitute t instead of x, y, z. So, w is, well, let's think of w as a function of three variables. OK, and then when I plug in the dependence of these three variables on t, then it becomes just a function of t. I mean, really, my w here is pretty much what I called f before. There's no major difference between the two. OK, any other questions? No? OK. So, let's see. Here's an application of what we've seen. So, let's say that you want to understand, actually, all these rules about taking derivatives in single variable calculus. So, what I showed you at the beginning, and then erased, basically justifies how to take the derivative of a reciprocal function. And, for that, you didn't need multivariable calculus. But, let's try to justify the product rule, for example, for the derivative. So, an application of this, actually, is to justify the product and quotient rules. So, let's think, for example, of a function of two variables, u and v, that's just the product, uv. And, let's say that u and v are actually functions of one variable, t. Then, well, df dt, so, let's say, d of uv over dt is given by the chain rule applied to f. This is df dt. So, df dt should be f sub u du dt plus f sub v times dv dt. But now, what is the partial of f with respect to u? It's v. So, that's v du dt. And, partial of f with respect to v is going to be just u. So, you get back the usual product rule. OK, that's a slightly complicated way of deriving it. But, that's a valid way of understanding how to take the derivative of a product by thinking of the product first as a function of two variables, which are u and v, and then saying, oh, but u and v were actually functions of a variable, t. And then, you do the differentiation in two stages using the chain rule. OK, similarly, you can do the quotient rule. Actually, let's do it just for practice. So, if I give you the function g equals u over v, so, right now, I'm thinking of it as a function of two variables, u and v. But, u and v themselves are actually going to be functions of t. Then, well, dg dt is going to be partial g, partial u. How much is that? How much is partial g, partial u? One over v times du over dt plus, well, next we need to have partial g over partial v. Well, what's the derivative of this with respect to v? Well, here we need to know how to differentiate the inverse. It's minus u over v squared times u prime. OK, and that's actually the usual quotient rule, just written in a slightly different way. I mean, just in case you really want to see it, if you clear the denominators to put the v squared, then you will see basically v, v prime times v minus v prime times u. OK. OK, now let's go to something even more crazy. I claim we can do chain rules with more variables. So, let's say that I have a quantity. Let's call it w today for now. So, let's say I have a quantity w as a function of, say, two variables, x and y. And so, in the previous setup, x and y depended on some parameter t. But actually, let's now look at the case where x and y themselves are functions of several variables, let's say of two more variables. Let's call them u and v. OK, so I'm going to stay with these abstract letters. But, you know, if it bothers you, if you are completely unmotivated, think about it maybe in terms of something you might know, say, polar coordinates. Let's say that I have a function that's defined in terms of the polar coordinate variables, r and theta. And then, I know actually I want to switch to usual coordinates, x and y. Or, the other way around, I have a function of x and y, and I want to express it in terms of the polar coordinates, r and theta. Then, I would like to know maybe how the derivatives, with respect to the various sets of variables, are related to each other. OK, so one way I could do it is, of course, to say, well, now, if I plug the formula for x and the formula for y into the formula for f, then w becomes a function of u and v. And, I can try to take its partial derivatives. If I have explicit formulas, well, that could work. But, maybe the formulas are complicated. Typically, if I switch between rectangular and polar coordinates, there might be inverse trig. There might be maybe arc tangent to express the polar angle in terms of x and y. And then, I don't really want to actually substitute arc tangents everywhere. I'd rather deal with the derivatives. So, how do I do that? So, the question is, what are partial w over partial u and partial w over partial v in terms of, so let's see, what do we need to know in order to understand that? Well, probably we should know how w depends on x and y. If we don't know that, then we are probably toast. OK, so partial w over partial x, partial w over partial y should be required. What else should we know? Well, it would probably help to know how x and y depend on u and v. If we don't know that, we don't really know how to do it either. So, we need also, well, u sub x, sorry, x sub u, x sub v, y sub u, y sub v. So, we have a lot of partials in there. Well, so let's see how we can do that. So, let's start by writing dw. So, we know that dw is partial f, well, I don't know why I have two names, w and f. I mean, w and f are really the same thing here. But, let's say f sub x dx plus f sub y dy. OK, so far, that's our new friend, the differential. Now, what do we want to do with it? Well, we'd like to get rid of dx and dy because we'd like to express things in terms of, you know, the question we are asking ourselves is, let's say that I change u a little bit. How does w change? So, of course, what will happen is if I change u a little bit, then x and y will change. How do they change? Well, that's given to me by the differential, right? So, dx is going to be, well, I can use the differential. Again, you know, dx, well, x is a function of u and v. That will be x sub u times du plus x sub v times dv. OK, that's, again, taking the differential of a function of two variables. Does that make sense? Then, we have the other guy, f sub y times, what is dy? Well, similarly, dy is y sub u du plus y sub v dv. And now, we have a relation between dw and du and dv. We are expressing how w reacts and how the change is in u and v, which was our goal. Now, let's actually collect terms so that we see it a bit better. So, that's going to be f sub x times x sub u plus f sub y times y sub u du plus f sub x x sub y y sub v dv. And so, now we have dw equals something du plus something dv. Well, that coefficient here has to be partial f, partial u. What else could it be? That's the rate of change of w with respect to u if I forget what happens when I change v. That's the definition of a partial. Similarly, this one has to be partial f over partial v. That's because it's the rate of change with respect to v if I keep u constant so that these guys are completely ignored. So, now you see how the total differential accounts for somehow all the partial derivatives that come as a result of the individual variables in these expressions. OK, so let me maybe rewrite these formulas in a more visible way, and then re-explain them to you. OK, so here's the chain rule for this situation with two intermediate variables and two variables that you express these two variables with. So, in our setting, we get partial f over partial u equals partial f over partial x partial x over partial u plus partial f over partial y times partial y over partial u. And, the other one, same thing with v instead of u, partial f over partial x times partial x over partial v plus partial f over partial y over partial y over partial v. So, I have to explain various things about these formulas because they look complicated. And, actually, they're not that complicated. So, a couple of things to know. First thing, so how do we remember a formula like that? Well, that's easy. We want to know how f depends on u. Well, what does f depend on, actually? It depends on x and y. So, we'll put partial f over partial x and partial f over partial y. Now, x and y, why are they here? Well, they're here because they actually depend on u as well. How does x depend on u? Well, the answer is partial x partial u. How does y depend on u? The answer is partial y partial u. So, see, the structure of these formulas is simple. To find the partial of f with respect to some new variable, you use the partials with respect to the variables that f was initially defined in terms of x and y. And, you multiply them by the partials of x and y in terms of the new variable that you want to look at, v here. And, you sum these things together. OK, so that's the structure of a formula. Why does it work? Well, let me explain it to you in a slightly different language. So, this asks us, how does f change if I change u a little bit? OK, well, why would f change if u changes a little bit? Well, it would change because f actually depends on x and y, and x and y depend on u. If I change u, how quickly does x change? Well, the answer is partial x partial u. And now, if I change x at this rate, how does that cause f to change? Well, the answer is partial f partial x times this guy. Well, at the same time, y is also changing. How fast is y changing if I change u? Well, at the rate of partial y partial u. But now, if I change y, how does f change? Well, the rate of change is partial f partial y. So, the product is the effect of how u changes changing y, and therefore changing f. Now, what happens in real life if I change u a little bit? Well, both x and y change at the same time. So, how does f change? Well, it's the sum of the two effects. OK, does that make sense? Good. Of course, if f depends on more variables, then you just have more terms in here. OK, here's another thing that may be a little bit confusing. So, what is tempting? Well, what's tempting here would be to simplify these formulas by removing these partial x's. So, let's simplify by partial x. Let's simplify by partial y. We get partial f partial u equals partial f partial u plus partial f partial u. Something is not working here. OK, so why doesn't it work? Well, the answer is precisely because these are partial derivatives. These are not total derivatives. And so, you cannot simplify them in that way. I mean, and that's actually the reason why we use this curly d rather than a straight d. It's to remind us, beware, there's these simplifications that we can do with straight d's that are not legal here. So, somehow, when you have a partial derivative, you get the urge of simplifying things. OK, so no simplifications in here. That's the simplest formula you can get. OK, any questions at this point? No? Yes? So, when would you use this, and what does it describe? Well, it's basically when you have a function given in terms of a certain set of variables because maybe there's a simple expression in terms of those variables. But, ultimately, what you care about is not those variables, x and y, but another set of variables, here, u and v. So, x and y are giving you a nice formula for f, but actually the relevant variables for your problem are u and v. And, you know how x and y are related to u and v. So, of course, what you could do is you could plug the formulas the way that we did substituting. But, maybe that will give you very complicated expressions. And, maybe it's actually easier to just work with the derivatives. So, the important claim here is basically we don't need to know the actual formulas. All we need to know is the rates of changes. If we know all these rates of change, then we know how to take these derivatives without actually having to plug in values. Yes? Yes, you could certainly do the same things in terms of t. If x and y were functions of t instead of being functions of u and v, then it would be the same thing. And, you would have the same formulas that I had, well, over there. I still have it. So, why does that one have straight d's? Well, the answer is I could put curly d's if I wanted. But, I end up with a function of a single variable. So, if you have a single variable, then the partial with respect to that variable is the same thing as the usual derivative. So, we don't actually need to worry about curly in that case. OK, but that one is indeed, it's a special case of this one where instead of x and y depending on two variables, u and v, they depend on a single variable, t. Now, of course, whether you call your variables any name you want, it doesn't matter. But, so this is just a slight generalization of that. Well, not quite because here I also had a z. See, I'm trying to just confuse you by giving you functions that depend on various numbers of variables. If you have a function of 30 variables, things work the same way, just they are longer, and you're going to run out of vectors in the alphabet before the end. OK, any other questions? No? Yes, what? Oh, yes? So, if u and v themselves depend on another variable, then you would continue with your chain rules. Maybe you would know how to express partial x, partial u in terms using that chain rule. Sorry, if u and v depended on yet another variable, then you could get the derivative with respect to that using first the chain rule to pass from u, v to that new variable. And then you would plug in these formulas for the partials of f with respect to u and v. So, in fact, if you have several substitutions to do, you can always arrange to use one chain rule at a time. You just have to do them in sequence. So, that's why we don't actually learn that. But, you can just do it by repeating the process. I mean, probably at that stage, the easiest to not get confused, actually, is to manipulate differentials because that's probably easier. Yes? Ah, so curly f does not exist. That's easy. OK, curly f makes no sense. By itself, it doesn't exist alone. What exists is only curly d, f over curly d, some variable. And then, that accounts only for the rate of change with respect to that variable, leaving the others fixed. While, straight d, f is somehow a total variation of f. And so, it accounts for all of the partial derivatives and their combined effects. OK, any more questions? No? OK, so let me just finish very quickly by telling you again, you know, one example where concretely you might want to do this is you have a function that you want to switch between rectangular and polar coordinates. So, just to make things a little bit more concrete, so if you have polar coordinates, that means that, you know, in the plane, instead of using coordinates x and y, you will use coordinates r, distance to the origin, and theta, the angle from the x-axis. So, the change of variables for that is x equals r cosine theta. And so, that means, well, if you have a function, f, that depends on x and y, then, in fact, you can plug these and get a function of r and theta. And then, you can ask yourself, well, what is partial f over partial r? Well, that's going to be, well, you want to take partial f over partial x, partial x over partial r plus partial f over partial y, partial y over partial r. And so, that will end up being, actually, f sub x times cosine theta plus f sub y times sine theta. And, you can do the same thing to find partial f over partial theta. And so, you can express derivatives either in terms of x, y or in terms of r and theta with simple relations between them. And, the, oh, yes, and one last thing I should say, so, on Thursday, we'll learn about more tricks we can play with variations of functions. And, one that's important, because you need to know it, actually, to do the p set, is the gradient vector. So, the gradient vector, it's simply a vector. So, you use this downward pointing triangle as the notation for the gradient. It's simply a vector whose components are the partial derivatives of a function. OK, so, it's, I mean, in a way, you can think of a differential as a way to package partial derivatives together into some weird object. Well, the gradient is also a way to package partials together. So, we'll see on Thursday what it's good for, but some of the problems on Thursday, we'll get to.