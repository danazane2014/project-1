 So let me use the mic to introduce Alex Townsend, who taught here at MIT, taught linear algebra 1806 very successfully. And now he's at Cornell on the faculty, still teaching very successfully. And he was invited here yesterday for a big event over in engineering. And he agreed to give a talk about a section of the book, section 4.3, which if you look at it, you'll see is all about his work. And now you get to hear from the creator himself. OK. Should we have that on? Yeah. OK, thanks. Thank you, Gil. Thank you for inviting me here. I hope you're enjoying the course. Today, I want to tell you a little about why there are so many matrices that are low rank in the world. So as computational mathematicians, Gil and myself, we come across low rank matrices all the time. And we started wondering as a community, why? What is it about the matrices, the problems that we're looking at? What makes low rank matrices appear? And today, I want to give you that story, or at least an overview of that story. So for this class, x is going to be an n by n real matrix, so nice and square. And you already know, or are very comfortable with, the singular values of a matrix. So the singular values of a matrix, as you know, are a sequence of numbers that are monotonically non-increasing that tell us all kinds of things about the matrix x. For example, the number of non-zero singular values tell us the rank of the matrix x. And they also, you probably know, tell us how well a matrix x can be approximated by a low rank matrix. So let me just write two facts down that you already are familiar with. So here's a fact that if I look at the number of non-zero singular values in x, so I'm imagining there's going to be k non-zero singular values, then we can say a few things about x. For example, x, the rank of x, as we know, is k, the number of non-zero singular values. But we also know from the SVD that we can decompose x into a sum of rank 1 matrices, in fact, a sum of k of them. So because x is rank k, we can write down a low rank representation for x. And it involves k terms like this. Each one of these vectors here is a column vector. So if I draw this pictorially, this guy looks like this. And we have k of them. So because x is rank k, we can write x as a sum of k rank 1 matrices. And we also have an initial fact that we already know that the dimension of the column space of x is equal to k and the same with the row space. So the col space of x equals the row space of x, the dimension, and that they all equal k. So there are three facts we can determine from looking at this sequence of singular values of a matrix x. Of course, the singular value sequence is unique. x defines its own singular values. What we're interested in here is what makes x, what are the properties of x that make sure that the singular values have a lot of zeros in that sequence? Can we try to understand what kind of x makes that happen? And we really like matrices that have a lot of zeros here for the following reason. We say x is low rank if the following holds, right? Because if we wanted to send x to our friend, we're imagining x is a picture where each entry is a pixel of that image. If that matrix, that image, was low rank, we could send the picture to our friend in two ways. We could send, one, every single entry of x. And for us to do that, we would have to send n squared pieces of information because we'd have to send every entry. But if x is sufficiently low rank, we could also send them, our friend, the vectors, u1, v1, uk, up to vk. And how many, how much pieces of data would we have to send our friend to get x to them if we sent in the low rank form? Well, here there's 2n here, 2n here numbers. There's k of them. So we'd have to send 2kn numbers. And we strictly say a matrix is low rank if it's more efficient to send x to our friend in low rank form than in full rank form. So this, of course, by a little calculation, just shows us that provided the rank is less than half the size of the matrix, we are calling the matrix low rank. Now, often in practice, we demand more. We demand that k is much smaller than this number so that it's far more efficient to send our friend the matrix x in low rank form than in full rank form. So the colloquial use of the word low rank is kind of this situation. But this is the strict definition of it. OK, so what do low rank matrices look like? And to do that, I have some pictures for you. I have some flags, the world flags. So these are all matrices x. These examples, because they're flags, happen to not be square. I hope you can all see this. But the top row here are all matrices that are extremely low rank. For example, the Austria flag, if you want to send that to your friend, that matrix is of rank 1. So all you have to do is send your friend two vectors. You have to tell your friend the column space and the row space. And the dimensions are one of both. For the English flag, you need to send them two column vectors and two row vectors, u1, v1, u2, and v2. And as we go down this row, they get slowly fuller and fuller rank. So the Japanese flag, for example, is low rank but not that small. The Scottish flag is essentially full rank. So it's very inefficient to send your friend the Scottish flag in low rank form. You're better off sending almost every single entry. So what do low rank matrices look like? Well, if the matrix is extremely low rank, like rank 1, then when you look at that matrix, like here, like the flag, it's highly aligned with the coordinates, with the rows and columns. So if it's rank 1, the matrix is highly aligned, like the Austria flag. And of course, as we add in more and more rank here, the situation gets a bit blurry. For example, once we get into the medium rank situation, which is a circle, it's very hard to see that the circle is actually, in fact, low rank. But what I wanted to do was try to understand why the Scottish flag, or diagonal patterns, are particularly a bad example for low rank. So I'm going to take the triangular flag to examine that more carefully. So the triangular flag, it looks like I take a square matrix and are coloring the bottom half. So this matrix is the matrix of 1's below the diagonal. And I'm interested in this matrix, and in particular, its singular values, to try to understand why diagonal patterns are not particularly useful for low rank compression. And this matrix of all 1's has a really nice property that if I take its inverse, it looks a lot like getting close to Gill's favorite matrix. So if I take the inverse of this matrix, it has an inverse because it's got 1's on the diagonal. Then its inverse is the following matrix, which people familiar with finite difference schemes will notice the familiarity between that and the first order finite difference approximation. In particular, if I go a bit further and times two of these together and do this, then this is essentially Gill's favorite matrix, except one entry happens to be different. Ends up being this matrix, which is very close to the second order central finite difference matrix. And people have very well studied that matrix and know its eigenvalues, its singular values. They know everything about that matrix. And you'll remember that if we know the eigenvalues of a matrix like X transpose X, we know the singular values of X. So this allows us to show by the fact that we know that that the singular values of this matrix are not very amenable to low rank. They're all non-zero. And they don't even decay. So I'm getting this from I rang up Gill, and Gill tells me these numbers. That allows us to work out exactly what the singular values of this matrix are from the connection to finite differences. And so we can understand why this is not good by looking at the singular values. So the first singular value of X from this expression is going to be approximately 2n over pi. And from this expression again, the last guy, the last singular value of X is going to be approximately 1.5. So these singular values are all large. They're not getting close to 0. If I plotted these singular values on a graph, so here's the first singular value, the second, and the nth, then what would the graph look like? Well, I'll plot these numbers. I'll divide by this guy so that they all are bounded between 1 and 0 because of the normalization, because I divided by sigma 1 of x. And so we can plot them, and they will look like this kind of thing. This number happens to be here where they come to be pi over 4n, which is me dividing this number by this number approximately. So triangular patterns are extremely bad for low rank. We need things, or we at least intuitively think that we need things aligned with the rows and columns. But the circle case happens to also be low rank. And so what happened to the Japanese flag? So why is the Japanese flag convenient for low rank? Well, it's the fact that it's a circle, and there's lots of symmetry in a circle. So if I try to look at the rank of a circle, the Japanese flag, then I can bound this rank by decomposing the Japanese flag into two things. So this is going to be less than or equal to the rank of a sum of two matrices. And I'll do it so that the decomposition works out. I have the circle. I'm going to cut out a rank 1 piece that lives in the middle of this circle. And I'm going to cut out a square from the interior of that circle. And I can figure out, of course, the rank is just bounded by the sum of those two ranks. This guy is bounded by rank 1 because it's highly aligned with the grid. So this guy is bounded by rank 1. And this guy is bounded by rank 2. So this guy is bounded by rank 1. So this thing here plus 1. And now I have to try to understand the rank of this piece. Now, this piece has lots of symmetry. For example, we know that the rank of that matrix is the dimension of the column space and the dimension of the row space. So when we look at this matrix, because of symmetry, if I divide this matrix in half along the columns, all the columns on the left appear on the right. So the columns, for example, the rank of this matrix is the same as the rank of that matrix because I didn't change the column space. Now I go again and divide along the rows. And now the row dimension of this matrix is the same as the top half because as I wipe out those, I didn't change the dimension of the row space because the rows are the same top, bottom. And so this becomes the rank of that tiny little matrix there. And because it's small, it won't have too large a rank. So this is definitely less than if I divide that up. Little guy here looks like that plus the other guy that looks like that plus 1. And so of course, the row space of this matrix cannot be very high because this is a very thin matrix. There's lots of zeros in that matrix, only a few ones. And so you can go along and do a bit of trig to try to figure out how many rows are non-zero in this matrix. And a bit of trig tells you, well, it depends on the radius of this original circle. If I make the original radius r of this Japanese flag, then the bound that you end up getting will be for this matrix r 1 minus square root 2 over 2. For this guy, a bit of trig. Better make sure that's an integer. And then again, here it's the same, but for the column space. So this is me just doing trig. And that's a bound on the rank. It happens to be extremely good. And if you work out what that rank is and try to look back, you'll find it's extremely efficient to send the Japanese flag to your friend in low rank form because it's not full rank because these numbers are so small. So this comes out to be approximately a half r plus 1. So much smaller than what you would expect. Because remember, a circle is almost the anti-version of aligned with the grid. But yet, it's still low rank. OK. Now, most matrices that we come up with in computational math are not exactly of finite rank. They are of numerical rank. And so I'll just define that. So the numerical rank of a matrix is very similar to the rank, except we allow ourselves a little bit of wiggle room when we define it. And so that amount of wiggle room will be a parameter called toll, called epsilon. That's a tolerance. I'm thinking of epsilon as a tolerance. That's the amount of wiggle room I'm going to give myself. And we say that the numerical rank, I'll put an epsilon there to denote numerical rank, is k. If k is the first singular value or the last singular value above epsilon in the following sense, I'm copying the definition above, but with epsilons instead of zeros. If this singular value is less than epsilon, relatively, and the k-th one was not below. So k plus 1 is the first singular value below epsilon in this relative sense. So of course, the rank of 0x, if that was defined, is the same as the rank of x. So this is just allowing ourselves some wiggle room. But this is actually what we're interested more in in practice. I don't want to necessarily send my friend the flag to exact precision. I would actually be happy to send my friend the flag up to 16 digits of precision, for example. They're not going to tell the difference between those two flags. And if I can get away with compressing the matrix a lot more once I have a little bit of wiggle room, that would be a good thing. So we know from the Urquhart and Young that the singular values tell us how well we can approximate x by a low-rank matrix. In particular, we know that the k plus 1 singular value of x tells us how well x can be approximated by a rank k matrix. For example, when the rank was exactly k, the sigma k plus 1 was 0. And then this came out to be 0, and we found that x was exactly a rank k matrix. Here, because we have the wiggle room, the epsilon, we get an approximation, not an exact. So this is telling us how well we can approximate x by a rank k matrix. That's what the singular values are telling us. And so this allows us to try our best to compress matrices, but use low-rank approximation rather than doing things exactly. And of course, on a computer, when we're using floating-point arithmetic, or on a computer because we always round numbers to the nearest 16-digit number, if epsilon was 16 digits, your computer wouldn't be able to tell the difference between x or x, the rank k approximation, if this number was satisfied this expression. Your computer would be able to tell the difference in this expression. Your computer would think of x and xk as the same matrix, because it would inevitably round both to epsilon, within epsilon. So what kind of matrices are numerically of low rank? Of course, all low-rank matrices are numerically of low rank, because the wiggle room can only help you. But it's far more than that. There are many full-rank matrices, matrices that don't have any singular values that are 0, but the singular values decay rapidly to 0. That are full-rank matrices with low numerical rank, because of the wiggle room. So for example, here is the classic matrix that fits this regime. If I give you this, this is called the Hilbert matrix. This is a matrix that happens to have extremely low numerical rank, but is actually full-rank. Which means that I can approximate h to buy a rank k matrix, where k is quite small, very well, provided you give me some wiggle room. But it's not a low-rank matrix, in the sense that if epsilon was 0 here, you didn't allow me the wiggle room, all the singular values of this matrix are positive. So it's of low numerical rank, but it's not a low-rank matrix. The other classical case, classic example, which motivated a lot of the research in this area was the van der Maan matrix. So here is the van der Maan matrix, an n by n version of it. Think of the xi's as real. This is van der Maan. This is the matrix that comes up when you try to do polynomial interpolation at real points. This is an extremely bad matrix to deal with, because it's numerically low-rank. And often, you actually want to solve a linear system with this matrix. And numerical low-rank implies that it's extremely hard to invert. So this is low-rank. Numerical low-rank is not always good for you. Often, we want the inverse, which exists, but it's difficult because v has low numerical rank. So people have been trying to understand why these matrices are numerically of low rank for a number of years. And the classic reason why there are so many low-rank matrices is because the world is smooth, as people say. They say, the world is smooth. That's why matrices are of numerical low rank. And to illustrate that point, I will do an example. This is classically understood by a man called Reed in 1983. And this is what his reason was. I have a picture of John Reed. He's not very famous, so I try to make sure his picture gets around. He's playing the piano. It's one of the only pictures I could find of him. So what is in this reason? Why do people say this? Well, here's an example that illustrates it. If I take a polynomial in two variables, and I, for example, this is a polynomial of two variables, and my x matrix comes from sampling that polynomial integers, for example, this matrix, then that matrix happens to be of low rank, mathematically of low rank, with epsilon equals 0. Why is that? Well, if I write down x, as in terms of matrices, you can easily see it. So this is made up of a matrix of all 1's plus a matrix of j's. So that's 1, 2, up to n, 1, 2, up to n, because every entry of that matrix just depends on the row index. And then this guy depends on both j and k. So this is a multiplication table. So this is n, 2, 4, up to 2n, n, 2n, n squared. OK. Clearly, the matrix of all 1's is a rank 1 matrix. The same with this guy. The column space is just of dimension 1. And this guy happens to, the last guy, also happens to be of rank 1, because I can write this matrix in rank 1 form, which is a column vector times a row vector. So this matrix X is of rank 3. I guess at most, rank 3 is what I've actually shown. OK. Now, of course, this isn't got to numerical low rank yet. So let's get ourselves there. So Reed knew this. And he said to himself, OK, well, if I can approximate, if X is actually coming from sampling a function, and I approximate that function by a polynomial, then I'm going to get myself a low rank approximation and get a bound on the numerical rank. So in general, if I give you a polynomial of two variables which can be written down, it's degree m in both X and Y. Let's say, keep these indexes away from the matrix index. I give you this such polynomial. And I go away and I sample it and make a matrix X. Then X, by looking at each term individually like I did there, will have low rank mathematically with epsilon equals 0. This will have at most m squared rank. And if m is 3 or 4 or 10, it possibly could be low because this X could be a large matrix. So what Reed did for the Hilbert matrix was said, OK, well, look at that guy. That guy looks like it's sampling a function. It looks like it's sampling the function 1 over X plus Y minus 1. So he said to himself, well, that X, if I look at the Hilbert matrix, then that is sampling a function. It happens to not be a polynomial. It happens to be this function. But that's OK because sampling polynomials, integers, gives me low rank exactly. Maybe sampling smooth functions, functions like this, can be well approximated by polynomials and therefore have low numerical rank. And that's what he did in this case. So he tried to find a P, a polynomial approximation to F. In particular, he looked at exactly this kind of approximation. So he has some numbers here so that things get dissolved later. He tried to find a P that did this kind of approximation. This approximates F. And then he would develop a low rank approximation to X by sampling P. So he would say, OK, well, if I let Y be a sampling of P, then from the fact that F is a good approximation to P, Y is a good approximation to X. And so this has finite rank. He wrote down that this must hold. And the epsilon comes out here because these factors were chosen just right. The divide by n was chosen so that the epsilon came out just there. So for many years, that was kind of the canonical reason that people would give that, well, if the matrix X is sampled from a smooth function, then we can approximate that function by a polynomial and get polynomial rank approximations. And therefore, the matrix X will be of low numerical rank. There's an issue with this reasoning, especially for the Hilbert matrix, that it doesn't actually work that well. So for example, if I take the 1,000 by 1,000 Hilbert matrix and I look at its rank, well, I've already told you this is full rank. You'll get 1,000. All the singular values are positive. If I look at the numerical rank of this 1,000 by 1,000 Hilbert matrix and I compute it, I compute the SVD, and I look at how many are above epsilon, where epsilon is 10 to the minus 15. So that means I can approximate the 1,000 by 1,000 Hilbert matrix by a rank 28 matrix and only give up a 15. There will be exact to 15 digits, which is a huge amount. So this is what we get from in practice. But Reed's argument here shows that the rank of this matrix, the numerical rank, is at most. So it doesn't do a very good job on the Hilbert matrix for bounding the rank. So Reed comes along, takes this function. He tries to find a polynomial that does this, where epsilon is 10 to the minus 15. He finds that he needs the number of terms that he needs in this expression here is around 719. And therefore, that's the rank that he gets, the bound on the numerical rank. The trouble is that 719 tells us that this is not of low numerical rank. But we know it is. So it's an unsatisfactory reason. So there's been several people trying to come up with more appropriate reasons that explain the 28 here. And so one reason that I've started to use is that another slightly different way of looking at things, which is to say the world is Sylvester. Now, Sylvester, what does that mean? What does the word Sylvester mean in this case? It means that the matrices satisfy a certain type of equation called the Sylvester equation. And so the reason is really many of these matrices satisfy a Sylvester equation. And that takes the form for some a, b, and c. So x is your matrix of interest. You want to show x is of numerical low rank. And the task at hand is to find an a, b, and c so that x satisfies that equation. For example, all the matrices, the two matrices I've had on the board, satisfy a Sylvester equation, a Sylvester matrix equation. There is an a, a b, and a c for which they do this. For example, remember the Hilbert matrix, which we have there still, but I'll write it down again, satisfies, has these entries. So all we need to do is to try to figure out an a, and b, and then a c so that we can make it fit a Sylvester equation. There's many different ways of doing this. The one that I like is the following. Well, if I put a half here and 3 over 2 here, all the way down to n minus 1 half times this matrix. So this is timesing the top of this matrix by 1 half, and then 3 over 2, and then 5 over 2. So we're basically timesing that matrix, each entry of this matrix, by j minus 1 half. And then I do something on the right of here, which I'm allowed to do because I've got the b freedom. And I choose this to be the same up to a minus sign. Then when you think about this, what is it doing? It's timesing the jk entry, this is, by j minus 1 half. That's what this is doing. And what's this doing? It's timesing the jk entry by k minus 1 half. So this is, in total, timesing the jk entry by j plus k minus 1 half minus 1 half, which is minus 1. So this is timesing the jk entry by j plus k minus 1. So it knocks out the denominator. And what we get from this equation is a bunch of 1's. So in this case, A and B are diagonal, and C is the matrix of all 1's. We can also do this for van der Mond. So van der Mond, you'll remember, looks like this. And then over here, we have this guy, the matrix that appears with polynomial interpolation. So if I think about this, I can also come up with an A, B, and C. And for example, here's one that works. I can stick the x's on the diagonal. So this is, if you imagine what that matrix on the left is doing, it's timesing each column by the vector x. So the first column of this matrix becomes x, the vector x. The second becomes the vector x squared, where squared is done entry-wise. And then the third entry is now x cubed. And when we get to the last, it's x to the n. So that's like multiplied each column by the vector x. So if I want to try to come up with a matrix, so what's left is of low rank, is of this form, what I can do is shift the columns. So I've noticed that this product here, this diagonal matrix, has made the first column x. So if I want to kill off that column, I can take the second column and permute it to the first column. I can take the third column and permute it to the second, the last column and permute it to the penultimate column here. And that will actually kill off a lot of what I've created in this matrix right here. So let me write that down. This is a circumshift matrix. This does that permutation. This does that permutation. I've put a minus 1 there. I could have put any number there. Doesn't make any difference. But this is the one that works out extremely nicely. Now this zeros out lots of things because of the way I've done the multiplication by x and the circumshift of the columns. And so the first column is 0 because this first column is x. This first column is x. So I got x minus x. This column was x squared minus x squared. So I got 0. And I just keep going along until that last column. That last column is a problem because the last column of this guy is x to the n, whereas I don't have x to the n in v. So there's some numbers here. You'll notice that C in both cases happens to be a low rank matrix. In these cases, it happens to be of rank 1. So people were wondering, maybe it's something to do with satisfying these kind of equations that makes these matrices that appear in practice numerically of low rank. And after a lot of work in this area, people have come up with a bound that demonstrates that these kind of equations are key to understanding numerical low rank. So if x satisfies a Sylvester equation like this, and A is normal, B is normal. I don't really want to concentrate on that, those two conditions. It's a little bit academic. Then people have found a bound on the singular values of any matrix that satisfies this kind of expression. And they found this following bound. So here, the rank of C is r. So that goes there. So in our cases, the two examples we have, r is 1. So we can forget about r. This nasty guy here is called the Zolotareff number. E is a set that contains the eigenvalues of A. And F is the set that contains the eigenvalues of B. Now, it looks like we have gained absolutely nothing by this bound, because I've just told you singular values are bound by Zolotareff numbers. That doesn't mean anything to anyone. It means a little bit to me, but not that much. So the key to this bound, the reason this is useful, is that so many people have worked out what these Zolotareff numbers actually mean. So these are two key people that worked out what this bound means. And we have gained a lot, because people have been studying this number. This is like a number that people cared about from 1870 onwards to the present day. And people have studied this number extremely well. So we've gained something by turning it into a more abstract problem that people have thought about previously. And now we can go to the literature on Zolotareff numbers, whatever they are, and discover this whole literature of work on this Zolotareff number. And the key part, I'll just tell you the key, is that the sets E and F are separated. So for example, in the Hilbert matrix, the eigenvalues of A can be read off the diagonal. What are they? They are between minus 1 half and n minus 1 half. And the eigenvalues of B lie in the set minus 1 half, minus n plus 1 half. And the key reason why the Hilbert matrix is of low numerical rank is the fact that these two sets are separated. And that makes this Zolotareff number get small extremely quickly with k. Now, you might wonder why there's a question mark on Penzel's name. There is an unofficial curse that's been going on for a while. Both these men died while working on the Zolotareff problem. They both died at the age of 31. One died by being hit by a train, Zolotareff. It's unclear whether he was suicidal or was accidental. Penzel died at the age of 31 in the Canadian mountains by an avalanche. I am currently not yet 31, but going to be 31 very soon. And I'm scared that I may join this list. But for the Hilbert matrix, what you get from this analysis based on these two people's work is a bound on the numerical rank. And the rank that you get is, let's say, a world record bound for the Hilbert matrix is 34, which is not quite 28, not yet. But it's far more descriptive of 28 than 719. And so this technique of bounding singular values by using these Zolotareff numbers is starting to gain popularity because we can finally answer to ourselves why there are so many low-rank matrices that appear in computational math. And it's all based on two 31-year-olds that died. And so if you ever wonder when you're doing computational science when a low rank appears and the smoothness argument does not work for you, you might like to think about Zolotareff and the curse. OK, thank you very much. Thank you, Rob. Excellent. How does it work now? We're good. OK. Yeah? I'm happy to take questions if we have a minute, if you have any questions. How near 31 are you? Well, I said I get a spotlight. I'm 31 in December. So they died at the age of 31. So next year is the scary year for me. So I will be not driving anywhere, not leaving my house until I become 32. Well, thank you all. Thanks. Thanks. Thank you. Thanks very much.