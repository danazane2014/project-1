 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. OK. So I'm going to talk about gradient descent today to get to that central algorithm of neural nets, deep learning, machine learning, and optimization in general. So I'm trying to minimize a function. And that's the way you do it. If there are many, many variables, too many to take second derivatives, then we settle for first derivatives of the function. OK. So I introduced, and you've already met the idea of gradient. But let me just be sure to make some comments about the gradient and the Hessian and the role of convexity before we see the big crucial example. So I've kind of prepared over here for this crucial example. The function is a pure quadratic. Two unknowns, x and y. Pure quadratic. So every pure quadratic I can write in terms of a symmetric matrix S. And in this case, x1 squared was bx2 squared. The symmetric, the matrix is just 2 by 2. It's diagonal. It's got eigenvalues 1 and b sitting on the diagonal. I'm thinking of b as being the smaller one. So the condition number, which we'll see is all important in the question of the speed of convergence, is the ratio of the largest to the smallest. In this case, the largest is 1. The smallest is b. So that's 1 over b. And when 1 over b is a big number, when b is a very small number, then that's when we're in trouble. The condition number of a matrix, when the matrix is symmetric, that condition number is lambda max over lambda min. If I had an unsymmetric matrix, I would probably use sigma max over sigma min, of course. But here, matrices are symmetric. OK, we're going to see something neat, is that we can actually take the steps of steepest descent, write down what each step gives us, and see how quickly they converge to the answer. And what is the answer? So I haven't put in any linear term here. So I just have a bowl sitting on the origin. So of course, the minimum point is x equals 0, y equals 0. So the minimum point, x star, is 0, 0, of course. So the question will be, how quickly do we get to that one? And you will say, pretty small example, not typical. But the terrific thing is that we see everything for this example. We can see the actual steps of steepest descent. We can see how quickly they converge to the x star, the answer, the place where this thing is a minimum. And we can begin to think what to do if it's too slow. OK, so I'll come to that example after some general thoughts about gradients, Hessians. So what does the gradient tell us? So let me just take an example of the gradient. Let me take a linear function, f of x, y equals, say, 2x plus 5y. I just think we ought to get totally familiar with these. We're doing something. We're jumping into an important topic. When I ask you, what's the gradient? That's a freshman question. But let's just be sure we know how to interpret the gradient, how to compute it, what it means, how to see it geometrically. So what's the gradient of that function? It's a function of two variables. So the gradient is a vector with two components. And they are the derivative with respect to x, which is 2, and the derivative with respect to y, which is 5. So in this case, the gradient is constant. And the Hessian, which I often call h after Hessian, or del squared f would tell us we're taking second derivatives. That will be the second derivative is obviously 0 in this case. So what shape is h here? It's 2 by 2, right? Everybody recognizes. 2 by 2. This h would have the second derivative of that. Sorry, the first derivative of that with respect to x, obviously 0. The first derivative with respect to y. The first derivative of that with respect to x, y. OK. Hessian 0 for sure. OK. So let me draw the surface. So x, y, and the surface, if I graph f in this direction, then obviously I have a plane. And I'm at a typical point on the plane, let's say. Yeah. Yeah. So I'm at a point x, y, I should say. I'm at a point x, y. And let me put the plane through it. OK, so how do I interpret the gradient at that particular point x, y? So what does 2x plus 5y tell me? Or rather, what does grad f tell me about movement from that point x, y? Of course, the gradient is constant, so it really didn't matter what point I'm moving from, but I've taken a point here. So what's the deal? If I move, what's the fastest way to go up the surface? If I took the plane that went through that point x, y, what's the fastest way to climb the plane? What direction goes up fastest? The gradient direction, right? The gradient direction is the way up. How am I going to put that in this picture? I guess I'm thinking of this plane as, so what plane? You could well ask, what plane have I drawn? Suppose I've drawn the plane 2x plus 5y equals 0 even. So I'll make it go through the area. And I've taken a typical point on that plane. Now, if I want to increase that function, I go perpendicular to the plane. If I want to stay level with the function, if I want it to stay at 0, I stay in the plane. So there are two key directions. Everybody knows this. I'm just repeating. This is the direction of the gradient of f out of the plane, steepest upwards. This is the downwards direction minus gradient of f perpendicular to the plane downwards. And that line is in the plane. That's part of a level set, 2x plus 5y equals 0 would be a level set. That's my pretty amateur picture. Just all I want to remember is these words, level and steepest up or down. Down with the minus sign that we see in steepest descent. So we're in steepest descent. And what's the Hessian telling me about the surface? If I take the matrix of second derivatives, so I have this surface. So I have a surface f equal constant. That's the sort of level surface. So if I stay in that surface, the gradient of f is 0. Gradient of f is 0 on the surface. The gradient of f points perpendicular. But what about the Hessian, the second derivative? What is that telling me about that surface in particular when the Hessian is 0 or other surfaces? What does the Hessian tell me about? I'm taking the Hessian at a particular point. So I'm getting 0 for the Hessian because the surface is flat. If the surface was convex upwards, if it was a convex graph of f, the Hessian would be. So I just want to make that connection now. What's the connection between the Hessian and convexity? The Hessian of the function and convexity of the function. So the point is that convexity, the Hessian tells me whether or not the surface is convex. And what is the test? Positive definiteness or semidefiniteness. I'm just looking for an excuse to write down convexity and strong or strict. Do I say strict or strong convexity? I've forgotten. Strict, I think. Strictly convex. So convexity, the Hessian is positive semidefinite or which includes, which I better say that right here, includes positive definite. And if I'm looking for a strict convexity, then I must require positive definite. H is positive definite. Semidefinite won't do. So semidefinite for convex. So in fact, the linear function is convex, but not strictly convex. Strictly means it really bends upwards. The Hessian is positive definite. The curvatures are positive. So this would include linear functions, and that would not include linear functions. They're not strictly convex. OK. Good, good, good. Some examples. OK, the number one example, of course, is the one we're talking about over here. So examples. f of x equal 1 half x transpose Sx. And of course, I could have linear terms minus A transpose x, a linear term, and I could have a constant term. OK. OK. So this function is strictly convex when S is positive definite, because H is now S for that function. For that function, H. Usually, H, the Hessian, is varying from point to point. The nice thing about the pure quadratic is it's constant. It's the same S at all points. OK. Let me just ask you, so that's a convex function. And what's its minimum? What's the gradient, first of all? What's the gradient of that? I'm asking really for differentiating, thinking in vector, doing all n derivatives at once here. I'm asking for the whole vector of first derivatives. And because here, I'm giving you the whole function with x for a vector x. Of course, we could take n to be 1. And then we would see that if n was 1, this would just be half Sx squared. And the derivative of a half Sx squared, let me just put that over here so we're sure to get it right, half of Sx squared. This is in the n equal 1 case. And the derivative is obviously Sx. And that's what it is here, Sx. Sx. It really just, it's obviously simple. But if you haven't thought about that line, it's asking for all the first derivatives of that quadratic function. Oh, it's not. What do I have to include now here? That's not right as it stands for the function that's written above it. What's the right gradient? Minus a, thanks. Because the linear function, it's partial derivatives. They're obviously just the components of a. OK. And the Hessian H is S, the derivatives of that guy. OK, good, good, good, good. And the minimum value, we might as well, oh, yeah, what's the right words for a minimum value? No, I'm sorry. The right word is minimum value, like F min. So I want to compute F min. Well, first, I have to figure out where is that minimum reached. And what's the answer to that? So we're putting everything on the board for this simple case. The minimum of F of x, remember, x's were in n dimensions, is at x equal what? Well, the minimum is where the gradient is 0. So what's the minimizing x? S inverse a, thanks. And what's the? Oh, sorry, that's not right. It's here that I meant to write it. Really, my whole point for this little moment is to be sure that we keep straight what I mean by the place where the minimum is reached and the minimum value. Those are two different things. OK. So the minimum is reached at S inverse a, because that's obviously where the gradient is 0. It's a solution to Sx equal a. And what I was going to ask you is, what's the right word? Well, sort of word, made up word for this point x star where the minimum is reached. So it's not the minimum value. It's a point where it's reached. And that's called, the notation for that point is? Arg min, thanks. Arg min of my function. And that means the place, the point where F equals F min. And I haven't said yet what the minimum value is. This tells us the point. And that's usually what we're interested in. We're, to tell the truth, not that interested in a typical example of what the minimum value is as much as where is it? Where do we reach that thing? And of course, so this is x min. This is then arg min of my function F. That's the point. And it happens to be, in this case, the minimum value is actually 0. So it goes, because there's no linear term a transpose x. I guess I feel, why am I talking about arg min when you've all seen it? I guess I think that somebody could just be reading this stuff, for example, learning about neural nets, and run into this expression arg min and think, what's that? So it's maybe a right time to say what it is. It's the point where the minimum is reached. And that's it's the, why those words, by the way? Well, arg isn't much of a word. It sounds like you're getting strangled. But it's sort of short. I assume it's short. Nobody ever told me this. I assume it's short for argument. The word argument is a kind of long word for the value of x. If I have a function f of x, f I call a function, and x is the argument of the function. You might more often see the word variable, but argument. And I'm assuming that's what that refers to. It's the argument that minimizes the function. OK, good. And here it is, S inverse A. Now, just by the way, what is f min? Do you know the minimum of a quadratic? I mean, this is the fundamental minimization question, to minimize a quadratic. Electrical engineering, the quadratic regulator problem is the simplest problem there. There could be constraints. And we'll see it with constraints included. But right now, no constraints at all. We're just looking at the function f of x. And let me remove the b, because that just shifts the function by b. If I erase that, that's just to say it didn't matter. It's really that function. So that function actually goes through 0, right? As it is, when x is 0, we obviously get 0. But it's still on its way down, so to speak. It's on its way down to this point, S inverse A. That's where it bottoms out. And when it bottoms out, what do you get for f? One thing I know, it's going to be negative, because it passed through 0, and it was on its way below 0. So let's just figure out what that f min is. So I have a half. I'm just going to plug in S inverse A, the bottom point, into the function and see where the surface bottoms out and at what level it bottoms out. So I have a half. So that's S inverse A is A transpose S inverse. S is symmetric, so I'll just write S inverse transpose S S inverse A from the quadratic term minus A transpose. And x is S inverse A. Have you done this calculation? It just doesn't hurt to repeat it. OK, so I've plugged in S inverse A. There, there, and there. OK, what have I got? Well, S inverse cancels S. So I have a half of A transpose S inverse A minus 1 of A transpose inverse A. So I get, finally, negative 1 half. Half of it minus 1 of it of A transpose S inverse A. Sorry, that's not brilliant use of the blackboard to squeeze that in there, but that's easily repeatable. OK, good. So that's what a quadratic bowl, a perfect quadratic problem, minimizes to where that's its lowest level. OK, I wanted to mention one other function, because I'm going to speak mostly about quadratics. But obviously, the whole point is that it's the convexity that's really making things work. So let me just put here a remarkable convex function. And the notes tell what's the gradient of this function. They don't actually go as far as the Hessian. Proving that this function I'm going to write down is convex, it takes a little thinking. But it's a fantastic function. You would never sort of imagine it if you didn't see it there sometime. So it's going to be a function of a matrix, a function of, those are n squared variables, xij. So it's a function of many variables. And here is this function. You take the determinant of the matrix. That's clearly a function of all the n squared variables. Then you take the log of the determinant and put in a minus sign, because we want convex. That turns out to be a convex function. And even to just check that for 2 by 2, well, for 2 by 2, you have four variables, because it's a 2 by 2 matrix. We could maybe check it for a symmetric matrix. That would be down to three variables. But I'd be glad anybody who's ambitious to see why that log determinant is a remarkable function. And let me see. I think it has, so the gradient of that thing is also amazing. The gradient of that function, let me, I'm going to peek so I don't write the wrong fact here. Yeah. Yeah. So the partial derivatives of that function are the entries of, these are the entries of A inverse. That's the, of x inverse. That's like, wow, where did that come from? OK. And it might be minus the entries, of course. Yeah. So we've got n squared function. So think of the, what is a typical entry in x inverse? What is a typical x inverse ij? Just to remember that bit of pretty old-fashioned linear algebra. The entries of the inverse matrix, I'm sure to divide by what? The determinant. That's the one thing we know. And that's the reason we take the log. Because when you take derivatives of a log, that will put determinant of x in the denominator. And then the numerator will be the derivatives of the determinant of x. Oh, can we get any idea what are the derivatives of the determinant? Oh, my god. How did I ever get into this? OK. So are you with me so far? This is going to be derivatives of determinant, with respect to all these variables, divided by the determinant. Because that's what the log achieved, right? So when I take the derivative of the log of something, the chain rule says, take the derivative of that, of that something, divide by the function, determinant of x. OK, so what's the derivative of the determinant of a matrix with respect to its 1, 1 entry? Yeah, sure, this is crazy to be doing this, but it's healthy. OK. So I have a matrix X. X, X, 1, 1, X, 1, n, et cetera, X, n, 1, X, n, n. OK. And what am I looking for? I'm looking for the derivatives of the determinant. Do I want the derivatives of the determinant? Yes. So what's the derivative of X of the determinant with respect to the first equals what? How could I figure out? If I change, so what's this asking me to do? It's asking me to change X, 1, 1 by delta X and see what's the change in the determinant, right? That's what derivatives are. Change X, 1, 1 a little bit, how much did the determinant change? And what's the, how do I, what has the determinant of the whole matrix got to do with X, 1, 1? You remember that there's a formula for determinants. So I need that fact. The determinant of X is X, 1, 1 times something, and is that something that I really want to know, plus X, 1, 2 times other something, plus say along the first row times another something. What are these factors that multiply the X's to give the determinant? What that gives is a linear combination of the first row times certain factors gives the determinant. And how do I know that there will be such factors? Because the fundamental property of the determinant is that it's linear in row 1. If I don't mess with other rows, it's a linear function of row 1. So it has the form X, 1, 1 times something. And what is that something? The determinant of this. So what does X, 1, 1 multiply when you compute determinants? X, 1, 1 will not multiply any other guys in its row, because you never are multiplying two X's in the same row or the same column. What X, 1, 1 is multiplying are all these guys. In fact, it turns out to be is the determinant. And what is this called? That one smaller determinant that I get by throwing away the first row and first column. It's called a? Minor is good. Yes, minor is good. There are two words that can be used, minor and cofactor. Yeah. And what is it? I mean, how do I compute it? What is the number? This is a number. And it's this number. But maybe I think of the minor as this determinant. Cancel that. Maybe I think of the minor as this smaller matrix and the cofactor, which is the determinant of the minor. Minor. And there is a plus or a minus. Everything about determinants, there's a plus or minus choice to be made, and we're not going to worry about that. So anyway, so it's the cofactor. Let me call it C11. C11. And so that's the formula for the determinant. That's the cofactor expansion of a determinant. And that will connect back to this amazing fact that the gradient is the entries of x inverse, because the inverse is the ratio of cofactor to determinant. So x inverse 11 is that cofactor over the determinant. So that's where this all comes from. Anyway, I'm just mentioning that as a very interesting example of a convex function. OK. I'll leave that. That's just for education. OK. Now I'm ready to go to work on gradient descent. So actually, the rest of this class and Friday's class about gradient descent are very fundamental parts of 18.065. And that will be one of our examples, and then the general case here. OK. So I'm using this. I'm not going to. It would be interesting to minimize that thing, but we're not going there. Let's hide it so we don't see it again. And I'll work with that example. OK, so here is gradient descent. Is xk plus 1 is xk minus sk, the step size, times the gradient of f at xk. OK. So the only thing left that requires us to input some decision making is the step size, the learning rate. We can take it as constant. If we take too big a learning rate, the thing will oscillate all over the place, and it's a disaster. If we take too small a learning rate, too small steps, what's the matter with that? Takes too long. Takes too long. So the problem is to get it just right. And one way that you could say get it right would be to think of optimize, choose the optimal sk. Of course, that takes longer than just deciding an sk in advance, which is what people do. So I think what people do on really big problems is take an sk, estimate a suitable sk, and then go with it for a while, and then look back to see, if it was too big, they'll see oscillations in the, it'll be bouncing all over the place. Or, of course, an exact line search. So you see this expression often, an exact line search. Choose sk to make my function f at xk plus 1 a minimum. On the line, on the search line, a minimum in the search direction. The search direction is given by the gradient. That's the direction we're moving. This is the distance we're moving, or a measure of the distance we're moving. And an exact search would be to go along there. If I have a convex function, then as I move along this line, as I increase sk, I'll see the function start down, because the gradient, negative gradient means down. But at some point, it'll turn up again, and an exact line search would find that point and stop there. That doesn't mean we would, we will see in this example where we will do exact line searches, that for a small value of b, it's extremely slow. That the condition number controls the speed. That's really what my message will be just in these last minutes and next time, the sort of key lecture on gradient descent. So an exact line search would be that. So a backtracking line search, backtracking would be take a fixed s, like 1. And then be prepared to come backwards. Cut back by half, see what you get at that point. Cut back by half of that to a quarter of the original step, see what that is. So the full step might have taken you back to the upward sweep. Halfway forward might still be on the upward sweep, might be too much. So backtracking cuts the step size in pieces and checks until it, so s0, half of s0, quarter of s0, or obviously a different parameter, a s0, a squared s0, and so on, until you're satisfied with that step. And there are, of course, many, many refinements. We're talking about a big algorithm here that everybody, depending on their function, has different experiences with. So here's my fundamental question. What if I do, let's think of an exact line search. How much does that reduce the function? How much does that reduce the function? So that's really what the bounds that I want are. How much does that reduce the function? And we'll see that the reduction involves the condition number, M over M. So why don't I turn to the example first, and then where we know exact answers. That gives us a basis for comparison. And then the math goal is prove, get bounds on the size of F that match what we see exactly in that example, where we know everything. We know the gradient. We know the Hessian. It's that matrix. We know the condition number. So what happens if I start at a point x0, y0 that's on my surface? Sorry, what do I want to do here? Yeah, I take a point, x0, y0, and I iterate. OK. OK. So here's the new x, y. K plus 1 is x, y, k minus the S, which I can compute, times the gradient of F. So I'm going to put in gradient of F. What is the gradient here? The gradient, the derivative is with respect to x. So I have a 2x, k, and 2by, k. Right, that's my step. And this is the step size. And for this small problem, where we're going to get such a revealing answer, I'm going to choose exact line search. I'm going to choose the best x, k. And what's the answer? So let me, I just want to tell you what the iterations are for that particular function, starting at x0, y0. So let me put start x0, y0. And I haven't done this calculation myself. It's taken from the book by Stephen Boyd and Vandenberg called Convex Optimization. Of course, they weren't the first to do this either. But I'm happy to mention that book, Convex Optimization. And Stephen Boyd will be on campus this spring, actually, and in April for three lectures. This is April, maybe? Yeah, OK, so it's this month in two or three weeks. And I'll tell you about that. So OK, so here are the x, k's, and the y, k's, and the f, and the function values. OK, so x, k. So where am I going to start? Yeah, so I'm starting from the point x0, y0 equal b1. Turns out that'll make our formulas very convenient. x0, y0 equals b1. Good. So OK, so x, k is b times the key ratio, b minus 1 over b plus 1 to the k-th power. And y, k happens to be 1 minus, it's just, it has this same ratio. And my function f has the same ratio too. This is f, k has that same ratio, 1 minus b over 1 plus b to the k-th times f0. That's the beautiful formula that we're going to take as the best example possible. Let's just see. At k equals 0, I have x, k equal b, y, k equal 1, starting at b1. And that tells me the rate of decrease of the function. It's this same ratio. So what am I learning from this example? What's jumping out is that this ratio, 1 minus b over 1 plus b, is crucial. If b is near 1, that ratio is small. If b is near 1, that's near 0 over 2. And I converge quickly. No problem at all. But if b is near 0, if my condition number is bad, so the bad case, the hard case, is small b. Because when b is small, that ratio is very near 1. So I get, it's below 1. The ratio is below 1, so I'm getting convergence. I do get convergence. I do go downhill. But what happens is I don't go downhill very far until I'm headed back uphill again. So the picture to draw for this, let me change that picture to a picture in the xy plane of the level sets. So the picture really to see is in the xy plane. The level sets, f equal constant. That's what a level set is. It's a set of points, x and y, where f has the same value. And what do those look like? Let's see. I think, what do you think? What do the level sets look like for this particular function? If I look at the curve x squared plus b y squared equal constant, that's what a level set is. This is x squared plus b y squared equal a constant. What kind of a curve is that? That's an ellipse. And what's up with that ellipse? What's the shape of it? So because there's no xy term, that ellipse is well lined up with the axes. The major axes of the ellipse are in the x and y directions because there is no cross term here. And we could always have diagonalized our matrix if it wasn't diagonal, and that wouldn't have changed anything. So this is just rotating the space, and we've done that. So what do the level sets look like? They're ellipses. And suppose b is a small number. Then what's with the ellipses? If b is small, I have to take a pretty large y to match a change in x. I think maybe they're ellipses of that sort. Are they? They're lined up with the axes, and I hope I'm drawing them the right direction. They're long and thin. Is that right? Because I would have to take a pretty big y to make up for a small b. OK. So what happens when I'm descending? This is a narrow valley then. Think of it as a valley which comes down steeply in the y direction, but in the x direction, I'm crossing the valley. Is that right? So what happens if I take a point there? Oh, yeah. Ah, I remember what to do. So let's start at that point on that ellipse. OK. Now, and those were the level sets, f equal constant. So what direction, what's the first search direction? What direction do I move from x0, y0? Do I move along the ellipse? Absolutely not, because along the ellipse, f is constant. The gradient direction is perpendicular to the ellipse. So I move perpendicular to the ellipse. And when do I stop? Pretty soon, because very soon, I'm going back up again. So I start some, well, I don't know whether you, yeah. I haven't practiced with this curve, but I know. And time's up, thank God. But what, so what do I know is going to happen? And by Friday, we'll make it happen. So what do we see for the curve, the track of the, say it? It's a zigzag, yeah. So we would like to get here, but we're not aimed here at all. So we zig, zig, zig, zag, and very slowly approach that point. And how slowly? With that multiplier, 1 minus b over 1 plus b. So that's what I'm learning from this example, that that's a key number. And then you could ask, well, what about general examples? This was one specially chosen example with exact solution. Well, we'll see at the beginning of next time that for a convex function, this is typical. It's this 1 minus b is the critical quantity, or 1 over b, or how small is b compared to 1. So that will be the critical quantity, and we see it in this ratio, 1 minus b over 1 plus b. So if b is 1 hundredth, this is 0.99 over 1.01. It's virtually 1. OK, so next time is sort of a key lecture to see what I've just said, that this controls the convergence of steepest descent, and then to see an idea that speeds it up. That idea is called momentum or heavy ball. So the physical idea is if you had a heavy ball right there and wanted to get it down the valley toward the bottom, you wouldn't go perpendicular to the level sets. Not at all. You'd let the momentum of the ball take over and let it roll down. So the idea of momentum is to model the possibility of letting that heavy ball roll instead of directing it by steepest descent at every point. So there's an extra term in steepest descent, the momentum term that accelerates it. OK, so Friday is the day. Good. See you then.