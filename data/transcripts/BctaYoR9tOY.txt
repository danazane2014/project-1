 OK, so let's continue with our discussion of Hilbert spaces. And so last time, we finished with the Riesz representation theorem, the only representation theory I remember, Riesz representation theorem, which states that if you have a Hilbert space, then for all f in the dual space, which remember, this is a set of bounded linear maps from h to complex numbers, there exists a unique v in h such that f of u is equal to u inner product v for all v, for all u and h. So every continuous linear functional on a Hilbert space can be realized as inner product with a fixed vector. So using this, we can revisit the subject which you touched upon in the assignment of adjoints. So let me just state this as a theorem. Let h be a Hilbert space and a going from h to h be a bounded linear operator. The conclusion is then there exists a unique bounded linear operator A star from h to h, which we call the adjoint of A, such that A star satisfies the following property. For all u, v, and h, Au inner product v is equal to u inner product A star v. And moreover, the operator norm of the dual equals, I mean, the operator norm of the adjoint equals the operator norm of A. So again, the same way we proved uniqueness for the Riesz representation theorem, you can prove that such a linear operator satisfying this identity for all u and v has to be unique. So I'll just state here that uniqueness of such an operator A star follows from the identity it satisfies. Just like the vector v, which spits out f of u when you take the inner product with u with it, just like that vector is unique regardless of how you chose to define it, I mean, how you created it based on that identity, the same thing holds for the adjoint. So just the same argument is what I mean. So how do we define this operator? Well, we basically define it by this relation. So let v be an H. Define a map f sub v from H to C via f sub v of u is equal to A u inner product with v. Now, this is clearly a linear map. Maybe I shouldn't check this. Maybe I should. But let's check it anyways. If I take f of v of lambda 1 times u1 plus lambda 2 u2, this is equal to A times lambda 1 u1 plus lambda 2 u2 v. And now we use the fact that A is a linear operator so that it acts on the linear combination by linearity. So then I get lambda 1 A u1 plus lambda 2 A applied to u2 inner product v. And because the inner product is linear in the first entry, it's conjugate linear in the second entry, but it's linear in the first entry. This is equal to lambda 1 times A u v plus lambda 2 A u2 v, which is lambda 1 fv of u1 plus lambda 2 fv of u2. So OK, so this thing is a linear map. I claim it's also continuous, i.e. it's an element of the dual. And we just check. So if the norm of u equals 1, then if I take the absolute value of f sub v applied to u, this is by definition A u v, which is less than or equal to by the Cauchy-Schwarz inequality the norm of A times of A u times the norm of v. And the norm of A u is less than or equal to the norm of A, since A is a bounded linear operator, times the norm of v. So this holds for all u with norm u equals 1. And therefore, norm of this operator is less than or equal to norm of A times norm of v. So that proves that it's an element. So thus, f sub v is an element of the dual. And now we use the Reese representation theorem. So by the Reese representation theorem, there exists a unique element, which we denote A star v and h, such that for all u and h, f v of u equals u inner product with A star v, i.e. for all u and h, A u v, because that's the definition of this guy on the left, is equal to u A star v. OK? OK. So this just defines for each v an element A star v and h. So now let's prove that this map v to A star v is a bounded linear operator. That then defines the adjoint. OK. So first we claim v, which maps to A star v, the element that satisfies this identity, is linear. So let lambda 1, let v1, v2 be in h, lambda 1, lambda 2 be complex numbers. Then for all u and h, we have what? If I take, let's see, how did I spell this out? So u inner product with the element A star lambda 1 v1 plus lambda 2 v2. So remember, this is the element such that this inner product is defined by this is equal to A u plus inner product with lambda 1 v1 plus lambda 2 v2. And this is equal to taking the complex. Well, so first off, the inner product is conjugate linear in the second entry. So this is equal to lambda 1 complex conjugate A u v1 plus lambda 2 complex conjugate A u v2. Now, how A star, how this symbol, if you like, is defined is that it's equal to that this quantity here is equal to lambda 1 times A u A star v1 plus lambda 2 bar u star v2. And pulling the complex numbers back in, this is equal to u lambda 1 A star v1 plus lambda 2 A star v2. Now, this holds for all u and h, i.e. the inner product of u with this minus this is 0 for all u in h. And the only thing orthogonal to everything in h is the 0 vector. So we conclude that this element is equal to. So this element corresponding to this linear combination is a linear combination of these elements. Thus, the map v gets mapped to A star v is linear. And so we denote this map now simply by A star. It acts on a vector v by A star v, by multiplying by A star. So now A star is a linear operator. And I write A star here. I mean the map that takes v to A star v. So now I want to check that this linear operator is a bounded linear operator. I need to check that it's continuous. And in the process, we'll end up showing that the norm of A star equals the norm of A. Now, suppose norm of v equals 1. So I want to bound A star applied to v. So if A star v equals 0, then clearly A star v is less than or equal to the norm of A. This, in the end, is what I will show. But I'm kind of dealing with the stupid case, which is when this thing on the left-hand side is 0. So suppose A star v is non-zero. Then if I take the norm of A star v squared, this is equal to A star v inner product A star v. Now, by the definition of the inner product, this is equal to A times A star v v, taking this A star and moving it over here. And by Cauchy-Schwarz, well, first off, all of these numbers are real. So it makes sense to actually write less than or equal to. But by Cauchy-Schwarz, this is equal to or less than or equal to A times A star v norm times norm of v, which equals 1. Now, this equals 1. And the norm of A applied to A star v, that's bounded by the norm of A times the norm of the input, which is A star v. And therefore, so we started off with A star v squared less than or equal to the norm of A times the norm of A star v. This is non-zero, so I can divide by it and conclude that A star v norm of that is less than or equal to norm of A. Now, remember, the norm of a linear operator is the soup of this for all v which have length 1. And since this is bounded above by the norm of A, that implies that norm of A star is less than or equal to the norm of A. OK? So we've shown that basically all of this theorem, with the exception of equality of the norms. So every bounded linear operator has an adjoint, which is a bounded linear operator and satisfies. And it's a unique linear operator satisfying that identity up there for A star. Now, note that for all u, v, and h, if I look at A star u inner product v, this is equal to the complex conjugate of v inner product A star u, which is equal to, by the definition of A star, moves over to A v u. And again, applying the complex conjugate switches the order of the entries. So this is equal to u A v. So what have we shown? We've shown that the adjoint of the adjoint is equal to A. So i.e., we've shown that A star u, v is equal to u times A v. And recall, this is supposed to be equal to u A star A star. So the adjoint of the adjoint. And we've shown that this quantity is equal to this quantity for all u and v. And therefore, the adjoint of the adjoint is equal to the operator again. Thus, by what we've done previously, the previous argument, if you like, applied now where A is replaced by A star, the norm of A, which is equal to the norm of A, the adjoint of the adjoint, because this operator is equal to A. By what we've done, we've shown that the norm of the adjoint is always less than or equal to the norm of the thing you're taking the adjoint of. So this is less than or equal to norm of A. And so we have this, and we have this, which implies norm of the adjoint is equal to the norm of the operator. OK? So I mean, what is this creature in practice? So let's take maybe a second simplest example. So the simplest example would be if you're on Cn and Rn. Well, OK, so let's talk about that. Suppose now we have a matrix. So u is equal to u1, un, and this is in Cn. And A applied to u, if I want the i-th coordinate of that, is given by sum of j equals 1 to n of uj, where these are just complex numbers, some fixed complex numbers. So A is just a matrix. Linear transformation on Cn is always representable by a matrix. And here I'm writing u in terms of the standard basis vectors 1, 0, and so on. OK? Then to determine what the adjoint is, we figure out what's the operator that satisfies the identity the adjoint has to satisfy. So if we take A, u, and take its inner product, we want to write this. So let me write this in a box. We want to be able to write this as u with some operator B applied to v. And then this would be, and therefore, that would be the adjoint. So if we write A, u, inner product v, this is equal to sum i equals 1 to n of vi complex conjugate conjugate. And this is, by definition, equal to, I'm just going to write sum over i and j. i and j are going from 1 to n of aij uij vj complex conjugate. And if I now switch this over and sum first with respect to i and then with respect to j, I can write this as uj sum i equals 1 to n of aij applied to vi complex conjugate. The complex conjugate there. And so this tells me that, so this is equal to sum j equals 1 to n uj and times what I'll call the adjoint applied to vj. How does the adjoint operate on an element v? This is equal to sum from, let's say I want the i-th member now. This is sum from j equals 1 to n aji complex conjugate applied to vj. So for matrices, again, this should be review. So if A is represented by a matrix, then the adjoint, which is also representable by a matrix, is representable by the matrix. So I should say the matrix for the adjoint, aij, is equal to aji complex conjugate. And so we did this for CN. There's no reason we can't do it for, let's say, little l2 now and make it a little more interesting. So suppose now I have now infinitely many numbers, aij, so a double sequence in CN such that sum ij sum aij squared, which is linked as n goes to infinity of sum of i equals 1 to n, j equals 1 to n, is finite. Now, let's define A now from little l2 to little l2 via A applied to a sequence, which I put an underline underneath for, or let's make it U. OK, we've been using A's for elements in little lp as sum j equals 1 to infinity aij aj. And here this is for a equals little l2. Now, by the Cauchy-Schwarz inequality, you can check that if this condition is satisfied, then this is a bounded linear operator. The absolute value of this thing will be less than or equal to the sum j equals 1 to infinity of aij sum and j squared, square root times the l2 norm of A. And therefore, I can sum that using, that's l2-summable using this, the fact that this is finite. Don't worry about the fact that I'm summing this double sum in a certain way by j going from 1 to n, i going from 1 to n. In fact, if it's absolutely summable in this, which it is, it's a sum of non-negative numbers. This sum doesn't depend on how I'm summing it in this way. OK, so then this is a bounded linear operator. And what's its adjoint? Its adjoint is going to be of the same flavor as what we got for the finite dimensional case. And for all a, b, and little l2, if I take the inner product of a applied to u, a, b, and little l2, this is equal to sum over i. I mean, the same proof applies. Essentially, sum ij, ai, bi, complex conjugate, which is equal to sum j, aj, i, aij, bi, complex conjugate, complex conjugate, which is equal to inner product a with a star b, where a star b is defined via sum. So I should have said this is the i-th entry in this new sequence in l2. And so here, the i-th entry in this new sequence in l2 is defined to be j equals 1 to infinity, aj, i, complex conjugate, bj. So just like in the finite dimensional case, for this case, if I switch the variables of this a, if you'd like a double infinite matrix, if you want to think of it that way, it's the same as in the finite dimensional case. Let's do one last example. So I'm not going to go through, again, the computations, which are very similar. Here, we're having finite sums. If you can do something with sums, you should consider whether or not you can do it for integrals. So let's suppose k is a continuous function on 0, 1 cross 0, 1. And we define a map a, which you can show goes from l2 to l2 via a f of x is equal to the integral from 0, 1 of k of x, y, f of y, dy. In fact, this is for each f in l2, a f, this thing, is, in fact, a continuous function. It's more than just an l2. It's, in fact, a continuous function. But continuous functions on 0, 1 are elements of l2 on 0, 1. Then you can check, just as we've done in these two other examples, that the adjoint of f is equal to, or let's make this a different, say, adjoint applied to g is equal to the integral from 0 to 1, k, yx, complex conjugate, g of y, dy. So again, it's like flipping the indices and taking the complex conjugate. Now, I said at the end of last class that you can tell something about solvability of equations based on how the adjoint, on properties of the adjoint. So we'll see another way this is manifested later, kind of as the simplest way. And we'll suppose H is a Hilbert space. And Ha from H to H is a bounded linear operator. Then I used RAN in the assignment, so I'll use it this way here. The range of A, orthogonal complement, so the range you can show is a subspace of H. In fact, I think that was in one of the earlier assignments. If you take the orthogonal complement of that, that is equal to the null space of the adjoint. So here, let me recall that range of an operator B. This is exactly what it is. This is a set of all vectors Bu, where U is an H, and null space of B. I might have called it the kernel. Might have labeled it as the kernel of B, but it both mean the same set. This is a set of all U and H, such that Bu equals 0. So in particular, if we know that we have an operator A, such that the range is a closed subspace, then being able to solve the equation Au equals V is equivalent to showing that the null space of the adjoint is simply 0. So let me make that as a remark. Suppose that the range of A is closed, then A from H to H is surjective if and only if the adjoint is injective, meaning the only thing that gets sent to 0 is a 0 vector. Because if the range of, if the null space is equal to the 0, so first off, it's easy to see. If the range of A is equal to H, then the orthogonal complement of H is a 0 vector. So null space of the adjoint is just a set containing the 0 vector. Now, if the null space of the adjoint is just a 0 vector, then taking the orthogonal complements of both sides and using the fact that the range is closed so that the orthogonal complement of the orthogonal complement gives me back the set, I conclude that the range of A equals the orthogonal complement of the 0 vector, which is H, the entire space. So we know that we have an operator which has closed range, then being surjective is equivalent to the adjoint being injective. So the proof of this, though, is pretty easy. So V is in the null space of A star if and only if U A star V equals 0 for all U and H. A star V equals 0 if and only if this holds for all U and H. And this is equivalent to, by the property of the adjoint, U inner product A star V is equal to A times U inner product V. So AUV equals 0 for all U and H. So V is a fixed thing. So this says that V is orthogonal to all of the elements of H of the form A times U, but that's just the range of A. And so this is equivalent to V is in the orthogonal complement of the range of A. OK. OK. So we're soon going to get into the realm of more refined things we can say about solving certain equations involving operators, meaning when can you solve AU equals V and so on. But now, kind of the best theorem about that that you should know or maybe heard at some point from linear algebra is the rank nullity theorem that says the dimension of the range plus the dimension of the null space equals the dimension of the, let's say you're going from spaces with the same dimension, then the dimension of the range plus the dimension of the kernel. I may be actually getting this wrong. It's the end of the day. It's the end of a long day. Equals the dimension of the whole space. So what this says is that, in essence, that in order to be able to solve a given equation, your input has to satisfy finitely many linear relations. And the solution of that equation is unique up to finitely many or a finite dimensional subspace. OK? Now, this is kind of the best thing you can say. Input has to satisfy finitely many linear relations and unique so that existence is predicated upon your data satisfying finitely many linear conditions. And you have uniqueness up to finite dimensional subspace, mainly the null space. Now, this is from finite dimensional linear algebra. It would be great if we could do the same thing for infinite dimensions, not just because it would be fun, but in the end, because we need to do these things, or I should say need to, but not in the sense that an airplane wing is going to fall off if we don't, but in order to learn more about certain problems that arise, it would be great to be able to do these things. And we'll be able to do them for certain operators which are in some way, these aren't the only operators, but for certain operators that are close to being matrices in a certain sense. So we'll get to that in a minute. Now, these operators that we will eventually study the solvability properties of have a very special nature or property on how it acts on bounded sequences. Now, something that you kind of take for granted in RN is that if I have a linear operator and I have a bounded sequence of vectors, then A maps that to a bounded sequence of vectors because a matrix is a bounded linear operator. It takes bounded sets to bounded sets. And therefore, by the Heine-Borel theorem, there will be a subsequence of the images of these vectors which converges. OK? So there's some compactness hidden in what you're doing on RN and CN. And why am I rambling right now? I'm getting to the final point that we need to study a little bit about compactness and Hilbert spaces to eventually get to the place where we can say more about being able to solve equations involving bounded linear operators now in infinite dimensions. OK? OK, so with that rambling buildup, hopefully I convinced you that we should study a little bit about what it means for sets to be compact in a Hilbert space. So let's look at, so now we'll compactness. So in a Hilbert space, so let me just, let me recall for you what it means for a set to be compact in a metric space. x is metric space. In a Hilbert space, we say the subset k and x is compact if every sequence of elements in k has a subsequence converging to an element in k. OK, so a set is compact if every sequence of elements in the set has a subsequence which converges to an element in the set. So the simplest on Earth types of compact sets are finite sets. OK? This just follows from the pigeonhole principle, basically. All right? So the simplest example are finite sets, finite subsets of a metric space, of any metric space. Now, we have this very cool theorem from intro analysis, which goes by the name of Heine-Borel that says a subset k and r, you can replace r by rn or cn, but this follows from the one-dimensional case, essentially. A subset k of r is compact if and only if k is closed subset of r and bounded. OK? So for example, the closed and bounded intervals, AB, those are all compact sets. AB, the set consisting of 1 over n, n natural number, union, the element 0, that's also a compact set. It's closed and it's bounded. OK, now, this is a theorem about elements of compact subsets of r. One can build off of this to build off the proof of this, really, to get the result for rn and cn. I mean, the metric properties of cn are the same as r to n. So let's say just rn, that a subset of rn is compact if and only if it's closed and bounded. Now, does that hold for arbitrary metric spaces? No. Does it hold for Banach spaces? No. You did an example on one of the assignments that proved that little lp is not compact. What about if we specialize the Hilbert spaces? The answer is still no. So let's make this a non-example since we did examples a minute ago. So all of these as subsets of r, these are all compact. So let's say we look at B equal to, let's suppose H is a infinite dimensional Hilbert space. Then the closed ball B equals the set of u and H. Let's make it F such that length of u is less than or equal to 1. This is a closed and bounded set. You can check that. This is not compact. Let's get back to compactness. So let's suppose H is an infinite dimensional. Let's even make it separable as all good Hilbert spaces are. Then the closed unit ball is not compact. So why is this? Let e n equals 1 to infinity be n. Or in fact, I mean, it doesn't have to be separable. In fact, if you just take any infinite dimensional Hilbert space, you can find by the, so let's, we don't even need to make it separable. You can always find an orthonormal, countably infinite orthonormal subset of H. Now, in the separable case, we could choose it so that it's an orthonormal basis. But that really doesn't matter. Just the fact that H is an infinite dimensional Hilbert space, we can find a countable subset. So we can find a countably infinite subset of orthonormal vectors. Why is this? Because we can find a countable set of linearly independent vectors in an infinite dimensional Hilbert space or infinite dimensional vector space. And then we can make them orthonormal by applying the Gram-Schmidt process to that collection and come up with a countably infinite orthonormal subset. Then for all n not equal to k, we get that En norm minus Ek squared. This is equal to, again, using the parallelogram law, this is, well, I don't need to apply the parallelogram law. I can just apply what this is. This is equal to norm En squared plus norm Ek squared plus 2 real part En Ek. And since they're orthogonal, this is 0. And since they're normalized to 1, this is equal to 2. So for any two distinct elements in the sequence, the distance between the two of them is equal to 2. So there's no way the sequence of these orthonormal vectors can have a convergent subsequence, right? Because a convergent subsequence would mean that if I go along the subsequence, or this subsequence has to be Cauchy, so that the difference between vectors has to get small. But it's never small. It's always at least bigger than or equal to 2, at least the length squared or distance squared. So this is a closed and bounded subset, which is not compact. There exist sequences which have no, or this sequence has no convergent subsequence. So the question is, what's the additional condition I need to conclude that a subset of a metric space is compact? So from real analysis, 18.100, every compact set has to be closed and bounded. One implication is that if something is compact, it's closed and it's bounded. What additional condition on the set, so being closed and bounded is a necessity of the set? What additional condition guarantees that it's compact? Maybe not if and only if, like we have for the Heine-Borel, but at least something simple to verify so that the subset of the Hilbert space is compact. Now, if you didn't see this in 18.100, that's fine. But this question can or could have been already asked in a different context in 18.100. And at least when I taught it, 100B, it was that you can ask the same question about subsets of continuous functions. So definition of a compact set is with respect to a metric space. It doesn't have to be an inner product space. So the space of continuous functions on, say, 0, 1, that's a perfectly good metric space with the infinity norm. You could ask, what are the conditions on a set to ensure that the set is compact? And the three conditions are closed, bounded, because these are necessary if the set is to be compact, and what's called equicontinuous. So for subsets of the space of continuous functions, if the subset is closed, bounded, and what's called equicontinuous, then the subset is compact. This is the famous Arzela-Ascoli theorem. And that extra condition, equicontinuity, is in some sense what allows you to reduce your compact, your problem of showing your space is compact, to the finite dimensional case. It takes care of the infinite dimensions, or it takes care of the infinite part, in a sense. That's not very specific, but it's needed to control all but finitely many things, in some sense. Equicontinuity in Arzela-Ascoli is what helps you control all but finitely many things. Now, there is a similar game, or not game, but theorem that you can prove now in Hilbert spaces. So let's just state the theorem. Part of the theorem will be a definition, unfortunately. But H be a Hilbert space. Well, OK, so we're not going to prove the theorem. I want to prove just yet. I just skipped ahead by one. But so we'll motivate a certain condition. So I got ahead of myself just a minute by just one theorem. But let me introduce the definition. So let H be a Hilbert space. A subset K of H has equi-small tails with respect to a countable orthonormal subset. So the new bit of terminology is equi-small tails, e n, if, so let's make this K. If, so here's a, in some sense, so by Bessel's inequality, the sum of the, if you like, Fourier coefficients or the sum of the squares of the inner product, e K with a fixed vector, this is always bounded by the norm of the vector squared. And therefore, it always converges. So equi-small tails means that somehow that series converges, or the tail end of that series converges uniformly with respect to this orthonormal subset. So in other words, for all epsilon positive, there exists a natural number n such that for all v in the set K, if I look at the tail, this is less than epsilon squared. So somehow I can, given epsilon, I can always choose an n, which is independent of the element in K so that the tail end of, if you like, the Fourier series is small. And this n can be chosen independent of the element in K. So I leave it to you to verify if K is simply a finite set. So this implies K has equi-small tails with respect to any subset or orthonormal subset. So what's the motivation for this definition? Is that the next simplest type of set, which is compact. So like I said, any finite subset of a metric space is compact. Now, I'll give an example of another more interesting compact set. But that compact set also satisfies this property that it has equi-small tails with respect to any countable orthonormal subset. And so then we should hope that if a set is closed bounded and has the property that with respect to an orthonormal basis, it has equi-small tails, then that set is compact. And that's eventually what we'll prove. But I don't think we'll get to it today. But so one more bit of motivation on why this might be the right extra condition to add to closed and bounded to produce compact sets is provided by the following. So again, let H be a Hilbert space and be in a convergent sequence. And now let EK be some arbitrary countable orthonormal subset. So then two things hold. One is that the set K equals the set of elements V in union the limit. This is compact. Proof is given in Melrose's notes. And I'm going to leave it to you to look up or work your way through. It's not that difficult to work your way through. And K has equi-small tails with respect to EK. So this subset consisting of a convergent sequence along with its limit is compact. And it has equi-small tails with respect to any countable orthonormal subset. OK? All right. So like I said, I'm going to leave one for you. Two, we will prove. Which time do I got? OK, I don't have much time left. But we should be able to finish this. All right, so we have to verify this definition. So let epsilon be positive. So why should we? What's the thinking on why we expect this set to have equi-small tails is because, so I have to check that I can find a large enough integer that all the squares of these coefficients is small. Now, the point is that the Vn's are essentially very close to V being V, at least for n very large. And for a fixed V, I can always choose a capital N so that this holds. And so that N will take care of infinitely many. And then I just need to choose N big enough for V and also the finitely many that are not very close, finitely many elements of this sequence which are not close to V. That's essentially the argument. So since Vn converges to V, there exists a natural number M such that for all n bigger than or equal to M, I have Vn minus V. The norm is less than epsilon over 2. Now, choose natural number N so large so that sum K bigger than N of Vek. This is small. So here V is fixed, right? And we always have that this whole sum converges by Bessel's inequality. And therefore, we can always choose an N so that the tail is small. And we can do the same for the first M minus 1 elements of the sequence. So plus max of 1 so that this is all less than epsilon squared over 4. So for each Vn, I can find a capital. So for V1, I can find an N1 so that this is less than epsilon squared over 8, say. And then for N2, I mean, for N equals 2, N equals 3, all the way up to M minus 1, I can find a capital N sub N. I then take capital N to be the maximum of those finitely many capital N sub N's along with the N that I need for this. So that's the argument on why I can find such a capital N. I just have finitely many things I need to make small, which I individually can. OK. Now, I claim this capital N works. So I have to show that if I take any element in K, then it has a small tail. So then I have that by how I've chosen N, sum from sum over K bigger than N of Vek squared. This is less than epsilon squared over 4, which is less than epsilon squared. And for all 1 less than or equal to N is less than or equal to M minus 1, the same bound. So I just need to check now that this is small when N is bigger than or equal to M. Now, if N is bigger than or equal to M, by Bessel's inequality, we have that sum K bigger than or N Vn Vk squared. If I can take the half power and show that's less than epsilon. So this is equal to K bigger than N of Vn minus V plus V. So I can Ek plus. So let's write it this way. Now, this is the little l2 norm of a sum of two sequences in K. Not N, but in K. So by the triangle inequality for little for sequences in little l2, this is less than or equal to sum K bigger than N of Vn minus V Ek 1 half plus sum from K bigger than N of V Ek raised to the 1 half power. Now, this quantity I already have control over. That's less than epsilon over 2. I'm taking the 1 half power here. And I can bound this by Bessel's inequality. This is less than or equal to norm of Vn minus V plus, and then using the bound for this, epsilon over 2. And now, remember, how is capital M chosen? Capital M is chosen to ensure that this is less than epsilon over 2, which we can do since the Vn's are converging to V. So this is less than epsilon over 2 plus epsilon over 2 equals epsilon, proving that this set consisting of the elements of a convergent sequence along with the limit has equi-small tails with respect to an arbitrary orthonormal subset. So next time, we will prove that if we have a subset of a separable Hilbert space, which is closed, bounded, and has equi-small tails with respect to a orthonormal basis, which exists because it's a separable Hilbert space, then the set is compact. And then we'll rephrase that in a way that doesn't involve Hilbert spaces and go from there. Start looking at some of these operators, I said, which are close to matrices. All right, we'll stop there.