 OK. So last time, we introduced pre-Hilbert spaces as vector spaces that come equipped with a Hermitian inner product. Hermitian inner product is linear. So it's a pairing between two elements that gives you a complex number. It's linear in the first entry. And if you switch the entries, that's equal to the pairing of the original two entries, taking the complex conjugate of that. And it's positive definite, meaning if I take the inner product of a vector with itself, it's non-negative and 0 if and only if the element is 0. And we proved the Cauchy-Schwarz inequality. So let me actually just recall. So let me actually just recall that we defined for if H is a pre-Hilbert space, we defined what we called, or using this norm notation, although I haven't proved it's a norm yet, to be the inner product of B with itself raised to the 1 half power. And this is a non-negative real number. This is a non-negative number, so taking the 1 half power is meaningful. And we proved at the end of last time the Cauchy-Schwarz inequality that for all u and v and H, if I take the inner product of u and v, take its absolute value, this is less than or equal to norm of u times the norm of v. So now let's use this to actually prove, or to prove that this thing that I've been denoting with this norm notation is, in fact, a norm on a pre-Hilbert space. So theorem, if H is a pre-Hilbert space, then this thing here, defined in this way, is a norm on H. OK? So remember, we have to prove three things for something to be a norm. We have to prove it's positive definite. And then we also have to prove homogeneity and the triangle inequality. So note that this quantity here equals 0 if and only if the inner product of v with itself is 0, which by the positive definite quantity of a Hermitian inner product implies that v equals 0. So that proves that this function here on H is, in fact, positive definite. Now, if lambda is a complex number and v is in H, if I take, so we kind of saw this at the proof of the Cauchy-Schwarz inequality, but if I take lambda times v and inner product lambda times v, this is equal to lambda times lambda bar. A scalar pulls out of the first entry unfazed. If a scalar is in the second entry, it comes out with a complex conjugate. And then, of course, for a complex number times this complex conjugate, I get the norm of that or the length of that, the absolute value of that complex number squared. So taking the square root of both sides of this, which is equal to this, I get that this quantity here is equal to the absolute value of lambda times this norm of v, which proves homogeneity of this function on H. So all that remains to prove that this is an actual norm, so I don't have to keep being, or at least trying to be, careful with what words I'm using, will prove that this is a norm. So now we need to prove the triangle inequality. If I let u, v be in H, then I compute u plus v squared. This is equal to u plus v, u plus v, which is equal to just how we computed it for when we had a t here and a t here when we did the proof of the Cauchy-Schwarz inequality. And we'll use this identity quite often, that the norm of u plus v squared is norm u squared plus norm v squared plus 2 times the real part of u and v, the real part of the inner product of u and v. Now, this is less than or equal to norm squared plus v squared plus the absolute value of the real part. Now, the absolute value of the real part of a complex number is less than or equal to the absolute value of that complex number. So this is less than or equal to the absolute value of the inner product of u and v. And by Cauchy-Schwarz, that is less than or equal to. So still what I have before, plus 2 times the inner product of u and v. And I guess I'm going to go to the next board. All the proof is essentially done. So what I had before is equal to norm of u plus norm of v squared, i.e. So I had started off with the norm of u plus v squared. And I had proved that's less than or equal to this thing here squared. So taking the square root of both sides, I get the norm of u plus v is less than or equal to the norm of u plus the norm of v. So this thing that I defined before is, in fact, a norm. And I'm just using that notation to denote an imposter. Now, using the Cauchy-Schwarz inequality, we can also prove that taking the inner product is a function on h cross h that's continuous. So let me label this as continuity of the inner product. So let me state it as the following. If we're in a pre-Hilbert space and un converges to u and vn converges to v in a pre-Hilbert space with norm as defined as before, so now we have a norm on a pre-Hilbert space. And so we can define convergence since it's a normed space. Pre-Hilbert space h with norm this, then un inner product vn converges to u inner product v. OK? All right, so why is this? So last time, in the previous proof, I mean, we really didn't use the full strength of the Cauchy-Schwarz inequality. We could have just gotten by with the real part, having just proven that the real part of the inner product is an absolute value less than or equal to the inner product of or the product of the norms of u and v. Here, we'll actually use the full Cauchy-Schwarz inequality. So the proof is quite simple. If un converges to u and vn converges to v, i.e., let me just spell this out for you, un minus u converges to 0 and norm of vn minus v converges to 0 as n goes to infinity, then we'll use the squeeze theorem to show that this quantity converges to u inner product with uv. So I have to show that this quantity here, let me not put that, we're not in 18.100. Typically, in 18.100, when I first start teaching the squeeze theorem, I always include the lower bound. But it should be clear that this is always less than, always bigger than or equal to 0, since it's the absolute value or of a complex number. So this is equal to un minus uvn plus u inner product vn minus v. And by the triangle inequality now, just for the modulus or absolute value of complex numbers, that's less than or equal to un minus uvn plus absolute value of uvn minus v. And this is less than or equal to, by the Cauchy-Schwarz inequality, norm of un minus u times the norm of vn plus norm of u times norm of vn minus v. And now, the vn's are converging to v. And therefore, the norms of the vn's are converging to the norm of v. So recall that if, simple fact, if vn converges to v, then norm of vn converges to norm of v. And again, so let me add, since one can prove, as you do in real analysis, the following reverse triangle inequality. Since if I have two vectors, a and b, and h, so this just works in any Banach space, not necessarily a pre-Hilbert space, the absolute value of the difference in norms is less than or equal to the norm of the difference. So this is a convergent sequence of real numbers, so it must be bounded. So I can say that's less than or equal to times sup of n, or let's use a different letter, k, times plus norm of vn minus v times norm of u. Now, this is something converging to 0 times a fixed number. This is something converging to 0 times a fixed number. And therefore, this goes to 0 as n goes to infinity. And therefore, the thing which we started with, so this quantity here, an absolute value, is less than or equal to something converging to 0. And it's also non-negative, so therefore, I get that. So this is by the Squeeze theorem, i.e. OK? OK, so that's a. So on a pre-Hilbert space, we can define a norm using the inner product. And this inner product is continuous with respect to this norm, OK? Because once we have a norm, we have convergence, so we can talk about, or once we have a norm, we have the notion of distance, so we can talk about convergence of sequences and things like that. So now, we have pre-Hilbert spaces. What is a Hilbert space? This is simply a pre-Hilbert space, which is complete with respect to this norm. So a Hilbert space H is a pre-Hilbert space, which is complete with respect to this norm, which, again, remember, was defined as, let's take a vector, the norm of a vector here is equal to the inner product of the vector with itself raised to the 1 half power. All right, so this is the new terminology. And as we'll see, or at least we'll explicitly spell out in the form of a theorem, but we will see probably by the end of this lecture, that really, there's for a reasonable Hilbert space, you're either one of two things. So let me first give the examples, basic examples of Hilbert space. So for example, Cn, which is the set of n-tuples. How do I denote this? Complex numbers, where the inner product of two vectors, z and w, is equal to sum j equals 1 to n, zj, wj. So this is an example of a Hilbert space, finite dimensional one, meaning it's a vector space of finite dimension. The other example we had is the space little l2, which this was the space of sequences such that each of these is a complex number. And this series is convergent, is finite. Sum of the absolute value of the ak squares. This is a Hilbert space as well, with what's the inner product of two elements of little l2. This is the sum from j equals 1 to infinity. Let me add it. Of ak, vk. And note, so this is pretty clear, but note that the norm I get from this inner product, this is simply which was the little l2 norm. Right? OK. So these are two basic examples of a Hilbert space. We will, in fact, show that every separable Hilbert space can be in a inner product and, therefore, link-preserving way, be mapped. So that's usually called an isometric isomorphism. Can be mapped isometrically to either cn or little l2. So these two, if one wants to go about categorizing all of the possible Hilbert spaces as far as separable Hilbert spaces, which are kind of the only reasonable ones, of course, one can come up with wild examples of Hilbert spaces which are not. Then these are the only two. I shouldn't say two, because this one's indexed by its dimension. But these are the only two types that you come up with, either a finite dimensional cn or it's isometric to little l2. But I will still write down another example. Since we went to all that work, if E is a measurable subset of R, l2 of E, the big l2 of E, which remember this was a space of measurable functions f from E to C such that f squared E is finite, this is also a Hilbert space. What's the inner product? With the inner product of two elements in l2 defined to be kind of just the analog of little l2 where we replace the sum with an integral f times g. The Lebesgue integral of f times the complex conjugate of g. OK. Now, I wrote down capital L2. I wrote down little l2. But what about the other little lp and big lp spaces or any of those Hilbert spaces? So of course, if I define the inner product in this way as I did before, then that only induces the little l2 and the big l2 norm. But is there perhaps some other kind of magic inner product out there that I can put on little lp or big lp so that I would get out the little lp or big lp norm when I define the norm according to how I've been doing it? So the question on hand is are the other little lp or big lp spaces also Hilbert spaces? So again, it's clear that if I were to define the inner product in the way I did in the two examples, then that only is going to give the l2 norm. I'm asking now, is there some magical inner product I can define on these spaces that spits out the little lp or big lp norm? So the answer is no. And there's, in fact, a lot of people who have asked me, well, is there a way to determine whether or not a space is a Hilbert space? Because if you think about it, the way we've come about introducing what a pre-Hilbert space is and a Hilbert space is, we first had a norm in our hand, and then we defined an inner product. Well, if that's the way you're building your space, then you know automatically that it's a Hilbert space because you had an inner product first, and then you defined an inner product. Because you had an inner product first, and then you defined the norm second. Let's suppose the data you're given is some norm on a space. When can you determine if that norm comes from an inner product? That's the question that is underlying this question I wrote on the board. If I have a norm space, so my initial data is the norm, when can I tell that that norm comes from an inner product? And so you'll prove this basically by direct calculation, but this is the following parallelogram law, which is the following, if H is a pre-Hilbert space, then for all u, v, and H, if I take the norm of u plus v squared, and I add the norm of u minus v squared, this is equal to twice the norm of u squared plus the norm of v squared. So this is a condition that is stated purely in terms of the norm. Right? Moreover, if H is a norm space satisfying star, then H is a pre-Hilbert space. In other words, although it seems like we defined Hilbert spaces or pre-Hilbert spaces initially in terms of an inner product, in fact, you can say a norm space is a pre-Hilbert space if and only if it satisfies this parallelogram law. So if you have on your hands just the norm, then as long as that norm satisfies this identity, that norm can be derived from an inner product. So using this theorem, you can check that the answer is only for p equals 2. OK? So in other words, you can come up with u and v so that this inequality is not satisfied when p is not equal to 2. OK? OK. So now we have the notion of a Hilbert space where this norm given in terms of an inner product, the space is complete. Now we have an inner product, so we can start talking about vectors being orthogonal to other vectors or with the normal sets, which when I first started lecturing about this stuff, I was already using the terminology, this thing being orthogonal to this thing or something like that. So of course, if you don't remember what those words mean from linear algebra, I'll briefly, I'll quickly remind you. So suppose we're in a pre-Hilbert space. We say that two elements u and v are orthogonal if their inner product is 0. And if instead of saying orthogonal, I want to write this in words, I'll write u perp v. OK? So throughout, H is going to be a pre-Hilbert space if I don't actually write it. If H is a pre-Hilbert space, a subset, which I'll denote by E lambda, lambda in capital lambda, so just some subset indexed by some indexing set, capital lambda, we say this set is orthonormal if for all lambda, each one of these vectors in the subset has unit length and any two different indexed elements implies that they are orthogonal. OK? So maybe this notation scares you because what's this indexing set? Typically, we just use the natural number. So let me just make that remark, although I'll make a few remarks in general about orthonormal sets that are not necessarily indexed by the natural numbers. We'll mainly be interested in a finite set or a countably infinite set. OK? So although an orthonormal subset of H could be a very crazy type of subset, mainly we're going to be interested only in finite or countably infinite orthonormal sets. So what are some examples? OK, so simplest examples, if so this is an example of a set of orthonormal or an orthonormal subset of C2 or 1. This is also an example of an orthonormal subset of C3. Let's say using notation from before, if I denote by e sub n, this is the sequence consisting of 0's up until I hit the n spot, and then 0 afterwards, and entry, which is an element of little l2, then this is an orthonormal subset of l2. OK? One other example is, let's look at the functions 1 over square root of 2 pi times e to the i n x. And let's think of these as elements of l2 minus pi to pi. OK, then this is an orthonormal subset of, might write o n instead of writing out orthonormal, but this is an orthonormal subset of l2, right? Since if I take the inner product of two of these guys, I get, let's say I just look at two different ones, m does not equal n. So then I take the inner product of i m x with inner product of i n x complex conjugate. So that's the inner product on big L2, and this is equal to 1 over 2 pi times minus pi to pi. Now, if m equals n, then this, I just get the length of e to the i m n x squared. The length of e to the i m x is 1, and therefore I would just get 1 here dx over 2 pi, which gives me 1. But now they're not equal, so I get e to the i m minus n x dx. And now the integral of this creature, so here e to the i, let's say I'm going to come up with some e to the i y. When y is a real number, this is by definition cosine of y plus i sine y. Now, you can check that fundamental theorem of calculus still holds for e to the i m minus n over x, and this equals e to the i m minus n x over i times m minus n. And e to the i m minus n times x is 2 pi periodic, so when I evaluate it at minus pi, I get the same value if I evaluate it at pi, so this equals 0. OK, so that's an orthonormal subset of big L2. So this collection of vectors, and I'm calling them vectors, but this collection of elements in big L2 is still countable, even though I'm indexing it by the integers rather than the natural numbers. OK, so we have the following. Now, most of what I'm going to say with regards to countable subsets of orthonormal, so countable orthonormal sets, still carries through to possibly uncountable orthonormal sets where now an infinite sum over an uncountable number of elements has to be defined in a precise way, but I will really just stick mostly to the countable case. And if you're interested, you can always look that stuff up. So I should say, what I haven't said is that whether or not the LP spaces, let's say over an interval a, b, or over r, are separable or not, or even over a, yeah, so let's stick to either a closed and bounded interval over or r. Now, why are these spaces separable, meaning they have a countably dense subset, so including big L2? The reason is the following. So what you proved in the assignment is that the continuous functions are dense and big LP, for p between p equals 1 to infinity, strictly less than infinity. They're not dense in L infinity. So as long as you stay away from that, they're dense. So continuous functions are dense in LP. Now, what's one way to approximate a continuous function? Going back to your introductory analysis class, hopefully you covered what's called the Weierstrass approximation theorem, which says that for every continuous function, you can approximate it uniformly on the interval by a polynomial. So that shows that polynomials are dense in all the LP spaces, of course, not L infinity. Now, how do you go from the set of polynomials, which is uncountable to a countable, which is dense in LP but is uncountable to a countable dense subset, which is dense in LP? Make everything rational. The set of polynomials with rational coefficients is, in fact, countable. And you can approximate every polynomial with real coefficients on a closed and bounded interval by a polynomial with rational coefficients. This is not too difficult to believe. And therefore, the polynomials with rational coefficients are dense in the LP spaces as long as I'm not in L infinity. And therefore, all the LP spaces are separable. Now, the little LP spaces are also separable because, except for L infinity also, which is not separable, because, first off, a dense subset of the, let's say, little l2, let's make things definite, is the subspace consisting of all sequences which terminate after some entry. In other words, it's 0 after that. Convince yourself that this subspace of all finitely terminating sequences, this is dense in little lp for p between 1 and infinity, not equal to infinity. So finitely terminating sequences are dense in little lp. Unfortunately, that's, again, an uncountable. I mean, it's a subspace, so it's going to be uncountable. So how do I get now a countable thing? Again, I replace everything by rational numbers. So if I can approximate every sequence which terminates after a certain point consisting of real numbers by a sequence full of just rationals terminating after a certain point by just choosing the rationals very close to those real numbers. We're using here, and I didn't say this explicitly a minute ago, that we have the density of the rationals in the reals. For every real, I can find a rational very close to it. So this is kind of thinking that goes in, goes on. But now the set of all finitely terminating sequences with rational coefficients, this is a countable set. And that countable set is dense in little lp. And therefore, little lp is separable as long as p is between 1 and infinity, excluding infinity. So I said at the beginning that we're going to be mainly interested in separable spaces without actually saying why little lp or little l2 and big l2 are separable. But I just gave you the argument by word of mouth now instead of actually writing it down. So we have the following Bessel's inequality for countable orthonormal subsets. So if En is, well, let me just put n here. If this is a countable, meaning it's either finite or it's countably infinite, but it's countable, orthonormal subset of a pre-Hilbert space H, then for all u and H, if I look at the sum of squares u inner product E sub n, this is less than or equal to the norm of u squared. So our discussion here of orthonormal subsets is taking place within a pre-Hilbert space. We don't need Hilbert spaces to talk about these concepts yet. But when we're in a Hilbert space and we have a certain orthonormal subset, that will be important that we're in a Hilbert space. OK, so the proof is let's do the finite case first. So suppose I have a finite orthonormal subset of H, or I'll often say finite collection of orthonormal vectors in H, an O-N subset of H, an O-N standing in for orthonormal. Then let me just record a few identities, which are pretty easy to verify, sum from n equals 1 to n of u inner product E n, E n. If I take the norm of this thing squared, let's compute this out. This is the inner product of with the understanding that n is going from 1 to capital N. Let's use a different index here. And this is equal to n M u E n. Now I have this sum here of, oh, I should be leaving out E n. U E m complex conjugate times the inner product of E n with E m. That's the two vectors that are taking the complex conjugate of. This is a number here. It just comes out. This number here gets hit with a complex conjugate when it comes out. Now the inner product of E n with E m is 0 when n does not equal m and is equal to 1 because it's equal to the norm of E n squared when n equals m. So all I pick up from this double sum, which is just a finite double sum, is when n equals m, and n going from 1 to n. And therefore, this is equal to U E n squared. And that's one formula I want to have. Another one is that if I take the inner product of U with n equals 1 to n of the sum, and maybe you recognize what this sum here actually is. I'll say so in a minute. So this is equal to sum from n equals 1 to n of U inner product E sub n times this number. This number comes out and gives me a complex conjugate. And therefore, 0, which is bigger than or equal to the norm of U minus n equals 1 to n of U E n E n. Now if you remember back from linear algebra or from calculus that if I have orthonormal vectors and I have a vector U, this quantity is nothing but the projection of U onto the span of those orthonormal vectors. So what I'm looking at here is, if you like, the norm of the part of U that's orthogonal to these finitely many vectors. So this thing's bigger than or equal to 0. We use that formula of how to compute the norm of something plus something. And this is equal to norm of U squared plus norm n equals 1 to n U E n E n squared minus 2 times the real part of U inner product sum from n equals 1 to n of U E n E n. And now we know what all of these things are. This is equal to this quantity here, as is this inner product. It's also equal to this thing here. So the real part is equal to this thing here, because this is a real number. And I get, since this comes with a 2, I cancel one of those. So I get norm of U squared minus sum from n equals 1 to n of U E n squared, which is exactly what I wanted to prove for the finite case. But the infinite case then follows from the finite case and by letting capital N go to infinity. So infinite case, suppose E n equals 1 to infinity is orthonormal subset of H. Then we know that for all N, capital N, we have that sum over n equals 1 to n of U E n. This is less than or equal to norm of U squared. So I can just send capital N to infinity to get that the sum equals 1 to infinity of norm of U E n squared is less than or equal to norm of U squared. OK. So orthonormal subsets we can define as a collection of vectors that have unit length and are mutually orthogonal to each other. Now just orthonormal subsets, just any old orthonormal subset is not really the most useful thing if we're trying to study the entire Hilbert's or pre-Hilbert space H because we may miss something if we leave out certain orthonormal vectors. So but a more useful type of orthonormal set is a maximal orthonormal set, which is defined as follows. An orthonormal subset E lambda, lambda and capital lambda, of a pre-Hilbert space H is maximal. So again, if having a possibly uncountable collection of orthonormal subsets indexed by some indexing set makes you uncomfortable, replace this with n, where n is going from 1 to capital N or n is going from 1 to infinity. So a countable collection, if you like. But I'm stating this so that you know that something more general is true. So this is maximal if what? The following holds. If u is in H and u is orthogonal to everything in this orthonormal subset, this implies that u is 0. OK? So an example, of course, is you can check that this collection here is a maximal orthonormal subset of C2. A non-example, you can maybe see this coming. The one we had a minute ago, this is not maximal since there's a vector that has inner product 0 with both of these but is non-zero. Since this should be C3. Since, we'll just write it kind of loosely this way. Since this vector 0, 1, 0 is orthogonal to both of these, but it's non-zero. Maximal means if you're orthogonal to everything in your collection, then it has to be 0. But this is non-zero and it's orthogonal to everything to these two vectors there. Another example is, again, with the notation from before, this is where this is the sequence that is 0 except for the n spot where it is 1. This is maximal orthonormal subset of little l2. Now, what we're going to see very shortly is that if we have a countably infinite maximal subset of a Hilbert space, then that set serves kind of the same purpose as a basis, as a orthonormal basis does in linear algebra. I mean, if you look at here already, this orthonormal set is maximal and it also forms a basis for C2. This was not maximal and you see it doesn't form a basis for C3. So maximal is going to kind of give us a condition which is kind of equally useful as being a basis. But it won't be a Hamil basis. These subsets won't be a Hamil basis in the sense that every vector can be written as a finite linear combination of the elements of a maximal orthonormal subset. But what is true is that we can write it as possibly an infinite sum involving the maximal orthonormal subset, which is in most cases just as good if you want to use that. So first off, does every pre-Hilbert space have a maximal subset? Let me state this as a theorem. In fact, I'll state two theorems. The first is every, I should say, non-trivial because we could have the Hilbert space just be the zero vector. Pre-Hilbert space has a maximal orthonormal subset. Whether it's separable or not, it has a maximal orthonormal subset. And the way you prove this is using, so I'm not going to give a proof of this. I'm going to give you a proof of something a little less strong but about as useful as we'll need. One proves this by using Zorn's Lemma, by taking your set that you're going to put a partial order on to be the collections of max of orthonormal subsets and then ordered by inclusion. And then one can do a Zorn's Lemma argument and apply Zorn's Lemma to obtain a maximal orthonormal subset. But that's kind of hands off. And maybe that scares you a little bit because Zorn's Lemma is equivalent to the axiom of choice. So if you don't like using the axiom of choice, maybe you have a problem with using it to construct a maximal orthonormal subset of a pre-Hilbert space. But we can actually do this by hand if the Hilbert space is separable. So this is a theorem we'll actually prove. Every non-trivial separable, meaning the pre-Hilbert space has a countably dense subset, every non-trivial separable of pre-Hilbert space has a countable maximal orthonormal subset. Which, as I said, this is kind of the main types of orthonormal subsets we'll be interested in just because defining infinite sums is easier to do over a countable index than it is an uncountable index. But anyways, so we're going to prove and actually construct this essentially by hand using the process that's the name of this little section, the Gram-Schmidt process. So if you remember from linear algebra, if you have a collection of vectors, you can always find an orthonormal collection of vectors that span the vector space that is spanned by the original set of vectors. And that's what we'll do. So since H is separable, let B be a countable dense subset of H. And this is a non-trivial pre-Hilbert space, so we can always make sure that the first one is a non-zero vector. So what countable means, dense remember, means that for any element in H, there exists an element from this sequence or from this collection that's with an epsilon of that vector from H. So now I'm going to make the following claim. This is essentially the Gram-Schmidt process, which we'll prove by induction. For all n, natural number, there exists another integer, natural number m of n, and an orthonormal subset e1 to em such that the following is true. So first is the span of e1 up to em of n equals the span of v1 up to vn. And so you see, n is changing. So maybe for each time I change n, I get a different orthonormal subset, or a wildly different orthonormal subset from the integer before it. So the property of these subsets are that I'm just simply adding a vector or not. So and e1 up to em of n, so this collection, is equal to the previous collection union either the empty set if vn is in the span of v1 up to vn minus 1, and some new vector e of m sub n. And I'll tell you what e of m sub n is otherwise. So what I'm saying here is that I have this countably infinite list of v's, and for each n, I can come up with a finite orthonormal subset that spans the same span as v1 up to vn. And at each stage, all I do is add a vector or not, depending on if the next v is in the span of the previous or not. So I hope that's clear. So to prove this by induction, so proof of claim, this is by induction. So let's do the base case, n equals 1. Let's take e1 to be v1 over length of v. So now we're started. Now we've got our first vector in this list that we're building up. Inductive step. Let's call this what I want to prove star. So suppose star holds for this whole claim here. I shouldn't say just a and b, but the whole claim holds for n equals k. And now I want to prove it holds for n equals k plus 1. So what kind of vector do I need to add to the previous collection of vectors to now span v1 up to vk plus 1? So if vk plus 1 is in span of v1 up to vk, then e1 up to em of k, I should say the span of these vectors, equals the span of v1 up to vk equals, because vk plus 1 is in this span, this span is also equal to vk plus 1. And therefore, this case is handled and we're in this spot where we don't add anything to the previous collection. This proves what we wanted to for n equals k plus 1 by not adding anything. So this was in the case that vk plus 1 is in the span of v1 up to vk. So now let's do the more interesting case of v1 not equal or not in the span of v1 up to vk. So now suppose vk plus 1 is not in span of v1 up to vk. I define wk plus 1 to be vk plus 1 minus its projection onto the previous list of orthonormal vectors. So sum from j equals 1 to m of k vk plus 1 ej ej. Then first note that this vector cannot be, so wk plus 1, this vector cannot be 0. Otherwise, this would imply that vk plus 1 is equal to this quantity here. And therefore, vk plus 1 is in the span of the ej's for j between 1 and m sub k, which is equal to the span of the vj's for 1 up to k. But we're assuming this is not in the span of, I should say this is equal to the span of e1 up to em sub k. That's what we're assuming. That's the inductive hypothesis. So then this vector does not equal 0. And define em sub k plus 1, or m to be the vector wk over its length of plus 1. Now this is a unit vector. And what? And if I take, so now I claim this is orthogonal to e1 up to e of m sub k. If I take emk plus 1 inner product e sub n sub j, so let's take this is equal to simply by definition 1 over length of mk plus 1 times the inner product of vk plus 1 minus sum from j equals 1 to m sub k. So remember, this is supposed to be the projection onto the e1 up to em sub k. And vk plus 1 minus that is supposed to be orthogonal to those guys. So we'll see that this ends up being 0. And let me change this j to an l since I'm, or no. And I get 1 over times the sum times vk plus 1 e sub l minus now e sub l inner product with the sum. Now this is the sum from j equals 1 to m sub k of this number here times the inner product of e sub j inner product e sub l. Now the inner product of e sub j and e sub l is 0 unless j equals l and 1 if j equals l. And so I only pick up the j equals l part of this sum when this inner product hits e sub l. In particular, I only pick up the coefficient in front since these have unit length. So I get v sub k plus 1 e sub l equals 0. Then this guy now does the job. So I would write a little bit more, but I'm running short on time. So we've proved the claim. And now let's use this to conclude that this collection of all of these ELs are maximal. So we let S to be e sub n, which equals. And S is an orthonormal subset of H. So I may not be adding any more vectors after a certain point and therefore just have a finite collection. That's also possible. But I may also have a countably infinite collection. So this is an orthonormal set by v. And so we now show x is maximal. All right, so we haven't used anything about the nature of this subset of H. All we've used is that it's countable. So we could do this step by step where we construct these orthonormal vectors that span each finite equal to the span of each finite collection of vectors. Now to show it's maximal, this is where we use the fact that this collection of v's were dense or are dense in H. So suppose u is in H. And for all l, u of e sub l equals 0. So this is either a finite set, so l is between 1 and n, or it's a countable set and l is going from 1 to infinity. So this is equal to e1 up to e m sub n for some n, or e1 is countably infinite. OK, so since vj, this is a dense subset of H, we can find a sequence of elements from this collection. So v of j sub k, k equals 1 to infinity, such that vj of k converges to u as k goes to infinity. Now by property A, the fact that this span is equal to that span, v of j sub k is in the span of e1 up to e m of j sub k. And therefore, by Bessel's inequality, and the fact that u is orthogonal to all of these orthonormal vectors, I get that v of j sub k squared. Now v of j of k, this is in this span. So you can show that in fact, this norm is in fact equal to the sum of the coefficients that I get from taking the inner product of. If I have a vector in the span of a collection of orthonormal vectors, a finite collection of orthonormal vectors, then the norm squared is equal to the sum of the coefficients, just like in Rn. And since u is orthogonal to each of these, this I can write as, and by Bessel's inequality, this is less than or equal to the sum of these things squared is less than or equal to squared. And this goes to 0 as k goes to infinity, since the v of j sub k is converged to u. But we started off with the norm of v of j sub k, which implies 0, and therefore u, which is the limit of these, must be 0, proving that this is a maximal subset. So next time, we'll prove that these maximal countable orthonormal subsets, in fact, form a pretty good analog of bases that you find in finite dimensional linear algebra. We'll stop there.