 OK, here we go with, quiz review for the third quiz that's coming on Friday. So one key point is that the quiz covers, the quiz covers through chapter six. Chapter seven on linear transformations will appear on the final exam, but not on the quiz. So I won't review linear transformations today, but they'll come into the full course review on the very last lecture. So today I'm reviewing chapter six, and I'm going to take some old exams, and I'm always ready to answer questions. And I thought, kind of helps our memories if I write down the main topics in chapter six. So already on the previous quiz, we knew how to find eigenvalues and eigenvectors. Well, we knew how to find them by that determinant of A minus lambda I equals zero. But of course there could be shortcuts, there could be, like, useful information about the eigenvalues that we can speed things up with. OK. Then the new stuff starts out with a differential equation. So I'll do a problem, I'll do a differential equation problem first. Then the, what's special about symmetric matrices? Can we just say that in words? I better write it down, though. What's special about symmetric matrices? Their eigenvalues are real. The eigenvalues of a symmetric matrix always come out real, and there always are enough eigenvectors. Even if they're repeated eigenvalues, there are enough eigenvectors, and we can choose those eigenvectors to be orthogonal. So if A equals A transpose, the big fact will be that we can factor it into, we can diagonalize it, and those eigenvector matrix with the eigenvectors in the column can be an orthogonal matrix. So we get a Q lambda Q transpose. That's the, that in three symbols expresses a wonderful fact, a fundamental fact for symmetric matrices. OK. Then we went beyond that fact to ask about positive definite matrices, when the eigenvalues were positive. I'll do an example of that. Similar matrices, that's, now we've left symmetry. Similar matrices are any square matrices, but two matrices are similar if they're related that way. And what's the key point about similar matrices? Somehow those matrices are representing the same thing in different basis in chapter seven language. In chapter six language, what's up with these similar matrices? What's the key fact, the key positive fact about similar matrices? They have the same eigenvalues. Same eigenvalues. So if one of them grows, the other one grows. If we, if one of them decays to zero, the other one decays to zero. If A, that was, that was, or powers of A, powers of, powers of A will look like powers of B. Because powers of A and powers of B only differ by an M inverse and an M way on the outside. So if these are similar, then B to the k-th power is M inverse A to the k-th power M, and that's why I say this M, it, it does change the eigenvectors but it doesn't change the eigenvalues. So same lambdas. And then finally, I've got to review the point about the SVD, the singular value decomposition. OK, so that's what's this quiz has got to cover. And now I'll just take problems from earlier exams, starting with a differential equation. OK. And always ready for questions. So here's an exam from about the year zero. And it has a three by three, so that was, but it's a pretty special looking matrix. It's got zeros on the diagonal, it's got minus ones above, and it's got plus ones like that. So that's the matrix A. OK. Step one is find the, well, I want to solve that equation. I want to find the general solution. I haven't given you a u of zero here, so I'm looking for the general solution. So now what's the form of the general solution? With, with three arbitrary constants going to be inside it, because those'll be used to match the initial condition. So the general form is u at time t is some multiple of the first special solution. The first special solution will be growing like the eigenvalue and it's the eigenvector. So that's a pure exponential solution, just staying with that eigenvector. Of course, I haven't found yet the eigenvalues and eigenvectors. That's normally the first job. Now, there'll be a second one growing like e to the lambda two and a third one growing like e to the lambda three. So we're all done. Well, we haven't done anything yet, actually. I've got to find the eigenvalues and eigenvectors, and then I would match u of zero by choosing the right three constants. OK. So now I ask, ask you about the eigenvalues and eigenvectors, and you look at this matrix and what do you see in that matrix? Well, I guess, of course, we might ask ourselves right away, is it singular? Is it singular? Because if so, then we really have a head start, we know one of the eigenvalues is zero. Is that matrix singular? I don't know, would you take the determinant to find out, or do you, maybe you look at the first row and third row and say, hey, the first row and third row are just opposite signs, they're linear dependent, the first column and third column are dependent, it's singular. So one eigenvalue is zero. Let's make that lambda one. Lambda one, then, will be zero. OK, now we've got a couple of other eigenvalues to find, and I suppose the best, simplest way is to look at A minus lambda I. So let me just put minus lambda in here, minus ones above, ones below. But, actually, before I do it, so that matrix is not symmetric, for sure, right? In fact, it's the very opposite of symmetric. That matrix A transpose, how is A transpose connected to A? It's negative A. It's an anti-symmetric matrix, skew-symmetric matrix. And we've met, maybe, a two by two example of skew-symmetric matrices, and let me just say, what's the deal with their eigenvalues? They're pure imaginary. They'll be on the imaginary axis. There'll be some multiple of I, if it's an anti-symmetric, skew-symmetric matrix. So I'm looking for multiples of I, and, of course, that's zero times I, that's on the imaginary axis, but maybe I just do it out here. Lambda cubed, well, maybe that's minus lambda cubed, and then a zero and a zero, a zero, and then maybe I have a plus a lambda and another plus lambda, but those go with a minus sign. Am I getting minus two lambda equals zero? So, so I'm solving lambda cubed plus two lambda equals zero. So one, one root factors out lambda, and the other, the rest is lambda squared plus two. OK, this is going the way we expect, right? Because this gives the root lambda equals zero, and this gives the other two roots, which are lambda equal what? The solutions of when is lambda squared plus two equals zero, then the eigenvalues, those guys are, what are they? They're, they're a multiple of I, they're just square root of two I. When I set, when I set this equal to zero, I have lambda squared equal to minus two, right? To make that zero. And the roots are square root of two I and minus the square root of two I. So now I know what those, I'll put those in now. Either the zero T is just a one. So I, so I don't even, just, it's just a one. This is square root of two I and this is minus square root of two I. So is the solution decaying to zero? Is this a completely stable problem where the solution is going to zero? No. In fact, all these things are staying the same size. This thing is getting multiplied by this number e to the I something, T, that's a number that has magnitude one and sort of wanders around the unit circle. Same for this. So that, the solution doesn't blow up and it doesn't go to zero. OK. And to find out what it actually is, we would have to plug in initial conditions. But actually, the next question I ask is, when does the solution return to its initial value? I won't even say what's the initial value. This is a case in which the, I think this, this solution is periodic. After, at T equals zero, it starts at, with c1, c2, and c3, and then at some value of T, it comes back to that. So that's a very special question. I'm not, I do, well, let's just take three seconds, because that special question isn't likely to be on the quiz, but it comes back to the start when? Well, whenever we have e to the two pi i, that's one, and we've come back again. So it comes back to the start, period, it's periodic, when this square root of two i, shall I call it capital T for the period? For that particular T, if that equals two pi i, then e to this thing is one, and we've come around again. So the period is, T is determined here. Cancel the i's, and T is pi times the square root of two. So that's pretty neat. We get all the information about all solutions. We haven't fixed on only one particular solution, but it comes around again. So this was probably my first chance to say something about the whole family of antisymmetric, skew-symmetric matrices. OK. And then finally I ask, take two eigenvectors, again, I haven't computed the eigenvectors, and it turns out they're orthogonal. They're orthogonal. The eigenvectors of a symmetric matrix or a skew-symmetric matrix are always orthogonal. I guess my conscience makes me tell you, what are all the matrices that have orthogonal eigenvectors? And symmetric is the most important class, that's the one we've spoken about, but let me just put that little fact down here. Orthogonal x's, eigenvectors. A matrix has orthogonal eigenvectors, the exact condition, it's quite beautiful, that I can tell you exactly when that happens. It happens when A times A transpose equals A transpose times A. Any time that, that's the condition for orthogonal eigenvectors. And because we're interested in special families of vectors, tell me some special families that fit. This is the whole requirement. That's a pretty special requirement. Most matrices have, I mean, so the average three by three matrix has three eigenvectors, but not orthogonal. But if it happens to commute with its transpose, then wonderfully the eigenvectors are orthogonal. Now, do you see how symmetric matrices pass this test? Of course. If A transpose equals A, then both sides are A squared, we've got it. How do anti-symmetric matrices pass this test? If A transpose equals minus A, then we've got it again. Because we've got minus A squared on both sides. So those, that's another group. And finally, let me ask you about our other favorite family, orthogonal matrices. Do orthogonal matrices pass this test if A is a Q? Do they pass the test for orthogonal eigenvectors? Well, if A is Q, an orthogonal matrix, what is Q transpose Q? It's I. And what is Q Q transpose? It's I. We're talking square matrices here. So yes, it passes the test. So the special cases are symmetric, anti-symmetric, I'll say skew-symmetric, and orthogonal. Those are the three important special classes that are in this family. OK, that's like a comment that could have been made back in section 6.4. OK. I can pursue this with --. I better pursue the differential equations, although this question didn't ask it, to ask you to tell me how would I find this matrix exponential, e to the A t. So can I erase this? I'll just stay with this same --. How would I find e to the A t? Because how does that come in? That's the key matrix for a differential equation, because the solution is, the solution is u of t is e to the A t u of zero. So this is like the fundamental matrix that multiplies the given function and gives the answer. And how would we compute it if we wanted that? We don't always have to find e to the A t, because I can go directly to the answer without any e to the A t-s, but hiding here is an e to the A t, and how would I compute it? Well, if A is diagonalizable, if, so I'm now going to put in my usual if, if A can be diagonalized, and everybody remembers that there is an if there, because it might not have enough eigenvectors. This example does have enough, random matrices have enough. So if we can diagonalize, then we get a nice formula for this, because an S comes way out at the beginning, an S inverse comes way out at the end, and we only have to take the exponential of lambda. And that's just a diagonal matrix, so that's just e to the lambda one t, these, these guys are showing up now in the, e to the lambda n t. OK? That's a really quick review of that formula. It's something we can compute it quickly if we have done the S and lambda part. If we know S and lambda, then it's not hard to take that step. OK, that's some comments on differential equations. I would like to go on to, a next question that I started here. And it's, got several parts and I can just read it out. What we're given is a three by three matrix, and we're told its eigenvalues, except one of these is, like, we don't know, and we're told the eigenvectors. And I want to ask you about the matrix. OK. So, first question. Is the thing, is the matrix diagonalizable? And I really mean for which, for which C, because I don't know C, so my questions will all be, for which, is, is there a condition on C, does one C work, but, but what I, your answer should tell me all the Cs that work. I'm not asking, like, to, for you to tell me, well, C equal four, yeah, that checks out. I want to know all the Cs that make it diagonalizable. OK? What's the deal on diagonalizable? We need enough eigenvectors, right? We don't care what those eigenvalues are, it's eigenvectors that count for diagonalizable, and we need three independent ones, and are those three guys independent? Yes. Actually, let's look at them for a moment. What do you see about those three vectors right away? They're more than independent. They're, can you see what's, why, those three got chosen? Because it'll come up in the next parts. They are, they're orthogonal. Those eigenvectors are orthogonal. They're certainly independent. So the answer to diagonalizable is yes. All C. All C. Doesn't matter. C could be a repeated guy, but we've got enough eigenvectors, so that's what we care about. OK. Second question. For which values of C is it symmetric? OK. What's the answer to that one? If we know the same, same setup, if we know that much about it, we know those eigenvectors and we've noticed they're orthogonal, then which C's will work? So the eigenvalues of that symmetric matrix have to be real. So all real C. If C was I, the matrix wouldn't have been symmetric. But if C is a real number, then we've got real eigenvalues, we've got orthogonal eigenvectors, that matrix is symmetric. OK. Positive definite. OK. So that's, now this is a sub-case of symmetric, so we need C to be real, so we've got a symmetric matrix, but we also want the thing to be positive definite. Now we're looking at eigenvalues, we've got a lot of tests for positive definite, but eigenvalues, if we know them, is certainly a good, quick, clean test. Could this matrix be positive definite? No. No, because it's got an eigenvalue zero. It could be positive semi-definite, you know, like consolation prize. If C was greater or equal zero, it would be positive semi-definite. But it's not. No. Semi-definite, if I put that comment in semi-definite, that the condition would be C greater or equal zero. Then we'd be all right. OK. Next part. Is it a Markov matrix? Hm. Is, could this matrix be, if I choose the number C correctly, a Markov matrix? Well, what do we know about Markov matrices? Mainly we know something about their eigenvalues. One eigenvalue is always one, and the other eigenvalues are smaller, not larger. So an eigenvalue two can't happen. So the answer is no. Not a, that's never a Markov matrix. OK? And finally, could one half of A be a projection matrix? So could it, could this, could this be twice a projection matrix? So let me write it this way. Could A over two be a projection matrix? OK. What are projection matrices? They're real. I mean, they're symmetric, so their eigenvalues are real. But more than that, we know what those eigenvalues have to be. What do the eigenvalues of a projection matrix have to be? See, any, any nice matrix, we've got, we've got an idea about its eigenvalues. So the eigenvalues of projection matrices are zero and one. Zero and one only. Because p squared equals p, let me call this matrix p, so p squared equals p, so lambda squared equals lambda, because eigenvalues of p squared are lambda squared, and we must have that, so lambda equals zero or one. OK. Now if, what, what value of c will work there? So then we need, there are some value that will work, and what, what will work? c equals zero will work, or what else will work? c equal to two. Because if c is two, then when we divide by two, this, this eigenvalue of two will drop to one and, and so will the other one. So, or c equal to two. OK. Those are the guys that will work, and then, and it was the fact that those eigenvectors were orthogonal, the fact that those eigenvectors were orthogonal carried us a lot of the way here. If they weren't orthogonal, then symmetric would have been dead, positive definite would have been dead, projection would have been dead. But those eigenvectors were orthogonal, so it came down to the eigenvalues. OK. That's a, that was like a chance to review a lot of this chapter. So I take the, so I jump to the singular value decomposition then as the third topic for, for the review. OK, so I'm going to jump to this. OK. So this is the singular value decomposition, known as, to everybody as the SVD. And that's a factorization of A into orthogonal times diagonal times orthogonal. And we always call those u and sigma and v transpose. OK. And the key to that, this is the, this, this is for every matrix, every A, every A, rectangular, doesn't matter, whatever, can be, has, has this decomposition, so it's really important. And the key to it is to look at things like A transpose A. Can we remember what happens with A transpose A? If I just transpose that, I get v sigma transpose u transpose. That's multiplying A, which is u sigma v transpose. And the result is v on the outside, u transpose u is the identity, because it's an orthogonal matrix. So I'm just left with sigma transpose sigma in the middle. That's a, that's a diagonal, possibly rectangular diagonal by its transpose, so the result is, this is orthogonal diagonal orthogonal. So, I guess, actually, this is the SVD for A transpose A. Here I see orthogonal diagonal and orthogonal. Great. But little more is happening for A transpose A. The difference is the orthogonal guys are the same. It's v and v transpose. What am I seeing here? I'm seeing the factorization for a symmetric matrix. This thing is symmetric. So in a symmetric case, u is the same as v. u is the same as v for this symmetric matrix, and of course we see it happening. OK. So that tells us right away what these, what v is. v is the eigenvector matrix for A transpose A. OK. Now, you, if you were here when I lectured about this topic, when I gave the lecture on singular value decompositions, you'll remember that I got into trouble. I'm sorry to remember that myself, but it happened. OK. How did it happen? I did, I was in great shape for a while, cruising along. So I found the eigenvectors for A transpose A. Good. I found the singular values. What were they? What were the singular values? The singular value, singular value number i for, these are the guys in sigma, this has, this is diagonal with the numbers sigma, and this diagonal is sigma one, sigma two, up to the rank, sigma r, those are the nonzero ones. So I found those, and what are they? Remind me about that? Well, here I'm seeing them squared, so their squares are the eigenvalues of A transpose A. Good. So I just take the square root. If I want the eigenvalues of A transpose, if I want the sigmas and I know these, I take the square root, the positive square root. OK. Where did I run into trouble? Well, then my final step was to find u. And I didn't read the book. So I did something that was practically right, but, well, I guess practically right is not quite the same. OK. So I thought, OK, I'll look at A A transpose. What happened when I looked at A A transpose? Let me just put it here and then I can conceal it. OK. So here's A A transpose. So that's u sigma v transpose, that's A, and then the transpose is v sigma transpose u transpose. Fine. And then in the middle is the identity again, so it looks great. u sigma sigma transpose u transpose. Fine. All good. And again, the, now the eigenvector, now these columns of u are the eigenvectors, that's u is the eigenvector matrix for this guy. That was correct, so I did that fine. Where did something go wrong? A, a sign went wrong. A sign went wrong because, and now I see, actually somebody told me right after class, we can't tell from this description which sign to give the eigenvectors. If I, if this is, if these are the eigenvectors of this matrix, well, if you give me an eigenvector and I change all its signs, you, we've still got another eigenvector. So what I wasn't able to determine, and I had a fifty-fifty chance and life let me down, the signs I just happened to pick for the eigenvectors, one of them I should have reversed the sign. So from this, I can't tell whether the eigenvector or its negative is the right one to use in there. So the right way to do it is to, having settled on the signs, the Vs also, I don't know which sign to choose, but I choose one. I choose one. And then the, then the, I should have used instead, I should have used the one that tells me what sign to choose, the rule that A times a V is sigma times the U. So having decided on the V, I multiply by A, I'll notice the factor sigma coming out, and there'll be a unit vector there, and I now know which, you know, what the, exactly what it is, and not only up to a change of sign. So that's the good, and, and of course, this is the main point about the SVD. That's the point that we've diagonalized. That's A times the matrix of Vs equals U times the diagonal matrix of sigmas. That's, that's the same as that. OK. So that's like correcting, the, the, the wrong sign from that earlier lecture. And that would complete that, so that's how you would compute the SVD. Now, on the quiz, am I going to ask, well, maybe on the final, so we've got quiz and final ahead, sometimes you might be asked to find the SVD if I give you the matrix. Let me come back now to the main board. Or I might give you the pieces. And I might ask you something about the matrix. For example, suppose I ask you, oh, let's say, well, I, if I tell you what sigma is, well, let's, OK, let's take one example. Suppose sigma is, so all, all that's how we would compute them, but now suppose I give you these. Suppose I give you sigma is, say, three, two. And I tell you that U is, has a couple of columns, and V has a couple of columns. OK. Those are orthogonal columns, of course, because U and V are orthogonal. I'm just sort of, like, getting you to think about the SVD, because we only had that one lecture about it and one homework, and, what kind of a matrix have I got here? What do I know about this matrix? All I really know right now is that it's singular values, those sigmas are three and two. And the only thing interesting that I can see in that is that they're not zero. I know that this matrix is nonsingular, right? That's invertible, I don't have any zero eigenvalues, any zero singular values, that's invertible. There's a typical SVD for a nice two by two, nonsingular, invertible, good matrix. If I actually gave you a matrix, then you'd have to find the U's and the V's as we just spoke, but there. Now what if the two wasn't a two, but it was, well, let me make an extreme case here, suppose it was minus five. That's wrong, right away. That's not a singular value decomposition, right? The singular values are not negative. So that's not a singular value decomposition and forget it. OK. So let me ask you about that one. What can you tell me about that matrix? It's singular, right? It's got a singular matrix there in the middle. And let's see, can you, so, OK, it's singular, maybe you can tell me, it's rank? What's the rank of A? It's clearly, somebody just say it, one, thanks. The rank is one. So the null space, what's the dimension of the null space? One, right? We've got a two by two matrix of rank one, so all that stuff from the beginning of the course is still with us. The dimensions of those fundamental spaces is still central. And a basis for them. Now, can you tell me a vector that's in the null space? And then I, that's, that'll be my last point to make about the SVD. Can you tell me a vector that's in the null space? So I'm, it's really, it's somehow, what would I multiply by and get zero here? I think the answer is probably v2. I think probably v2 is in the null space, because I think that must be the eigenvector going with this zero eigenvalue. Yeah. Have a look at that. And the, and, and I could ask you the null space of A transpose. And I could ask you the column space, all that stuff. Everything is sitting there in the SVD. The SVD takes a little more time to compute, but it displays all the good stuff about a matrix. OK. Any question about the SVD? Let me keep going with, further topics. Now, let's see. Similar matrices we've talked about? Let me, let me see if I've got another, OK. Here's a true-false, so we can do that, easily. So, question. A given. A is symmetric and, and orthogonal. OK. OK. So, beautiful matrices like that don't come along every day. But, what can we say first about its eigenvalues? Actually, of course. We got, here are two most important classes of matrices, and we're looking at the intersection. So those really are neat matrices. And what can you tell me about the, what could the possible eigenvalues be? Eigenvalues can be what? What do I know about the eigenvalues of a symmetric matrix? Lambda is real. What do I know about the eigenvalues of an orthogonal matrix? Ha. Maybe nothing, but no, that can't be. What do I know about the eigenvalues of an orthogonal matrix? Well, what feels right? Basing mathematics on just a little gut instinct here. The eigenvalues of an orthogonal matrix ought to have magnitude one. Orthogonal matrices are like rotations, they're not changing the length. So orthogonal, the eigenvalues are one. Let me just show you why. Why? So the matrix, let's, let's call it, can I call it Q for orthogonal for the moment? If I look at Qx equal lambda x, how do I see that this thing has magnitude one? I take the length of both sides. This is taking lengths, taking lengths, this is whatever the magnitude is, times the length of x. And what's the length of Qx, if Q is an orthogonal matrix? This is something you should know. It's the same as the length of x. Orthogonal matrices don't change lengths. So, so lambda has to be one. Right. OK. That's worth committing to memory, that could show up again. OK. So what's the answer now to the, to, to this question, what can the eigenvalues be? There's only two possibilities, and they are one and the other one, the other possibility is negative one. Right. Because these have the right magnitude and they're real. OK. OK. True, OK, true or false, A is sure to be positive definite. Well, this is a great matrix, but is it sure to be positive definite? No. If it could have an eigenvalue minus one, it wouldn't be positive definite. True or false, it has no repeated eigenvalues. That's false, too. In fact, it's going to have repeated eigenvalues, if it's as big as three by three, one of these, one of these at least will have to get repeated. Sure. So it's got repeated eigenvalues. But is it diagonalizable? It's got these many, many repeated eigenvalues. If it's fifty by fifty, it's certainly got a lot of repetitions. Is it diagonalizable? Yes. All symmetric matrices, all orthogonal matrices can be diagonalized. And in fact, the eigenvectors can even be chosen orthogonal. So it can be sort of like diagonalized the best way with a Q and not just any old S. OK. Is it non-singular? Is the, is a symmetric orthogonal matrix non-singular? Sure. Orthogonal matrices are always non-singular. And obviously we don't have any zero eigenvalues. Is it sure to be diagonalizable? Yes. Prove that, now here's a final step, to show that one half of A plus I is a, this is proof, one half of A plus I is a projection matrix. OK? Let's see, what do I do? I could see two ways to do this. I could check the properties of a projection matrix, which are what? A projection matrix is symmetric, well, that's certainly symmetric, because A is. And what's the other property? I should square it and hopefully get the same thing back. So can I do that, square and see if I get the same thing back? So if I square it, I'll get one quarter of A squared plus two A plus I, right? And the question is, does that agree with the thing itself, one half A plus I? I guess I'd like to know something about A squared. What is A squared? That's our problem. What is A squared? If A is symmetric and orthogonal, A is symmetric and orthogonal. This is what we're given, right? It's symmetric and it's orthogonal. So what's A squared? I. A squared is I. Because A times A, if A, A equals its own inverse. So A times A is the same as A times A inverse, which is I. So this A squared here is I. And now we've got it. We've got two identities over four, that's good. And we've got two As over four, that's good. OK. So it turned out to be a projection matrix safely. And we could also have said, well, what are the eigenvalues of this thing? What are the eigenvalues of a half A plus I? If the eigenvalues of A are one and minus one, what are the eigenvalues of A plus I? Just stay with it this last thirty seconds here. What, what are the, if I, if I know these eigenvalues of A and I add the identity, the eigenvalues of A plus I are zero and two. And then when I divide by two, the eigenvalues are zero and one. So it's symmetric, it's got the right eigenvalues, it's a projection matrix. OK. You're seeing a lot of stuff about eigenvalues and special matrices and that's what the quiz is about. OK. So good luck on the quiz.