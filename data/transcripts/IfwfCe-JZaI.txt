 OK. I want to begin by giving some comments regarding the Wikipedia assignment. So I sent out an email about this last night. And so first of all, thank you for your contributions to this assignment to Wikipedia. It plays a really important role in educating a wider audience what the subject is about. Because as many of you have experienced, the first time if you heard of some term, you have no idea what it is. You put it into Google. And often Wikipedia's entry is one of the top results that come up. And what gets written there actually plays a fairly influential role in educating a broader audience about what this topic is about. And so I want to emphasize that this is not simply some homework assignment. It's something that is a real contribution. And it's something that contributes to the dissemination of knowledge. And for that, it is really important to do a good job, to do it right, to do it well, so that next time someone, maybe even yourselves, if you've forgotten what the subject is about and go back and you want to look it up again and remind yourself, you will have a useful resource to look into. But also, let's say someone wants to find out what is extremal graph theory about? What is additive combinatorics about? You want them to land on a page that points you to the right type of places, that points you to useful resources, that opens doors so that you can explore further. And some of the contributions indeed serve very well in that purpose. It opens doors to many things. And part of the spirit of this assignment is for you to do your own research, do your own literature research, to learn more about the subject, more than what has been taught in these lectures, so that you can write about it on Wikipedia. You can link to more references, show the world what this subject is about. Continuing with our program, so we've spent the past few lectures developing tools regarding the structure of set addition so that we can prove Freiman's theorem. So that's been our goal for the past few lectures. And today, we'll finally prove Freiman's theorem. But let me first remind you the statement. So in Freiman's theorem, we would like to show that if you're in the integers, you have a set A that has bounded doubling, doubling constant, constant k. Then the set must be contained in a small, generalized arithmetic progression. Bound the dimension and size only a constant factor larger than A. We developed various tools the past three lectures, building up to various intermediate results. But we also collected this very nice set of tools for proving Freiman's theorem. So let me review some of them, which we'll encounter again today. Planck-Aruzza inequality tells you that if you have a set with small doubling, then the further iterated sums are also controlled. So I want you to think of these parameters as k is a constant. So k to the some power is still a constant. But also, I don't really care about polynomial changes in k. So I should ignore polynomial changes in k and view this constant more or less as the original k itself. So if the A plus A is around the same size as A, then further iterations also do not change the sizes very much. Ruzza covering lemma, so this was some statement that if x plus b looks like it could be covered by copies of b just in terms of their sizes alone, then in fact, x could be covered by a small number of translates of a slightly larger ball. But here, b can be any set. We have this thing called Ruzza modeling lemma. In particular, a consequence of it is that if A has small doubling, then there exists a prime n that's not too much bigger than the size of A and a very large proportion, an eighth of a subset of A, such that the subset A prime is prime and 8-isomorphic to a subset of Z mod n. So even though you start with a set that's potentially very spread out, provided that it has small doubling, I can pick out a pretty large piece of it and model it by something in a fairly small cyclic group. And here, the modeling is 8-isomorphic, so it preserves sums up to 8-term sums. We had Bogolyubov's lemma, so now we're inside a small cyclic group. With a large subset of a small cyclic group, then Bogolyubov's lemma says that 2A minus 2A contains a large Bohr set, a large structure within this iterated sum set. And last time, we showed that via geometry of numbers, via Minkowski's second theorem, one can deduce that every Bohr set of small dimension and small width contains, of large width, contains a proper GAP that's pretty large. So putting these two together, putting the last two things together, we obtain that if you have a subset of the cyclic group and n is prime, so here, the previous statement, n is prime, so n is prime, then an A is pretty large, then 2A minus 2A contains a proper generalized arithmetic progression of dimension, at most, alpha to the minus 2, and size at least 1 over 4D to the D times n. So just starting from the size of A, 2A minus 2A contains a pretty large GAP. So we're going to put all of these ingredients together and show that you can now contain the original set A in a small GAP. So just from knowing that some subset of it, 2A minus 2A, so 2A prime minus 2A prime contains this large GAP, we're going to use it to boost it up to a cover. OK, so now let's prove Freiman's theorem. Using the modeling lemma, using the modeling lemma, the corollary of the modeling lemma, we find that since A plus A is size, at most, k times the size of A, there exists some prime n, at most, 2k to the 16th times A. And so I'm just copying the consequence of this modeling lemma. So I find a pretty large subset of A, such that A prime is Freiman 8-isomorphic to a subset of Z mod n. Now, applying the final corollary with alpha being the size of this A prime, which is at least the size of A over n, which is at least 1 over 16 times k to the power 16, so all constants, we see that 2A prime minus, so let me actually change the letters and call A prime B, so I don't have to keep on writing primes. So subset of A is called B. So 2B minus 2B now contains a large GAP. And the GAP has dimension d bounded. So the dimension is bounded by alpha to the minus 2. So it's some constant. And the size is pretty large. So size is at least 1 over 4D to D. If you only care about constants, just remember that everything that depends on k or D is a constant. OK, because B is Freiman 8-isomorphic, B is Freiman 8- isomorphic to, sorry. B is A prime is a subset of A. And B is a subset of Z mod. B is a subset of Z mod n, that A prime is 8-isomorphic to. So since B is 8-isomorphic to A prime, every GAP in B, so if you think about what 8-isomorphism preserves, you find that if you look at 2B minus 2B, it must be 2 Freiman 2-isomorphic to 2A prime minus 2A prime. So the point of Freiman isomorphism is that we just want to preserve enough additive structure. Well, we don't need to preserve all the additive structure, just enough additive structure to do what we need to do. And being able to preserve an arithmetic progression, or in general, a generalized arithmetic progression, requires you to preserve Freiman 2-isomorphism. And that's where the A comes in. So I want to analyze 2B minus 2B, and I want that to preserve 2-isomorphisms. So initially, I want B to preserve 8-isomorphism. So 2B minus 2B is Freiman isomorphic to 2A prime minus 2A prime. So the GAP, which we found earlier in 2B minus 2B, is mapped via this Freiman isomorphism to a proper GAP, which we'll call Q, now sitting inside 2A minus 2A, and preserving the same dimension and size. So Freiman isomorphisms are good for preserving these partial additive structures like GAPs. Yes? AUDIENCE 2 So we're using this smaller structure, B, A prime, to show a larger, a much stronger correspondence between 2B minus 2B and 2A prime minus 2A prime. YUFEI ZHAO Questions? We're using, so because we have to pass the 2B minus 2B, we want 2B minus 2B to be Freiman isomorphic to 2A prime minus 2A prime. So that's why, in the proof, I want B to be 8-isomorphic to A prime. So you see, so I'm skipping details of this step, but if you read the definition of Freiman s-isomorphism, you see that this implication holds. AUDIENCE 2 2-isomorphism is a stronger condition, right? YUFEI ZHAO No, 2-isomorphism is a weaker condition. 2-isomorphism just means that you are preserving 2y sums. Think about the definition of Freiman 2-isomorphisms. In particular, if two sets are Freiman 2-isomorphic, and you have a arithmetic progression in 1, then that arithmetic progression is also an arithmetic progression in the other. So it's just enough additive structure to preserve things like arithmetic progressions and generalized arithmetic progressions. So we found this large GAP in 2A minus 2A. So this is very good. So we wanted to contain A in the GAP. Seems like right now we're doing something slightly in the opposite direction. We find a large GAP within 2A minus 2A. But something we've seen before, we're going to use this to boost ourselves to a covering of A via the Ruzsa covering lemma. So once you find this large structure, you can now try to take translates of it to cover A. And this is, if there's any takeaway in the spirit of this proof, is this idea. Even though I want to cover the whole set, it's OK. I just find a large structure within it. And then I use translates to cover. How do we do this? So since Q is containing 2A minus 2A, we find that Q plus A is containing 3A minus 2A. Therefore, by Plunnecke-Ruzsa inequality, the size of Q plus A is at most the size of 3A minus 2A, which is at most k to the fifth power times the size of A. And I claim that this final quantity is also not so different from the size of Q. Because all of these, I mean, the point here is we are doing all of these transformations, passing down to subsets, putting something bigger, getting to something smaller. But each time, we only lose something that is polynomial in k. We're not losing much more. So we're only losing a constant factor. There is sometimes a bit more than polynomial. But in any case, we're losing only a constant factor at each step. So in particular, since n upper bounds the size of A prime, here where we ended up embedding into Z mod n, n is larger than A prime, which is at least a constant fraction of A. And the size of Q is at least 1 over 4D raised to D times n. So we find that this upper bound earlier on Q plus A, we can write it in terms of size of Q, where k prime is, we put all of these numbers together, what it is. Specifically, it doesn't matter so much. Other than that, it is a constant. D is polynomial in k. So what we have here is something that is exponential in the polynomial of k. So now we're in a position to apply the Ruzsa covering lemma. So look at that statement up there. So what is it saying? That A plus Q looks like it could be covered by Q, just in terms of size. So I should expect to cover A by a small number of translates of Q minus Q. So by covering lemma, A is contained in some x plus Q minus Q for some x in A, where the size of x is, at most, k prime. I claim, so we've covered A by something. Q is a GAP. X is a bounded size set. And I claim that this is the type of object that we're happy to have. Just to spell out some details, first, note that x is contained in a GAP of dimension x, so x minus 1, with length 2 in each direction. So add a new direction for every element of x. It's wasteful, but everything's constant. And recall that the dimension of Q as a GAP is d. So x plus Q minus Q is contained in a GAP of dimension. So what's the dimension? So when I do Q minus Q, it's like taking a box and doubling its length. I'm not changing the number of dimensions. So the dimension of Q minus Q is still d. The dimension of x is, at most, size of x. All of these things are constants. So we're happy. But to spell it out, the constant here is k prime is what we wrote up there. So this is a constant. And the size, so what is the size of the GAP that contains this guy here? So I'm expanding x to a GAP by adding a new direction for every element of x. And I might expand that size a little bit. But the size of this GAP that contains x is no more than 2 to the power x, 2 raised to the size of x. What is the size of the GAP Q minus Q? So Q is a GAP of dimension d. And we know that a GAP of dimension d has doubling constant at most 2 to the d, 2 to the d times the size of Q. And because Q is contained in 2a minus 2a, we find that Q is contained in 2a minus 2a. And 2 to the x, well, I know what the size of x is bounded by, k prime plus the size of x is at most k prime. And then I have 2 to the d over here. So 2a minus 2a by Plunnecke-Ruzsa is at most k to the 4 times the size of a. You put everything together, we find that this bound here is doubly exponential in the polynomial in k. And that's it. This proves Freiman's theorem. To recap, we went through several steps. So first, using the modeling lemma, we know that if a set A has small doubling, then we can pass a large part of A to a relatively small cyclic group. Going to work inside that cyclic group. Using Bogolyubov lemma and its geometry of numbers corollary, we then find that inside the cyclic group, the corresponding set, which we call B, is such that 2B minus 2B contains a large GAP. We pass that GAP back to the original set A, because we are preserving eight isomorphisms, Freiman 8 isomorphisms in Ruzsa modeling lemma. So we can pass to the original set A and find a large GAP in 2a minus 2a. Once we find this large GAP in 2a minus 2a, then we're going to use the Ruzsa covering lemma to contain A inside a small number of translates of this GAP. You put all of these things together, and the appropriate bounds coming from Plunnecke-Ruzsa inequalities, and you get the final theorem. And this is a proof of Freiman's theorem. AUDIENCE 2 How do you make it proper? YUFEI ZHAO The question is, how do you make it proper? Up until the step with q, it is still proper. So the very last step over here, it is you might have destroyed properness. So this proof here doesn't give you properness. So I mentioned in the beginning that in Freiman's theorem, you can obtain properness via an additional argument. So that I'm not going to show. It's some more work which is related to geometry of numbers. So for example, you can look up in the textbook by Tao and Vu to see how to get from a GAP to contain it in a proper GAP without losing too much in terms of size. So in terms of the memory, think about it this way. When do you have something which is not proper? When you have some linear dependence, when you have some integer linear dependence. And in that case, you kind of lost the dimension. When you have improperness, you actually go down a dimension. But then you need to salvage to the size, to make sure that the size doesn't blow up too much. And so there are some arguments to be done there. And we're not going to do it here. AUDIENCE 2 Well, I guess my question is more like, do they change, tweak within the proof, like say, get a stronger Q or whatever? Or do they use the same proof, but later on say that any non-proper GAP can somehow transfer to a proper GAP? YUFEI ZHAO So the question is, to get properness, do I have to modify the proof? Or can I use Freiman's theorem as written as a black box? So my understanding is that you can use the statement as a black box and obtain properness. But if you want to get good bounds, maybe you have to go into the proof. Although that, I'm not sure. Any more questions? So this took a while. So this was the most involved proof we've done in this course so far, improving Freiman's theorem. We had to develop a large number of tools. And we came up, so we eventually arrived at, it's a beautiful theorem. So this is a fantastic result that gives you an inverse structure. So something we know that GAPs have small doubling. And Convert says, if something has small doubling, it has to, in some sense, look like a GAP. And you see that the proof is quite involved and has a lot of beautiful ideas. In the remainder of today's lecture, I want to present some remarks on additional extensions and generalizations of Freiman's theorem. And while we're not going to do any proofs, there's a lot of deep and beautiful mathematics that are involved in this subject. So this is, I want to take you to a tour through some of the more things that we can talk about when it comes to Freiman's theorem. But first, let me mention a few things that I mentioned very quickly when we first introduced Freiman's theorem, namely some remarks on the bounds. OK? So the proof that we just saw gives you a bound which is basically exponential in the dimension and doubly exponential for the size blow-up. I mean, they're all constants. So if you only care about constants, then this is just fine. But you might ask, are we losing too much here? What is the right type of dependence? So what is the right type of dependence? So we saw an example. So we saw an example where if you start with A being a highly dissociated set where there is basically no additive structure within A, then you do need, so this example shows that you cannot do better than polynomial, well, actually, than linear in k in the dimension and exponential in the size blow-up. So in particular, you do need to blow up the size by some exponential quantity in k. So here, k is roughly the size of A over 2 in this example. And you can create modifications of the example to keep k constant and A getting larger. But the point is that you cannot do better than this type of dependence simply from that example. And it's conjecture that that is the truth. We're almost there improving this conjecture, but not quite, although the proof that we just gave is somewhat far, because you lose an exponent in each bound. There is a refinement of the final step in the argument. So let me comment on that. So we can refine the final step, the final steps in the proof to get polynomial bounds, which is much more in the right ballpark compared to what we got. And the idea is basically, over here, we used Ruzsa covering lemma. So we started with that q up there. Up until this point, you should think of this step as everything coming from Bogolyubov and its corollary. So that stays the same. And now the question is, starting with that q, how would you use this q to try to cover A? Well, we did as well, we said, apply Ruzsa covering lemma. Remember how the proof of Ruzsa covering lemma goes? So you take a maximal set of translates, destroying translates, and if you blow everything up by a factor of 2, then you get a cover. But it turns out to be somewhat wasteful. You see, there was a lot of wasting going from x to 2 to the x. So you could do that step more slowly. So starting with q, cover now some, not all of A, so cover parts of A by translates of q minus q set. So when you do Ruzsa covering lemma, you don't cover the whole thing, but nibble away. Cover a little bit. And then look at the thing that you get, which is the q becomes some new thing, let's say q1. And now cover more by q1 minus q1. So apparently, if you do the covering step more slowly, you can obtain better bounds. And that's enough to save you this exponent to go down to exponential bound, to go down to polynomial-type bounds for Freiman's theorem. So I'm not giving details, but this is roughly the idea. So you can modify the final step to obtain this bound. The best bound so far is due to Tom Sanders, who proved Freiman's theorem for bounds on dimension that's like k times polylog k and the size blow up to be e to the k times polylog k. So in other words, other than this polylogarithmic factor, it's basically the right answer. So this proof is much more sophisticated. So it goes much more in depth into analyzing the structure of set addition. So Sanders has a very nice survey article called the Structure of Set Addition that analyzes some of the modern techniques that are used to prove these types of results. There is one more issue which I want to discuss at length in the second half of this lecture, which is that you might be very unhappy with this exponential blow up. Because if you think about what happens in these examples, or not the examples, but if you think about what happens, like the spirit of what we're trying to say, Freiman's theorem is some kind of an inverse theorem. And to go forward, you're trying to say that if you have a GAP of dimension d, then the size blow up is like 2 to the d. So we want to say some structure applies small doubling. And Freiman's theorem tells the reverse, that you have small doubling, then you obtain this structure. And it seems like you are losing. I mean, getting from here to here, there is a polynomial type of loss, whereas going from here to here, it seems that we're incurring some exponential type of loss. And it would be nice to have some kind of inverse theorem that also preserves these relationships quantitatively. So that may not make sense in this moment, but we'll get back to it later this lecture. So the point is there's much more to be said about the bounds here, even though right now it looks as if they're very close to each other. One more thing that I want to expand on is we've stated and proved Freiman's theorem in the integers. And you might ask, what about in other groups? We also proved Freiman's theorem in F2 to the m, or more generally, groups of bounded exponent or bounded torsion, so abelian groups of bounded exponent. For general abelian groups, so Freiman's theorem in general abelian groups, you might ask, what happens here? And in some sense, what is even the statement of the theorem? We want something which combines two different types of behavior. On one hand, you have z, which is what we just did. And here, the model structures are GAPs. And on the other hand, we have, which we also proved, things like F2 to the m, where the model structures are subgroups. And there's a sense in which these are not the GAPs and subgroups. They have some similar properties, but they're not really like each other. So now, if I give you a general group, which might be some combination of infinite torsion or very large torsion elements versus very small torsion elements, so for example, take a Cartesian product of these groups. Is there a Freiman's theorem? And what does such a theorem look like? What are the structures? What are the subsets of bounded doubling? So that's the kind of thing we want to think about. So it turns out, for Freiman's theorem in general abelian groups, so there is a theorem. So this theorem was proved by Green and Ruzsa. So following a very similar type of proof framework, although the individual steps, in particular the modeling lemma, needs to be modified. And let me tell you what the statement is. So the common generalization of GAPs and subgroups is something called a coset progression. So a coset progression is a subset which is a direct sum of the form P plus H, where P is a proper GAP. So the definition of GAP works just fine in every abelian group, where you start with an initial point, a few directions, and you look at a grid expansion of those directions. P is a proper GAP, and H is a subgroup. And here, the direct sum refers to the fact that if P plus H equals to P prime plus H prime for some P and P prime in the set P and H and H prime in the set H, then P equals to P prime and H equals to H prime. So every element in here is written in a unique way as some P plus some H. So that's what I mean by direct sum. For such an object, such a coset progression, I call its dimension to be the dimension of the GAP, P. And its size, in this case, actually, is just the size of the set, which is also the size of P times the size of H. So the theorem is that if A is a subset of an arbitrary abelian group, and it has bounded doubling, then A is contained in a coset progression of bounded dimension and size bounded blowup of the size of A. And here, these constants, d and k, they are universal. They do not depend on the group. So there are some specific numbers, functions you can write down. They do not depend on the group. So this theorem gives you the characterization of subsets in general abelian groups that have small doubling. Any questions? Yeah? That's a good question. So I think you could go into their paper and see that you can get polynomial-type bounds. And I think Sander's results also work for this type of setting to give you these type of bounds. But I, yeah, so you should look into Sander's paper, and he will explain. I think in Sander's paper, he works in general abelian groups. The next question I want to address is, well, what do you think is the next question? Non-abelian groups. So Freiman's theorem in non-abelian groups, or rather the Freiman problem in non-abelian groups. Now, here's a basic question. If I give you a non-abelian group, what subsets have bounded doubling? Of course, the examples from abelian groups also work in non-abelian groups. You have subgroups. You have generalized arithmetic progressions. But are there genuinely new examples of sets in non-abelian groups that have bounded doubling? So think about that, and let's take a quick break. Can you think of examples in non-abelian groups that have small doubling that do not come from the examples that we have seen before? So let me show you one construction. And this is an important construction for non-abelian groups. So it has a name. It's called a discrete Heisenberg group, which is the matrix consisting of matrices that look like what I've written. So you have integer entries above the diagonal, 1 on the diagonal, and 0 below the diagonal. So let's do some elementary matrix multiplication to see how group multiplication in this group works. So if I have two such matrices, I multiply them together. And then you see that the diagonal is preserved, of course, but this entry over here is simply addition. So this entry here is just addition. This entry over here is also addition. And the top right entry is a bit more complicated. It's some addition, but there's an additional twist. So this is how matrix multiplication works in this group. I mean, this is how matrix multiplication works, but in terms of elements of this group, it's what happens. So you see, it's kind of like an abelian group, but there's an extra twist. So it's almost abelian. So the first step, you can take away from abelian. And there's a way to quantify this notion. It's called nilpotency, and we'll get to that in a second. But in particular, if you set S to be the following generators, if you take S to be these four elements, and you ask, what does the r-th power of S look like? So I look at all the elements which can be written by r or at most r elements from S. What do these elements look like? What do you think? So if you look at elements in here, how large can this entry, the 1, 2 entry, be? r, right? So each time you do addition, so it's at most r. So let me be a bit rough here and say it's big O of r. And likewise, the 2, 3 entry is also big O of r. What about the top right entry over here? So it grows like r squared, because there's an extra multiplication term. So you can be much more precise about the growth rate of these individual entries. But very roughly, it looks like this ball over here. So the size of S, the r-th ball of S, is roughly on the order of fourth power of r. So in particular, the doubling constant, if r is reasonably large, is what? So what happens when you go from r to 2r? So the size increases by a factor of around 16. 16. OK. So that's an example of a set in a non-abelian group with bounded doubling, which is genuinely different from the examples we have seen so far. So that's non-abelian. Yeah? AUDIENCE MEMBER 2. The size is at least order of the product of the two? YUFEI ZHAO. The question is, is the size? We've shown the sizes. This is, well, I'm not being very precise here, but you can do upper bound and lower bound. So the size turns out to be the order of r to the 4. So you want to show that there are actually enough elements over here that you can fill in, but OK, so I'll leave that to you. OK. Yeah? Can you build other examples like this one? Yeah? AUDIENCE MEMBER 3. How do we know that this isn't similar to a coset for the direct sum or a coset for gradient? YUFEI ZHAO. OK, question is, how do we know this isn't like a coset progression? Well, for one thing, this is not abelian. S, if you multiply entries of S in different orders, you get different elements. So already in that way, it's different from the examples that we have seen before. But no, you're right. So maybe we can write this as a semidirect product in terms of things we have seen before. And it is, in some sense, a semidirect product. But it's a very special kind of semidirect product. OK. For that example, you can build bigger examples, of course, with more entries in the matrix. But more generally, these things are what are known as neopotent groups. So as an example of a neopotent group, I remind you the definition of a neopotent group is a group where the lower central series eventually terminates, in particular, which means that if you look at, so this is the commutator of G. So look at all the elements that can be written as x, y, x inverse, y inverse, a set of elements that can be written this way. So that's a subgroup. And if I repeat this operation enough times, I eventually would get just the identity. And you could try it on that group. If you do the commutator, so you essentially you get rid of abelianness. And you move up the whole diagonal. And when you take the commutator, you get rid of these, you zero out these two entries. So you get z alone. If you do it one more time, you zero out that entry. And so more generally, all of these neopotent groups have this phenomenon, have this polynomial growth phenomenon. So if you take a set of generators and look at a ball and look at the volume of the ball, how does the volume of the ball grow with the radius, it grows like a polynomial. And so let me define that. So given G, a finitely generated group, so generated by set S, we say that G has polynomial growth if the size S to the R grows like at most a polynomial in R. It's worth noting that this definition is really a definition about G. It does not depend on the choice of generators. You can have different choices generators for the group. But if it has polynomial growth with respect to one set of generators, then it's the same. It also has polynomial growth with regards to every other set. So we've seen an example of groups with polynomial growth. Abelian groups have polynomial growth. So if you think of polynomial growth, think lattice, z to the n. So if you take a ball growing, so it has size growing like R to the dimension. But neopotent groups, another example of groups with polynomial growth. And you may wonder, and these are intuitively, at least for now, related to bounded doubling. If a polynomial growth, then it has bounded doubling. So is there a classification of groups with bounded, with polynomial growth? So if I tell you a group, so infinite group always, because otherwise finite, then it maxes out already at some point. So I give you an infinite group. I tell you it has polynomial growth. What can you tell me about this group? Is there some characterization that's an inverse of what we've seen so far? And the answer is yes. And this is a famous and deep result of Gromov. So Gromov's theorem on polynomial, groups of polynomial growth from the 80s, Gromov showed that a finitely generated group has polynomial growth if and only if it's virtually neopotent, where virtually is an adverb in group theory. When you have some property, like a billion, or solvable, or whatever, so virtually p means that there exists a finite index subgroup with property p. So virtually neopotent means that there's a finite index subgroup that is neopotent. So it completely characterizes groups of polynomial growth. So basically, all the examples we've seen so far are representative. So up to changing by a finite index subgroup, which, as you expect, shouldn't change the growth nature by so much. There are some analogies to be made here with, for example, in geometry. You ask, in Euclidean space, how fast does the ball of radius r grow? In dimension d, it grows like r to the d. What about in the hyperbolic space? Does anyone know how fast, in hyperbolic space, the ball of radius r grows? It's exponential in the radius. So for non-negatively curved spaces, the balls grow polynomially. But for something that's negatively curvature, in particular, the hyperbolic space, the ball growth might be exponential. You have a similar phenomenon happening here. The opposite of polynomial growth is, well, superpolynomial growth. But one specific example is that of a free group, where there are no relations between the generators. In that case, the balls, they grow exponentially. So the balls grow exponentially in the radius. Gromov's theorem is a deep theorem. And its original proof used some very hard tools coming from geometry. And Gromov developed a notion of convergence of metric spaces, somewhat akin to our discussion of graph limits. Starting with discrete objects, he looked at some convergence to some continuous objects, and then used some very deep results from the classification of locally compact groups to derive this result over here. So this proof has been quite influential. And it's related to something called Hilbert's fifth problem, which concerned characterizations of lead groups. So all of these are inverse type problems. I tell you some structure has some property. Describe that structure. OK, what does this all have to do with Freiman's theorem? Well, already you see some relation. So there seems, at least intuitively, some relationship between groups of polynomial growth versus subsets of bounded doubling. One implies the other, although not in the converse. And they are indeed related. And this comes out of some very recent work. Oh, I should also mention that Gromov's theorem has been later simplified by Kleiner, who gave an important simplification, a more elementary proof of Gromov's theorem. So let's talk about the non-Abelian version of Freiman's theorem. So we would like some result that says that it's true that every set, most every set of, OK, so previously, we had small doubling. You want to have some similar notion, although it might not be exactly small doubling. But let me not be very precise and to say, quote, small doubling, in literature, these things are sometimes also known as approximate groups. So if you look this up, you will get to the relevant literature on this subject. Most every set of small doubling in some non-Abelian group behaves like one of these known examples, something which is some combination of subgroups and nilpotent bonds. So these combinations are sometimes known as coset nil progressions. So this was something that was only explored in the past 10 years or so in a series of very difficult works. Previously, it had been known, and still, it's being investigated for various special classes of matrix groups or special classes of groups, like solvable groups and whatnot, that are more explicit or easier to handle or closer to the Abelian analog. There was important work of Khrushchevsky, which was published about 10 years ago, who showed using model theory techniques, so using methods from logic, that a weak version of Freiman's theorem is true for non-Abelian groups. And later on, Breillat, Green, and Tao building on Khrushchevsky's work. So this actually came quite a bit later, even though the journal publication dates are the same year. So they were able to build on Khrushchevsky's work, and were greatly expanding on it and going back to some of the older techniques coming from Hilbert's fifth problem, and as a result, proved an inverse structure theorem that gave some kind of answer to this question of non-Abelian Freiman. So we now do have some theorem, which is like Freiman's theorem for Abelian groups, that says in a non-Abelian group, if you have something that resembles small doubling, then this set must, in some sense, look like a combination of subgroups and neopotent balls. But let me not be precise at all. The methods here are built on Khrushchevsky, and Khrushchevsky used model theory, which is kind of, well, it's something where, in particular, one feature of all of these proofs is that they give no bounds, similar to what we've seen earlier in the course in proofs that involved compactness. What happens here is that the arguments use ultra-filters. So there are these constructions from mathematical logic, and like compactness, they give no bounds. So it remains an open problem to prove Freiman's theorem for non-Abelian groups with some concrete bounds. You have a question? AUDIENCE 1. Can you explain what is a neopotent ball? YUFEI ZHAO. So what is neopotent ball? I don't want to give a precise definition, but roughly speaking, it's balls that come out of those types of constructions. So you take a neopotent subgroup, you take a neopotent group, you look at the image of a neopotent group into your group, and then look at the image of that ball. So something that looks like one of the previous constructions. So that's what I want to say about non-Abelian extensions of Freiman's theorem. Any questions? Yeah. AUDIENCE 2. Can you explain one more time what you mean by approximate groups? YUFEI ZHAO. So what I mean by it, so you can look in the papers to see the precise definitions, but roughly speaking, it's that if you have different kinds of definitions, and most of them are equivalent, but one version is that you have a set A such that A is coverable by k translates of A. So it's a bit more than just the size information, but it's actually related to size information. So we've already seen this course how many of these different notions can go back and forth from one to the other, from covering to size and whatnot. OK. The final thing I want to discuss today is one of the most central open problems in additive combinatorics, going back to the Abelian version. So this is known as the polynomial Freiman-Ruzsa conjecture. So we would like some kind of a Freiman theorem that preserves the constants up to polynomial changes without losing an exponent. Now, from earlier discussions, I showed you that the bounds that we almost proved is close to the truth, that you do need some kind of exponential loss in the blow-up size of the GAP. But it turns out those kind of examples are slightly misleading. So let's look at the examples, the constructions again. So if A, so just for simplicity and exposition, I'm going to stick with F2 to the n, at least initially. So if A is an independent set of size n, then k, being the doubling constant of A, is roughly like n over 2. And yet, the subgroup that contains A has size 2 to the something on the order k times A. So you necessarily incur an exponential loss over here. Now, you might complain, well, the size of A here is basically k. But of course, I can blow up this example by considering what happens if you take each element here and blow it up into an entire subspace. So the E's are the coordinate vectors. So now I'm sitting inside F2 to the n plus n. And I give you this set. The doubling constant is still the same as before. And yet, we see that the subgroup generated by A still has this exponential blow-up in this constant, exponential in the doubling constant. But now, you see, in this example here, even though the subgroup generated by A can be much larger than A by everything is still constant, so much larger in terms of the function of the doubling constant, A has a very large structure. So A contains a very large subspace. So subspace, I mean, affine subspace. And this subspace here is comparable to the size of A itself. So you might wonder, if you don't care about containing A inside a single subspace, can you do much better in terms of bounds? And that's the content of the polynomial Freiman-Ruzsa conjecture. The PFR conjecture for F2 to the n says that if you have a subset of F2 to the m, and A plus A is size at most k times the size of A, then there exists a subspace V of size at most A, such that V contains a large proportion of A. And the large here, we only lose something that is polynomial in these doubling constants. So that's the case over here. So instead of containing A inside an entire subspace, I just want to contain a large fraction of A in a subspace. And the conjecture is that I do not need to incur exponential losses in the constants. AUDIENCE 2. So V is an affine subspace? YUFEI ZHAO. OK, question is, V an affine subspace? You can think of V as an affine subspace. You can think of V as a subspace. It doesn't actually matter in this formulation. There's an equivalent formulation, which you might like better, where initially, you might complain, well, initially, PFR is initially Freiman's theorem is about covering A. And now we've only covered a part of A. Of course, we saw from early arguments, you can use Ruzsa covering lemma to go from covering a part of A to covering all of A. Indeed, if it's the case that this formulation is equivalent to the formulation that if A is in F to the N, and A plus A size at most K times A, then there exists some subspace V with the size of V no larger than the size of A, such that A can be covered by polynomial in K many cosets of V. We see that here. Here, A has doubling constant K, which is around the same as N. And even though I cannot contain A by a single subspace of roughly the same size, I can use K different translates to cover A. Any questions? So I want to leave it to you as an exercise to prove that these two versions are equivalent to each other. So it's not too hard. So it's something I, if I have more time, I would show you. It uses Ruzsa covering lemma to prove this equivalence. The nice thing about the polynomial Freiman-Ruzsa conjecture, PFR conjecture, is considered a central conjecture in additive combinatorics because it has many equivalent formulation and relate to many problems that are central to the subject. So we would like some kind of inverse theorem that gives you these polynomial bounds. And I'll mention a couple of these equivalent formulations. Here's an equivalent formulation which is rather attractive, where instead of considering subsets, we're going to formulate something that has to do with approximate homomorphisms. So the statement, so it's still conjecture, is that if f is a function from a Boolean space to another Boolean space, is such that f is approximately a homomorphism in the sense that the set of possible errors, so if it's actually a homomorphism, then this quantity is always equal to 0. But it's approximately a homomorphism in the sense that the set of such errors is bounded by k in size. The conclusion, so the conjecture claims that then there exists an actual homomorphism, an actual linear map G such that f is very close to G, as in that the set of possible discrepancies between f and G is bounded, where you only lose at most a polynomial in k. So if you're in approximate homomorphism in this sense, then you're actually very close to an actual linear map. Now, it is not too hard to prove a much weaker, quantitatively weaker version of the statement. So I claim that it is trivial to show upper bound of at most 2 to the k over here. So think about why. So if I give you an f, I can just think about what the values of f are on the basis and extend it to a linear map. Then this set is necessarily a span of that set, so has size at most 2 to the k. But it's open to show you only have to lose a polynomial in k. There's also a version of the polynomial Freiman-Ruzsa conjecture, which is related to things we've discussed earlier regarding Szemeredi's theorem. And in fact, the polynomial Freiman-Ruzsa conjecture came back into popularity partly because of Gowers' proof of Szemeredi's theorem that used many of these tools. So let me state it here. So we've seen some statement like this in an earlier lecture, but not very precisely, or not precisely in this form. And I won't define you all the notation here, but hopefully you get a rough sense of what it's about. So we want some kind of an inverse statement for what's known as a quadratic uniformity norm, quadratic Gowers' uniformity norm. So recall back to our discussion of proof of Roth's theorem, the Fourier analytic proof of Roth's theorem. We want to say that, but now think about not 3-APs, but 4-APs. So we want to know that if you have a function f on the Boolean cube, and this function is one bounded, and I'm going to write down some notation which we are not going to define, but the Gowers' U3 norm is at least some delta. So this is something which is related to 4-AP counts. So in particular, if this number is small, then you have a counting lemma for four-term arithmetic progressions. If this is true, then there exists a quadratic polynomial Q in n variables over F2 such that your function F correlates with this quadratic exponential in Q. And the correlation here is something where you only lose a polynomial in the parameters. So previously, I quoted something where you lose something that's only a constant in delta. And that is true. That is known. But we believe, so it's conjecture that you only lose a polynomial in this parameter. So this type of statement, remember, in our proof of Roth's theorem, something like this came up. So something like this came up as a crucial step in the proof of Roth's theorem. If you have something where you look at counting lemma and you exhibit something like this, then you can exhibit a large Fourier character. And in higher-order Fourier analysis, something like this corresponds to having a large Fourier transform. It turns out that all of these formulations of polynomial Freiman-Ruzsa conjectures are equivalent to each other. And they're all equivalent in a very quantitative sense, so up to polynomial changes in the bounds. So in particular, if you can prove some bounds for some version, then that automatically leads to bounds for the other versions. So the proof of equivalence is just not trivial. But it's also not too complicated. Takes some work, but it's not too complicated. The best bounds for the polynomial Freiman-Ruzsa conjecture, and hence for all of these versions, is, again, due to Tom Sanders. And he proved a version of PFR with quasi-polynomial bounds. Whereby quasi-polynomial bounds, I mean, for instance, over here, instead of k, he proved it for something which is like e to the poly log k. So like k to the log k, but k to the poly log k. So it's almost polynomial, but not quite there. And it's considered a central open problem to better understand the polynomial Freiman-Ruzsa conjecture. And we believe that this is something that could lead to a lot of important new tools and techniques that are relevant to the rest of additive combinatorics. AUDIENCE 2 Using the fact that all of these are equivalent, is it possible to get a proof of Freiman's theorem using the bound of 2 to the k for the approximate norm of the Markovian theorem? YUFEI ZHAO So the question is, so we know that that up there has 2 to the k. So you're asking, can you use this 2 to the k to get some bound for polynomial for something like this? And the answer is yes. So you can use that proof to go through some proofs and get here. I don't remember how this equivalence goes. But now remember that the proof of Freiman's theorem for F2 to the n wasn't so hard. So we didn't use very many tools. Unfortunately, I don't have time to tell you the formulations of polynomial Freiman-Ruzsa conjecture over the integers and also over arbitrary abelian groups. But there are formulations over the integers. And that's one that people care just as much about. And there are also different equivalent versions. But things are a bit nicer in the Boolean case. Yeah? AUDIENCE 2 Is it like kind of the same thing as the one with the second condition? YUFEI ZHAO I'm sorry, can you repeat the question? AUDIENCE 2 Like for this for a third line? Yeah, I was asking. YUFEI ZHAO You're asking what does this mean? So this is what's called a Gowers uniformity norm. So something I encourage to look up. In fact, there's an unassigned problem in the problem set that's related to the Gowers uniformity norm before U2, which is related to Fourier analysis. But U3 is related to Fourier piece and quadratic Fourier analysis.