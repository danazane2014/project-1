 Το επόμενο πρόγραμμα προσφέρεται υπό εμπρός δικαιωματικής κοινωνίας. Η υποστηρική σας θα βοηθήσει να συνεχίσει να προσφέρει υψηλές ειδικές πιθανότητες ειδικές πιθανότητες αξιωματικά για ελεύθερα. Για να κάνετε μια διεγραφή ή να παρακολουθήσετε περισσότερα υλικά από χιλιάδες μαθητές MIT, επισκεφθείτε το MIT OpenCourseWare στηn ocw.mit.edu. Λοιπόν, θα αρχίσουμε τώρα με ένα νέο κεφάλαιο. Θα μιλήσουμε για τα πρόσφατα Markov. Η καλή νέα είναι ότι αυτό είναι ένα θέμα που είναι πολύ πιο αυτοκίνητος και απλός σε πολλά τρόπα από, ας πούμε, τα πρόσφατα Poisson. Ελπίζω ότι θα είναι ευτυχισμένο. Η πρόσφατα Markov είναι μια κλασσική κλίμακα πρόσφατων. Σε κάποιο σημείο είναι πιο εξελίξης από τα πρόσφατα Bernoulli και Poisson γιατί τώρα θα έχουμε εξελίξεις μεταξύ διαφορετικών χρονών, αντί να έχουμε διαδικασίες χωρίς θέματα. Η βασική ιδέα είναι η επόμενη. Στη φυσική, για παράδειγμα, γράφετε εξελίξεις για το πώς εξελίξει ένα σύστημα που έχει την κοινή γραμμή. Το νέο στήμα του σύστημα, 1 δευτερόλεπτο μετά, είναι κάποια εξελίξη του παλιού στήματος. Οι Ν.Ε. και όλα αυτά, στη φυσική, επιτρέπουν να γράφετε εξελίξεις αυτού του είδους. Άρα αν ξέρετε ότι ένα πράγμα κινδύει σε μια συγκεκριμένη γρήγορη πλήρη και είναι σε κάποια θέση, μπορείτε να προδοθείτε πότε θα είναι, λίγο μετά. Οι Markov πρόσφατες έχουν το ίδιο γεμό, εκτός από το γεμό που υπάρχει μέσα στο σύστημα. Αυτό είναι ο πρόσφατος Markov πρόσφατος. Είναι, ο οποίος οριζόμενά ο σύστημας, ή μερικές διαφορετικές, αλλά στην παρουσία κάποιων ήχους, ώστε η κινήση αυτή είναι λίγο αρκετή. Λοιπόν, αυτό είναι ένα αρκετό γενικό πλαίσιο. Οπότε, κάθε χρησιμοποιημένο ή ενδιαφέρουσιο πρόσφατο πρόσφατο που μπορείτε να σκεφτείτε, μπορείτε να το οριζόμενο το οριζόμενο το οποίο είναι ένα Markov πρόσφατο, αν καθαρίζετε καθαρά την νοητική του κόσμου. Λοιπόν, αυτό που θα κάνουμε είναι να παρουσιάσουμε το κλάσμα Markov πρόσφατων, παράδειγμα, μιλώντας για το κερδί μεταξύ εξοπλισμού στο αγοράδι. Λοιπόν, θα αποκλειστρωνουμε από το παράδειγμα μας, ώστε να δούμε μια πιο γενική κατεστημένη. Και τότε θα κάνουμε μερικές πράγματα, όπως πώς να προδοθήσουμε τι θα συμβεί μετά από εκατομμύρια χρόνια, αν ξεκινήσουμε σε ένα συγκεκριμένο στήμα, και μετά θα μιλήσουμε λίγο για κάποια στρογγυστικά πραγματικά των Markov-πρόσφατων ή Markov-χρήματα. Λοιπόν, εδώ είναι το παράδειγμα μας. Πηγαίνετε στο κερδί μεταξύ εξοπλισμού στο αγοράδι, και στέκετε εκεί και παρακολουθείτε τους κατοικητές που έρχονται. Λοιπόν, οι κατοικητές έρχονται, τους καταφέρνουν σε σειρά, και οι κατοικητές εξοπλισμούν ένα από τα χρόνια. Λοιπόν, η συζήτηση θα είναι σε τρόπο αγοράδια μεταξύ εξοπλισμού, αλλά η ίδια ιστορία εξελίξει κάθε υπηρεσιακό σύστημα. Μπορείτε να έχετε έναν υποστηρίξη, οι δουλειές έρχονται σε αυτόν τον υποστηρίξη, οι κατοικητές εξοπλισμούν σε σειρά, και ο υποστηρίξης εξοπλισμούς αυτά τα δουλειά ένα από τα χρόνια. Τώρα, για να κάνουμε ένα πραγματικό μοντέλο, πρέπει να κάνουμε κάποια αποφάσιοση για τα προσέγγιση και τα προσέγγιση. Και θέλουμε να κρατήσουμε τα πράγματα όσο απλό όσο μπορεί, για να ξεκινήσουμε. Ένα άλλο τρόπο να σκεφτείτε τον πρόοδο προσέγγισης, όχι πως συμβαίνει, αλλά βοηθά μαθηματικά, είναι να σκεφτείτε κάποιον που σκεφτεί ένα κοϊν, με πλήρη αρχή πλήρη π. Και κάθε φορά που το κοϊν πλύνει κεφάλαια, τότε έρχεται ένα κουστό. Λοιπόν, είναι σαν να υπάρχει ένα κοϊν που σκεφτεί από την φύση που αποφάσιζε τα προσέγγιση των κουστών. Λοιπόν, γνωρίζουμε ότι το κοϊν που σκεφτεί για να αποφάσισε τα προσέγγιση των κουστών είναι το ίδιο που έχουν γεωμετρική ανεπιέρχεια. Το γνωρίζουμε από το διεξεργασία μας για το πρόοδο του Bernoulli. OK, και τώρα πως για τα χρόνια εφαρμογής κουστών. Θα αποφάσισουμε ότι αν δεν υπάρχει κουστό στην ομάδα, κανένας δεν είναι υπηρετής, τότε, φυσικά, κανένας δεν θα αποφάσισε από την ομάδα. Αλλά αν υπάρχει κουστό στην ομάδα, τότε αυτός ο κουστός ξεκινά να επηρεάζεται και θα επηρεάζεται για αρκετή ορισμό της χρόνιας, και φυσικά, αυτοί χρειάζονται για να υπηρετήσουν το κουστό. Έχει γεωμετρική ανεπιέρχεια με κάποια γνωστή ορισμότητα Q. Οπότε, η χρόνια που χρειάζεται για να υπηρετήσει ένα κουστό είναι αρκετή, γιατί είναι αρκετή πόσες πράγματα έχουν στον πόδι τους, και πόσες κουπόντς έχουν να φυσικοποιήσουν, και τ. Π. Στην πραγματική κοινότητα, έχει κάποια διευθυντική ανταπτυξιακή, ας μην ανησυχούμε ακριβώς για το τι θα ήταν στον πραγματικό κόσμο, αλλά ως μοντέλων ορισμός, ή μόνο για να ξεκινήσουμε, ας φροντίσουμε ότι οι χρόνια του κουστού είναι καλύτερα αναγνωριστεί από γεωμετρική ανταπτυξιακή, με ορισμότητα Q. Ένα αντίδρατο τρόπο να σκεφτείς τις χρόνια του κουστού, μαθηματικά, θα ήταν, ξανά, σε τελός κλίμακας κεφαλαίων, δηλαδή, ο κουστός έχει κεφαλαίο με βιώσιμη κλίμακα, και κάθε φορά που ο κουστός κλίμακε το κεφαλαίο, με ορισμότητα Q, η δουλειά είναι πάνω, με ορισμότητα 1-Q, συνεχίζετε το δουλειο. Ένα αντιδρά που θα κάνουμε είναι ότι οι κεφαλαίες που συμβαίνουν εδώ για να αποδεχθούν τις προσέγγιες, είναι όλες ανεξαρτημένες από τα άλλα. Οι κεφαλαίες που αποδεχθούν το τέλος του δουλειού, είναι επίσης ανεξαρτημένες από τα άλλα, αλλά επίσης οι κεφαλαίες που συμβαίνουν εδώ είναι ανεξαρτημένες από τις κεφαλαίες που συμβαίνουν εκεί. Οπότε, πώς συμβαίνουν οι προσέγγιες, είναι ανεξαρτημένοι με τι συμβαίνει με το δουλειο. Ωραία. Ας πούμε τώρα, θέλετε να απαντήσετε μια ερώτηση όπως η επόμενη. Το στήμα είναι 7 π.μ. Ποιο είναι η πιθανότητα ότι ένα κουστό θα αποδεχθεί σε αυτό το συγκεκριμένο στιγμό? Λοιπόν, θα πείτε, εάν η σειρά είναι αλήθεια σε αυτό το στιγμό, τότε είστε σίγουροι ότι δεν θα έχετε μια αποδεχθή κουστό. Αλλά αν η σειρά δεν είναι αλήθεια, τότε υπάρχει η πιθανότητα, ότι μια αποδεχθή θα συμβαίνει σε αυτό το στιγμό. Λοιπόν, η απάντηση σε μια ερώτηση όπως αυτή έχει κάτι να κάνει με το στήμα του σύστηματος σε αυτό το στιγμό. Είναι απίστευτο τι είναι η σειρά. Και αν σας ζητώ, θα είναι αλήθεια η σειρά να είναι αλήθεια σε 7 π.μ. 10 π.μ.? Λοιπόν, η απάντηση σε αυτήν την ερώτηση εξαρτάται από αν, σε 7 π.μ., η σειρά ήταν αλήθεια ή όχι. Λοιπόν, γνωρίζοντας κάτι για το στήμα του στήματος σήμερα, μου δίνει αρκετή πληροφορία για αυτό που μπορεί να συμβαίνει στο μέλλον. Λοιπόν, τι είναι το κόσμο του σύστηματος? Επομένως, ήρθαμε να ξεκινήσουμε με αυτό το τερμό. Ο κόσμος, βασικά, αντιμετωπίζει κάτι που είναι αρκετό, κάτι που συμβαίνει σήμερα, που είναι αρκετό με το τι μπορεί να συμβαίνει στο μέλλον. Γνωρίζοντας τη μεγάλη μεγάλη μεθοδότηση του στήματος σήμερα, είναι χρήσιμη πληροφορία για εμένα να κάνω προδοκίες για το τι μπορεί να συμβαίνει 2 λεπτά μετά από τώρα. Λοιπόν, σε αυτό το συγκεκριμένο παράδειγμα, μια αρκετή επιλογή για το στήμα είναι να πληροφορήσουμε πόσους κουστανιούς έχουμε στο σύστημα. Ας υποστηρίξουμε ότι το κτήριο της αγοράς δεν είναι τόσο μεγάλο, οπότε μπορεί να κρατήσει μόνο 10 άτομα. Λοιπόν, θα λιμήσουμε το στήμα. Αντί να πάμε από 0 στο παντοδύναμο, θα το τραγουδήσουμε το μοντέλο μας στο 10. Λοιπόν, έχουμε 11 δυο δυνατός κώδικα, αντιμετωπίζοντας 0 κουστανιούς στο Q, 1 κουστανιού στο Q, 2 κουστανιούς, και τ. Ι. Λοιπόν, αυτές είναι οι δυο δυνάμες δυνατός κώδικα του σύστημα, υποστηρίζοντας ότι η αγορά δεν μπορεί να αντιμετωπίσει πάνω από 10 κουστανιούς. Λοιπόν, αυτό είναι το πρώτο βήμα, να γράψουμε κάτω το σύστημα δυνατόν κώδικων του σύστημα μας. Λοιπόν, το επόμενο πράγμα να κάνουμε είναι να ξεκινήσουμε να περιγράψουμε τις δυνατόν κωδικές μεταξύ των κώδικών. Σε κάθε στιγμή, τι μπορεί να συμβεί? Μπορούμε να έχουμε ένας προσφυγικός προς το οποίο κινείται το κώδικο ένα πιο υψηλό. Μπορούμε να έχουμε έναν προσφυγικό πρόσφυγικό που κινεί το κώδικο ένα λιγότερο. Υπάρχει η πιθανότητα ότι δεν συμβεί τίποτα, σε οποίο το σύστημα παραμείνει το ίδιο. Και υπάρχει επίσης η πιθανότητα που συνεχώς υπάρχει έτος και η απορρίπηση, σε οποίο το σύστημα πάλι παραμείνει το ίδιο. Λοιπόν, ας γράψουμε κάποιες αντιπροσωπικές προσφυγικές. Αν έχουμε δύο συμπληρώντες, η πιθανότητα ότι κατά τη διάρκεια αυτής της στήρης, πηγαίνουμε κάτω, αυτή είναι η πιθανότητα ότι έχουμε ένα στήριο, αλλά κανένα συμπληρώντες. Λοιπόν, αυτή είναι η πιθανότητα που συμμετείχε με αυτή την στήρη. Η άλλη πιθανότητα είναι ότι υπάρχει ένα συμπληρώντες, που συμβαίνει με πιθανότητα P, και δεν έχουμε ένα συμπληρώντες, και λοιπόν η πιθανότητα αυτής της συμπληρωτής είναι αυτό το σημείο. Και τότε, τελικά, η πιθανότητα ότι παραμείνουμε στο ίδιο κόσμο. Αυτό μπορεί να συμβεί σε δύο δυνατόν τρόπο. Ένα τρόπο είναι ότι έχουμε ένα έτος και ένα πηγαίνοντας, συνεχώς. Και η άλλη πιθανότητα είναι ότι δεν έχουμε έτος και δεν έχουμε πηγαίνοντας, οπότε το κόσμο παραμείνει το ίδιο. Λοιπόν, αυτές οι πιθανότητες της συμπληρωτής θα ήταν τα ίδια, ξεκινώντας από κάθε άλλο κόσμο, κόσμο 3 ή κόσμο 9 και τ. p. Οι πιθανότητες της συμπληρωτής γίνονται λίγο διαφορετικές στις σύνορες, στις σύνορες αυτής της διαγραφής, επειδή αν είστε σε κόσμο 0, τότε δεν μπορείτε να έχετε κανέναν κουστό πηγαίνοντας, δεν υπάρχει κανένας να παραμείνει υποστηρικτός. Αλλά υπάρχει μια πιθανότητα P που έρχεται ένα κουστό, σε οποία περίπτωση το ποσό κουστός στο σύστημα πηγαίνει σε 1, και πιθανότητα 1-P που δεν συμβαίνει τίποτα. Συμπληρωτικά με τις περιπτώσεις, αν το σύστημα είναι πλήρος, δεν υπάρχει χώρο για ένα άλλο έτος, αλλά μπορούμε να έχουμε μια περιπτώση που συμβαίνει με πιθανότητα Q, και δεν συμβαίνει τίποτα με πιθανότητα 1-Q. Λοιπόν, αυτό είναι το πλήρος διαγραφή, αναφερθεί με πιθανότητες μετατραπτώσεων, και αυτό είναι μια ολοκληρωμένη ορισμόση μιας σύμφωνας χρονικής στιγμής, σύνορας κλίμακας. Λοιπόν, αυτό είναι ένα ολοκληρωμένο πιθανότητας μοντέλο. Μετά από όλα αυτά τα πράγματα πληροφορίας, μπορείτε να ξεκινήσετε να καταπληκτείτε πράγματα και να προφανείτε τι θα συμβαίνει στο μέλλον. Τώρα ας αποκλειστούμε από αυτό το παράδειγμα και να πάμε σε μια πιο γενική κατεστημιότητα. Λοιπόν, έχουμε αυτό το κίνημα του κόσμου, που αναφερθεί τη στιγμή και το σύστημα που εξετάζουμε. Το σύστημα τώρα είναι δίκαιο, οπότε θα το θυμόμαστε ως δίκαιο διευθύνος. X-sub-n είναι το σύστημα, n μεταφράσεις μετά από το σύστημα που άρχισε να λειτουργεί. Οπότε το σύστημα αρχίζει να λειτουργεί σε κάποιο αρχικό σύστημα X0, και μετά από n μεταφράσεις, ανοίγει σε σύστημα Xn. Τώρα έχουμε ένα σύστημα δυνατών σύστημα. Σύστημα 1, σύστημα 2, σύστημα 3, και, γενικά, σύστημα i και σύστημα j. Για να κρατήσουμε τα πράγματα απλά, αποφασίσαμε ότι το σύστημα των δυνατών σύστημα είναι ένα φινάλιο σύστημα. Όπως μπορείτε να φανταστείτε, μπορούμε να έχουμε σύστημα, στο οποίο το σύστημα θα είναι αφιερωμένο. Μπορεί να είναι διευθύνος ή συνεχός, αλλά όλα αυτά είναι πιο δύσκολες και πιο συγκλειστές. Και αρκετά σημαντικά να ξεκινήσουμε από το πιο απλό διάστημα, όπου αντιμετωπίζουμε μόνο το φινάλιο σύστημα. Και ο χρόνος είναι διευθυντικός, οπότε μπορούμε να σκεφτούμε το σύστημα, στο αρχήν, μετά από ένα μεταλλόν, δύο μεταλλόν, και τ. Ι. So, είμαστε σε διευθυντικό χρόνο, και έχουμε φινάλια πολλές σύστημα. Οπότε, το σύστημα ξεκινά κάπου. Και σε κάθεiodance στο afraidix, ο σώματος προζίγεται μόνο από την Geezersman. Ε Ivanovan? Σύναιθρον,OK? Unearthed? Et ce que vous objectez against 0. On a une very belle notion, 1. Un grand message. A Dementia s'est redevenue... to describe the statistics of these transitions. If I am at that state, how likely is it that next time I'm going to find myself at that state? Well, we describe the statistics of this transition by writing down a transition probability, the transition probability of going from state 3 to state 1. So this transition probability is to be thought of as a conditional probability. Given that right now I am at state i, what is the probability that next time I find myself at state j? So given that right now I am at state 3, P31 is the probability that next time I'm going to find myself at state 1. Similarly here, we would have a probability P3i, which is the probability that given that right now I'm at state 3, next time I'm going to find myself at state i. Now one can write such conditional probabilities down in principle, but we need to make, so you might think of this as a definition here, but we need to make one additional big assumption. And this is the assumption that makes a process to be a Markov process. This is the so-called Markov property. And here's what it says. Let me describe it first in words here. Every time that I find myself at state 3, the probability that next time I'm going to find myself at state 1 is this particular number, no matter how I got there. That is, this transition probability is not affected by the past of the process. It doesn't care about what path I used to find myself at state 3. Mathematically, it means the following. You have this transition probability that from i, you jump to state j. Suppose that I gave you some additional information, that I told you everything else that happened in the past of the process. Everything that happened, how did you get to state i? The assumption we're making is that this information about the past has no bearing in making predictions about the future, as long as you know where you are right now. So if I tell you right now you are at state i, and by the way you got there by following a particular path, you can ignore the extra information of the particular path that you followed. You only take into account where you are right now. So every time you find yourself at that state, no matter how you got there, you will find yourself next time at state 1 with probability p31. So the past has no bearing into the future, as long as you know where you are sitting right now. For this property to happen, you need to choose your state carefully in the right way. In essence, the state needs to include any information that's relevant about the future of the system. Anything that's not in the state is not going to play a role, but the state needs to have all the information that's relevant in determining what kind of transitions are going to happen next. So to take an example, before you go to Markov process, just from the deterministic world, if you have a ball that's flying out in the air, and you want to make predictions about the future, if I tell you that the state of the ball is the position of the ball at a particular time, is that enough for you to make predictions where the ball is going to go next? No. You need to know both the position and the velocity. If you know position and velocity, you can make predictions about the future. So the state of a ball that's flying is positioned together with velocity. If you were to just take position, that would not be enough information. Because if I tell you current position, and then I tell you the past position, you could use the information from the past position to complete the trajectory and make the prediction. So information from the past is useful if you don't know the velocity. But if you know both position and velocity, you don't care how you got there or what time you started. From position and velocity, you can make predictions about the future. So there is a certain art or a certain element of thinking, a non-mechanical aspect into problems of this kind, to figure out which is the right state variable. When you define the state of your system, you need to define it in such a way that it includes all information that has been accumulated that has some relevance for the future. So the general process for coming up with a Markov model is to first make this big decision of what your state variable is going to be. Then you write down maybe a picture of the different states. Then you identify the possible transitions. So sometimes the diagram that you're going to have will not include all possible arcs. You will only show those arcs that correspond to transitions that are possible. For example, in the supermarket example, we did not have a transition from state 2 to state 5, because that cannot happen. You can only have one arrival at any time. So in the diagram, we only showed the possible transitions. And for each one of the possible transitions, then you work with the description of the model to figure out the correct transition probability. So you annotate the diagram by writing down transition probabilities. OK, so suppose you got your Markov model. What will you do with it? Well, what do we need models for? We need models for in order to make predictions, to make probabilistic predictions. So for example, I tell you that the process started at that state. You let it run for some time. Where do you think it's going to be 10 times steps from now? That's a question that you might want to answer. Since the process is random, there's no way for you to tell me exactly where it's going to be. But maybe you can give me probabilities. You can tell me with so much probability, the state will be there. With so much probability, the state will be there, and so on. So our first exercise is to calculate those probabilities about what may happen to the process, a number of steps, in the future. It's handy to have some notation in here. So somebody tells us that this process starts at a particular state i. We let the process run for n transitions. It will land at some state j, but that state j at which it's going to land is going to be random. So we want to give probabilities. Tell me with what probability the state n times steps later is going to be that particular state j. The shorthand notation is to use this symbol here for the n step transition probabilities that you find yourself at state j, given that you started at state i. So the way these two indices are ordered, the way to think about them is that from i, you go to j. So probability that from i, you go to j if you have n steps ahead, in front of you. Some of these transition probabilities are, of course, easy to write. For example, in 0 transitions, you're going to be exactly where you started. So this probability is going to be equal to 1 if i is equal to j, and 0 if i is different than j. OK, that's an easy one to write down. If you have only one transition, what's the probability that one step later, you find yourself in state j, given that you started at state i? What is this? These are just the ordinary one step transition probabilities that were given in the description of the problem. So by definition, the one state transition probabilities of this form, this equality is correct just because of the way that we defined those two quantities. Now we want to say something about the n step transition probabilities when n is a bigger number. So here, we're going to use the total probability theorem. So we're going to condition into different scenarios and break up the calculation of this quantity by considering the different ways that this event can happen. So what is the event of interest? The event of interest is the following. At time 0, we start i. We're interested in landing at time n at the particular state j. Now this event can happen in several different ways, in lots of different ways, but let us group them into subgroups. One group, or one sort of scenario, is the following. During the first n minus 1 time steps, things happen and somehow you end up at state 1. And then from state 1, at the next time step, you make a transition to state j. This particular arc here actually corresponds to lots and lots of different possible scenarios, or different paths, or different transitions. In n minus 1 time steps, there's lots of possible ways by which you could end up at state 1, different paths through the state space. But all of them together, collectively, have a probability which is the n minus 1 step transition probability that from state i, you end up at state 1. And then there's other possible scenarios. Perhaps in the first n minus 1 steps, you follow the trajectory that took you at state m. And then from state m, you did this transition, and you ended up at state j. So this diagram breaks up the set of all possible trajectories from i to j into different collections, where each collection has to do with a set of possible has to do with which one happens to be the state just before the last time step, just before time n. And we're going to condition on the state at time n minus 1. So the total probability of ending up at state j is the sum of the probabilities of the different scenarios, of the different ways that you can get to state j. If we look at that type of scenario, what's the probability of that scenario happen? With probability Ri1 of n minus 1, I find myself at state 1 at time n minus 1. This is just by the definition of these multi-step transition probabilities. This is the number of transitions, the probability that from state i, I end up at state 1. And then given that I found myself at state 1, with probability P1j, that's the transition probability, next time I'm going to find myself at state j. So the product of these two is the total probability of my getting from state i to state j through state 1 at the time before. Now where exactly did we use the Markov assumption here? No matter which particular path we used to get from i to state 1, the probability that next time I'm going to make this transition is that same number, P1j. So that number does not depend on the particular path that I followed in order to get there. If we didn't have the Markov assumption, we should have considered all possible individual trajectories here. And then we would need to use the transition probability that corresponds to that particular trajectory. But because of the Markov assumption, the only thing that matters is that right now we are at state 1. It does not matter how we got there. So now once you consider this scenario, then this scenario, and that scenario, and you add the probabilities of all these different scenarios, you end up with this formula here, which is a recursion. It tells us that once you have computed the n-1 step transition probabilities, then you can compute also the n step transition probabilities. This is a recursion that you execute or you run for all i's and j's simultaneously. That is, for a particular n, you calculate this quantity for all possible i, j's, k's. You have all those quantities, and then you use this equation to find those numbers, again, for all possible i's and j's. Now this is a formula which is always true. And there's a big idea behind that formula. And now there's variations of this formula, depending on whether you're interested in something that's slightly different. So for example, if you were to have a random initial state, and somebody gives you the probability distribution of the initial state, so you're told that with probability such and such, you're going to start at, say, state 1. With that probability, you're going to start at state 2, and so on. And you want to find the probability that at time n, you find yourself at state j. Well, again, total probability theorem, you condition on the initial state. With this probability, you find yourself at that particular initial state. And given that this is your initial state, this is the probability that n times steps later, you find yourself at state j. Now building again on the same idea, you can run recursions of this kind by conditioning at different times. So here's a variation. You start at state i. After one time step, you'll find yourself at state 1 with probability Pi 1. And you'll find yourself at state m with probability Pi m. And once that happens, then you're going to follow some trajectories. And there is a possibility that you're going to end up at state j after n minus 1 times steps. This scenario can happen in many possible ways. There's lots of possible paths from state 1 to state j. There's many paths from state m to state j. What is the collective probability of all these transitions? This is the event that starting from state 1, I end up at state j in n minus 1 times steps. So this one has here probability R1j of n minus 1. And similarly down here. And then by using the same way of thinking as before, we get the formula that Rij of n is the sum over all k's of Pik, and then the R of kj n minus 1. So this formula looks almost the same as this one, but it's actually different. The indices and the way things work out are a bit different, but the basic idea is the same. Here we use the total probability theory by conditioning on the state just one step before the end of our time horizon. Here we use the total probability theory by conditioning on the state right after the first transition. So this general idea has different variations. They're all valid, and depending on the context that you're dealing with, you might want to work with one of these or another. So let's illustrate these calculations in terms of an example. So in this example, we just have two states, and somebody gives us transition probabilities to be those particular numbers. Let's write down the equations. So the probability that starting from state 1, I find myself at state 1 n time steps later. This can happen in two ways. At time n minus 1, I might find myself at state 2, and then from state 2, I make a transition back to state 1, which happens with probability 0.2. And another way is that from state 1, I go to state 1 in n minus 1 steps, and then from state 1, I stay where I am, which happens with probability 0.5. So this is a recursion for R11 of n. Now R12 of n, we can write a similar recursion for this one. On the other hand, since these are probabilities, the state at time n is going to be either state 1 or state 2, so these two numbers need to add to 1. So we can just write that this is 1 minus R11 of n. And this is enough of a recursion to propagate R11 and R12 as time goes on. So after n minus 1 transitions, either I find myself at state 2, and then there's a 0.2 transition that I go to 1, or I find myself at state 1, which is with that probability, and from there I have probability 0.5 of staying where I am. So now let's start calculating. As we discussed before, if I start at state 1, after 0 transitions, I'm certain to be at state 1, and I'm certain not to be at state 2. If I start from state 2, I'm certain to not be at state 1 at that time, and I'm certain that I am right now at state 2. After I make one transition, starting from state 1, there's probability 0.5 that I stay at state 1, and there's probability 0.5 that I move to state 2. If I were to start from state 2, the probability that I go to 1 in one time step is this transition that has probability 0.2, and the other one is 0.8. OK, so the calculation now becomes more interesting. If we want to calculate the next term. How likely is it that at time 2 I find myself at state 1? In order to be here at state 1, this can happen in two ways. Either the first transition left me there, and the second transition is the same. So this corresponds to this 0.5 that the first transition took me there, and the next transition was also of the same kind. That's one possibility. But there's another scenario. In order to be at state 1 at time 2, this can also happen this way. So that's the event that after one transition I got there, and the next transition happened to be this one. So this corresponds to 0.5 times 0.2. It corresponds to taking the one-step transition probability of getting there times the probability that from state 2 I move to state 1, which in this case is 0.2. So basically we take this number, multiply it with 0.2, and then add those two numbers. And after you add them, here you get 0.35. And similarly here, you're going to get 0.65. And now to continue with the recursion, we keep doing the same thing. We take this number times 0.5 plus this number times 0.2, add them up, you get the next entry. Keep doing that, keep doing that, and eventually you will notice that the numbers start settling into a limiting value 2.7, and let's verify this. If this number is 2.7, what is the next number going to be? The next number is going to be 2 7ths, not 2.7. It's going to be 2 7ths, that's the probability that I find myself at that state, times 0.5. That's the next transition that takes me to state 1. Plus 5 7ths, that would be the remaining probability. That I find myself at state 2, times 1 5th. And so that gives me again 2 over 7. So this calculation basically illustrates that if this number has become 2.7, then the next number is also going to be 2.7. And of course, this number here is going to have to be 5 over 7, and this one will have to be, again, the same, 5 over 7. So the probability that I find myself at state 1, after a long time has elapsed, settles into some steady state value, so that's an interesting phenomenon. We just make this observation. Now we can also do the calculation about the probabilities starting from state 2. And here you do the calculations. I'm not going to do them. But after you do them, you find that this probability also settles to 2 over 7, and this one also settles to 5 over 7. So these numbers here are the same as those numbers. What's the difference between these? This is the probability that I find myself at state 1, given that I started at 1. This is the probability that I find myself at state 1, given that I started at state 2. These probabilities are the same no matter where I started from. So this numerical example sort of illustrates the idea that after the chain has run for a long time, what the state of the chain is does not care about the initial state of the chain. So if you start here, you know that you're going to stay here for some time, a few transitions, because this probability is kind of small. So the initial state does tell you something. But in the very long run, transitions of this kind are going to happen. Transitions of that kind are going to happen. There's a lot of randomness that comes in. And that randomness washes out any information that could come from the initial state of the system. We describe this situation by saying that the Markov chain eventually enters a steady state. Where steady state, what does it mean? Does it mean that the state itself becomes steady and stops at one place? No. The state of the chain keeps jumping forever. The state of the chain will keep making transitions, will keep going back and forth between 1 and 2. So the state itself, the Xn, does not become steady in any sense. What becomes steady are the probabilities that describe Xn. That is, after a long time elapses, the probability that you find yourself at state 1 becomes a constant, 2 over 7. And the probability that you find yourself at state 2 becomes a constant. So jumps will keep happening. But at any given time, if you ask what's the probability that right now I am at state 1, the answer is going to be 2 over 7. Incidentally, do the numbers sort of make sense? Why is this number bigger than that number? Well, this state is a little more sticky than that state. Once you enter here, it's kind of harder to get out. So when you enter here, you spend a lot of time here. This one is easier to get out, because that probability is 0.5. So when you enter there, you tend to get out faster. So you keep moving from one to the other, but you tend to spend more time on that state. And this is reflected in this probability being bigger than that one. So no matter where you start, there's 5 7th probability of being here, 2 7th probability of being there. So there were some really nice things that happened in this example. The question is whether things are always as nice for general Markov chains. The two nice things that happened were the following. As we keep doing this calculation, this number settles to something. The limit exists. The other thing that happens is that this number is the same as that number, which means that the initial state does not matter. Is this always the case? Is it always the case that as n goes to infinity, the transition probabilities converge to something? And if they do converge to something, is it the case that the limit is not affected by the initial state i at which the chain started? So mathematically speaking, the question we are raising is whether Rij of n converges to something. And where that something to which it converges to has only to do with j. It's the probability that you find yourself at state j. And that probability doesn't care about the initial state. So it's the question of whether the initial state gets forgotten in the long run. So the answer is that usually, or for nice chains, both of these things will be true. You'll get the limit, which does not depend on the initial state, but if your chain has some somewhat peculiar or unique structure, this might not happen. So let's think first about the issue of convergence. So convergence as n goes to infinity, to a steady value, really means the following. If I tell you a lot of time has passed, then you'll tell me, OK, the probabilities are equal to that value without having to consult your clock. If you don't have convergence, it means that Rij can keep going up and down without settling to something. So in order for you to tell me the value of Rij, you need to consult your clock to check if right now it's up or is it down, so there's some kind of periodic behavior that you might get when you do not get convergence. And this example here illustrates it. So what's happening in this example? Starting from state 2, next time you go here or there with probability 1.5. And then next time, no matter where you are, you move back to state 2. So this chain has some randomness, but the randomness is kind of a limited type. You go out, you come in. You go out, you come in. So there's a periodic pattern that gets repeated. It means that if you start at state 2, after an even number of steps, you are certain to be back at state 2. So this probability here is 1. On the other hand, if the number of transitions is odd, there's no way that you can be at your initial state. If you start here, at even times you will be here. At odd times you will be there or there. So this probability is 0. As n goes to infinity, this probability, the n-step transition probability, does not converge to anything. It keeps alternating between 0 and 1. So convergence fails. This is the main mechanism by which convergence can fail if your chain has a periodic structure. And we're going to discuss next time that if periodicity is absent, then we don't have an issue with convergence. The second question is if we have convergence, whether the initial state matters or not. In the previous chain, where you could keep going back and forth between states 1 and 2, numerically one finds that the initial state does not matter. But you can think of situations where the initial state does matter. Look at this chain here. If you start at state 1, you stay at state 1 forever. There's no way to escape. So this means that R11 of n is 1 for all n. If you start at state 3, you will be moving between states 3 and 4, but there's no way to go in that direction. So there's no way that you go to state 1. And for that reason, R31 is 0 for all n. So this is a case where the initial state matters. R11 goes to a limit as n goes to infinity. Because it's constant, it's always 1. So the limit is 1. R31 also has a limit. It's 0 for all times. So these are the long-term probabilities of finding yourself at state 1. But those long-term probabilities are affected by where you started. If you start here, you're sure that in the long-term, you'll be here. If you start here, you're sure that in the long-term, you will not be there. So the initial state does matter here. And this is a situation where certain states are not accessible from certain other states. So it has something to do with the graph structure of our Markov chain. Finally, let's answer this question here, at least for large n's. What do you think is going to happen in the long-term if you start at state 2? If you start at state 2, you may stay at state 2 for a random amount of time. But eventually, this transition will happen or that transition will happen. Because of the symmetry, you're as likely to escape from state 2 in this direction or in that direction. So there's probability 1 half that when the transition happens, the transition happens in that direction. So for large n, you're certain that the transition does happen. And given that the transition has happened, it has probability 1 half that it has gone that particular way. So clearly here, you see that the probability of finding yourself in a particular state is very much affected by where you started from. So what we want to do next is to abstract from these two examples and describe the general structural properties that have to do with periodicity and that have to do with what happened here with certain states not being accessible from the others. We're going to leave periodicity for next time. But let's talk about the second kind of phenomenon that we have. So here, what we're going to do is to classify the states in a transition diagram into two types, recurrent and transient. So a state is said to be recurrent if the following is true. If you start from state i, you can go to some places. But no matter where you go, there is a way of coming back. So what's an example of a recurrent state? This one. Starting from here, you can go elsewhere. You can go to state 7. You can go to state 6. That's all where you can go to. But no matter where you go, there is a path that can take you back there. So no matter where you go, there is a chance and there is a way of returning where you started. Those states we call recurrent. And by this, 8 is recurrent. All of these are recurrent. So this is recurrent. This is recurrent. And this state 5 is also recurrent. You cannot go anywhere from 5 except to 5 itself. Wherever you can go, you can go back to where you started. So this is recurrent. If it is not recurrent, we say that it is transient. So what does transient mean? You need to take this definition and reverse it. Transient means that starting from i, there is a place to which you can go and from which you cannot return. If it's recurrent, anywhere you go, you can always come back. Transient means there are places where you can go from which you cannot come back. So state 1 is recurrent because starting from here, there's a possibility that you get there, and then there's no way back. State 4 is recurrent. Starting from 4, there's somewhere you can go. And sorry, transient, correct. State 4 is transient. Starting from here, there are places where you can go and from which you cannot come back. And in this particular diagram, all of these four states are transients. Now if a state is transient, it means that there is a way to go somewhere where you're going to get stuck and not be able to come back. As long as your state keeps circulating around here, eventually one of these transitions is going to happen, and once that happens, then there's no way that you can come back. So the transient state will be visited only a finite number of times. You will not be able to return to it. And in the long run, you're certain that you're going to get out of the transient states and get to some class of recurrent states and get stuck forever. So let's see. In this diagram, if I start here, could I stay in this lump of states forever? Well, as long as I'm staying in this lump of states, I will keep visiting states 1 and 2. Each time that I visit state 2, there's going to be a positive probability that I escape. So in the long run, if I were to stay here, I would visit state 2 an infinite number of times, and I would get infinite chances to escape. But if you have infinite chances to escape, eventually you will escape. So you're certain that with probability 1, starting from here, you're going to move either to those states or to those states. So starting from transient states, you only stay at the transient states for a random but finite amount of time. And after that happens, you end up into a class of recurrent states. And when I say class, what I mean is that in this picture, I divide the recurrent states into two classes or categories. What's special about them? These states are recurrent. These states are recurrent. But there's no communication between the two. If you start here, you're stuck here. If you start here, you are stuck there. And this is a case where the initial state does matter, because if you start here, you get stuck here. If you start there, you get stuck there. So depending on the initial state, that's going to affect the long-term behavior of your chain. So the guess you can make at this point is that for the initial state to not matter, we should not have multiple recurrent classes. We should have only one. But we're going to get back to this point next time.