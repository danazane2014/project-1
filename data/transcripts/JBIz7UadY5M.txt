 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. I minus xi transpose t. OK, I just pick whatever notation I want for my variable and I say it's t. OK, so that's the least squares estimator. And it turns out that, as I said last time, it's going to be convenient to think of those things as matrices. So here, I already have vectors. I've already gone from one dimension, just real valued random variables to random vectors when I think of each xi. But if I start stacking them together, I'm going to have vectors and matrices that show up. So the first vector I'm getting is y, which is just a vector where I have y1 to yn. So that's a boldface vector. Then I have x, which is a matrix where I have, well, the first coordinate is always 1. So I have 1 and then x1 xp minus 1. And that's for observation 1. And then I have the same thing all the way down for observation n. OK, everybody understands what this is? So I'm just basically stacking up all the xi's, right? So this i-th row is xi transpose. I am just stacking them up. And so if I want to write all these things to be true for each of them, all I need to do is to write the vector epsilon, which is epsilon 1 to epsilon n. And what I'm going to have is that y, the boldface vector, now is equal to the matrix x times the vector beta plus the vector epsilon. And it's really just exactly saying what's there. So this is a vector, right? This is a vector. And what is the dimension of this vector? n, right? So this is n observations. And for two vectors to be equal, I need to have all the coordinates to be equal. And that's exactly the same thing as saying that this holds for i equal 1 to n. But now, when I have this, I can actually rewrite the sum for i equals 1 to n of yi minus xi transpose beta squared. This turns out to be equal to the Euclidean norm of the vector y minus the matrix x times beta squared. And I'm going to put a 2 here so we know we're talking about the Euclidean norm. This just means this is a Euclidean norm. That's the one we've seen before when we talked about chi square. That's the square norm. It's the sum of the square of the coefficients. And then I take a square root. But here, I have an extra square. So it's really just the sum of the square of the coefficients, which is this. And here are the coefficients. So now that I write this thing like that, then minimizing. So my goal here now is going to solve minimum over t in our p of y minus x times t 2 squared. And just like we did for one dimension, we can actually write optimality conditions for this. I mean, this is a function. So this is a function from r, p to r. And if I want to minimize it, all I have to do is to take its gradient and set it equal to 0. So minimum set gradient to 0. So that's where it becomes a little complicated. Now I'm going to have to take the gradient of this norm. It might be a little annoying to do. But actually, what's nice about those things, I mean, I remember that it was a bit annoying to learn. I mean, it's just like basically rules of calculus that you don't use that much. But essentially, you can actually expand this norm. And you will see that the rules are basically the same as in one dimension. You just have to be careful about the fact that matrices do not commute. So let's expand this thing. y minus x t squared. Well, this is equal to the norm of y squared plus the norm of x t squared plus 2 times y transpose x t. That's just expanding the square in more dimensions. And this I'm actually going to write as y squared plus. So here, the norm squared of this guy, I always have that the norm of x squared is equal to x transpose x. So I'm going to write this as x transpose x. So it's t transpose x transpose x t plus 2 times y transpose x t. So now, if I'm going to take the gradient with respect to t, I have basically three terms. And each of them has some sort of a different nature. This term is linear in t. And it's going to differentiate the same way that I differentiate a times x. I'm just going to keep the a. This guy is quadratic. t appears twice. And this guy, I'm going to pick up a 2. And it's going to differentiate just like when I differentiate a times x squared. It's 2 times ax. And this guy is a constant with respect to t. So it's going to differentiate to 0. So when I compute the gradient, now, of course, all these rules that I give you can check by looking at the partial derivative with respect to each coordinate. But arguably, it's much faster to know the rules of differentiability. It's like if I give you the function exponential x, and I said, what is the derivative? And you started writing, well, I'm going to write exponential x plus h minus exponential x divided by h and let h go to 0. That's a bit painful. AUDIENCE 1 Why is b transpose to 1 x transpose with respect to 0? I'm sorry? The alpha over 1 equals b transpose x. The transpose of 2ab is b transpose a transpose. If you're not sure about this, just make a and b have different size. And then you will see that there's some not in compatibility. There's basically only one way to not screw that one up. So that's easy to remember. So if I take the gradient, then it's going to be equal to what? It's going to be 0 plus we said here, this is going to differentiate. So think a times x squared. So I'm going to have 2ax. So here, basically, this guy is going to go 2x transpose xt. Now, I could have made this one go away. But that's the same thing as saying that my gradient is, I can think of my gradient as being either a horizontal vector or a vertical vector. So if I remove this guy, I'm thinking of my gradient as being horizontal. If I remove that guy, I'm thinking of my gradient as being vertical. And that's what I want to think of typically, vertical vectors, column vectors. And then this guy, well, it's like this guy is just think a times x. So the derivative is just a. So I'm going to keep only that part here. Sorry, I forgot a minus somewhere. Yeah, here. Minus 2y transpose x. And what I want is this thing to be equal to 0. Right, so t, the optimal t is called beta hat and satisfies. Well, I can cancel the 2's and put the minus on the other side. And what I get is that x transpose xt is equal to y transpose x. And that's not working for me. Yeah, that's because when I took the derivative, I still need to make sure. So it's the same question of whether I want things to be columns or rows, right? So this is not a column. If I remove that guy, y transpose t is a row. So I'm just going to take the transpose of this guy to make things work. And this is just going to be x transpose y. And this guy is x transpose y so that I have columns. OK, so this is just a linear equation in t. And I have to solve it. So it's of the form some matrix time t is equal to another vector. And so that's a basic linear system. And the way to solve it, at least formally, is to just take the inverse of the matrix on the left. So if x transpose x is invertible, then, sorry, that's beta hat is the t I want. I get that beta hat is equal to x transpose x inverse x transpose y. And that's the least squares estimator. So here, I use this condition, right? I want it to be invertible so I can actually write its inverse. Here, I wrote rank of x is equal to p. What is the difference? Well, there's basically no difference, right? Basically, here, I have to assume what is the size of the matrix x transpose x? AUDIENCE MEMBER 1. Same where it's on the p? PHILIPPE RIGOLLET. Yeah, so what is the size? p by p, right? So this matrix is invertible if it's of rank p. If you know what rank means. If you don't, that just rank p means that it's invertible. So it's full rank, and it's invertible. And the rank of x transpose x is actually just the rank of x, because this is the same matrix that you apply twice. And that's all it's saying. So if you're not comfortable with the notion of rank that you see here, just think of this condition just being the condition that x transpose x is invertible. And that's all it says. What it means for it to be invertible, this was true, right? We made no assumption up to this point. If x is not invertible, it means that there might be multiple solutions to this equation, right? In particular, for a matrix to not be invertible, it means that there's some vector v. So if x transpose x is not invertible, then this is equivalent to there exists a vector v, which is not 0, and such that x transpose x v is equal to 0, right? That's what it means to not be invertible. So in particular, if beta hat is a solution, so this equation is sometimes called score equations, because the gradient is called the score. And so you're just checking if the gradient is equal to 0. So if beta hat satisfies star, then so does beta hat plus lambda v for all lambda in the real line, right? And the reason is because, well, if I start looking at what is x transpose x times beta hat plus lambda v, well, by linearity, this is just x transpose x beta hat plus lambda x transpose x time v. But this guy is what? It's 0, right? Just because that's what we assumed. We assumed that x transpose x v was equal to 0. So we're left only with this part, which by star is just x transpose y. So that means that x transpose x beta hat plus lambda v is actually equal to x transpose y, which means that there's another solution, which is not just beta hat, but any move of beta hat along this direction v by any size. So that's going to be an issue, right? Because you're looking for one estimator, and there's not just one in this case. There's many. And so this is not going to be well-defined, and you're going to have some issues. So if you want to talk about the least squares estimator, you have to make this assumption. What does it imply in terms of, can I think of p being 2n, for example, in this case? What happens if p is equal to 2n? AUDIENCE 1. The rank of your matrix is only p over 2. So the rank of your matrix is only p over 2, right? So that means that this is actually not going to happen. I mean, it's not only p over 2. It's at most p over 2, right? It's at most the smallest of the two dimensions of your matrix. So if your matrix is n times 2n, it's at most n, which means that it's not going to be full rank, so it's not going to be invertible. So every time the dimension p is larger than the sample size, your matrix is not invertible, and you cannot talk about the least squares estimator. So that's something to keep in mind. And it's actually a very simple thing. It's essentially saying, well, if p is larger than n, it means that you have more parameters to estimate than you have equations to estimate it, right? So you have this linear system. There's one equation per observation, right? Each row, which was each observation, was giving me one equation. But then the number of unknowns in this linear system is p. And so I cannot solve linear systems that have more unknowns than they have equations. And so that's basically what's happening. Now, in practice, if you think about what data sets look like these days, for example, people are trying to express some phenotype. Some phenotype is something you can measure on people, maybe the color of your eyes or your height or whether you have diabetes or not, things like this, right? So things that are sort of macroscopic. And then they want to use the genotype to do that, right? They want to measure your, they want to sequence your genome and try to use this to predict whether you're going to be responsive to a drug or whether your eyes are going to be blue or something like this. Now, the data sets that you can have, people maybe for a given study about, I don't know, some sort of disease, maybe you will sequence the genome of maybe 100 people, right? n is equal to 100. p is basically the number of genes they're sequencing. This is of the order of 100,000. So you can imagine that this is a case where n is much, much smaller than p. And you cannot talk about the least squares estimator. There's plenty of them. There's not just one line like that, lambda times v, that you can move away. There's basically an entire space in which you can move. And so it's not well-defined. So at the end of this class, I will give you a short introduction on how you do this. Like, this actually represents more and more, becomes a more and more preponderant part of the data sets you have to deal with because people just collect data, right? When I do the sequencing, the machine allows me to sequence 100,000 genes. I'm not going to stop at 100 because doctors are never going to have cohorts of more than 100 patients. So you just collect everything you can collect. And this is true for everything. Cars have sensors all over the place, much more than they actually gather data. There's data. We're creating, we're recording everything we can. And so we need some new techniques for that. And that's what high-dimensional statistics is trying to answer. So this is way beyond the scope of this class. But towards the end, I will give you some hints about what can be done in this framework because, well, this is the new reality we have to deal with. OK, so here we're in a case where p is less than n, that typically much smaller than n. So the kind of orders of magnitude you want to have is maybe p is of order 10 and n is of order 100, something like this. You can scale that, but maybe 10 times larger. All right, so maybe you cannot solve this guy, b, for b hat. But actually, you can talk about x times b hat, even if p is larger than n. And the reason is that x times b hat is actually something that's very well-defined. So what is x times b hat? Remember, I started with the model. So if I look at this definition, essentially, what I had as a original thing was that the vector y was equal to x times beta plus the vector epsilon. That was my model, right? So beta is actually giving me something. Beta is actually some parameter, some coefficients that are interesting. But a good estimator for, so here, it means that the observations that I have are of the form x times beta plus some noise. So if I want to just remove the noise, a good candidate to the noise is x times beta hat. x times beta hat is something that should actually be useful to me, which should be close to x times beta. So in the one-dimensional case, what it means is that if I have, let's say this is the true line, and these are my x's. So I have these are the true points on the real line. And then I have my little epsilons that just give me my observations that move around this line, right? So this is one of the epsilons, say, epsilon i. Then I can actually either talk to say that I recovered the line. I can actually talk about recovering the right intercept or recovering the right slope for this line, right? Those are the two parameters that I need to recover. But I can also say that I've actually found a set of points that's closer to being on the line, that are closer to this set of points right here than the original crosses that I observed. So if we go back to the picture here, for example, what I could do is say, well, for this point here, there was an x here. Rather than looking at this dot, which was my observation, I can say, well, now that I've estimated the red line, I can actually just say, well, this point should really be here. And actually, I can move all these dots so that they're actually on the red line. And this should be a better value, something that has less noise than the original y value that I should see. It should be close to the true value that I should be seeing without the extra noise. So that's definitely something that could be of interest. For example, in imaging, you're not trying to understand. So when you do imaging, y is basically an image. So think of a pixel image, and you just stack it into one long vector. And what you see is something that should look like some linear combination of some feature vectors. So people have created a bunch of features. For example, Gabor frames or wavelets, transforms with just well-known libraries of variables x, such that when you take linear combinations of those guys, this should look like a bunch of images. And what you want for your image, you don't care what the coefficients of the image are in this basis that you came up with. What you care about is the noise in the image. And so you really want to get x beta. So if you want to estimate x beta, well, you can use x beta hat. And what is x beta hat? Well, since beta hat is x transpose x inverse x transpose y, this is x transpose. That's my estimator for x beta. Now, this thing, actually, I can define even if I'm not low rank. So why is this thing interesting? Well, there's a formula for this estimator. But actually, I can visualize what this thing is. So let's assume for the sake of illustration that n is equal to 3. So that means that y lives in a three-dimensional space. And so let's say it's here. And so I have my y is here. And I also have a plane that's given by the vectors x1 transpose, x2 transpose, which is, by the way, 1. Sorry, that's not what I want to do. I'm going to say that n is equal to 3 and that p is equal to 2. So I basically have two vectors, 1, 1, and another 1. Let's assume that it's, for example, a, b, c. So those are my two vectors. This is x1 and this is x2. And those are my three observations for this guy. So what I want when I minimize this, I'm looking at the point which can be formed as a linear combination of the columns of x. And I'm trying to find the guy that's the closest to y. So what does it look like? Well, the two points, 1, 1, 1, is going to be, say, here. That's the point 1, 1, 1. And let's say that a, b, c is this point. OK? So now I have a line that goes through those two guys. OK, that's not really. OK, let's say it's going through those two guys. And this is the line which can be formed by looking only at linear combinations. So this is the line of x times t for t in R2. That's this entire line that you can get. Why is it? Yeah, sorry. It's not just a line. I also have to have t, all the zeros thing. So that actually creates an entire plane, which is going to be really hard for me to represent. I don't know. Maybe I shouldn't do it in these dimensions. OK, so I'm going to do it like that. So this plane here is the set of xt for t in R2. OK? So that's a two-dimensional plane. Definitely goes through 0. And those are all these things, right? So think of a sheet of paper in three dimension. Those are the things I can get. So now what I'm going to have is y is not necessarily in this plane. y is actually something in this plane, x beta, plus some epsilon. OK? y is x beta plus epsilon. So I start from this plane, and then I have this epsilon that pushes me maybe outside of this plane. And what least squares is doing is saying, well, I know that epsilon should be fairly small. So the only thing I'm going to be doing that actually makes sense is to take y and find the point that's on this plane that's the closest to it. And that corresponds to doing an orthogonal projection of y onto this thing. And that's actually exactly x beta hat. OK? So in one dimension, just because this is actually a little hard. In one dimension, so that's if p is equal to 1. OK? So let's say this is my point. And then I have y, which is in two dimensions. So this is all on the plane. What it does, this is my, the point that's right here is actually x beta hat. That's how you find x beta hat. You take your point y, and you project it on the linear span of the columns of x. And that's x beta hat. This does not tell you exactly what beta should be. And if you know a little bit of linear algebra, it's pretty clear. Because if you want to find beta hat, that means you should be able to find the coordinates of a point in the system of columns of x. And if those guys are redundant, there's not going to be a unique coordinate for this guy. So that's why it's actually not easy to find. But x beta hat is uniquely defined. It's a projection. Yeah? AUDIENCE 1 Epsilon is the distance between the y. No, epsilon is the vector that goes from, right? So there's a true x beta. That's the true one. x beta hat is unlikely to be exactly equal to x beta, right? And then the epsilon is the one that starts from this line. It's the vector that pushes you away. So really, this is this vector. That's epsilon. So it's not a length, right? The length of epsilon is the distance. But epsilon is just the actual vector that takes you from one to the other. So this is all in two dimension. And it's probably much clearer than what's here. And so here, I claim that this x beta hat, so from this picture, I sort of implicitly claim that forming this, this operator that takes y and maps it into this vector, x times x transpose y, blah, blah, blah, this should actually be equal to the projection of y onto the linear span of the columns of x. That's what I just drew for you. And what it means is that this matrix must be the projection matrix. So of course, who knows linear algebra here? So what are the conditions that a projection matrix should be satisfying? AUDIENCE 1. Squares to itself. Squares to itself, right? If I project twice, I'm not moving. If I keep on iterating projection, once I'm in the space I'm projecting onto, I'm not moving. What else? Does it have to be symmetric, maybe? It's an orthogonal projection. Yeah, so this is an orthogonal projection. It has to be symmetric. And that's pretty much it, right? So from those things, you can actually get quite a bit of things. But what's interesting is that if you actually look at the eigenvalues of this matrix, they should be either 0 or 1, essentially. And they're 1 if the eigenvector associated is within the space, and 0 otherwise. And so that's basically what you can check. This is not an exercise in linear algebra, so I'm not going to go too much into the details. But this is essentially what you want to keep in mind. What's associated to orthogonal projections is Pythagoras' theorem. And that's something that's going to be useful for us. What it's essentially telling me is that if I look at this norm squared, it's equal to this norm squared. Sorry, this norm squared plus this norm squared is equal to this norm squared. And that's something, the norm of y squared, right? So Pythagoras tells me that the norm of y squared is equal to the norm of x beta hat squared plus the norm of y minus x beta hat squared. Agreed? It's just because I have a straight angle here. So that's this plus this is equal to this. So now, to define this, I made a new assumption. Epsilon could be as wild. I was just crossing my finger that epsilon was actually small enough that it would make sense to project onto the linear span, because I implicitly assumed that epsilon did not take me all the way there, so that I'm actually, it sort of makes sense to project back. And so for that, I need to somehow make assumptions that epsilon is well-behaved, and that it's not completely wild, that it's sort of moving uniformly in directions around, in all directions of the space. There's no privileged direction where it's always going. Otherwise, I'm going to make a systematic error. And I need that those epsilons are going to average somehow. So here are the assumptions we're going to be making so that we can actually do some statistical inference. The first one is that the design matrix is deterministic. So I started by saying I have xi, yi, and maybe they're independent here. They are, but the xi's I want to think as deterministic. If they're not deterministic, it can sort of condition on them. But otherwise, it's very difficult to think about this thing if I think of those entries as being random, because then I have the inverse of a random matrix, and things become very, very complicated. So we're going to think of those guys as being deterministic. We're going to think of the model as being homoscedastic. And actually, let me come back to this in a second. Homoscedastic, well, I mean, if you try to find the etymology of this word, homo means the same, scedastic means scaling. So what I want to say is that the epsilons have the same scaling. And since my third assumption is that epsilon is Gaussian, then essentially what I'm going to want is that they all share the same sigma squared. They're independent, so this is definitely the identity covariance matrix. And I want them to be centered as well. That means that there's no direction that I'm always privileging when I'm moving away from my plane there. So these are important conditions. It depends on how much inference you want to do. If you want to write t-test, you need all these assumptions. But if you only want to write, for example, the fact that your least squares estimator is consistent, you really just need the fact that epsilon has variance sigma squared. The fact that it's Gaussian won't matter, just like Gaussianity doesn't matter for a law of large number. Yeah? AUDIENCE 2 So the first assumption that x has to be deterministic, so x is made up of this x1, x2. PHILIPPE RIGOLLET. x is the matrix. AUDIENCE 2 Yeah, so those are random variables, right? PHILIPPE RIGOLLET. No, that's the assumption. AUDIENCE 2 OK. So I mean, once you collect the data and put it in the matrix, it becomes deterministic. So maybe I'm missing something. PHILIPPE RIGOLLET. So this is for the purpose of the analysis, right? I can actually assume that I look at my data and I think of this, right? So what is the difference between thinking of data as deterministic or thinking of it as random? When I talked about random data, the only assumptions that I made were about the distribution. I said, well, if my x is a random variable, I want it to have this variance, and I want it to have maybe this distribution, things like this. Here, I'm actually making an assumption on the values that I see. I'm saying that the value that you give me is the matrix is actually invertible. X transpose x will be invertible. So I've never done that before, assuming that some random variable, like assuming that some Gaussian random variable was positive, for example. We don't do that, because there's always some probability that things don't happen if you make things at random. And so here, I'm just going to say, OK, forget about here. It's basically a little stronger. I start my assumption by saying, the data that's given to me will actually satisfy those assumptions. And that means that I don't actually need to make some modeling assumption on this thing, because I'm actually putting directly the assumption I want to see. So here, either I know sigma square or I don't know sigma square. So is that clear? So essentially, I'm assuming that I have this model, where this guy now is deterministic. And this is some multivariate Gaussian with mean 0 and covariance matrix identity of Rn. That's the model I'm assuming. And I'm observing this, and I'm given this matrix X. Where does this make sense? You could say, well, if I think of my rows as being people and I'm collecting genes, it's kind of a little intense to assume that I actually know ahead of time what I'm going to be seeing and that those things are deterministic. That's true. But it still does not prevent the analysis to go through for one. And second, a better example might be this imaging example that I described, where those X's are actually libraries. Those are libraries of patterns that people have created, maybe from deep learning nets or something like this. But they've created patterns, and they say that all images should be representable as a linear combination of those patterns. And those patterns are somewhere in books, so they're certainly deterministic. Everything that's actually written down in a book is as deterministic as it gets. Any questions about those assumptions? Those are the things we're going to be working with. There's only three of them. One is about X. Actually, there's really two of them. This guy already appears here. So there's two, one on the noise, one on the X's. That's it. Those things allow us to do quite a bit. They allow me to write the distribution of beta hat, which is great, because when I know the distribution of my estimator, I know its fluctuations. If it's centered around the true parameter, I know that it's going to be fluctuating around the true parameter. And if you tell me what kind of distribution the fluctuations are, I actually know how to build confidence intervals. I know how to build tests. I know how to build everything. It's just like when I told you that asymptotically, the empirical variance was Gaussian with mean theta and standard deviation that depended on n, et cetera. That's basically the only thing I needed, and this is what I'm actually getting here. So let me start with this statement. So remember, beta hat satisfied this, so I'm going to rewrite it here. So beta hat was equal to X transpose X inverse X transpose Y. That was the definition that we found. And now I also know that Y was equal to X beta plus epsilon. So let me just replace Y by X beta plus epsilon here. Yeah? AUDIENCE MEMBER 2. Isn't it X transpose X inverse X transpose Y? PHILIPPE RIGOLLET. Yes, X transpose. Thank you. So I'm going to replace Y by X beta plus epsilon. So that's OK. And here comes the magic. I have an inverse of a matrix, and then I have the true matrix. I have the original matrix. So this is actually the identity times beta. And now this guy, well, this is a Gaussian, because this is a Gaussian random vector, and I just multiply it by a deterministic matrix. So we're going to use the rule that if I have, say, epsilon, which is n0 sigma, then B times epsilon is n0. Can somebody tell me what the covariance matrix of B epsilon is? AUDIENCE MEMBER 2. What is WB? PHILIPPE RIGOLLET. It's just a matrix. And for any matrix, I mean any matrix that I can pre-multiply, that I can post-multiply with epsilon. Right? Yeah? AUDIENCE MEMBER 3. B transpose B. PHILIPPE RIGOLLET. B transpose. And sigma is gone. AUDIENCE MEMBER 3. Oh, times sigma is gone. PHILIPPE RIGOLLET. That's a matrix, right? AUDIENCE MEMBER 3. B transpose sigma B. PHILIPPE RIGOLLET. Almost. Anybody wants to take a guess at the last one? I think we've removed all other possibilities. It's B sigma B transpose. So if you ever answered to the question, do you know Gaussian random vectors, but you did not know that, there's a gap in your knowledge that you need to fill, because that's probably the most important property of Gaussian vectors. When you multiply them by matrices, you have a simple rule on how to update the covariance matrix. OK? So here, sigma is the identity. Right? And here, this is the matrix B that I had here. So what this is is basically n, some multivariate n, of course. Then I'm going to have 0. And so what I need to do is B times the identity times B transpose, which is just BB transpose. And what it's going to tell me, it's X transpose X. Sorry, that's inverse. Inverse X transpose. And then the transpose of this guy, which is X, X transpose X, inverse transpose. But this matrix is symmetric, so I'm actually not going to make the transpose of this guy. And again, magic shows up. Inverse times the matrix, so those two guys cancel. And so this is actually equal to beta plus some n0, X transpose X inverse. Yeah? AUDIENCE 1 So you define that as the B matrix, and what happens? So I just apply this rule, right? So if I multiply a matrix by a Gaussian, then let's say this Gaussian had mean 0, which is the case of epsilon here. Then the covariance matrix that I get is B times the original covariance matrix times B transpose. So all I did is write this matrix times the identity times this matrix transpose. And the identity, of course, doesn't play any role, so I can remove it. It's just this matrix times the matrix transpose. And what happens? So what is the transpose of this matrix? So I sort of use the fact that if I look at X transpose X inverse X transpose, and now I look at the whole transpose of this thing, that's actually equal to, and I use the rule that AB transpose is B transpose A transpose. Let me finish. And it's X X transpose X inverse. Yes? Yes? I'm supposed to be for epsilon, that it was sigma squared. Oh, thank you. There's a sigma squared somewhere. So this was sigma squared times the identity, so I could just pick up a sigma squared everywhere. So here, in our case, so for epsilon, this is sigma. Sigma squared times the identity, that's my covariance matrix. You seem perplexed. It's just kind of a new idea for me to think of a maximum likelihood estimator as a random variable. Oh, it should not be. Any estimator is a random variable. Oh, yeah, that's a good point. And I have not told you that this was the maximum likelihood estimator just yet, right? An estimator is a random variable. There's a word some people use, estimate, just to differentiate the estimator while you're doing the analysis with random variables and the values when you plug in the numbers in there. But then, of course, people use estimate because it's shorter, so then it's confusing. All right, so any questions about this computation? Did I forget any other Greek letter along the way? All right, I think we're good. So one thing that it says, and actually, thank you for pointing this out, I said there's actually a little hidden statement there. OK, by the way, this answers this question, right? Beta hat is of the form beta plus something that's centered. So it's indeed of the form Gaussian with mean beta and covariance matrix sigma square x transpose x inverse, right? So that's very nice. As long as x transpose x is not huge, I'm going to have something that is close to what I want. Sorry, x transpose x inverse is not huge. All right, so there's a hidden claim in there, which is that the least squares estimator is equal to the maximum likelihood estimator. Why does the maximum likelihood estimator just enter the picture now? We've been talking about regression for the past 18 slides, and we've been talking about estimators, and I just dumped on you the least squares estimator, but I never really came back to this thing that we know, maybe the method of moments, or maybe the maximum likelihood estimator. It turns out that those two things are the same, but if I want to talk about a maximum likelihood estimator, I need to have a likelihood, and in particular, I need to have a density. And so if I want a density, I have to make those assumptions, such as the epsilons have this Gaussian distribution. So why is this the maximum likelihood estimator? Well, remember, y is x transpose beta plus epsilon. So I actually have a bunch of data. So what is my model here? Well, it's the family of Gaussians on n observations with mean x beta, variance sigma square identity, and beta lives in our p. Here's my family of distributions. That's the possible distributions for y. And so in particular, I can write the density of y. Well, what is it? It's something that looks like p of x. Well, p of y, let's say, is equal to 1 over. So now it's going to be a little more complicated, but it's sigma squared times 2 pi to the p over 2 exponential minus norm of y minus x beta squared divided by 2 sigma squared. So that's just the multivariate Gaussian density. I just wrote it. That's the density of a multivariate Gaussian with mean x beta and covariance matrix sigma squared times the identity. That's what it is. So you don't have to learn this by heart, but if you are familiar with the case where p is equal to 1, you can check that you recover what you're familiar with. And this sort of makes sense as an extension. So now I can actually write my log likelihood, right? How many observations do I have of this vector y? Do I have n observations of y? I have just one, right? This, oh, sorry. I shouldn't have said p. This is n. Everything is in dimension n. So I can think of either having n independent observations of each coordinate, or I can think of having just one observation of the vector y. So when I write my log likelihood, it's just the log of the density at y. And that's the vector y, which I can write as minus n over 2 log sigma squared 2 pi minus 1 over 2 sigma squared norm of y minus x beta. And that's, again, my boldface y. And what is my maximum likelihood estimator? Well, this guy does not depend on beta. And this is just a constant factor in front of this guy. So it's the same thing as just minimizing, because I have a minus sign, overall beta in our p y minus x beta squared. And that's my least squares estimator. OK? Is there anything that's unclear on this board? Any question? So all I used was I wrote my log likelihood, which is just the log of this expression where y is my observation. And that's indeed the observation that I have here. And that was just some constant minus some constant times this quantity that depends on beta. So maximizing this whole thing is the same thing as minimizing only this thing. The minimizers are the same. And so that tells me that I actually just have to minimize this squared norm to get my maximum likelihood estimator. But this used heavily the fact that I could actually write exactly what my density was. And that when I took the log of this thing, I had exactly the square norm that showed up. If I had a different density, if, for example, I assume that my coordinates of epsilons were, say, iid exponential, double exponential random variables, right? So it's just half of an exponential on the pluses, half of an exponential on the negatives. So if I said that, then this would not have the square norm that shows up. This is really idiosyncratic to Gaussians. If I had something else, I would have maybe a different norm here, or something different that measures the difference between y and x beta. And that's how you come up with other maximum likelihood estimators that leads to other estimators that are not the least squares, maybe the least absolute deviation, for example, or this fourth moment, for example, that you suggested last time. So I can come up with a bunch of different things. And they might be tied. Maybe I can come up from them from the same perspective that I came from the least squares estimator. I said, let's just do something smart and check, then, that it's indeed the maximum likelihood estimator. Or I could just start with the modeling and check, then, what was the implicit assumption that I put on my noise. Or I could start with the assumption of the noise, compute the maximum likelihood estimator, and see what it turns into. All right. So that was the first thing. I've just proved to you the first line. And from there, you can get what you want. So all the other lines are going to follow. So what is beta hat? So for example, let's look at the second line, the quadratic risk. Beta hat minus beta, from this formula, has a distribution which is n, n, 0. And then I have x transpose x inverse, right? Sorry, the dimension of what? AUDIENCE 2 A beta hat minus beta. Isn't beta only a p-dimensional? Oh, yeah, you're right. That was all p-dimensional there. Sorry. Yeah, because if b here, the matrix that I'm actually applying, has dimension p times n. So even if epsilon was an n-dimensional Gaussian vector, then b times epsilon is a p-dimensional Gaussian vector now. So that's how I switch from p to n, from n to p. Thank you. So you're right. This is beta hat minus beta is this guy. And so in particular, if I look at the expectation of the norm of beta hat minus beta squared, what is it? It's the expectation of the norm of some Gaussian vector. Sorry, of the norm of some Gaussian vector. And so it turns out, so maybe we don't have, well, OK, that's just also a property of a Gaussian vector. So if epsilon is n0 sigma, then the norm, the expectation of the norm of epsilon squared, is just the trace of sigma. Actually, we can probably check this by saying that this is the sum from j equal 1 to p of the expectation of beta hat j minus beta j squared. Since beta j squared is the expectation, beta j is the expectation of beta hat, this is actually equal to the sum from j equal 1 to p of the variance of beta hat j, just because this is the expectation of beta hat. And how do I read the variances in a covariance matrix? They're just the diagonal elements, right? So that's really just sigma jj. And so that's really equal to, so that's the sum of the diagonal elements of this matrix. Let's call it sigma. So that's equal to the trace of x transpose x inverse. OK, the trace is the sum of the diagonal elements of a matrix. And I still had something else. I'm sorry, this was sigma squared. I forget it all the time. So the sigma squared comes out. It's there. And so the sigma squared comes out, because the trace is a linear operator. I multiply all the entries of my matrix by the same number, then all the diagonal elements are multiplied by the same number. So when I sum them, the sum is multiplied by the same number. OK, so that's for the quadratic risk of beta hat. And now I need to tell you about x beta hat, right? x beta hat was something that was actually telling me that that was the point that I reported on the red line that I estimated, right? That was my x beta hat. That was my y minus the noise. Now, this thing here, so remember, we had this line, and I had my observation. And here, I'm really trying to measure this distance squared. This distance is actually quite important for me, because it actually shows up in Pythagoras' theorem, right? And so you can actually try to estimate this thing. So what is the prediction error? So we said we have y minus x beta hat. So that's the norm of this thing we're trying to compute. But let's write this for what it is for one second, right? So we said that beta hat was x transpose x inverse x transpose y, and we know that y is x transpose beta plus epsilon. So let's write this, x beta plus epsilon plus x. OK? And actually, maybe I should not write it. Let me keep the y for what it is now. OK? So that means that I have essentially the identity of Rn times y minus this matrix times y, so I can factor y out. And that's the identity of Rn minus x x transpose x inverse x transpose the whole thing times y. We said that this, we call this matrix P, because this was the projection matrix onto the linear span of the x's. So that means that if I take a point x and I apply p times x, I'm projecting onto the linear span of the columns of x. What happens if I do i times p times i minus p times x? It's x minus px, right? So if I look at the point on which, so this is the point on which I project. This is x. I project orthogonally to get p times x. And so what it means is that this operator, i minus px, is actually giving me this guy, this vector here, x minus p times x. Let's say this is 0. This means that this vector, I can put it here. It's this vector here. And that's actually the orthogonal projection of x onto the orthogonal complement of the span of the columns of x, right? So if I do x, if I project x, or if I look at x minus its projection, I'm basically projecting onto two orthogonal spaces. What I'm trying to say here is that this here is another projection matrix p prime. It's just the projection matrix onto the orthogonal projection onto orthogonal of column span of x. Orthogonal means the set of vectors that's orthogonal to everyone in this linear space. So now, when I'm doing this, this is exactly what, I mean, in a way, this is sort of illustrating this Pythagoras theorem. And so when I want to compute the norm of this guy, the norm squared of this guy, I'm really computing, if this is my y now, this is px of y, I'm really controlling the norm squared of this thing. OK, so if I want to compute the norm squared. So I'm almost there. So what am I projecting here onto the orthogonal projector? So here, y now, I know that y is equal to x beta plus epsilon. So when I look at this matrix p prime times y, it's actually p prime times x beta plus p prime times epsilon. What's happening to p prime times x beta? Let's look at this picture. So we know that p prime takes any point here and projects it orthogonally on this guy. But x beta is actually a point that lives here. It's something that's on the linear span. So where do all the points that are on this line get projected to? The origin, to 0, right? They all get projected to 0. And that's because I'm basically projecting something that's on the column span of x onto its orthogonal. So that's always 0 that I'm getting here. So when I apply p prime to y, I'm really just applying p prime to epsilon. So I know that now this actually is equal to the norm of some multivariate Gaussian. What is the size of this Gaussian? What is the size of this matrix? Well, I actually had it there. It's i n, right? So it's n dimensional. So it's some n dimensional with mean 0. And what is the covariance matrix of p prime times epsilon? No? AUDIENCE MEMBER 2 P. p transpose. Yeah, p prime, p prime transpose, right? Which we just said p prime transpose is p. So that's p squared. And we said that when we project twice, it's as if we projected only once. So here, this is n 0 p prime p prime transpose. That's the formula for the covariance matrix. But this guy is actually equal to p prime times p prime, which is equal to p prime. So now what I'm looking for is the norm squared of the trace. So that means that this whole thing here is actually equal to the trace. Oh, did I forget again an epsilon, a sigma square? Yeah, I forgot it only here, which is good news. So I should assume that sigma square is equal to 1. So sigma square is here. And then what I'm left with is sigma square times the trace of p prime. At some point, I mentioned that the eigenvalues of a projection matrix were actually 0 or 1. The trace is the sum of the eigenvalues. So that means that the trace is going to be an integer number. It's the number of non-zero eigenvalues. And the non-zero eigenvalues are just the dimension of the space onto which I'm projecting. Now, I'm projecting from something of dimension n onto the orthogonal of a space of dimension p. What is the dimension of the orthogonal of a space of dimension p when thought of a space in dimension n? n minus p. That's the so-called rank theorem, I guess, as a name. And so that's how I get this n minus p here. This is really just equal to n minus p. Yeah? AUDIENCE 2 Here is the expectation of the whole thing. PHILIPPE RIGOLLET Yes, you're right. So that's actually the expectation of this thing that's equal to that, absolutely. But I actually have much better, right? I know even that the norm that I'm looking at, I know it's going to be this thing. What is it going to be the distribution of this guy? Norm squared of a Gaussian. Chi-square. So there's going to be some chi-square that shows up. And the number of degrees of freedom is actually going to be also n minus p. And maybe it's actually somewhere. Yeah, right here. n minus p times sigma hat squared over sigma squared. So this is my sigma hat squared. If I multiply n minus p, I'm left only with this thing. And so that means that I get sigma square times, because I always forget my sigma square, I get sigma square times this thing. It turns out that the square norm of this guy is actually exactly a chi-square with n minus p degrees of freedom. So in particular, we know that the expectation of this thing is equal to sigma square times n minus p. So if I divide both sides by n minus p, I'm going to have that something whose expectation is sigma square. And this something I can actually compute. It depends on y and x that I know and beta hat that I've just estimated. I know what n is and pr are the dimensions of my matrix x. So I'm actually given an estimator whose expectation is sigma square. And so now I actually have an unbiased estimator of sigma square. That's this guy right here. And it's actually super useful. That's something that's basically, so those are called the normalized sum of square residuals. Those are called the residuals. Whatever is residual when I project my points onto the line that I've estimated. And so in a way, those guys, if you go back to this picture, this was yi, and this was xi transpose beta hat. So if beta hat is close to beta, the difference between yi and xi transpose beta should be close to my epsilon i. It's some sort of epsilon i hat. Agreed? And so that means that if I think of those as being epsilon i hat, they should be close to epsilon i. And so their norm should be giving me something that looks like sigma squared. And so that's why it actually makes sense. It's just magical that everything works out together, because I'm not projecting on the right line. I'm actually projecting on the wrong line. But in the end, things actually work out pretty well. There's one thing. So here, the theorem is that this thing not only has the right expectation, but also has a chi-square distribution. That's what we just discussed. So here, I'm just telling you this. But it's not too hard to believe, because it's actually the norm of some vector. You could make this obvious. But again, I didn't want to bring in too much linear algebra. So to prove this, you actually have to diagonalize the matrix P. So you have to invoke the eigenvalue decomposition and the fact that the norm is invariant by rotation. So for those who are familiar with what I can do, let's just look at the decomposition of P prime into u du transpose, where this is an orthogonal matrix and this is a diagonal matrix of eigenvalues. And when I look at the norm squared of this thing, I mean, I have basically the norm squared of P prime times some epsilon. It's the norm of u du transpose epsilon squared. The norm of a rotation of a vector is the same as the norm of the vector. So this guy goes away. This is not actually, I mean, you don't have to care about this if you don't understand what I'm saying. So don't freak out. It's really for those who follow. What is the distribution of u transpose epsilon? I take a Gaussian vector that has covariance matrix sigma squared times the identity, and I basically rotate it. What is its distribution? Yeah? It's the same. It's the same. It's completely invariant, right? Because the Gaussian think of all directions as being the same. So it doesn't really matter if I take a Gaussian or a rotated Gaussian. So this is also a Gaussian, so I'm going to call it epsilon prime. And I'm left with just the norm of epsilon primes. So this is the sum of the dj's squared times epsilon j squared. And we just said that the eigenvalues of P are either 0 or 1, because it's a projector. And so here I'm going to get only 0's and 1's. So I'm really just summing a certain number of epsilon i squared, so squared of standard Gaussians, sorry, with a sigma squared somewhere. And basically, how many am I summing? Well, the n minus p, the number of non-zero eigenvalues of p prime. So that's how it shows up. This, when you see this, what theorem am I using here? Cochrane's theorem. This is this magic book. I'm actually going to dump everything that I'm not going to prove to you and say, well, this is actually Cochrane's. No, Cochrane's theorem is really just telling me something about orthogonality of things, and therefore independence of things. And Cochrane's theorem was something that I used when I wanted to use what? That's something I used just one slide before, student t-test. I used Cochrane's theorem to see that the numerator and the denominator of the student statistic were independent of each other. And this is exactly what I'm going to do here. I have, I'm going to actually write a test to test maybe if the beta j's are equal to 0. I'm going to form a numerator, which is beta hat minus beta. This is normal. We know that beta hat has a Gaussian distribution. I'm going to standardize by something that makes sense to me. And I'm not going to go into details because we're out of time. But there's the sigma hat that shows up. And then there's a gamma j, which takes into account the fact that my x's, if I look at the distribution of beta, which is gone, I think. Yeah, beta is gone. Oh, yeah, that's where it is. The covariance matrix depends on this matrix x transpose x. So this will show up in the variance. In particular, the diagonal elements are going to play a role here. And so that's what my gammas are. The gammas is the j-th diagonal element of this matrix. So we'll resume that on Tuesday. So don't worry too much. If this is going too fast, I'm not supposed to cover it. But just so you get a hint of why Cochrane's theorem actually was useful. All right, so I don't know if we actually ended up recording. I have your homework. And as usual, I will give it to you outside.