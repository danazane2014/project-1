 OK, so last time we discussed orthonormal bases, and then we considered the concrete question of the complex exponentials being an orthonormal basis for L2 of minus pi to pi, that Fourier series actually converge to an L2 function in the L2 norm. So now we're going to go back to a general discussion of Hilbert spaces. Kind of the way the rest of the course will be will be some general theory followed by some concrete applications scattered in concrete in the sense that it just won't be about general Hilbert spaces, but about maybe specific operators, specific problems we're trying to solve. OK, so now we're going to discuss link minimizers. And what do I mean by that? I mean by the following is if I have a closed subspace of a Hilbert space, then I could define, of course, so way back when we could then define a norm on H modulo this closed Hilbert space, and the norm on that is the infimum of the norm of v minus w, where w, little w, is in the linear subspace capital W. A natural question is, is that infimum, is that minimum, that minimal distance, actually achieved by some element in the subspace? Now, the answer is, in fact, yes. And it's, in fact, true for a larger class of subsets of Hilbert spaces than closed subspaces. So we have the following. Suppose C is a subset of a Hilbert space, of a Hilbert space H, such that three conditions hold, kind of the silliest. C is non-empty. C is closed. And the final is that C is what's called convex. This means the following, that if v1 and v2 are in the subset C, and T is in 0, 1, then T v1 plus 1 minus T v2 is in C. So another way of stating this last condition is that for any two elements in C, the line segment from v1 to v2 is contained in C. So this is nothing but an element on the line segment joining v1 to v2. So here we have what would look like a convex set C. And for every v1, v2, it's convex, meaning the line segment joining them is in there. So you wouldn't have, let's say, you wouldn't have. So this is convex. And so for example, something like a, this picture is not convex, because I could take two elements v1, v2 here, and the line segment does not stay in the set C. OK? All right. So if we assume that we have a non-empty closed and convex subset of the Hilbert space H, then there exists a unique element v in C such that, I should say, yeah, v in C such that v is equal to the minimum of the length of u in C. OK? Now, we'll see when we apply this theorem how such C's pop up. One of them is, like I said a minute ago, that, OK, I didn't exactly say it this way, but C being a vector plus, so C could be some fixed vector plus a subspace, meaning the set of all vectors of the form v plus w, where w is in some subspace. But that's not necessarily the only kind we'll come across. OK, so if we have a closed convex non-empty subset of a Hilbert space, then there exists a unique minimizer in this subspace. So of course, I encourage you to think about if I drop either of these conditions, whether or not this theorem remains true. So it's clear you cannot drop C being closed. So for example, so this is just a remark. You cannot drop the condition C closed. For example, let's say you have an open ball in R2 now. And you take just any point outside the ball. Then the minimum, let's see, let's, I've got this backwards. Let's take C to be everything outside the ball. So it's neither closed nor convex. Then of course, this will not have a minimum length because, or this subset will not have a vector that has minimum length because the minimum length will want to be on this, let's say that's radius 1, on the circle of radius 1, which is not included in the set C. OK? All right. And OK, so that one's taken away. Both conditions that C is closed and convex. Of course, you can do something where it's just convex but not closed. For example, take a square now in R2. Then let's say this is minus 1, 1. This is 1, the corner's at 1, 1. And there's the origin. Then the vector of minimum length will want to be right here at the point 0, 1. But that's not in C. OK? So you can also show that for C given by this rectangle missing a spot, it will not have a minimizer. OK? And then you can play with this and find a closed subset that is not convex. That doesn't have a unique minimizer. OK, so these conditions are necessary for this theorem to hold. It's not just me giving you a lesser or a theorem that's with assumptions that are much stronger than you need. All right, so OK. So what's the proof? So this infimum, so let me recall just real quick that, so something, let's say A, is equal to the infimum of a set S where S is a subset of the real numbers, if and only if two things, A is a lower bound for S and there exists a sequence Sn and S such that Sn converge to A. OK, so this should have been covered in 18-100. All right? So let's call this number d. So let d be the infimum of u, which we know exists because norms are bounded below by 0. So this set of norms where u ranges over c is bounded below by 0, so the infimum exists. OK, so let d be that. Then there exists a sequence un of elements in C such that the norms of these un converge to d. OK? So I now make the claim that this sequence, what we're trying to do is come up with some element in C that achieves this d, right, so that the norm of v is equal to this d. So I now claim that this sequence is, in fact, Cauchy. Cauchy. All right? And you'll see how we use the Hilbert space structure of or the inner product structure that comes with the Hilbert space. So we'll just do this the old-fashioned way. Let epsilon be positive. We now need to find a capital N so that the difference between un and um is less than epsilon and norm for all n and m bigger than or equal to capital N. So since the norm of un is converging to d, there exists a natural number n such that for all n bigger than or equal to n, I have that 2 un squared is less than 2 d squared plus epsilon squared over 2. So this thing here is converging to 2 times d squared as n goes to infinity. So if I perturb 2 d squared by a little bit, then since this is converging to 2 d squared, that will be less than this for insufficiently large. Now, I claim this capital N works. And for all n, m bigger than or equal to capital N, if I take the norm, now I'm going to use this parallelogram law that I have for norms, which crucially relies on the Hilbert space property. So this I can write as 2 norm u squared plus 2 norm um squared minus 4 times un plus um over 2 squared. Now, what's the point? Now, un plus um divided by 2, that's, if you like, t equals 1 half in condition c. Now, since un and um are in c, un plus um over 2 is in c as well. So this thing is in c. And therefore, its norm squared is bigger than or equal to d squared. So when it gets hit with a negative, that switches the inequality. This norm squared is less than or equal to, or minus 4, this norm squared is less than or equal to minus 4 d squared. So this is less than or equal to 2 un squared plus 2 um squared minus 4 d squared, again, because d is a lower bound for the norms of all elements in c, and this element is in c. So and this minus sign flips. And now, based on how we've chosen capital N, we've chosen it so that we have this inequality here. I get this is less than 2d squared plus epsilon squared over 2 from the first one plus 2d squared plus epsilon squared over 2 from the second one, and then minus d squared from this last one, and that equals epsilon squared. So for all n, m bigger than or equal to capital N, the norm squared of un minus um is less than epsilon squared. And therefore, that proves the claim that this sequence is Cauchy. Now, since this sequence is Cauchy, and we're in a Hilbert space, a complete inner product space, there's a limit such that un converges to v. Now, c is closed, so v is also in H. Since c is closed, it contains all limits, all sequential limits. So c is closed is equivalent to for every sequence converging to something, that something has to be in the set. v is in c. Finally, since the un's converge to v, we have v norm is equal to the limit as n goes to infinity of the norms of the un's. And remember, where did we come up with these un's? They are supposed to be, their norms are supposed to be converging to d. So v is an element in c whose norm gives me d. Now, there's one last statement in the theorem that v is unique. There can't be two of them. And it follows from a similar argument that we made here. So what we've shown so far is that there exists a v in c whose norm gives me d. Now we claim that there can't be more than one. Suppose v and v bar are in c, and both of their norms give me d, this infimum of norms over c. Now I use the parallelogram law again. I get that norm of v minus v bar squared, this is equal to norm squared of v plus norm of v bar squared with a 2 plus 2 minus 4 norm v plus v bar squared. OK? Now, this equals d squared. This equals d squared. So this combines and gives me 4d squared minus the same thing. And again, since c is convex and both v and v bar are in c, v bar does not mean complex conjugate. It just means something other than v. So since both of these elements are in c, their midpoint, which is, again, t equals 1 half if you like in condition c for this theorem, this is also in c. And since d is the smallest of all the norms, the norm squared has to be bigger than or equal to d squared. And when I hit a minus sign, that flips the inequality. So that's less than or equal to 4d squared minus 4d squared equals 0. And therefore, this norm squared has to be 0, i.e. v equals v bar. OK? Now, this simple theorem has important consequences, which we're going to obtain from it. So the first application of this theorem is going to be to a way to always decompose a Hilbert space if we're given a closed linear subspace, which you're kind of used to if you're working in Rn or Cn. But we haven't touched on yet for Hilbert spaces. And there's a reason, because we didn't have the technology yet. But now, let's discuss orthocomplements. So let's see. OK, I actually spelled it correctly. When I was at the University of Chicago, I sent my thesis to my advisor. And he had some questions about math points of the thesis that I needed to make a little bit clearer. These weren't big deals math-wise. But of course, every time I got an email from my advisor with a question about my thesis, it struck the fear of God in me. But everything was fine. Everything was fixable. And then the last comment he made was there was like 50 times throughout the paper that I needed to, or the thesis that I needed to change complement, because I kept spelling it complement with an I. So complement meaning what's not in there. Complement is something nice you say. And so mistakes get made sometimes. OK, so for the complements we're interested in, we have the following, if H is a Hilbert space and W is a linear subspace, then the following set, W perp, which is equal to the set of all U and H, which are orthogonal to everything in W. So U inner product W equals 0 for all W and W. This is a closed linear subspace of H. So that's the first part. If the subspace we started with, W, is closed, then in fact, we can write H as the direct product of, or the direct sum, I'm sorry, of W and its orthogonal complement. What does this mean? Again, I'll recall from linear algebra what this means, i.e. make sure I don't leave anything out. i.e. for all U and H, there exists a unique W and W, W perp and W perp, little w, such that U is equal to W plus W perp. OK. OK. So it's simple to show that. So to see that W perp is a subspace of H, what I have to check is that linear combinations of elements of W perp remain in W perp. But this is kind of clear. If U inner product W1 is equal to 0 for all U in capital W, no, no, backwards. If U1 inner product W is equal to 0 for all W in capital W, and U2 inner product W is equal to 0 for all W in capital W, then the linear combination of U1 and U2 will be orthogonal to W for all little w in capital W. So that's pretty easy to see why it's a subspace. And the only thing these two subspaces have in common is the 0 vector, right? Because if I have something in W and W perp, then it must be orthogonal to itself. And one of the conditions that we have for an inner product is that it's positive definite. If I have something orthogonal to itself, it has to be the 0 vector. So that's why we get these fairly quickly. Why is W perp, why is this closed? This follows from the continuity of the inner product. So to show the W perp is closed, let a un be a sequence in W perp and a u in H such that un converges to u. So W perp is closed if we show that u is, in fact, in W perp. That's the condition of a subset of a metric space being closed. It's closed. I mean, there's several different ways to phrase this, but the most useful one is often that a subset is closed if and only if it's closed under taking sequential limits. Every limit of a sequence is contained in the set. OK, so we need to verify that if I have a sequence of elements in W perp converging to something, then that something has to be in C. Now let little w be in capital W, then the inner product of this limit u with w by continuity of the inner product, since the un's are converging to u, this is equal to the inner product of un with w, taking the limit. And all of these are 0 for all n, so this equals 0. And therefore, this inner product is 0 for all w in capital W. But this is the condition that u is in W perp. Thus, u is closed. Now let's do the second part. So maybe I should have numbered these. 1, 2, so this is a proof of 1. So now we're on to proof of the statement 2. If w is closed, then h is equal to the direct product of w and w perp. So now suppose w is closed. Suppose if w is just the entire Hilbert space, then clearly the only thing orthogonal to everything is the 0 vector. And we have the decomposition as trivially. So let's assume w is not the entire space. So now we actually have something to check. Let u be in H, take away w. I can't remember if the backslash goes that way or that way. I don't mean H modulo w. I mean H take away the set w. And let's define set C to be, OK, maybe it looks like I'm just lying to you about not having to deal with H mod w. But in any case, the actual set u plus capital W, meaning set u plus little w, little w and capital W. All right, so just this set. Now I claim that C is, so first off, C is clearly non-empty. It contains u. So it's non-empty. I claim it's also closed. So well, first, let's do the easier bit that it's convex. Convex since if u plus w1 is in C, u plus w2 is in C. So these are two elements now in C. They're of the form u plus an element of capital W. w1, w2 are in capital W. And t is between 0 and 1. Then t times u plus w1 plus 1 minus t times u plus w2. This equals t times u plus 1 minus t times u just gives me back u plus tw1 plus 1 minus tw2. And now you note that w1 and w2, they're part of a, they're elements in a subspace capital W. And therefore, this linear combination of them is also in W. So therefore, u plus this element is in C, which is all elements of the form u plus w for w and capital W. So C is convex. Now let's just show C is closed. Now why is C closed? So suppose u plus wn, so this is a sequence of elements in C. So each of these is in C, converges to an element. Let's call it v in H. We want to show that v is in C. Want to show v is in C. Now u plus wn converging to v implies that wn converges to v minus u. OK? And since wn is coming from a closed subspace, remember, this is actually what we're using, the fact that w is closed. So this implies, since w is closed, v minus u is in W. And since v is in, since v minus u is in W, this implies that v is equal to u plus some element w with w in capital W, i.e. v is in C. Again, capital C is the set of all elements of the form little u plus something from capital W. OK? So we've shown that if we have a sequence converging to an element in H, then that limit must be in the set. So C is closed. All right? So let me draw a picture now of what's going on. Let's imagine that W, the subspace W is, so imagine H is R2, and W is just the x-axis. And u is this vector here. u plus w is now the horizontal line that goes through this point. This is u plus w. Let's just call it C. OK? OK? Now, we have this set C. We have W. We would like to break up u into an element which is parallel to W and something that's perpendicular to W. Right? OK? Now, based on this picture, what does, let's call this element v. This is not the same v from before. So new v. We would like for this v to be perpendicular to C. And based on this picture, what would it satisfy? It would be the element of C of minimal length. Right? Right? So that's how we'll define v. Or if you like, that's the element that's the W perp. It'll end up being the perpendicular part. OK? So since C is closed and convex, there exists a unique element v in C such that norm of v equals the infimum of all elements, the norms of the elements in C. But I'll write it a slightly different way, W and capital W, of norm of u plus W. Right? Because this is all of the elements in C come in this way. So this is the way this norm of v is, or the way this infimum can be written. OK? So I've identified a candidate for the part that will be orthogonal, and then simply u minus v will be the part that's in C, hopefully. And let's check this. OK? So claim. OK. So first off, let's note a simple thing. Note that v in C implies that u minus v is in W. Right? v is of the form u plus little w. So u minus that has to be an element of W. And we have that u is equal to u minus v plus v. So this would be the element of W. And we hope to show that this element is in W perp. OK? OK. So now we claim little w is in capital W perp. Now, how we do this is what's called, I guess, a variational argument, or Euler-Lagrange equations argument. But in any case, something is the infimum of this, if and only if v satisfies certain equations. OK? Or not if and only if, but v being the infimum of this implies that v satisfies certain equations. If you've taken classical mechanics, those equations end up being the Euler-Lagrange equations. But anyways, I claim v is in W perp. So let w be in capital W. We want to show v inner product W equals 0. Let f of t be the norm of v plus tw squared, which this is just a polynomial in t. This is equal to norm v squared plus t squared norm w squared plus 2 real part of 2t real part vw. So it's just a polynomial. Now, what do we know? So an f of t has a minimum at t equals 0, right? Because for each t, this is an element of capital C, right? And the norm of everything in capital C is minimized exactly at v, which is t equals 0. So this has a minimum at t equals 0, which implies f prime evaluated at t equals 0 is 0. And therefore, if I take the derivative of f prime of t and set t equals to 0, then I just pick up twice the real part of vw equaling 0. And therefore, I get real part of vw equals 0. So I got that the real part of the inner product is 0. Now, it's not. And so what we can do is then repeat the previous argument with i times w in place of w to get that the real part of v inner product iw, which is equal to, in fact, the imaginary part of vw equals 0. And therefore, the inner product of v with w is 0 since its real part and imaginary part equals 0. And therefore, vw equals 0, and v is in the orthogonal complement. And so v is in the orthogonal complement, and u can be written as something in w plus something in w perp. Now, why is this? So let's take just two seconds to say why this decomposition is unique. It's unique because the only thing the two subspaces have in common is the 0 vector. 0 vector. So if I have u is equal to two different decompositions, w1 plus w1 perp equals w2 plus w2 perp, where each of these is in capital W, each of these is in the orthogonal complement of capital W. I didn't say I actually, that's the terminology I'm using, but W perp, I'll call the orthogonal complement. Then this implies that w2 minus w1 is equal to w1 perp minus w2 perp. And this is in W. This is in W perp. And therefore, since the only thing in W and W perp is 0, that implies that the left and right side have to be 0. And therefore, w equals w1. w1 perp equals w2 perp. And that gives us the uniqueness of the decomposition. So it's in the assignment, but I should say the optional assignment, but subspace. So I have a subspace W. I can take its orthogonal complement. If it's closed, then h is equal to W plus W perp. But if I just have an arbitrary subspace and I take its orthogonal complement, I can then take the orthogonal complement of that. What do I get? Well, the orthogonal complement of a set is always closed. So I may not get back the actual subspace again, but I will get back its closure. So the closure of W, which you can check is, again, a subspace, is equal to the orthogonal complement of the orthogonal complement. So in particular, if W is closed, then the orthogonal complement of the orthogonal complement is the set again. So this is in the optional subspace, in the optional assignment from the last week. All right, now, given a closed linear subspace, I can define an operator. So let's just call it a map for now that takes a vector u and spits out, let's say, the part that's in capital W. It's W part. Now, what kind of operator is that? Or I could have said it takes an element u and spits out the part that's in the orthogonal complement of capital W. What kind of map is that part, or is that? So there's a very special name for that. So of course, in R2, if I have an element u, so let's say W is the x-axis, W perp would then be the y-axis. And then this would be the part that's in W. This vector would be the part that's in W perp. Now, what exactly is this usually called, at least going back to your calculus days, usually referred to it as the projection of u onto the, let's say, x-axis. But that name has, or that word has a very specific meaning. And then we'll show that what I was just discussing, taking u to its W part or its W perp part is, in fact, a projection. So a bounded linear operator P going from H to H is a projection if P squared equals P. So this is a new bit of terminology. So for example, it doesn't have to exactly look like this or come from this way I was describing of obtaining a map from H to W or W perp. That's not the definition of a projection. The definition of a projection is this. So for example, taking everything to 0 is certainly a projection, which I guess you could think of as projecting onto the subspace consisting only of 0. But what I want to say is that, in fact, the map that we just kind of outlined is, in fact, a projection as defined here. So let H be Hilbert space as usual, W a closed subspace. So then by the previous theorem, we have H is equal to the direct sum of W and its orthogonal complement. The map pi sub W going from H to H given by H is defined by the following. If V is equal to W plus W perp, so I take an element in the Hilbert space, decompose it as a part that's in W and as a part that's in W perp, then the definition of this map evaluated on V is just W. This map is a projection. So we need to show that it is a bounded linear operator, and its square gives you back the original map. OK, so first let's show this map is linear. So first claim is that pi is linear. So if I have V1 is equal to W1 plus W1 perp, V2 is equal to W2 plus W2 perp, and I have two scalars, lambda 1, lambda 2, complex numbers, then lambda 1 times V1 plus lambda 2 V2, this is equal to multiplying out and combining, this is equal to lambda 2 W2 plus lambda 1 W1 perp plus lambda 2 times W2 perp. Now, since W1 and W2 are in W, this linear combination of them is also in W. And this part also, since that's in the orthogonal complement of W and so is that, their linear combination is also in the orthogonal complement. So the decomposition of this linear combination of V1 and V2 is a linear combination of the decompositions. And therefore, by how we define this map as the part that's in W, this is equal to lambda 1 W1 plus lambda 2 W2. And W1, that's just pi sub W. That's just projection. I'm calling it projection, although I haven't proved that yet. This thing applied to V1 and W2 is this thing applied to V2. By definition, this is pi W2 is pi by definition. So this is equal to lambda 1 V1 plus lambda 2 V2. And therefore, this map is linear. OK, so it's linear. Why is it bounded? So now, pi is bounded. V is equal to W plus W perp. W is, again, equal to pi of V. Then because these two things are orthogonal, I get that norm of V squared is equal to norm of W plus W perp squared. And now, what do I pick up? I pick up the norm of W squared plus norm of W perp squared plus 2 times the real part of the inner product of W and W perp, but that's 0. So I just get sum of the norms. And that's bigger than or equal to, or since it's a sum of two non-negative things, that's bigger than or equal to one of them, which is W squared. Or rephrasing, since W is equal to pi applied to V, I've said that this is less than or equal to the norm of V, so that pi is a bounded linear operator. OK? In fact, what we've shown is that its norm is less than or equal to 1. OK. And finally, so the last piece that we need to check is that pi squared equals pi, but that's pretty easy to check. Simply note that if V is less than or equal to 1, we simply note that if V is equal to W plus W perp, then I need to check that I get pi W of V again. So this is equal to the part of this that's in W. Now, this is equal to W, little w. And again, this picks out the part of the element in here that's in capital W, but this is in capital W, so this is just equal to W. And this is, by definition, again, equal to pi of V. So we've shown that pi squared of V equals pi of V. OK? OK, so one last application we'll do of minimizers, which is probably the most important application. Which one could prove for separable Hilbert spaces based on what we know and have done so far, but this proof works also for non-separable Hilbert spaces. And OK, so what is this theorem I'm referring to? It's probably one of the most important theorems in all of this business. It's the Riesz representation theorem. OK, so the only category theory I know is the bare category theorem. The only representation theory I remember is the Riesz representation theorem, which tells us we can identify a Hilbert space, the dual of a Hilbert space with the Hilbert space itself. So if H is a Hilbert space, then for all f in the dual, there exists a unique element V and H such that f of u, so this is an element in the dual, meaning it takes u to a complex number and is linear in u. You can write it as u inner product with this element V. All right, so every element of the dual can be realized as the inner product with a vector. Now, we saw this in a certain form already in, I think, it was maybe the first or second assignment when you computed the or proved that the dual space of little lp is little lq, where 1 over p plus 1 over q equals 1. When p equals 2, q is 2. So you saw that the dual space could be identified with itself when we're looking at little l2, which is the only Hilbert space out of all the little lps. And remember, how we proved that little lq was dual to little lp was via pairing between the two, which specifically was the sum of the sequences multiplied entry by entry. And now, so this theorem says that wasn't a fluke. This is, in fact, true for every Hilbert space, that the dual can be identified with the space itself in sort of this canonical way, where every element of the dual can be realized as taking the inner product with a vector. OK, so for the proof, so first off, we note that v is unique. If such a v exists, it's unique, since if f of u is equal to u inner product v is equal to u inner product v tilde for all u implies that u inner product v minus v tilde equals 0 for all u in H, which by setting u equal to v minus v tilde tells me v equals v tilde. All right, so all we need to do is come up with, given an element of the dual, come up with a vector that whenever I stick in a u into the dual vector, it's equal to this inner product. OK, so the easiest case to deal with is, of course, f equals 0. In other words, it maps u to 0 no matter what u is. We just choose this vector that we just choose this vector v to be 0. And we'll always have f of u equal to u inner product with the 0 vector, OK? So suppose now f does not equal 0, then there exists, let's say, a u1 in H such that f of u1 does not equal 0. Then if I take u0 to be u1 over f of u1, this implies that f of u0 equals 1, OK? OK, so now let c be the set of all elements u and H, which give me 1 when I stick it into f, OK? Now, this set is non-empty because I just gave you an element that when I stick it into f gives me 1, OK? So this is non-empty. And what is this? Actually, this is the inverse image of the singleton in the set of complex numbers. Now, a singleton is a closed set. f is a continuous function, right? An element of the dual is a bounded linear map from the Hilbert space to the complex numbers, and therefore it's continuous. So the inverse image by a continuous function of a closed set is closed. So this is also closed. So c is that, which is a non-empty closed subset of H. OK, you can see where this is going. Now let's check that c is convex. That writing is a bit skew. Now, if u1, u2 are in c, t is in 0, 1, then f of t times u1 plus 1 minus t u2. This is equal to, now f is linear, so this is equal to t times f of u1 plus 1 minus t f of u2. Now, this is equal to 1. That's equal to 1. So I get 1. So c is also convex. Now, you can see how the proof is going, but we might guess that if you're just looking at the proof, why am I doing this? Well, in fact, almost the minimum vector, or the vector with the smallest length, will be the vector that satisfies that inequality. So since c is a closed convex non-empty set, there exists a v0 in c such that v0 is equal to the inf of, let's say, u in c such that we're looking at the norm of u. OK? So let v equal norm of v0 over norm squared of v0. So first off, note that v0 cannot be 0. If v0 is 0, then f of v0 is equal to 0, but v0 is in c. f of v0 has to be 1. So v0 is non-zero. And I claim this v does the job. For all, OK, I was using u's to denote elements in c, but claim, I'll just write v does the job. Ie, for all u and capital H, f of u is equal to u inner product with v. OK? All right, to see that, let n be the null space of f, the set of all vectors that get sent to the number 0. So this is a set of all w and H such that f of w equals 0. This is a closed linear subspace of n, I mean, of H. Then it's not too difficult to convince yourself that I can write c as, in fact, v0 plus w, where w is in this subspace. OK, and what is v0 again? v0 is the infimum of the norms of all of these guys. So is equal to the infimum over, if I'm writing c in that way, w in n norm v0 plus w. All right, why am I doing this? Because of the argument we gave a minute ago, I can conclude that v0 is orthogonal to everything in n. So by the previous argument, just, I mean, look back at, is it up there or no, it was on this board and I erased it. But where we define this function f of t equal to, in this case, it would be v0 plus tw squared. Since that has a minimum at t equals 0, we conclude that v0 is orthogonal to w. So by previous argument for when we showed that H can be written as w plus it's orthogonal, the direct sum of a closed linear subspace and its orthogonal complement. So by the previous argument from that proof, we can show that v0 is an element of the orthogonal complement of n, the set of all vectors that get sent to 0. OK, now we're almost done. So let u be an H. I want to show that f of u is equal to v inner product u. Just needed that little bit of fact there. So remember, then if I look at f of u minus f of u v0, this is equal to, so f is linear, remember, so scalars pop out. This is just a complex number. So this is equal to f of u minus f of u f of v0. This equals 0, right? And therefore, u, which is equal to u minus f of u v0 plus f of u v0. Now, this thing here, because f applied to it gives me 0, this is an n, right? And this is in, again, remember, in perp. Although I'm not going to use that specifically, I'm going to use the fact that v0 inner product with everything from n is 0. And therefore, if I take the inner product of u with v, remember, v was v0 over v0 squared. This is equal to u 1 over v0 squared inner product u v0. This is equal to 1 over norm v0 squared. Now, again, this element is an n. So when I take the inner product of this line with v0, when v0 hits this, I get 0, right? Because it's an n. So if you like, call this thing w, which is an n. So this is equal to w inner product v0 plus f of u v0 inner product v0. And again, this thing is an n. v0 is in the orthogonal complement of n, right? So this inner product is 0 times f of u times v0 inner product with itself is the norm squared. And I get f of u. So I found a vector, namely this certain minimizer over its length squared, so that f of u is equal to u inner product with this vector for all u and h. And that completes the proof of the Riesz representation theorem. Now, next time, we'll talk about adjoints, which you had on an assignment at one point when we were talking about, I think I defined the adjoint for a general Banach space. So if you have a map, let's say, from a Banach space to itself, then we define the adjoint in a certain way to be now a linear map that goes from the dual space to the dual space. Now, in the case of a Hilbert space, the dual space is equal to the space itself. So the adjoint for a Hilbert space will be a map, again, from the space to itself. And it will satisfy a certain identity. And we will see the connection between adjoints and what this has to do with when a map is or a bounded linear operator is onto. So adjoints pop up now when we're trying to solve equations on Hilbert spaces. The properties of the adjoint can tell us when we can always solve our equation. And not only that, they're going to end up they're the analog of the transpose that hopefully you saw, maybe it was also referred to the adjoint, in linear algebra and finite dimensions. OK? And so this is way ahead. But hopefully what you proved in linear algebra and finite dimensions was the spectral theorem, which is that if you have a matrix that is equal to its adjoint, more generally that it's normal, that it commutes with its adjoint, then and for finite dimensions, the adjoint of a matrix was you switch the entries, switch i and j, and take the complex conjugate. If that's equal to the matrix again, then what you can conclude, the statement of the spectral theorem is that you can find an orthonormal basis of Cn, say, or Rn that diagonalizes the matrix. And we'll see something like that also holds in the Hilbert space setting, but it's not so simple as the eigenvalue, as diagonalizing just does not mean finitely many eigenvalues. And we'll go over more. I'll go more into that when we get to it. All right, we'll stop there for now.