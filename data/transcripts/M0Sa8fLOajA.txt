 Okay. This is a lecture where complex numbers come in. It's, complex numbers have slipped into this course because even a real matrix can have complex eigenvalues. So we met complex numbers there as the eigenvalues and complex eigenvectors. And we are, this is probably the last, we have a lot of other things to do about eigenvalues and eigenvectors. And that will be mostly real. But at one point somewhere we have to see what do you do when the numbers become complex numbers. What happens when the vectors are complex, when the matrices are complex, when the, what's the inner product of two, the dot product of two complex vectors? We just have to make the change. Just see what is the change when numbers become complex. Then can I tell you about the most important example of complex matrices? It comes in the Fourier matrix. So the Fourier matrix, which I'll describe, is a complex matrix. It's certainly the most important complex matrix. It's the matrix that we need in, Fourier transform. And the really, the special thing that I want to tell you about is what's called the fast Fourier transform, and everybody refers to it as the FFT, and it's in all computers and it's used, it's being used as we speak in a thousand places, because it has, like, transformed whole industries to be able to do the Fourier transform fast. Which means multiplying, how do I multiply fast by that matrix, by that n by n matrix? Normally, multiplications by an n by n matrix would normally be n squared multiplications. Because I've got n squared entries and none of them is zero, this is a full matrix. And it's a matrix with orthogonal columns, I mean, it's just, like, the best matrix. And this fast Fourier transform idea reduces this n squared, which was slowing up the calculation of Fourier transforms down to n log n. n log n, log to the base two, actually. And it's this, when that hit, when that possibility hit, it made a big difference. Everybody realized gradually what, that this simple idea, you'll see it's just a simple matrix factorization, but it changed everything. Okay. So I want to talk about complex vectors and matrices in general, recap a little bit from last time, and the Fourier matrix in particular. Okay. So what's the deal? All right. The main point is, what about length? I'm given a vector, I have a vector x. Or let me call it z, as a reminder that it's complex for the moment. But I can, later I'll call the components x, they'll be complex numbers. But it's a vector z1, z2, down to zn. So the only novelty is, it's not in Rn anymore, it's in complex n-dimensional space. Each of those numbers is a complex number. So this z, z1 is in cn, n-dimensional complex space instead of Rn. Just a different letter there. But now the point about its length is what? The point about its length is that z transpose z is no good. z transpose z, if I just put down z transpose here, it would be z1, z2, to zn. Doing that multiplication doesn't give me the right thing. Why not? Because the length squared should be positive. And if I multiply, suppose this is like one and i. What's the length of the vector with components one and i? What if I do this? So n is just two, I'm in c2, two-dimensional space, complex space, with the vector whose components are one and i. All right. So if I took one times one and i times i and added z transpose z would be zero. But I don't, that vector is not, doesn't have length zero, the vector with components one and i. This multiplication, what I really want is z1 conjugate z1. You remember that z1 conjugate z1 is, so you see that first step will be z1 conjugate z1, which is the magnitude of z1 squared, which is what I want. That's like three squared or five squared. Now if it's, if z1 is i, then i multiplied by minus i gives one. Plus one. So the component of length, the component i, its modulus squared is plus one. That's great. So what I want to do then is do that, I want z1 bar z1, z2 bar z2, zn bar zn. And remember that, you remember this complex conjugate. So, so there's the point. Now I can erase the no good and put is good. Because that now gives the answer zero for the zero vector, of course, but it gives a positive length squared for any other vector. So it's a, it's the right definition of length. And essentially the message is that we're always going to be taking, when we transpose, we also take complex conjugate. So let's, let's find the length of one. So the vector one i, that's z, that's that vector z. Now I take the conjugate of one is one, the conjugate of i is minus i, I take this vector, I get one plus one, I get two. So that's a vector, and that's a vector of length square root of two. Square root of two is the length and not the zero that we would have got from one minus i squared. Okay. So the message really is, whenever we transpose, we also take conjugates. So here's a symbol, one symbol, to do both. So that symbol H, it stands for a guy named Hermite, who didn't actually pronounce the H, but let's pronounce it. So I would call that Z Hermitian Z. I'll, let me write that word, Herm- so his name was Hermite, and then we make it into an adjective Hermitian. So Z Hermitian Z, Z H Z. Okay. So, that's the, that's the, length squared. Now what's the inner product? Well, it should match. The inner product of two vectors, so inner product, is no longer, used to be y transpose x. That's for real vectors. For complex vectors, whenever we transpose, we also take the conjugate. So it's y Hermitian x. Of course, it's not real anymore, usually. That, the inner product will usually be complex number. But if y and x are the same, if they're the same Z, then we have Z H Z, we have the length squared, and that's what we want. The inner product of a vector with itself should be its length squared. So this is, like, forced on us, because this is forced on us. So, so this, this, everybody's picking up what this equals. This is Z one squared plus Z n squared. That's the length squared. And that's the inner product that we have to go with. So it could be a complex number now. One more change. Well, two more changes. We've got to change the idea of a symmetric matrix. So I'll, I'll just recap on symmetric matrices. Symmetric means A transpose equals A. But not, no good if A is complex. So what do we, what do we instead, if I, that applies perfectly to real matrices. But now if my matrices are complex, I want to take the transpose and the conjugate to equal A. So there's, that's the, the right complex version of symmetry. The, the symmetry now means when I transpose it, flip across the diagonal and take conjugate. So, for example, here would be an example. On the diagonal, it better be real, because when I flip it, the diagonal's still there, and it has to, and then when I take the complex conjugate, it has to be still there. So it better be a real number, let me say two and five. What about entries off the diagonal? If this entry is, say, three plus I, then this entry had better be, because I want whatever this, when I transpose, it'll show up here, and I conjugate. So I need three minus I there. So there's a matrix with, that's, that's, that cor- that corresponds to symmetry, but it's complex. And those matrices are called Hermitian matrices. Hermitian matrices. A H equals A. Fine. Okay, that's, and those matrices have real eigenvalues, and they have perpendicular eigenvectors. What does perpendicular mean? Perpendicular means the inner product, so let's go on to perpendicular. Well, when I had perpendicular vectors, for example, they were like q1, q2, up to qn. That's my, q is my letter that I use for perpendicular. Actually, I usually, I, I, I also mean unit length, so those are perpendicular unit vectors, but now what is, so it's a orthonormal basis, I'll still use those words, but how do I compute perpendicular? How do I check perpendicular? This means that the inner product of qI with qJ, but now I not only transpose, I must conjugate, right? To get zero if I is not J and one if I is J. So it's a unit vector, meaning unit length, orthogonal, all the angles are right angles, but these are angles in complex n-dimensional space. So it's q1, q1, qI bar transpose. Or, for short, qIH qJ. So it will still be true, so let me, again, I'll create a matrix out of those guys. The matrix will have these q's in its columns, q2 to qn. And I want to turn that into matrix language, just like before. What does that mean? That means I want all these inner products, so I take these columns of q, multiply by their rows, so it was, it used to be q, it used to be q transpose q equals I, right? This was an orthogonal matrix. But what's changed? These are now complex vectors. Their inner products are, involve conjugating the first factor. So it's not, it's the conjugate of q transpose, it's q bar transpose q, qH. So can I call this, let me call it qHq, which is I. So that's our new, you see, I'm just translating, and the book, on one page, gives a little, like, dictionary of the right words in the real case, Rn, and the corresponding words in the complex case for the vector space Cn. Of course, Cn is a vector space, the numbers we multiply are now complex numbers, we're just moving in the complex n-dimensional space. Okay. Now, actually, I have to say, we changed the word symmetric to Hermitian for those matrices. People also changed this word orthogonal into another word that happens to be unitary, as a word that applies, that signals that we might be dealing with a complex matrix here. So what's a unitary matrix? It's just like an orthogonal matrix. It's a square, n by n matrix with orthonormal columns, perpendicular columns, unit vectors, unit vectors computed by and, and, and perpendicularity computed by remembering that there's a conjugate as well as a transpose. Okay. So those are the words. Now I'm ready to get into the substance of the lecture, which is the most famous complex matrix which happens to be, one of these guys. It, it has orthogonal columns. And it's named after Fourier because it comes into the Fourier transform, so it's the matrix that's all around us. Okay. Let me tell you what it is, first of all, in the n by n case. Then often I'll let n be four, because four is a good size to work with. But here's the n by n Fourier matrix. Its first column is the vector of ones. It's n by n, of course. Its second column is the powers, the, actually, better if I move from the math department to EE for this one half hour, and then please let me move back again. Okay. What's the difference between those two departments? It's just, math starts counting with one and, electrical engineers start counting at zero. Actually, they're probably right. So, anyway, we'll give them humor them. So this is really the zeroth column. And the first column up to the n minus one, that's the one inconvenience spot in electrical engineering. All these expressions start at zero, no problem, but they end at n minus one. Well, that's, that's the difficulty of, that course six has to do. So what's, they're the powers of a number that I'm going to call w. w squared, w cubed, w to the, now what is the w here? What's the power? This was the zeroth power, first power, second power, this will be n minus first power. That's the column. What's the next column? It's the powers of w squared, w to the fourth, w to the sixth, w to the two, n minus one. And then more columns and more columns and more columns, and what's the last column? It's the powers of, let's see, actually, if we look along rows, this matrix is symmetric, it's symmetric in the old, not quite perfect way, not perfect because these numbers are complex. And so it's, that first row is all ones, one w w squared up to w to the n minus one. That's the, the last column is the powers of w to the n minus one, so this guy matches that, and finally we get w to something here. I guess we could actually figure out what that something is. What are the entries of this matrix? The i, j entry of this matrix, are, am I going to, are you going to allow me to let i go from zero to n minus one? So i and j go from zero to n minus one. So the one, the zero zero entry is a one. It's just this same w guy to the power i times j. Let's see, I'm jumping into formulas here and I have to tell you what w is and then you know everything about this matrix. So w is the, well, shall we finish here? What was this, this is the n minus one, n minus one entry. This is w to the n minus one squared. Everything's looking like a mess here. Because we have, not too bad, because all the entries are powers of w. None of them are zero. This is a full matrix, but w is a very special number. w is the special number whose nth power is one. In fact, well, actually there are n numbers like that. One of them is one, of course. But the one we, the w we want is the angle is two pi over n. Is that what I mean? n over two pi. No, two pi over n. w is e to the i and the angle is two pi over n. Right. Where is this w in the complex plane? It's on the unit circle, right? This is, it's the cosine of two pi over n plus i times the sine of two pi over n. But actually, forget this. It's never good to work with the real and imaginary parts, the rectangular coordinates, when we're taking powers. To, to take that to the tenth power, we can't see what we're doing. To take this form to the tenth power, we see immediately what we're doing. It would be e to the i twenty pi over n. So when our matrix is full of powers, so it's this formula, and where is this on the complex plane? Here are the real numbers, here's the imaginary axis, here's the unit circle of radius one, and this number is on the unit circle at this angle, which is one nth of the full way round. So if I drew, for example, n equals six, this would be e to the two pi, two pi over six, it would be one sixth of the way round, it'd be sixty degrees. And where is w squared? So I, I, I, my w is e to the two pi i over six, in this case, in the six by, for the six by six Fourier transform, it's totally constructed out of this number and its powers. So what are its powers? Well, its powers are on the unit circle, right? Because when I square a mat- square a number, a complex number, I square its absolute value, which gives me one again. All the powers have, are on the unit circle. And they, the angle gets doubled to a hundred and twenty, so there's w squared, there's w cubed, there's w to the fourth, there's w to the fifth, and there is w to the sixth, as we hoped, w to the sixth coming back to one. So those are the sixth, can I say this on TV? The sixth, sixth roots of one, and it's this one, that primitive one, we say, the first one, which is w. Okay. So what, let me change, let me, I said I would probably switch to n equal four. What's w for that? It's the fourth root of one. w to the fourth will be one. w will be e to the two pi i over four now. What's that? This is e to the i pi over two. This is a quarter of the way around the unit circle, and that's exactly i, a quarter of the way around. And sure enough, the powers are i, i squared, which is minus one, i cubed, which is minus i, and finally i to the fourth, which is one, right. So there's w, w squared, w cubed, w to the fourth. I'm really ready to write down this Fourier matrix for the four by four case, just so we see that clearly. Let me do it here. F4 is, all right, one, one, one, one, one, one, one, w. Ah, it's i. i squared, that's minus one. i cubed is minus i. I- I could write i squared and i cubed, why don't I, just so we see the pattern for sure. i squared, i cubed, i squared, i cubed, i fourth, i sixth, i fourth, i sixth, and i ninth. You see the exponents fall in this nice -. The exponent is the row number times the column number, always starting at zero. Okay. And now I can put in those numbers if you like. One, one, one, one, one, i, minus one, minus i, one, minus one, one, minus one, and one, minus i, minus one, i. No. Yes. Right. What's -? Why do I think that matrix is so remarkable? It's the four by four matrix that comes into the four-point Fourier transform. When we want to find the Fourier transform, the four-point Fourier transform of a vector with four components, we want to multiply by this f4, or we want to multiply by f4 inverse. One way we're taking the transform, one way we're taking the inverse transform. Actually, they're so close that it's easy to confuse the two. The inverse of this matrix will be a nice matrix also. So, and that's, of course, what makes it that that, I guess Fourier knew that. He knew the inverse of this matrix. As you'll see, it just comes from the fact that the columns are orthogonal. From the fact that the columns are orthogonal, we will quickly figure out what is the inverse. What Fourier didn't know, didn't notice, I think Gauss noticed it, but didn't make a point of it, and then it turned out to be really important, was the fact that this matrix is so special that you can break it up into nice pieces with lots of zeros, factors that have lots of zeros and multiply by it or by its inverse very, very fast. Okay. But how did it get into this lecture first? Because the columns are orthogonal. Can I just check that the columns of this matrix are orthogonal? So the inner product of that column with that column is zero. The inner product of column one with column three is zero. How about the inner product of two and four? Can I take the inner product of column two with column four? Or even the inner product of two with three. Let's see, does that --? Let me do two and four. Okay. What? Oh, I see, yes. Hm. Hm. Hm. Let's see, I believe that those two columns are orthogonal. So let me take their inner product and hope to get zero. Okay, now if you hadn't listened to the first half of this lecture, when you took the inner product of that with that, you would have multiplied one by one, i by minus i, and that would have given you one. Minus one by minus one would have given you another one. Minus i by i would have been minus i squared. That's another one. So do I conclude that the inner product of columns --? I said columns two and four, that's because I forgot those are columns one and three. I'm interested in their inner product, and I'm hoping it's zero, but it doesn't look like zero. Nevertheless, it is zero. Those columns are perpendicular. Why? Because the inner product, we conjugate. You remember that one of the vectors in the inner product has to get conjugated. So when I conjugate it, it changes that i to a minus i, changes this to a plus i, changes those, that second sign and that fourth sign, and I do get zero. So those columns are orthogonal. So columns are orthogonal. They're not quite orthonormal, but I could fix that easily. They all, those columns, have length two. Length squared is four, like this, the four I had there, this length squared, one plus one squared, one squared, one squared, one squared is four, square root is two. So if I really wanted them, suppose I really wanted to fix life up perfectly, I could divide by two, and now I have columns that are actually orthonormal. So what? So I can invert right away, right? Orthonormal columns means, now I'm keeping this one half in here for the moment, means F4 Hermitian, can I use that? Conjugate transpose times F4 is i. So I see what the inverse is. The inverse of F4 is, it's just like an orthogonal matrix. The inverse is the transpose, here the inverse is the conjugate transpose. So, fine. That tells me that anything good that I learn about F4, I'll know the same, I'll know a similar fact about its inverse, because its inverse is just its conjugate transpose. Okay, now, so what's good? Well, first, the columns are orthogonal. That's a key fact. That's the thing that makes the inverse easy. But what property is it that leads to the fast Fourier transform? So now I'm going to talk in these last minutes about the fast Fourier transform. What the- here's the idea. F6, our six by six matrix, will- there's a neat connection to F3, half as big. There's a connection of F8 to F4. There's a connection of F64 to F32. Shall I write down what that connection is? What's the connection of F64 to F32? So F64 is a 64 by 64 matrix whose w is the 64th root of one. So it's one 64th of the way around in F64. And F32 is a 32 by 32 matrix. Remember, they're different sizes. And the w in that 32 by 32 matrix is the 32nd root of one, which is twice as far. See that key point? That's the- that's how 32 and 64 are connected, in the w's. The w for 64 is one 64th of the way- so all I'm saying is that if I square the w- w64, that's what I'm using for the one over w sixty- this is wn is e to the i 2pi over n, so w64 is one 64th of the way around it. When I square that, what do I get but w32? Right? If I square this matrix, I double the angle, if I square this number, I double the angle, I get the w32. So somehow there's a little hope here. To connect F64 with F32. And here's the connection. Okay. Let me go back, yeah, let me- I'll do it here. Here's the connection. F64, the 64 by 64 Fourier matrix, is connected to two copies of F32. Let me leave a little space for the connection. This is 64 by 64. Here's a matrix of that same size, because it's got two copies of F32 and two zero matrices. Those zero matrices are the key. Because when I multiply by this matrix, just as it is, regular multiplication, I would take need 64- I would have 64 squared little multiplications to do. But this matrix is half zero. Well, of course, the two aren't equal. I'm going to put an equal sign, but there has to be some fix-up factors, one there and one there, to make it true. The beauty is that these fix-up factors will be really almost all zeros. So that as soon as we get this formula right, we've got a great idea for how to get from the 64 squared calculations. So this originals- originally we have 64 squared calculations from there, but this one will give us- this is- this will- we don't need that many. We only need two times 32 squared, because we've got that twice, and plus the fix-up. So I have to tell you what's in this fix-up matrix. The one on the right is actually a permutation matrix, a very simple odds and evens permutation matrix. The ones show up- I haven't put enough ones, I really need a- thirty-two of these guys, a double space, and then you see it's a permutation matrix. What it does- shall I call it P for permutation matrix? So what that P does when it multiplies a vector, it takes the odd- the even-numbered components first and then the odds. You see this one skipping every time is going to pick out x0, x2, x4, x6, and then below that will come- will pick out x1, x3, x5. And of course, that can be hardwired in the computer to be instantaneous. So that says- so far, what have we said? We're saying that the sixty-four by sixty-four Fourier matrix is really separated into- separate your vector into the odd- into the even components and the odd components, then do a thirty-two-sized Fourier transform onto those separately. And then put the pieces together again. So the pieces, putting them together, turns out to be I and a diagonal matrix and I and a minus, that same diagonal matrix. So the fix-up cost is really the cost of multiplying by D, this diagonal matrix. Because there's essentially no cost in- in the I part or in the permutation part, so really it's the fix-up cost is essentially, because D is diagonal, is thirty-two multiplications. That's the- there you're seeing, of course, we didn't check the formula or we didn't even say what D is yet, but I will, this diagonal matrix D is powers of w. One, w, w squared down to w to the thirty-first. So you see that when I- to do a multiplication by D, I need to do thirty-two multiplications. There they are. Then, but the other, the more serious work is to do the F thirty-two twice on the- separately on the even-numbered and odd-numbered components, so twice thirty-two squared. So sixty-four squared is gone now. And that's the new count. Okay, great. But what next? So that's- we now have the key idea, we would have to check the algebra, but it's just, checking a lot of sums that come out correctly. This is right- the right way to see the fast Fourier transform, or one right way to see it. Then you've got to see what's the next idea. The next idea is to break the thirty-twos down. Break those thirty-twos down. So we have this factor and now we have the F thirty-two, but that breaks into some guy here, F thirty- F six- F sixteen, F sixteen. Each F thirty-two is breaking into two copies of F sixteen. And then we have a permutation and then the- so this is a, like, this was a sixty-four size permutation, this is a thirty-two size permutation. Ah, I guess I've got it twice, because I'm just using the same idea recursively. Recursion is the key word. Then on each of those F thirty-two, so here's zero, zero, it's just, to get F thirty-two, this is the odd-even permutations, so you see we're, the combination of those permutations, what's it doing? This guy separates into odds and into evens and odds, and then this guy separates the evens into the ones, the numbers that mul- the even-evens, which means zero, four, eight, sixteen. And even-odds, which means two, six, ten, fourteen. And then odd-evens and odd-odds. You see, together, these permutations then break it, break our vector down into x, even-even, and three other pieces. Those are the four pieces that separately get multiplied by F sixteen, separately fixed up by these i's and d's and i's and minus d's. So this count is now reduced. This count is now, what's it reduced to? So that's going to be gone. Because thirty-two squared, that's the change I'm making, right? The thirty-two squared, so it's this that's now reduced. So I still have two times it, but now what's thirty-two squared? It's gone in favor of two sixteen squareds plus sixteen. That's ins- that- that was- and then the original thirty-two to fix. Maybe you see what's happening even easier than this formula is. What's- when I do the recursion more and more times, I get simpler and simpler factors in the middle. Eventually I'll be down to two point or one point for A transforms. But I get more and more factors piling up on the right and left. On the right I'm just getting permutation matrices. On the left I'm getting these guys, these i's and d's, so that there was a thirty-two there and a thir- each one of these is costing thirty-two. Each one of those is costing thirty-two. And how many will there be? See, see the thirty-two for this original fix-up, because d had thirty-two numbers? Thirty-two for this next fix-up, because d is sixteen and sixteen more. I keep going, so the count in the middle goes down to zip, but these fix-up counts are all that I'm left with. And how many factors, how many fix-ups have I got? Log n. From sixty-four, one step to thirty-two, one step to sixteen, one step to eight, four, two and one. Six steps, so I have six fix-up factors. Finally, I get to six times the thirty-two. That's my final count. Instead of sixty-four squared, this is log to the base two of sixty-four times sixty-four, actually half of sixty-four. So actually, the final count is n log to the base two of n, that's the thirty-two, ah, a half. So can I put a box around that wonderful, extremely important and satisfying conclusion? That the fast Fourier transform multiplies by an n by n matrix, but it does it not in n squared steps, but in one half n log n steps. And if we just complete by doing a count, let's suppose a typical case would be two to the tenth. Now n squared is bigger than a million. So it's a thousand twenty-four times a thousand twenty-four. But what is n, what is one half, what is the new count, done the right way? It's n, a thousand twenty-four, times one half, and what's the logarithm? It's ten. So times ten over two. So it's five times, it's five times a thousand twenty-four, where this one was a thousand twenty-four times a thousand twenty-four. We've reduced the calculation by a factor of two hundred, just by factoring the matrix properly. This was a thousand times n, we're now down to five times n. So we can do two hundred Fourier transforms, where before we could do one, and in real scientific calculations where Fourier transforms are happening all the time, we're saving a factor of two hundred in one of the major steps of modern scientific computing. So that's the idea of the fast Fourier transform, and you see the whole thing hinged on being a special matrix with orthonormal columns. Okay. That's actually it for complex numbers. I'm back next time really to real numbers. Eigenvalues and eigenvectors and the key idea of positive definite matrices is going to show up. What's a positive definite matrix? And it's terrific that this course is going to reach positive definiteness, because those are the matrices that you see the most in applications. Okay. See you next time. Thanks.