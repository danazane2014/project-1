 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. Well, so three things to mention. One was, you remember last time I made a list of six or seven different situations where they x equal b and problems that could arise. The last one was when a was just way too big to fit into core. But in the middle were other methods, so other issues, like the columns being nearly dependent when Graham-Schmidt will come up. First, I want to say that I know there are typos in the two pages. I thought you might just like to see the very first draft of two pages of the book. So people sometimes ask me, how long does the book take to write? So I started when 18065 started a year ago. So I'm into the second year, and it usually takes two years. And the system is I write by hand. So I wrote those opening pages that you saw, two pages, by hand. Then I scan them to Mumbai, where my best friend in the world types them with typos. No problem, because I'm going to make so many changes that a few typos are nothing. Anyway, so he scans them back to me typed. And then I start making changes. If I'm lucky and have the chance to talk about it in here, then I realize better things to do. Then I scan back to him and back to me and back to him and back to me. So that's where the two years disappear. Anyway, I'm quite happy with those two pages until I start improving them. And one other topic from the past that I thought that came out in class wasn't in the notes yet. Do you remember the day that we minimized different norms? L1 or L2 or max, L infinity norm, with the condition of solving with a constraint that this equation was satisfied. I'm in 2D to be able to draw a picture. And the constraint is one line. And that's about what the line looks like. So I'm going to draw here again. So what I'm doing is putting some numbers in to that insight that I drew about a week ago. Do you remember that? Because I thought that really illustrates how L1 and L2 and L infinity are different. Let me draw the L2 one here. So where is the point on this line? So x has to lie on this line. And where is the point that has the smallest sum of squares norm, standard L2 norm? So geometrically, where is that point? Well, what does the set of points with norm 1 look like for L2? It's a circle, right? So we just blow that circle up or shrink it down until it touches this thing. And now, where it touches, it'll touch where our radius is perpendicular. So there's the circle. There's our best point in L2. Because if we picked another point, we'd have to blow the norm. The norm would have to be bigger to go through that point. So that's clearly the first one. And actually, we can probably see what it is. Because if we know those are perpendicular, I know the slope of this line. So I think that the slope of this line is something like 3 4ths, probably coming from there. Or maybe 4 3rds. 3 3rds. We'll figure it out. I think it's 4 3rds. Yeah. I think that line. I'll find that point. Then the most interesting one was the L1 norm. Because what was the shape of the unit ball for L1? A diamond. Right, a diamond. So the diamond that first touches the line is here. That's the winning point. And if the line is 3x1 plus 4x2 equal 1, then I know that that point is x1 will be 0, and x2 will be 1 quarter. So that's the winning point in L1. And I think I calculated this right, that the winning point in L2, I think, will be, let's see, this goes up to, I'm sort of moving down the line. I think this would have 3 25ths, 4 25ths. I won't stop to derive that, but at least now, yeah, that slope is looking like 4 3rds. Goes up 4 when it crosses 3. And that 4 and the 3 came from there. And of course, you notice I've scaled it to fit the line. 3 times 3 25ths is 9 25ths plus 16 25ths. 25 25ths is 1. And finally, what about L infinity? What was the picture there? What's the unit ball look like in 2D for the max norm? It's a square. Right. So the square will hit there. These two on the square are, this is a 45 degree line now. On that square, it hits it at that sharp point. And so the x1 and the x2 are equal. And I think they're probably, let's see, if they're equal, if we made them 1 7th, that would be, then I'd have 3 7ths plus 4 7ths equals 7 7ths. Yeah, I think we make them 1 7th, 1 7th. So that would be the x infinity point. 3 7ths plus 4 7ths give 1. So why do I mention this? First, because when I did it before, I just drew pictures without really solving the problem. And secondly, because in thinking ahead about projects, this is the kind of project that I would think is quite interesting. Obviously, this is p equal 1. This is p equal 2. This is p equal infinity. And I guess it's pretty clear from the pictures that as p increases, this point starts up here and moves down the line and ends up here. And I have no idea what the solution is for a different p and how it moves. And to make it more of a project, what happens in 3D? What happens in 3D? In 3D, if I have one equation in 3D, then I have a plane. And this would become a diamond, a 3D diamond. This would be a 3D sphere. This would be a 3D cube. They would expand to hit that plane. And I don't know how many zeros you get in that case. So that would be the case of one equation. So there would be a plane that these diamond sphere and cube expand until they hit it. Or you could have two constraints, two equations. So if we had two equations and three unknowns, that would be a line again. But how many zeros would we get in these different cases? How sparse is L1 going to be? OK, that's like a recapture of what we did. I thought it was nice occasionally to have pictures showing where the solution is. Now I'm coming to the topic of the day, which is Gram-Schmidt. And so Gram-Schmidt, number one, is the standard way that would be taught in 1806. And the result is, so what's Gram-Schmidt about? And I'll just put down here the general facts of Gram-Schmidt. We start with a matrix A. It's got n columns. But they're not orthogonal. And in fact, they may be badly conditioned. Columns might be nearly dependent on the others. I'm going to assume the columns are independent, but they might be barely independent. So Gram-Schmidt, so those lines then would be sort of like pointing very nearly parallel. But Gram-Schmidt opens up the picture to get a matrix Q, an orthogonal matrix with columns Q1 to Qn, which are orthonormal. So it gets a perfect basis of Q's. And so that's what Gram-Schmidt does. And these are different ways to do it, different ways to organize the computation. I really only put the standard way in. What is this mysterious R? I claim that the Q's are combinations of the A's. So there's some matrix to tell me what those combinations are. Or if I go backwards and say, well, the A's are combinations of the Q's, that's what I'm about to do. If I say each A is a combination of Q's, that means that my A matrix is my Q matrix times some R matrix, which tells me the combinations. When I multiply by R on the right, I'm taking combinations of the columns of Q and getting the columns of A. So just like LU, you go forward with the algorithm to reach U. Here we go forward with the algorithm to reach Q. But then when we want to understand, put it in one simple equation, it turns out to be better to go backwards and say, how is the original A related to the final Q? And there has to be some R. I always feel when I talk about Gram-Schmidt, I usually end with that A equal QR. And of course, the MATLAB command is exactly QR. So in MATLAB, you're asking for the command would be QR of A instead of LU of A. So it would give you Q and R. That's what MATLAB will output. Now, as I say, Q is the thing we're constructing. R is the combinations that we need to get what we want. And so it's a little, it comes at the end, what the heck was R? But actually, R is really a simple idea. I want to show that at the beginning instead of the end. OK. So I'm going to move Q over here as Q inverse. But what is Q inverse? Q transpose, right, because I've created an orthogonal matrix. So this mysterious R is Q transpose A. And let me just put here the, let me sort of make it grow out into matrices. That has the Q's along the rows, Q transpose, of course. Qn transpose, Q1 transpose. I just, I'm transposing the matrix above it. So these columns become rows times the A's, A1 to An. So what is a typical entry in R? That's really why I want to say nothing mysterious about this. You can see what you end up with. It'll be right in front of us here. What is a typical, what is the entry in row I, column J of R? OK, this says that all those entries in R are Qi transpose times Aj, right? That's the old way to multiply matrices. And it's the best way for this, a row times a column. In other words, the R's are just the inner products, the dot products of the Q's with the A's, of the Q's with the A's. That's sort of like nothing mysterious about R. Because Q is an orthogonal matrix, we were able to put it over here, get a nice expression for R, and see what it really is. So R, you could do R at the end or on the root. But it's just inner products of the Q's with the A's. OK, now what's Gram-Schmidt? I'm sort of thinking you've seen the basic ideas of Gram-Schmidt, but let's review. So I start with A. So what does Gram-Schmidt begin with? A1. It takes that first column. So these A's are not orthogonal, generally. But the first direction is OK. I have no complaints about the first direction, except that A1 might not be a unit vector. So Q1 would just be A1 over its norm to have a unit vector. The whole idea of Gram-Schmidt is in Q2. So what is Q2? The whole idea is coming here. It's the only thing you need to know. And the picture shows it. So Q2, I start with A2. But it's not orthogonal to A1. So what do I do? I figure out the component of A2 in the A1 direction, and I remove it. So I take that vector away, and I'm left with this vector. So there is a vector. I'll call that A2. A2. So A2 is the original little a2 with the A1 direction removed. So A2 is like, so what's the formula for what I just did? This is the whole, the key step that Gram-Schmidt repeats over and over and over again. It's truly boring. So it subtracts. Well, remember that this is the same direction as Q1. And it's better to work with Q1, because we've found that guy. We've got it. And we know it's a unit vector. So here's my linear algebra question. What's the component of A2 that I want to subtract off? It's the component in the direction of Q1. It's this in the direction of Q1. And let me just remember. So obviously, that angle is coming into it. So that will be an A2 transpose Q1 times Q1. That's it. That's the component that we remove. And maybe I'd prefer to write it as Q1 transpose A2. I don't know. Doesn't matter, of course. The two dot products are the same. Maybe I will just. Yeah. Well, maybe not. Fine. Good. OK, now, what is that vector supposed to achieve? It's supposed to be this vector. This vector I'm really going to call A2, because it's in the right direction for Q2. But it is not yet normal. So what is Q2, then? So I'm saying this guy got the direction right. This thing subtracted off this vector, got that direction right. It got it as A2. What is Q2 now that I want to finish? I've got the direction. All I want to do is get it to be a unit vector. So I just take A2 over its normal. So that double step is the whole thing in Gram-Schmidt, the whole thing. Subtract off the components in the directions already set. Then you get something in a totally new direction called A, capital A. And then you divide by its length to make it a unit vector. And that gives you the new Q. So you see, just to show that we've got the point, what about the next step, aiming for Q3? So tell me what A3 should be. A3. I'm going to start with this. And I'm going to subtract off some stuff. What am I going to subtract off? Is the component A3 transpose right times Q1. Q1. And I didn't yet check that this came out orthogonal to Q1. I'll come back to that. Now, have I done everything I should do here with A3? No, I've got one more step to take. And I should take the two steps separately. It's called modified Gram-Schmidt. What I want to do is subtract off the Q2 component. So what multiple of Q2 do I need? Because Q2 has been set by the time I get to A3. So what goes here? A3 transpose Q2. Thanks. Thanks. If you look at a code, say a MATLAB code to do Gram-Schmidt, oh, what's the final Q3 then? Normalize it. So you take A3, which is in the right direction. And you divide by its length to get a unit vector. Yeah. Let me just come back and check that I did it right, that I got the right direction. So what do I mean by the right direction? What should I check about that guy? I should check that its inner product with Q1 is, if this is the right direction to go that way, then I should check, I have to check, hopefully I got the formula right, that its inner product with Q1 is 0. Thank you. So let me just, so is it obvious that it is? Take the inner product of the dot product of that with Q1. What do you get? You get this same number, Q1 A2 transpose Q1. You get that number. And over here, you're getting a Q1 transpose with Q1. So you see what I'm doing? And probably it looks like it would have been better. I'm checking that Q1 transpose A2 is 0. It is. Q1 transpose A2 here. Q1 transpose A2 or A2 transpose Q1, I don't mind. And I have another Q1 transpose Q1 here. And what is that? What is Q1 transpose Q1? It is 1. So check. OK, that's Gram-Schmidt, standard Gram-Schmidt, which you have met before. Now I'm ready for a better Gram-Schmidt. You could say a better Gram-Schmidt. Because here, so what's going to be the difference? Here, I took the A's in their original order. Now, suppose I did that with elimination. Elimination, usually we write as acting on the row. So thinking of elimination, I'm thinking of the rows. What would be the danger in taking the rows in order, doing no row exchanges, just figure out the pivot each time and kill the rest of the column and then move on? So taking the rows in the order they came, whatever it might be, what's the risk? And why would MATLAB not do that? Because something can be very small and totally blow up your calculation. And that's that pivot number. So the question is, if A2 is very near A1, so let me draw a new picture. So here's the risk. So A1 was whatever it was. If A2 is really close to the same direction, then I'm subtracting off almost all of it. And I've got some tiny little bit for the new direction. That's like the pivot in elimination. It's the number that sort of measures what's new, what the new row in elimination or the new column in Gram-Schmidt gives you. And if that's too small, I mean, like in elimination, we would, as I say, we would never use an elimination code on a general matrix that didn't check the size of the pivot and exchange rows when necessary. Well, similarly with Gram-Schmidt, it can take the columns in order. That's the standard Gram-Schmidt, taking the columns in order. But only if it checks each time that the little bit, well, that the new part, what would be the new part, is big enough to be able to, we have to divide by the thing. And if that A2 is a tiny vector, this dividing by A2 and onwards is building in round-off error that we can't remove again. We're stuck with it. So that's the column exchange, column pivoting idea in a sort of a more professional Gram-Schmidt. And it leads to do it. So I have to be able to check, compare this little bit, if it is little, with what? What am I going to compare with? In elimination, I'd look down the rest of the column for a bigger number. Here, I want to know, I guess what I have to do is I have to find this component, not just from A2, but from all the remaining A's. And I'll pick the biggest. So there, in that sentence, I said the main idea of column exchanges is, once you get Q1 set, which was certainly Q1 was the easiest one in the world, but maybe even for Q1, I guess it could even happen. That would be starting with a 0 up in the upper left corner for elimination. What a way to start the day. Here, if A1, if my matrix A had a tiny little A1, then I should look for a bigger one to get the very first Q chosen. Let's suppose, give ourselves a reasonable chance here. Let's suppose A1 was a decent size, as I've drawn. The next step might not be. If I use A2, as I say, it could be same direction as A1, virtually. And then I'm just working with that little piece. So what do I have to do differently? I have to be able to compare this little piece with all the other potential possibilities. So let me just write down what you have to do. You have to do in a different order. So this is now with column pivoting, column exchange, column pivoting allowed. Allowed, or possible. So to make it possible, I have to find not only A2, the piece of little A2. I have to find A2, the piece. I'm just going to copy that. I have to take my second column, subtract off the Q1 part. And that could be small. So I have to compare it with, oh, I haven't written this page up, so I haven't got a notation in mind yet. I won't give it a name. I have to also compute at this step before deciding Q2. This is now I'm describing how to decide Q2, the second vector. And I'm saying that the way to decide Q2 is not only to take the piece of A2, but also the piece of A3. Look at this piece. And look at all the other pieces. And now what will be my policy? Standard Gram-Schmidt accepted this one and didn't look at these. But now I'm going to look at them all, and I'm going to take the largest. I'm going to take the largest. And that will be the A2 that I want. So it might not be this one. If this guy is largest, then I'm taking column 3 first, and that will be my A2. And then I'll say, fine, and then Q2 will be that A2 over its norm. So you see the difference? It's not exciting. And you might think, wait a minute, this is a heck of a lot more work. But it isn't. It isn't actually more work, because these are all the things, these ones that look like we're paying a price. We're computing all these alternatives. But we had to do that eventually anyway. Do you see that? Let me say it again. That the standard way took all the components, like for here. The standard way waited until you got to column 3, and then subtracted off both pieces. Waited until you got to column 4, subtracted off three pieces. This way, you're subtracting off the first piece as soon as you know what it should be. As soon as you know Q1, you remove it from all the remaining vectors, and you look to see what's the biggest. You pick the biggest one. I said, maybe it's this guy. So you move that one. Some permutation matrix is going to move it to the second column. See, it started in the third column, but I'm going to move it to the second, because it's the biggest. Then I do the right thing. I find Q2. And now I go on to the third, toward Q3. And how will I find Q3? It'll be I want to pick the biggest column to work with, so I subtract off the Q2 components. This is easy to say, but I had never figured it out before. So let me just say it again, and then I'll leave that with you. How do I get Q3? I've fixed two columns. They happen to be maybe not necessarily the first two, but two columns, two Qs are set, and I'm looking for the next one. OK, I go on, and I look at all the remaining columns, all of which have had subtracted off their Q1 and Q2 parts. So I've orthogonalized with respect to Q1 and Q2. I look at all the remaining things that I have to work with and pick the biggest, just like picking the biggest number to go into the pivot. OK, I don't think I can say it anymore without just repeating myself. And I bring it here to class because I had not sort of appreciated the point that no extra work was involved. You just did these subtractions for all the remaining columns of A before you started on the next job. Is that OK? Eventually, the notes will describe that. Maybe they even do. Yeah, I think they even do. I wrote it, but I didn't understand it. Now, a little improvement. OK. So yes? AUDIENCE 2 So are we permuting every time to get the biggest pivot? GILBERT STRANGEROFFEYSSINCZKOWSKI Yeah, yeah. Only we don't call them pivots, or maybe we should. I don't know what word is used. To get the biggest column remaining or something. Yeah, yeah, yeah, each time. Right. If the columns were in a stupid order, this puts them in the right order. OK, finally, come these weird names. Krylov, Russian, Arnoldi. Actually, I don't know what he is, and I shouldn't admit that on tape. So what's the idea there? So again, we're solving Ax equal b. So this is going to be Krylov. What was his idea? I want to solve Ax equal b. A is a big matrix, pretty big. Of course, I don't plan to invert it. That would be insane. What I can do with a big matrix A, especially if it's sparse, which may, so that would be a large sparse A would be a good candidate for Krylov. So what is it that you could do cheap and fast with a large, I mean, really large, but really sparse matrix A? You can do a matrix times a vector. And here is our matrix, and there is our vector. Or yeah, we could start with a vector b. We can multiply A times b. We can multiply A times A times b. And of course, I write it that way. I never, I mean, like, if you multiply A times A first, then like, you know, turn in your MATLAB account. Because you just have to do it that way. OK, and then you keep going, which of course, that's A squared b, but you didn't form A squared. And then on up to, in the end, you get to some A to the, say, k, k minus 1 b. But of course, that's computed as A times the previous one, which was A times the previous one. So there is a bunch of vectors, which are likely to be independent. So they span a space, and it's called the Krylov space. So these span. They're combinations. Combinations give, oh, I don't like that letter k, because that's also Krylov. So what shall I say? J. So I have J vectors. The original b, A b, A squared b, up to that. So combinations give the Krylov space, say. We'll name it after Krylov, and we need a subscript J to show how big it is, its dimension. So that will be the idea. Well, let me complete the idea. The idea will be, so they're combinations. So that's a space, a subspace, pretty big if J is big. And I'm going to look for the best solution in that space. So I'm not going to solve Ax equal b exactly. I'm going to find the best solution, the closest solution, the least square solution, in this Krylov space. I'm going to let J be pretty big. So this space has got plenty of vectors in it. I have a basis for this space. And some combination of these basis vectors will be my xj. So again, xj will be the best vector, or the closest vector, the closest vector in this Krylov space, Kj. It will be the best vector in that space, the closest one. So I know what this space is. I've reduced the dimension down to J. And I can find this best vector. There is just one catch, and it's the same catch that Graham-Schmidt were aiming to help to remove. That is, this basis that I'm, right now, I'm working with all combinations of these guys. And those could be very, very dependent. That might be a terrible basis. Anytime you want to do big computations, what kind of a basis do you want? Yes? So what sort of a basis is good to project onto, to find the best solution within that subspace? So you're sort of finding a projection. And you've got vectors that span the space, so you know what you're projecting onto. But those vectors might not be, they might be nearly dependent. They might all be pointing almost the same direction, in which case your calculations are terrible. So what do you do? Orthogonalize, right. And that's where Arnoldi comes in. And there is also a Hungarian guy named Lanchos. So that's what they contribute is how to orthogonalize that basis. And then once you've done that, you have an orthogonal basis. And of course, an orthogonal basis is perfect to do a projection. Do you see, everybody has to know that. Why is an orthogonal basis so great, orthonormal even? Let's just remember. Suppose I have a vector x. It's unknown here, but suppose I have it. And I want to write it as a combination of these orthonormal guys. So you say, what is it about orthonormal q's that makes this easy to do, which it would not be with a arbitrary basis? So this is really q times c, right? q times this vector of c's. The q's are in the columns of q. The c's, we're multiplying a matrix by a vector. That's just a combination of the columns. That's what we get. And what's the point of when the q's are orthogonal, what's the answer? We can get the answer straight away. So here, we're trying to find the coefficients with respect to the basis vectors q of a given vector x. And what's the answer to that question? The point is, usually, to find the coefficients, c would have to be q inverse x. We'd have to solve that system of equations. We do have to solve that system of equations. But where is the payoff from orthonormal basis? q inverse is q transpose. That's the payoff. So it's telling me that to find c1, how do I find c1? This says, take the first q1 transpose with x. I'll say the same thing here. Take the first component, the first vector with x. That'll be about c1 q1 transpose q1. I'm just taking the dot product of everything there with q1. And then a c2 q1 transpose q2 and so on. But what's good? Tell me again. These are all 0. And the q1 transpose q1 is 1. So it's perfect. c1 is q1 transpose x. And that's exactly what that tells us. The first component is the first row of q transpose, which is q1 transpose with x. So that's the idea. So that's the idea here. That's the reason for Arnoldi and Lanchos being famous, is that they figured out a good way to orthogonalize that basis. Do we want to see what they did? Or just those would be in the notes. Well, how do you do it? So this is the basis. This is our not good basis. And then our good basis is going to be q's. So I'll take b to be q1 will be. What would be the right choice for q1? Well, I'll take the first vector and normalize it. We're just doing Gram-Schmidt. What would q2 be? How would I find q2? Following the Gram-Schmidt idea. I take a b. I subtract off its component in this q1 direction. And I normalize. And all the Arnoldi-Lanchos algorithm is is that same Gram-Schmidt idea applied to these Krylov vectors. So Arnoldi-Lanchos, Arnoldi is for any matrix, and Lanchos is for a symmetric matrix where you get some special benefit. So what they did is just you could say now, they just wrote down Gram-Schmidt. In fact, probably the standard Gram-Schmidt, because this is a case where we really don't want to exchange columns. I don't want suddenly to be pushed into this one. I'd rather take them in order, because it just turns out right. And this is in the notes. So let me tell you where this is. This would be section 2.1, so part 2 of the book, which is where we are, and the first section. So what altogether is in this first section when you look at it? That section is standard numerical linear algebra, what any course in MIT offers, 18.3, I'm not sure of the number, 330 maybe, which is all things like this. Krylov would be there, Arnoldi, Lanchos. Of course, Gram-Schmidt would be there. That's five people who've thought of the same thing. And so that section 2.1 summarizes what's in really good, a lot of textbooks. And let me mention a favorite, a book by Trefesen and Bao, or the Bible. So this is maybe a moment to tell you about two books on classical numerical linear algebra, what you do for matrices of order 1,000, not for matrices of order millions. That you have to rethink. So Trefesen and Bao is called numerical linear algebra. And you know the authors of the Bible of numerical linear algebra. So that's a textbook. And what I'm going to write down now finally is 750 pages it's grown to in its fourth edition. It's the Bible for all numerical linear algebra people. And it's written by Golub and Van Loon. So Gene Golub was a remarkable guy. He probably didn't write more than about 11 pages of this. Charlie wrote most of it. But Golub was an amazing person who traveled the world and connected people and left behind papers to be written and books to be written. And so this Golub Van Loon is now in the fourth volume. And it has so much good stuff and references that it's like the good reference. And this is the good textbook if you were going to teach a course on numerical linear algebra. So I think I've come to the point to finish. So I really have finished along with the extra attraction of this different problem. I finished with Ax equal b. And well, at least I now move on to what to do with really, really large matrices.