 All righty, why don't we get started? So welcome back, nice to see you all. And what have we been doing in theory of computation? We have been talking about Turing machines and about the power of Turing machines. We started at the beginning by showing a bunch of decidability theorems that exhibit the power of Turing machines to calculate properties of finite automata, context-free grammars, and so on in some cases. And last lecture, we talked about the limitations of the power of Turing machines by proving undecidability theorems. So we showed that this language ATM, the acceptance problem for Turing machines itself, is an undecidable problem. That was the first of many undecidable problems that we're going to encounter. And though we proved the undecidability of ATM using the diagonalization method, as hopefully you remember, we're going to introduce a new method, which we basically previewed last time, called the reducibility method, which is the way other problems are typically shown to be undecidable. And so we're going to stick with that for this lecture and also next lecture. We're going to be talking about undecidability. And I think there's going to be a few additional discussions after that. But this is one of the important themes of the course is to understand that threshold between decidability and undecidability or the limitations of computation. So today, as I mentioned, we're going to talk about the reducibility method for proving problems undecidable and also for proving problems non-Turing-unrecognizable. We're going to introduce this notion of a reducibility in general. And we'll also talk about a very specific kind of reducibility called the mapping reducibility. So today, as promised, we're going to talk about using reducibilities to prove problems are undecidable or unrecognizable. So that's going to be our general method. Oops, make myself smaller. Thank you. I always forget. Thank you for the reminder. So using reducibilities to prove problems are undecidable or unrecognizable. And the basic way that works is we're going to leverage some problem we already know is undecidable, say, or unrecognizable, to prove other problems are unrecognizable. So we did a quick example of that last time. We're going to go over that example again just to set the stage. And then we're going to talk about that in greater detail. So as you recall from last time, we had this problem HALT TM, which is the problem of testing for a given Turing machine and an input to that Turing machine whether the Turing machine halts, either accepting or rejecting, but just whether it halts, which is a somewhat different problem, closely related, obviously, but somewhat different than the ATM problem, which is just testing whether the Turing machine accepts. So we already showed that ATM is undecidable. Now, conceivably, HALT TM might be decidable. It's not exactly the same problem. But we're going to show that HALT TM is likewise undecidable. We did this last time, but I'm just going over it again. We're going to likewise show that HALT TM is undecidable. We could go back to the diagonalization method and do it from scratch, but generally, that's not what's done. Generally, what people do is they use a reducibility from a known undecidable problem. And so what we're going to show is a proof by contradiction, which says that if HALT TM were decidable, then ATM would also be decidable. And we know it isn't. And that's by virtue of what we call a reducibility from ATM to HALT TM. And I'll explain with the terminology, and we'll have a chance to play with the concept all lecture long. So we're going to see it in all sorts of different variations. So as I said, we'll assume HALT TM is decidable and use that to show that ATM is decidable, which we know is not true. So quickly going through it, because we did it already once before, we're going to assume that HALT TM is decidable. Let's say Turing machine R is the decider. And now we're going to show that ATM is decidable by constructing a Turing machine S, which uses R to decide ATM. That's going to be our contradiction. So here is the machine S. The machine is, you have to keep in mind what the goal of S is. We're going to design S to solve ATM, which we know is not decidable. So don't get confused by that. We're aiming for a contradiction. So we're going to use S as typically, well, there might be other variations. But for now, S is going to be used to decide ATM. So we can try to figure out how can we decide ATM. And the way we're going to do it is use our HALT TM tester that we assumed to have. And we'll first take our M and W, where we're trying to determine does M accept W. And we'll first test whether M halts on W. If it doesn't, we're done. Because it couldn't be accepting W. M couldn't be accepting W if it's not even halting on W. So if R reports doesn't halt, we can reject right off. But even if R says it does halt, we're still in good shape. Because now we can run M on W until completion. Because R has promised us that it's going to halt. R is stated and R is assumed to be correct. R is stated that M halts on W. So now we don't have to worry about getting into a loop, which we're not allowed to do since we're making a decider. We're trying to decide ATM here. But now we can run M on W to completion. We can find out what M does on W. And then we can act accordingly. OK. So we're using the halt TM decider to decide ATM. That's the name of the game here. OK? And that's a contradiction. And so therefore, our assumption that halt TM was decidable had to be false, so it's undecidable. OK? Important to understand this. Because this is sort of the prototype for all of the other undecidability proofs that we're going to do going forward. OK? So we can just take a few seconds here. If there's something that you're not getting about this, it's a good time to ask. Not seeing many messages here or any, so why don't we go on? But if you ask, I can get to it next slide too. All right. Here we go. So here's the concept of reducibility. And I know I've taught this course many times. I know where the bumpy places are in terms of people struggling with material. The concept of reducibility is a bit tricky. So don't feel bad if you don't get it right away. So that's why I'm going to try to go slowly in this lecture to make sure we're all together on understanding how reducibility works. OK. So the concept of reducibility is that we say one problem is reducible to another. Say A reducible to B. It means that you can use B to solve A. That's what it means for A to be reducible to B. So I'm going to give a bunch of sort of informal examples of that or easy examples of that. And then we'll start to use it for real. So example one. This is sort of really outside material from the course, but I think it's something you can appreciate. Everybody knows you can measure the area of a rectangle by measuring the lengths of the two sides, measuring the length and width of the rectangle. So in other words, if you had the problem of determining the area, you could reduce that problem to the problem of measuring the length and width of the rectangle. So here, we're taking one problem and reducing it to another problem. It's conceivable that measuring the length and width is easier than it would be to measure the area directly by somehow covering the space with tiles as one way of measuring it. But it tells you you don't have to do that. The problem of measuring the area is easier than covering with tiles. You can just measure the length and width, and you're done. So reducibility is a way of making problems easier by translating them into some easier problem. So here's another example that we've already seen. We didn't call it a reducibility, but if you remember back a couple of weeks ago, we were talking about the languages ANFA and ADFA, the acceptance problems for NFAs and DFAs. And we gave a way of solving the ADFA problem. As you remember, the Turing machine simulated the finite automaton. And we solved the ANFA problem not by doing it directly, but by converting the NFA to a DFA and then using the solution for ADFA. In effect, what we were doing was we were reducing the ANFA problem to the ADFA problem. So let's do another example. Here's a problem. Here's an example that you, again, probably didn't think about it this way, but from your homework. We had this pusher problem, the problem of determining whether a pushdown automaton ever pushes on its stack for any input. I know a bunch of you were struggling with that problem, working on it, hopefully solving it in one way or another. So there's one way to solve it, is in effect, by reducing the pusher problem to the ECFG, the emptiness for CFGs, which is equivalent to the emptiness for PDAs. Why is that? Because you can take your, I mean, this is the solution I had in mind, which is a particularly simple and short solution, of course, not the only solution. You can take your pushdown automaton, where you're trying to determine if it ever pushes. And you can take the states that are about to make a push. And instead of making them make a push, you make them accept states. And you get rid of the original accept states. So now you've converted this automaton to one that accepts every time the original pushdown automaton pushes. And it accepts, and then it has to move to the end of the input, of course. So it goes into an accept state and moves to the end of the input. So every time the original machine was about to push, the new machine that you're just creating here is going to go into an accept state at the end of the input. Now, to test whether the original machine ever uses stack, it's enough to test whether the new machine ever accepts a string. So that's a way, I don't want to overcomplicate this right here and get you thinking about the homework again. But this is a way of reducing one problem to another problem. And if you don't quite get this one, just focus on the other two examples. I don't want to spend time on the homework set too right now. So we can address that all separately if you want. It's also the solution that's written up in the solution set that's posted on the home page, by the way. So getting back, let's see, thinking about reducibility. What I have in mind, again, this is sort of rephrasing it, but I'm trying to hammer it in, that if A is reducible to B, then solving B gives a solution to A. That's what happens in each of these examples. Now, how are we going to use that? We're going to use that in the following two ways. One is to observe that if A is reducible to B, and B is an easy problem, then A must also be easy, because we have a way of converting A problems into B problems. We have a way of solving A using B. So B is easy. Then now you can solve A too easily, because you can solve A using B, which is easy. Maybe that's the clearest up here in example one. Where measuring the area might seem at first glance hard, because I have to walk out over the whole area, but it's not hard, because you only have to measure the length and the width. So the fact that B is easy tells you that A is easy. But actually, this is not the way we're going to be using it most typically. We're going to be most typically using it in the second version, which is a little bit more convoluted. But this is the way you're going to have to get used to this. So if A is reducible to B, and you know A is hard, undecidable, unrecognizable, whatever the form of hard you care about, if you know A is hard and A is reducible to B, then that tells you B also has to be hard. Why? Because if B were easy, then A would be easy. But we know by we're assuming that A is hard. So B also has to be hard. So there's a, I'm inverting the logic here, but this is logically equivalent. So you have to mull that over a bit. So why don't you think about that? And let me just take a few questions on the chat. And don't forget, the TAs are there too. So they're happy to answer your questions. Don't make them sit there lonely. All right. So somebody's asking, is it possible that A is reducible to B and that B is also reducible to A? So that's a good question. That can certainly happen. In that case, in a certain sense, A and B are going to be equivalent. So solving one is going to be just as easy or hard as solving the other one. So they're going to be equivalent from the perspective of the difficulty of solving it. So somebody's asking, and this is a perennial confusion. Somebody's asking, so in the case, in the previous slide here, maybe I'll just flip back to it here. So which direction are we doing? Are we reducing A TM to whole TM or whole TM to A TM? The way it's written on the slide is what I have in mind. Here, we're reducing A TM to whole TM because we're using whole TM to solve A TM. And that's reducing A TM to whole TM. Just like here, measuring the area is reducible to measuring the length of the sides. We're using measuring the length of the sides to solve the area. So we're reducing the area to the lengths, the area to the length of the sides. But I know you're going to have to play with it, digest it, get used to it. All right. OK. So let's continue. So as I said, this latter one, because the focus on this course is mainly on the limitations of computation. So we're going to be looking at ways of showing problems are difficult. It could be difficult in principle, like undecidable, or it could be difficult in terms of complexity, which is what we're going to focus on in the second half. But in both cases, we're going to be using the concept of reducibility. So reducibility is going to be a theme. You've got to get comfortable with reducibility. So we're going to be focusing more on the notion that if you reduce A to B and you know A is hard, that tells you B is also hard. So I'm going to try to say that a few times during the course of today's lecture to try to help you get it. All right. Here's a check-in. A little bit sort of off to the side, but I thought it was a fun check-in. The question is, some people say biology is reducible to physics. Well, maybe everything is reducible to physics, since physics tells you about the laws of the universe. And biology is part of the universe. So my question to you is, do you think? And there's no right answer here. Do you think, in your opinion, is biology reducible to physics? Maybe yes. Or maybe there are some things like consciousness, which cannot be reduced to physics. Or maybe we don't know. So curious to know your thoughts. But it does kind of use, in a sense, the notion of reducible in the spirit of what I have in mind here, in the sense that if you could fully understand physics, would that allow you to fully understand biology? OK, here we are. We're almost kind of interesting, though not too unexpected, I suppose. OK, so we are, I think, just about done. Five seconds. Pick anything of you want to get credit for this and you haven't selected yet. Ready to go, ending polling. Here are the results. And as I say, there's no right answer here. But if I had been in the class, I would have picked B. But I'm not surprised that, especially in an MIT crowd, that A is a winner. All right, let's continue. OK, so now we're going to use reducibility again. This is going to be yet another example, like the whole TM example, but a little bit harder. And we're going to be doing this. In the next lecture, we're going to be doing more reducibilities, but much harder. So we really got to get really comfortable. All right, I'm going to show ETM. So ETM is the emptiness problem for Turing machines. Is its language empty? I'm just going to give you a machine. I want to know, is its language empty or not? Does it accept something, or is its language empty? That's going to be undecidable, no surprise, proof by contradiction. And we're going to show that ATM is reducible to ETM. So these things often take a very similar form. And I'm going to try to use the same form. So if you're feeling shaky on it, at least you'll get the form of the way the solution goes. And that will help you maybe plug in to solve problems. So proof by contradiction, assume that ETM is decidable, opposite of what we're trying to show, and then show that ATM is decidable, which we know is false. So let's say we have a decider for ETM, R, using the same letters on purpose here, just to try to get the pattern for you. So R deciding ETM, construct S deciding ATM. So now let's think about it together for a minute before I just put it up there. So S, I'm trying to make a decider for ATM, using my decider for the emptiness problem. So we have R, which can tell us whether M's language is empty. So why don't we just, I don't know, stick M into that emptiness tester and see what it says. I'm not saying this is the solution, but this is how one might think about coming up with the solution. OK? So are you with me? We're going to take M. We have an emptiness tester. Let's take M and plug it into R, see what R says. R is going to come back and tell us whether M's language is empty or not. Now, one of those answers will make us happy. Why? Suppose R tells us that M's language is empty. Why is that good? Well, now we're done. Because what S is trying to figure out, we're trying to figure out exactly. Somebody told me the answer, which is correct. Because now we can reject. If M's language is empty, it's clearly not accepting W, because it's not accepting anything. So if R says M's language is empty, then we're good. The only problem is R may also say M's language is not empty. And then what do we know? Well, not much. Not much that's useful for testing whether M accepts W. We just know M accepts something, but that something may or may not be W. OK, so what do we do? Well, the problem is that M is possibly accepting all sorts of strings besides W, which are kind of mucking up the works. They're making, they're interfering with the solution that we'd like. We'd like to be able to use R on M to tell us whether M is accepting W. But M is accepting other things, and that's making the picture complicated. So what I propose we do, why don't we modify M so that it never accepts anything besides W? The very first thing M does in the modified form is it looks at its input and sees whether it's different from W. If it's different from W, it immediately rejects. Now we take that modified machine and we feed it into the emptiness tester. Now the emptiness tester is going to give us the information we're looking for. Because if the emptiness tester says the modified machine's language is empty, well, we know that M is not accepting W because we haven't changed how M behaves when it's given W. But if R says M's language is not empty, well, then it must be that M is accepting W because we've already filtered out all of the other possibilities when we've modified the machine. So let me repeat that on the slide and write it down a little bit more formally. So what I'm going to do is I'm going to transform M to a new Turing machine. I'm going to call it M sub W to emphasize the fact that this new machine depends on W. It's going to actually have W built into as part of the rules of the machine. So for a different W, we're going to end up with a different machine here. So this is a machine whose structure is going to depend on knowing W. And that machine is going to be very much like the original machine M, except that when it gets an input, let's say it's called x now, that machine is going to compare x with W and reject if it's not equal. If otherwise, if x is equal to W, it's going to run M on W as before. So it's not going to change the behavior when the input is W. It's only going to change the behavior when the input is something different than W, and then it's going to reject. All right? So let's just, I'm going to look at two aspects of this. First, let's understand the language of this new machine, and then we'll also talk about how we go about doing this transformation. So first of all, just for emphasis, so MW works just like M. It has all the rules of M in it, except some extra rules. It always, at the very first step, it tests whether x is equal to W or not. And if it's not equal to, it rejects. Not equal to, W rejects. So the language of that new machine is either going to be just the string W when M accepts W, because everything else is filtered out, or the empty set if M rejects W. Got it? So it's important that you understand the behavior, at least, of this new machine. It's just like M, except filtering out all of the inputs which are not W. Those are going to be automatically rejected. Now, so it's also important that S be able to make this transformation. But I claim that you'll have to accept this if you don't totally see it. But the transformation is simply taking M and adding some new rules, some new transitions and states. So that the very first thing that M sub W does is it just has a sequence of moves where it's checking that the input string is equal to W or not. And if it's not equal to W, it just rejects. So it's easy to modify M. You could easily write a program which would modify the states and transitions of M to make it do that test at the beginning. So I'm not going to elaborate on those kinds of things in the future. But just for the very first time, I just want to make sure you understand that we're not doing anything fishy here. This is a completely legitimate thing for S to be able to do. So S can modify M to this new machine MW, which filters that new machine, filters out all strings except for W, and rejects them. So S takes that new machine. And what is it going to do with it? Is it ever going to run that machine? No. This machine is built not for running. This machine is built for feeding into R. Because as you remember, feeding M into R had the problem that M might accept things besides W. And that confuses the result that we get from R in the sense that it's not useful. But if we feed MW into R, now we're good. Because the information about whether MW's language is empty from over here tells us whether or not M accepts W. If MW's language is not empty, then M accepts W. If M's language is empty, M rejects W. So I'm starting to get some good questions here. So let me just finish the description of S. So somebody is asking here. So this is an excellent question. How do we know that MW halts on W or whatever? We don't. MW may not halt on W. We don't care. We're never going to run MW on anything. We're going to take MW as a machine, and we're going to feed it into R as a description. We're going to take the description of MW and feed it into R. Then it's R's problem. But R has been assumed to answer emptiness testing. So we're going to just take the original machine, modified it so that the only possible thing it could accept is W, and now feed it into the emptiness tester to see whether its language is empty or not. Now if its language is not empty, it has to be accepting W because it's built not to accept anything else. So we don't care whether MW might end up looping. We're never going to run MW. I acknowledge it's a leap for many of you. So you're going to have to mull it over. So I'm going to use R to test whether MW's language is empty. If yes, that means that M rejects W, so then we're going to reject. If we know that MW's language is empty, that must have been that M rejected W. So now as an ATM decider, which is what S is, S is supposed to reject, which is what we have here in the description. And if no, that means the language is non-empty, so M accepts W, and so therefore we are accepting. So there's a little bit of a twist here also. So let's take some more. I'm expecting some questions here. So somebody is asking, how do you determine if a language is decidable? That's what we're doing. You can show a language is decidable by exhibiting a Turing machine which decides it. And you can show a language is not decidable, which is what we're doing here, by proving that it's not possible for a Turing machine to decide it. We did that first with ATM. We got that contradiction by diagonalization. And here we're doing a reducibility to show as a method of proof. All right, let's continue. So now we're going to talk about a special kind. So so far, we talked about reducibility. We didn't define it in a precise way because there are several different ways to get at the notion of reducibility precisely. And I'm going to introduce a one version which is a little bit more restrictive, somewhat more restrictive, and a little bit different way of looking at it than we have been doing so far. But there are going to be some benefits to looking at this particular kind of reducibility, which we're calling mapping reducibility. It's going to have several benefits for us immediately and down the road. But this is also a little technical, so you're going to have to don't get scared off. It might look complicated at first. We'll try to unpack it for you. So first of all, if we have here a, first of all, talk about the notion of a computable function. So generally, when we've had Turing machines, they're doing yes, no. They're doing accept, reject kinds of things. So it's like a function, just sort of a computing, sort of a binary function. For here, we're going to want to talk about Turing machines that are computing some more general, it's a function which converts one string to another string. So it's a mapping from strings to strings. And it could be like the function which reverses the string, for example. That's one possible function you could be having here. But there are, of course, zillions of possible functions here. And we're going to talk about functions that you can compute with a Turing machine. And that basically means you provide the input to the function as input to the Turing machine. And the output of the function, the value of the function, comes out as the output of the Turing machine, which is, let's just say, it leaves that value on its tape when it holds. It holds with the value of the function on the tape. But just we're thinking about algorithms here. Come up with your favorite method of thinking about algorithms. It has an input and an output. And the algorithm just computes the function by taking as input w, and the output is f of w. Doesn't have to be Turing machines. Just any algorithm that can compute something is good enough. They're all equivalent. And now we're going to use this notion of a function that you can compute to define a kind of reducibility called mapping reducibility. I'm going to say that A is mapping reducible to B, written with this less than or equal to sub m symbol. And you're going to see that a lot. It's on the homework also, by the way. If there is some computable function, as I just described, where whenever w is in A, f of w is in B. And the way to think about it is with this picture. So A and B are languages, written like here's A and here's B. And now there are strings. So w might be in A. It might be out of A. And you think of you're trying to solve A. You're trying to decide membership in A. So you want to test whether w is in A or not. A mapping reducible is a function which maps things from this space over to that space in a way that strings that are in A get mapped to strings that are in B. So if you start out with w in A, f of w is in B. And if w is not in A, then f of w is not in B. Pictorially, it's a simple idea. We'll have to make sure we understand why this fits with our concept of a reducibility. But we'll do that. But anyway, let's first understand just what we're doing here. We're just coming up with a function that can do this kind of a mapping. It sort of translates inputs which may or may not be in A into other strings which may or may not be in B, but sort of maintaining the same input property, the same membership property. So if you start out with something in A, when you apply f, you're going to end up with something in B. And conversely, if you're not in A, then you won't be in B. Somebody's asking just a couple of questions here. Not necessarily one-to-one. No. So the function doesn't have to be one-to-one. There could be multiple things that map to the same point. And is there any restriction on alphabet? No. So before we actually get into the example, let me try to give you a sense about why we call this a reducibility. And the reason is, suppose we have such an f which can do the mapping as I described. And we also have a way of deciding membership in B. So B is decidable. So that's going to tell us right away that A is decidable. Because if you have some input and you want to know, is it in A or not, you can now apply f and test whether f of w is in B. So the test of w is in A, you're going to instead test whether f of w is in B. And we're assuming that shows that A is reducible to B. So if you could solve the B problem, that gives you a way to solve the A problem. Again, we're going to say this several times in several different ways. So if you didn't quite get it yet, don't panic. So here's going to be an example. Sort of building on what we just showed last time in the previous slide, ATM, we're going to show how ATM is actually mapping reducible to the complement of ETM. And the complement is necessary here. The computable function that we're going to give, which is basically if you're going to, the computable function is going to translate problems about ATM to problems about ETM. Of course, we're mapping reducing ATM to the complement of ETM. So what we're doing here is we're mapping M and w to the machine MW. Kind of in a way, we're boiling out the essence, boiling down to the essence of the proof that we gave in the previous slide. This is really the core of the proof. This translation of MW, where you want to know, is M accepting w to a new machine MW, where you're testing whether MW's language is empty. And so remember MW from before. It's the machine that filters out all the non-w's and rejects them. And the reason why this reduction function works is that MW is in ATM if and only if M sub w is in the complement of ETM. So M accepts w exactly when MW's language is not empty. So M accepts w if and only if the language of MW is not empty. You have to mull this over a bit. Realize it's, I know this can be a little tricky. But I think what we're going to do here, I think we're at the break, the time for the break. So oh, no, there's one more slide. I apologize. So let's talk about this. And then we're going to have our coffee break. So there are the reason why, so these properties are really going to be getting at what makes mapping reducibility fit with our understanding of what a reducibility should be. So if a is mapping reducible to b and b is decidable, then so is a. So that fits with what we want. Because if a is reducible to b and b is easy, then a is easy. So here, easy means decidable. And here's the proof. Let's take a Turing machine that decides b and construct a Turing machine that decides a as claimed. S operates like this. It takes its input, computes f of that input, tests whether f of w is in b using the R machine that we were assuming. We have R deciding b. And if R halts, then output the same result. So if R accepts, we're going to accept. If R halts and rejects, we'll reject. And of course, we're going to be simul running R. So if R is not going to be halting, we're not going to end up halting either. So the corollary is, and this is the way we're going to be using it, if a is reducible to b and a is undecidable, then so is b. So this is, as I mentioned, the focus for us is going to be on undecidability. And you may want to think about a is like the ATM problem, which we know is undecidable. We're going to show the ATM is mapping reducible to some other problem to show that that other problem is undecidable. And the important thing about mapping reducibility is that it also applies to recognizers. So if a is mapping reducible to b and b is recognizable, then so is a. So if you're reducing a to a recognizable problem, then a is also recognizable. Same proof, because you can just map your w to f of w and feed it into the recognizer. That's going to give you a recognizer for the original language. And the corollary is that if a is mapping reducible to b and a is unrecognizable, then so is b. So this is, again, that sort of inverted logic. So now I think we're, oops. I meant to put this picture up earlier. OK, so here's a check-in. It'd be more of a check-in for me to see how well you're following me. So these are some properties of, so I'll give you a minute here to think about this, some properties of mapping reducibility. Suppose a is mapping reducible to b. What can we conclude? Does that mean that we can flip it around? If a is mapping reducible to b, does that mean that b is mapping reducible to a? What about this one? If a is mapping reducible to b, is the complement of a mapping reducible to the complement of b? Or maybe neither. So you can check all that apply. Multiple choice. OK, five seconds. Sorry to pressure you, but we have to move on here. Pick anything if you don't know. OK, 1, 2, 3, the end. So well, the majority is correct. In fact, it's only b. Now, a really is not in the spirit of reducibility because, as suggested even by the inequality sign there, a being reducible to b is really a rather different thing than b being reducible to a. So that's something to, we're not going to prove that right here, but that's something that you could think about. But part b, I think if you just look here at the definition of mapping reducibility, it maps strings in to in and out to out. Well, that's just going to be, if you exchange in and out, as you do when you're flipping complements on both sides, it's still by the same f going to still work as a mapping reduction. So now we're at our break. So we're going to take five minutes here. And I'll be happy to take questions here. Don't forget the TAs. They're here too. OK. So this is a fair question here. So we had this notion of a general reduction and a mapping reduction. They're not the same. So any time you have a mapping reduction, it's going to be an example of a general reduction, but not the other way around. So if you go back and look at the reduction that we offered for HALT TM, where we showed ATM is reducible to HALT TM, where we started, it's actually not a mapping reduction because we're doing something more complicated than just translating an ATM problem to a HALT TM problem. We're kind of reusing it. We're using the HALT TM decider in a more complicated way. And there are cases where that's actually necessary. So we won't not going to discuss that here. But it's actually maybe an interesting homework problem perhaps or some kind of problem to think about. So Turing machines for Fs, does F have to be a, it's not really a decider, but it has to be a, well, I guess it does have to be a decider in the way it always holds. The Turing machines for F has to always hold. It always has to have an output. So F, for the computing the function, always has to hold. So someone is asking me, can I explain the statement that if R holds, then output the same result? I just mean that in that previous slide or two slides back, if R accepts, holds and accepts, then we're going to halt and accept. And if R holds and rejects, then we halt and reject. So I don't know if this is a good idea, but we can just pull that back here. That was the statement here. If R holds and output the same result, I just mean that S is going to do the same. We're translating an A problem to a B problem and then answering the B problem. And we're going to give the same value, the same answer there. So whatever R says, we're going to say, too, if that's helpful. Well, that's a good question here. If A is reducible to B, why can't we just get B reducible to A by inverting the function? That's a great question. I like that question. The reason is because the function that's mapping onto B doesn't have to be onto, surjective, I guess. So you can't always, you're not going to be able to, if it was onto, so if it covered all of B, then I think then you would get an invertible function. And you would get the reduction going the other way as well. But I'm not sure what happens when you have, it doesn't matter if you have collisions. It turns out that's not going to matter. But anyway, let's not get it too complicated here. But the problem with inverting it is that it's not necessarily onto the whole range of B. So we're kind of out of time here. Is A reducible to A complement? Let me just end with that. No, not necessarily. It's reducible to A, OK, we'll get there. A is reducible to A complement, but not mapping reducible to A complement. But A is general reducible to A complement. So actually, we'll talk, I have a slide on that. So let us move on. OK, mapping, I think it's actually this slide, mapping versus general reducibility. So we're going to kind of contrast it to a bit. So mapping reducibility, which is what we've just been talking about, has this picture, which is, I think, a very useful picture to remember. And because the way I like to think of a mapping reduction as a problem translator. Your problem's sort of in the A domain, and the mapping reduction allows you to translate that problem into the B domain. And then if you have a way of solving it in the B domain, combining that with the reduction, you get a solution to the problem in the A domain. So that's why if A is mapping reducible to B, and B is solvable, then A is also solvable. So mapping reduction is a special kind of reducibility, as opposed to the general notion, general reducibility, where we started. And it's particularly useful to prove Turing unrecognizability. So when you want to prove Turing unrecognizability, as we'll see, the general reducibility is not fine enough, in a way. It doesn't sort of differentiate things as well as mapping reducibility does. And for that reason, it's not always going to be useful to prove Turing unrecognizability. It's better for proving undecidability. So what we call reducibility, or general reducibility, is where we just use a solver for B to solve A in sort of a most general possible way. So I'm writing that as the picture here. You want to solve A, you're going to use the B solver as a subroutine to solve A. That's the way we did the whole TM reduction at the beginning. But we didn't necessarily translate an ATM problem to a whole TM problem. A little slightly different. So you can go back and look at that. So I find that people struggle more with the mapping reducibility concept, and that the general reducibility is what people naturally gravitate towards. And so in some sense, it's conceptually simpler. And it's useful to proving undecidability. But you really have to be comfortable with both. And especially in the complexities part, we're going to be focusing on mapping reducibility. So one noteworthy difference here, as sort of foreshadowed by the person who made this question, which is a good question, is that A is reducible using a general reduction to A complement, which kind of makes sense. I mean, if I can test whether things are in A complement, well, I can test whether things are in A. I just invert the answer. But A may not be mapping reducible to A complement, because there it's a very special kind of reduction. And you have to just translate things in the language to things in the language, and things out of the language to things out of the language. And they don't necessarily allow you to do that inversion. So for example, ATM complement is not mapping reducible to ATM. Because as we pointed out, anything that's reducible to a recognizable language is going to be recognizable. Anything mapping reducible to a recognizable language is going to be recognizable. But we know that ATM complement is not recognizable. We showed that before. So it couldn't be mapping reducible to ATM. Coming a little fast, I realize I'm going to have to digest it. So here's the last check-in for today. We showed that if A is mapping reducible to B, and B is Turing recognizable, then so is A. So let's just say that again carefully. If A is mapping reducible to B, and B is Turing recognizable, then so is A. And here, the emphasis on Turing recognizable as opposed to decidable. Is the same true if we use general reducibility instead of mapping reducibility? So you got it? So we're saying A is mapping reducible to B, using this picture over here. And we're going to assume that B is Turing recognizable, so that we have a machine which holds an accept when you're inside B and is going to reject possibly by looping when you're not inside B. Now, that allows you to get a recognizer for A if you have a mapping reduction. Does it always work to give you a recognizer if you have just a general reduction? If you just had now assumed you have a B solver, and you're going to build an A solver out of that. OK, so mull that over while I'm setting this thing up. Well, the right answer is winning, but not by much. I suppose I shouldn't be laughing about it. But I knew that this is going to be challenging. So I think it's the kind of thing you're going to have to work at. So let's see, we're almost done here. Five seconds to go. Better answer it. I can see a few of you, either you've left the room or you're, OK, two seconds. One, two, three. Somebody hasn't answered it. There we go. So the correct answer is B. It's not the same. The reason is that in a general reduction, the picture's right here. Let's see. So how do I explain this? So we know that a language is going to be, OK, if we're using general reducibility, and A is just reducible to B. So we know that a language is always reducible to its complement in using general reducibility. So if this were true, then we would have here. So if this were true, when A, when a language is reducible to its complement, if the complement were recognizable, the language would also be recognizable. That clearly is not going to be the case, because ATM complement is reducible to ATM using general reducibility. But ATM complement is not recognizable, even though ATM is recognizable. So we kind of have a proof that this has to be no. But as you can see, I'm even getting myself confused. So you have to stare at it. So let me see. We can try to take a couple of questions, see if I can clear up people's confusion. OK, so why, again, is A reducible to its complement in the general sense? So I'm saying, if you have a solver, if you have a decider for A complement, it gives you a solver for A. You just ask the solver, is the string in the language or not? And now you just give the opposite answer if you want to solve the complementary problem. So the A complement is general reducible to A. You just invert the answer for whatever the solver is doing for A. But you can't just do that inversion when you have a mapping reduction. It's a much more kind of specific translation that's allowed. I mean, the fundamental difference between general reducibility and mapping reducibility, I'm trying to bring it out here. It's just a difference in the nature of the way things are used. Mapping reducibility is a special kind of general reducibility. So to answer the question about what's the fundamental difference, one is using the problem as a subroutine, and the one is using it as a transformation. So anyway, I think we're going to have to move on here. And I have a couple of examples which may help. And then there's office hours too after the lecture. OK, oh, yeah. So I wanted to, again, to help you, I'm putting these down as sort of templates for how do you use reducibility. I'm not saying you should just apply things blindly. But I think it's sometimes good just to see the pattern and then to understand how the pattern works once you just start to understand the pattern of how things are used. So to show a language is undecidable, to prove a language B is undecidable, show undecidable language is reducible to B. Using just a general reduction is going to be good enough. And the template for that is assume we have R deciding B, which you then can use as a subroutine when you make a machine, Turing machine, or S deciding A. And that's going to be your contradiction if A was originally known to be undecidable. But now to prove something unrecognizable, this kind of reduction is not going to be, is not in a sense, it's not as restrictive enough. Because this kind of reduction allows for complementation, which is not going to be satisfactory when you're trying to prove Turing unrecognizable. So you're going to have to prohibit that complementation. And that's really one of the effects of the mapping reducibility, if that's sort of getting at the essence of it. So you're going to show Turing unrecognizable A is mapping reducible to B. Often, you start off with the complement of A, T, M, which is a language we know is Turing unrecognizable, as we showed before. Here, the template is you give the reduction function F, that computable function. So here are going to be two examples, one showing that ETM is Turing unrecognizable. We showed it was undecidable before. Now we're going to show it's even, in a sense, worse. It's not even recognizable. And the way we'll do that is to reduce a known unrecognizable language to ETM, again, the emptiness language. So here is the picture that we have when we're doing mapping reductions. We're going to map strings that are in the complement of A, T, M, so strings that are outside of A, T, M, if you wish, to strings where the language is empty, to machines where the language is empty. And here, strings describing machines where the language is empty. Here, we're going to take ETM problems and map them to machines where the language is not empty. And the thing that's going to do the trick is going to be that same reduction function that we saw earlier. We're going to take that machine w from before, the machine that filters out all the non-w's. And we're going to take Mw, which is an A, T, M complement problem. So if M rejects w, then it's in the complement of A, T, M. And that's supposed to map to a string, a machine, which is where the language is empty. So if Mw is in the complement of A, T, M, so M rejects w, then Mw's language is going to be empty, which is what you want to have happen. Let me move on to my last. I mean, this example is, in a way, kind of similar to the one we did before. And I really want to get to the last example here. OK, so we'll have to just talk through this rather than having it build. Let's take EQTM. That's the equivalence problem for Turing machines. Do they recognize the same language? So this is a language of a new kind for us. This is a language where neither it nor its complement are going to be recognizable, Turing recognizable. So the way we get that is we're going to reduce the complement. The way we show problems are not recognizable is mapping reduce a non-recognizable language to a typically the complement of A, T, M. So we're going to mapping reduce the complement of A, T, M to both EQTM and to the complement of EQTM to show that both of those are not recognizable. And here we're going to introduce a new machine that we're going to be building inside the reduction function. And that's going to be a machine I'm going to call TW. And TW is a machine that always behaves the way M behaves on W for every input. So if M accepts W, T is going to accept everything. If M rejects W, T is going to reject everything. So it copies the behavior of M on W onto all inputs. And the way I describe that machine TW is it ignores its input. Whatever the input is, it just simulates M on W. You could easily, given M and W, you can build the machine TW. It just always runs M on W, no matter what input it gets. And so now we're going to give a function which maps A, T, M problems, which have the form MW, so A, T, M complement problem. So we want to test if M accepts W or not. So that's an A, T, M complement type problem. And I want to map that to an EQTM problem of the form EQTM problems are pairs of machines now and where we're going to be testing equivalence. So I'm just trying to give you the form of the output of the reduction function F. And specifically, what it's going to look like is when we have M is F is processing on MW, it's going to produce two machines. One of them is going to be TW, which always behaves the way M behaves on W, but expanded to all inputs. And then a machine I'm going to call T reject, which just is designed to reject everything. Now, just walk through the logic with me. If M rejects W, TW rejects everything. And so we'll be equivalent to the machine T reject. That's what we want. If M rejects W, so we're in the language A, T, M complement, then these two machines that I produced for you are going to be in the EQTM language. That's what I want to have happen for a reduction from A, T, M complement to EQTM. Similarly, to do part two, I'm going to make here a different F. Maybe I should call it F prime or F1 and F2 for the two different parts. So these are two different Fs. I'm going to make F here on instead of generating TW and T reject, I'm going to have TW and T accept, which is a Turing machine that always accepts its input. Now, if M rejects W, it's in A, T, M complement, then TW is going to reject everything. And it's going to be different from its companion here, T accept. And so it won't be in EQTM complement. But if M accepts W, then TW is going to accept everything. And it's going to be equivalent to T accept. And you will be equivalent. So here is where we're taking A, T, M complement and mapping it to the complement for EQTM. Too many complements here, I realize. Complements are confusing. But anyway, why don't you mull this over? And just to summarize, OK, yeah, we're out of time here. But why do we use reducing when we talk about reductions? It's because when we reduce A to B, we kind of bring A's difficulty down to B's difficulty. That's where the reducing comes from. Or we bring B's difficulty up to A's difficulty, because it's really A's difficulty relative to B that we're talking about when we're reducing A to B. So that's why the term reducing seems a little out of place when we're proving things undecidable or unrecognizable. But that's where it's coming from. Anyway, quick review. We introduced the reducibility method. We defined mapping reducibility as a special kind of reducibility. We showed ETM is undecidable and unrecognizable, and that EQTM is both it and its complement are unrecognizable. So we're out of time. I will shut this down. But I'll take a few questions here, actually. I'll stick around for a few questions. And then I'll move to the other chat room for office hours. So question to go over the case for the complement of EQTM. So I will do that. So that's in this slide here. OK, so this is proof part two for the person who asked me to go over it. But I think it's helpful for those of you who might be a little bit shaky on this. I want to mapping reduce the complement of ATM to the complement of EQTM. By the way, I don't know if this is going to be helpful. But as we pointed out in the check-in a while back, that's completely equivalent to having a mapping reduction from ATM to EQTM. You can complement both sides, and you get an equivalent statement. Maybe let's stick with the complements here, so I hope that doesn't make it too confusing. OK, we're trying to show the complement of ATM is mapping reducible to the complement of EQTM. What does that mean? So that means when M rejects W, so you're in the complement, we want the two Turing machines to be inequivalent. No, yeah, so we're in the complement of EQTM. Yeah. So in other words, when we're in the complement of ATM, we want the result of F to be in the complement of EQTM. So in other words, when M rejects W, the two machines should be inequivalent. When M accepts W, the two machines should be equivalent. Because when we're not in this language, so we're in ATM, we want to be not in that language. So we should be in EQTM. So when M accepts W, we should be equivalent. When M rejects W, we should be inequivalent. That's what we want. Let's go down here. So if M accepts W, we want them to be equivalent. So if M accepts W, TW accepts everything, and it's equivalent to T accept. When M rejects W, TW rejects everything. And so it's not equivalent to the machine that accepts everything. So I hope that's just go through the logic yourself. You'll see why it's working. All right, bye-bye, everyone.