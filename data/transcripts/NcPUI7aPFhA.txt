 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. OK, let me start one minute early. So this being MIT, I just came from a terrific faculty member, Andy Lowe, in the Sloan School. And I have to tell you what he told us. And then I had to leave before he could explain why it's true. But this is like an amazing fact, which I don't want to forget. So here you go. This is everything will be on that board. So it's an observation about us or other people, maybe not us. So suppose you have a biased coin. Maybe the people playing this game don't know, but it's 75% likely to produce heads, 25% likely to produce tails. And then the player has to guess for one flip after another, heads or tails. And you get $1 if you're right. You pay $1 if you're wrong. So you just want to get as many right choices as possible from this coin flip that continues. So what should you do? Well, what I guess I hope we would do is we would not know what the probabilities were. So we would guess maybe heads the first time, tails the second time, heads the third time, and so on. But the actual result would be mostly heads. So we would learn at some point, maybe not quite as soon as that, we would eventually learn that we should keep guessing heads. And then that would be our optimal strategy, to guess heads all the time. But what do people actually do? They do start like this, same way. And then they're beginning to learn that heads is more common. So maybe they do more heads than tails. But sometimes tails is right. And then after a little while, they maybe see that it's, yeah. Or they're just, well, maybe they're not counting. They're just operating like ordinary people. And what do ordinary people actually do in the long run? You would think guess heads every time, right? But they don't. In the long run, people, and maybe animals, and whatever, guess heads 3 quarters of the time and tails 1 quarter of the time. Isn't that unbelievable? They're guessing tails a quarter of the time when the odds are never changing. Anyway, that's something that economists and other people have to explain. And if I had been able to stay another hour, I could tell you about the explanation. But anyway, that's, oh, I see. I've written that on a board that I have no way to bury. So it's going to be there. And it's not the subject of 18.065. But it's kind of amazing. So there's good math problems everywhere. OK, can I just leave you with what I know? And if I learn more, I'll come back to that question. OK, please turn attention this way, right? Norms, a few words on norms, just like that should be a word in your language. And so you should know what it means. And you should know there are a few of the important norms. Again, a norm is a way to measure the size of a vector or the size of a matrix or the size of a tensor, whatever we have, or a function. Very important. A norm would be a function like sine x from 0 to pi. What would be the size of that function? Well, if it was 2 sine x, the size would be twice as much. So the norm should reflect that. So yesterday, or Wednesday, I told you the p equals 2, 1, actually infinity. And then I'm going to put in the 0 norm with a question mark, because you'll see that it has a problem. But let me just recall from last time. So these are the p equal to 2 is the usual sum of squares, square root, usual length of a vector. p equal 1 is this very important norm. So I would call that the L1 norm. And we'll see a lot of that. Really, I mentioned that it plays a very significant part now in compressed sensing. It really was a bombshell in signal processing to discover, and in other fields too, to discover that some things really work best in the L1 norm. The maximum norm has a natural part to play. And we'll see that, or its matrix analog. So I didn't mention the L0 norm. All this LP business, so the LP norm for any p is you take the p-th power to the p-th power up here. p was 2. And you take the p-th root. So maybe I should write it to the 1 over p. Then that way, taking p-th powers and p-th roots, we do get the norm of 2v has a factor 2 compared to the norm of v. So p equal to 2, you see that we've got it right there. p equal 1, you see it here because it's just the sum of the absolute values. p equal infinity, if I move p up and up and up, it will pick out. As I increase p, whichever one is biggest is going to just take over. And that's why you get the max norm. Now, the 0 norm, where I'm using that word improperly, as you'll see, because if so what is the 0 norm? So let me write it sort of complete. It's the number of non-zeros, of non-zero components. It's the thing that you'd like to know about in question of sparsity. Is there just one non-zero component? Are there 11? Are there 101? You might want to minimize that because sparse vectors and sparse matrices are much faster to compute with. You've got good stuff. But now I claim that's not a norm, the number of non-zero components. Because what's the norm of 2? How does the norm of 2v compare with a norm of v, the 0 norm? It would be the same. 2v has the same number of non-zeros as v. So it violates the rule for a norm. So I think with these norms and all the p's in between, so actually the math papers are full of let p be between 1 and infinity. Because that's the range where you do have a proper norm, as we will see. I think the good thing to do with these norms is to have a picture in your mind. The geometry of a norm is good. So the picture I'm going to suggest is plot all the vectors, let's say, in 2D. So two-dimensional space, R2. So I want to plot the vectors that have v equal 1 in these different norms. So let me ask you what. So here's 2D space, R2. And now I want to plot all the vectors that have the ordinary L2 lengths equal 1. So what does that picture look like? I just think a picture is really worth something. It's a circle. Thanks. It's a circle. It's a circle. The circle has the equation, of course, v1 squared plus v2 squared equal 1. So I would call that the unit ball for the norm or whatever is a circle. OK, now here comes more interesting. What about in the L1 norm? All right, so again, tell me how to plot all the points that have v1 plus v2 equal 1. What's the boundary going to look like now? It's going to be, let's see. Well, I can put in a certain number of points. They're out at 1, and they're at 1, and they're at minus 1, and they're at minus 1. That would reflect the vector 1, 0. And this would reflect the vector 0 minus 1. So yeah, OK, so those are like four points easy to plot, easy to see the L1 norm. But what's the rest of the boundary here? It's a diamond. Good. It's a diamond. We have linear set equal to 1. Up here in the positive quadrant, it's just v1 plus v2 equal 1. And the graph of that is a straight line. So all these guys, this is all the points with v1 plus v2 equal 1. And over here, and over here, and over here. So the unit ball in the L1 norm is a diamond. And that's a very important picture. It reflects in a simple, very simple way something important about the L1 norm and the reason it's just exploded in importance. Let me continue, though. What about the max norm? The max, or v infinity equal 1. So again, let me plot these guys. And these guys are certainly going to be in it again. The 0, the plus or minus i, and plus or minus j that are good friends. What's the rest of the boundary look like now? Now, this means max of the v's equal 1. So what are the rest of the points? You see, it does take a little thought, but then you get it and you don't forget it. So what's up? So suppose the maximum is v1. I think you're going to go, I think it's going to look like that, out to 1, 0 and up to 0, 1. And up here, the vector would be 1.4 or something. So the maximum would be 1. Is that OK? So somehow, what really sees as you change this number p, you start with p equal 1, or diamond, and it kind of swells out to be a circle at p equal to 2. And then it kind of keeps swelling to be a square at p equal infinity. That's an interesting thing. And yeah, yeah. Now, what's the problem with the 0 norm? This is the number of non-zeros. OK, let me draw it. Where are the points with 1 non-zero? So I'm plotting the unit ball. Where are the vectors in this thing that have 1 non-zero, not 0 non-zero? So that's not included. So what do I have? I'm not allowed the vector 1 third, 2 thirds, because that has 2 non-zeros. So where are the points with only 1 non-zero? Yeah, on the axes. Yeah, that tells you. So it can be there and there. Oops, without that guy. And of course, those just keep going out. So it totally violates the, you know. So maybe the point that I should make about these figures. So like, what's happening when I go down to 0? I'm sort of, really, that figure should be at the other end, right? Oh, no, it shouldn't. This guy is in the middle. This is a badly drawn figure. L2 is kind of the center guy. L1 is at one end. L infinity is at the other end. And this one has gone off the end at the left there. The diamond has, yeah, what's happened here is that 1 goes down towards 0. None of these will be OK. These balls will, or these sets will lose weight. So they'll always have these points in, but they'll be like this, and then like this, and then finally in the unacceptable limit. But none of those. This was not any good either. This was for p equal 1 half, let's say. Sort of that's a p equal to half, and that's not a good norm. Yeah, so maybe the thing, so there's a property of the circle, the diamond, and the square, which is a nice mass property of those three sets, and is not possessed by this. As this thing loses weight, I lose the property. And then, of course, that's totally lost over there. Do you know what that property would be? Explain? Concave, convex, convex, I would say. Convex. This is a true norm, has the convex unit ball. Maybe for ball, I'm taking all of these less or equal 1. Yeah, so I'm allowing the insides of these shapes. So this is not a convex set. That set, which I should maybe, so not convex would be this one with, like so. And that reflects the fact that the rules for a norm are broken, the triangle inequality is probably broken, and the other stuff. I think that's sort of worth remembering. And then one more norm that's natural to think about is, so S, as in the Piazza question, S does always represent a symmetric matrix in 18.065. And now I'm going to ask, my norm is going to be, so I'm going to call it the S norm. So S is a, actually, it's a positive definite symmetric matrix. F is a positive definite symmetric matrix. And what will I do? I'll take V transpose AV, SV, sorry, SV. OK, what's our word for that? The energy. That's the energy in the vector V. And I'll take the square root so that I now have the length of 2 if I double V, from V to 2V. Then I get a 2 here and a 2 here, and when I take the square root, I get an overall 2, and that's what I want. I want the norm to grow linearly with the 2 or 3 or whatever I multiply by. But what is the shape of this thing? So what is the shape of, let me put it on this board. I'm going to get a picture like that. So what is the shape of V transpose SV equal 1, or less or equal 1, if S is, this is a positive, symmetric positive definite. People use those three letters to tell us. I'm claiming that we get a bunch of norms. When do we get the L2 norm? What matrix S would this give us the L2 norm? The identity, certainly. Now what's going to happen if I use some different matrix S? This circle is going to change shape. I'm going to have a different norm depending on S, and a typical case would be S equal 2, 3. Say that's a positive definite symmetric matrix. And now I would be drawing the graph of 2 V1 squared plus 3 V2 squared. That would be the energy, right? Equal 1. And I just want you to tell me what shape that is. So that's a perfectly good norm. You could check all its properties. They all come out easily. But I get a new picture, a new norm that's kind of adjustable. You could say it's a weighted norm. Weights mean that you kind of have picked some numbers sort of appropriate to the particular problem. Well, suppose those numbers are 2 and 3. What shape is the unit ball in this S norm? It's an ellipse, right? It's an ellipse. And I guess it will actually be, so the larger number 3 will mean you can't go as far as the smaller number 2. I think it would probably be an ellipse like this. And the axis length of the ellipse would probably have something to do with the 2 and the 3. OK, so that's just, now you know really all the vector norms that are sort of natural to use. These come up in a natural way. They're all, as we said, the identity matrix brings us back to the 2 norm. So these are all sort of variations on the 2 norm. And these are variations as p runs from 1 up to 2 on to infinity and is not allowed to go below 1. OK, that's norms. And then maybe you can actually see from this picture. Here is a somewhat hokey idea of why it is that this norm, minimizing the error in this norm. So what do I mean by that? Here would be a typical problem. Minimize subject to Ax equal b the L1, sorry, I'm using x now, the L1 norm of x. So that would be an important problem. Actually, it has a name. People have spent a lot of time thinking of a fast way to solve it. It's almost like least squares. What would make it more like least squares would be change that to 2. Yeah, yeah. Can I just sort of like sketch without making a big argument here the difference between L equal 1 or 2 here? I'll just draw a picture. Now, I'll erase this ellipse, but you won't forget it. So this is our problem. With L1, it has a famous name, basis pursuit. Well, famous to people who work in optimization. For L2, it has an important name. Well, it's sort of like least squares, ridge regression, all sorts of. This is like a beautiful model problem. Among all solutions to Ax, suppose this is just one equation, like c1 x1 plus c2 x2 equals some right side b. So the x's, the constraint says that the vectors x have to be on a line. Suppose that's a graph of that line. So among all these x's, which one? Yeah, I'm realizing what I'm going to say is going to be smart. I mean, it's going to be nice. Not going to be difficult. Let's do the one we know best, L2. So here's a picture of the line. Let me make it a little more tilted so you. Yeah. Yeah. Yeah, like 2, 3. OK. This is the xy plane. Here's x1. Here's x2. Here are the points that satisfy my condition. Which one minimizes, which point on that line minimizes as the smallest L2 norm? Which point on the line has the smallest L2 norm? Yeah, you're drawing the right figure with your hands. The smallest L2 norm, L2, remember, is just how far out you go. It's circular here. So it doesn't matter what direction. They're all giving the same L2 norm. It's just how far. So we're looking for the closest point on the line, because we don't want to go any further. We want to go a minimum distance. I'm doing L2 now. So where is the point at minimum distance? Yeah, it should show me again once more with hands or whatever. It'll be that. I didn't want 45 degree angles there. I'm going to erase it again. Really, this time I'm going to get angles that are not 45. Is that obvious? All right, brilliant. Got it. OK, that's my line. OK, and what's the nearest point in the L2 norm? Here's the winner in L2, right, the nearest point? Everybody sees that picture? So that's a basic picture for minimizing something with a constraint, which is the fundamental problem of optimization, of neural nets, of everything, really, of life. Well, I'm getting philosophical. So OK, but of course, the question always is, and maybe it's true in life too, which norm are you using? OK, now that was the minimum in L2. That's the shortest distance, where distance means what we usually think of it as meaning. But now let's go for the L1 norm. Which point on the line has the smallest L1 norm? So now I'm going to add the two. So if this is some point A0, and this is some point 0B, right there. So those two points are obviously important. And that point, we could figure out the formula for, because we know what the geometry is. But I've just put those two points in. So did I get a 0B? Yeah, that's a 0. So let me just ask you the question. What point on that line has the smallest L1 norm? Which has the smallest L1 norm? Somebody said it? Just say it a little louder so that you're on tape forever. 0B, this point. That's the winner. This is the L1 winner. And this was the L2 winner. And notice what I said earlier, and I didn't see it coming. But now I realize this is a figure to put in the notes. The winner has the most 0's. It's the sparsest vector. It's got a 0. Well, out of two components, it didn't have much freedom. But it has a 0 component. It's on the axes. It's the things on the axes that have the smallest number of components. So yeah. So this is the picture in L2. Sorry, the picture in two dimensions. So I'm in 2D. And you can see that the winner has a 0 component. Yeah. Yeah. And that's a fact that extends into higher dimensions, too, and that makes the L1 norm special, as I've said. Yeah. Is there more to say about that example? For a simple 2D question, that really makes the point that the L1 winner is there. It's not further. You don't go further up the line, right? Because then you have even that's bad in all ways. When you go up further, you're adding some non-zero first component, and you're increasing the non-zero second component. So that's a bad idea. That's a bad idea. This is the winner. And in a way, here's the picture. Yeah. Oh, yeah. I should prepare these lectures, but this one's coming out all right anyway. So the picture there is the nearest ball hits at that point. And what is it? Can you see that? So that star is outside the circle. This is the L1 winner. So when that's the nearest, that's the blow up the L1 norm until it hits. That's the point where the L1 norm hits. Do you see it? Just give it a little thought. Another geometric way to see the answer to this problem is you start at the origin, and then you blow up the norm until you get a point on the line. That satisfies your constraint. And because you were blowing up the norm, when it hits first, that's the smallest blow up possible. That's the guy that minimizes. Yeah. Just think about that picture, and I'll draw it better somewhere, too. Well, that's vector norms. Vector norms. And then I introduced some matrix norms. And let me just say a word about those. Yeah. OK. A word about matrix norms. So the matrix norms were the, so now I have a matrix A. And I want to define those same three norms again for a matrix. And this was the 2 norm. And what was the 2 norm of a matrix? Well, it was sigma 1. Turned out to be. So that doesn't define it. Or it could define it. Just say, OK, the largest singular value is the 2 norm of the matrix. But actually, it comes from somewhere. So I want to speak about this one first. The 2 norm. So it's the 2 norm of a matrix. And one way to see the 2 norm of a matrix is to connect it to the 2 norm of vectors. I'd like to connect the 2 norm of matrices to the 2 norm of vectors. And how shall I do that? I think I'm going to look at the 2 norm of Ax over the 2 norm of x. So in a way, to me, that ratio is like the blow-up factor. If A was 7 times the identity, to take an easy case, if A is 7 times the identity, what will that ratio be? Say it, yeah? 7. If A is 7i, this will be 7x. And this will be x. And norms, the factor 7 comes out. So that ratio will be 7. OK. For me, the norm is, that's the blow-up factor. So here's the idea of a matrix norm. Now I'm doing matrix norm from vector norm. And the answer will be the maximum blow-up. The maximum of this ratio. I call that ratio the blow-up factor. That's just a made-up name. The maximum over all x's. I look to see which vector gets blown up the most. And that is the norm of the matrix. I know I've settled on norms of vectors. That's done upstairs there. Now I'm looking at norms of matrices. And this is one way to get a good norm of a matrix that kind of comes from the 2-norm. So there would be other norms for matrices coming from other vector norms. And those we haven't seen. But the 2-norm is, this is a very important one. So what is the maximum value of this, of that ratio for a matrix A? The claim is that it's sigma 1. I put a big equality there. Now, can we see why is sigma 1 the answer to this problem? I can see a couple of ways to think about that. But that's a very important fact. In fact, this is a way to discover what sigma 1 is without all the other sigmas. If I look for the x that has the biggest blow-up factor, and by the way, which x will it be? Which x will win the max competition here and be sigma 1 times as large as the ratio will be sigma 1? That'll be sigma 1. When is this thing sigma 1 times as large as that? For which x? Not for an eigenvector. If x was an eigenvector, what would that ratio be? Lambda. But if A is not a symmetric matrix, maybe the eigenvectors don't tell you the way, the exact way to go. So what vector would you now guess? It's not an eigenvector. It is a singular vector. And which singular vector is it probably going to be? V1. Yeah, V1. Makes sense. Winner. So the winner of this competition is x equal V1, the first right singular vector. And we better be able to check that. So again, this maximization problem, the answer tells you is in terms of the singular vector. So that's a way to find this first singular vector without finding them all. And let's just plug in the first singular vector and see that the ratio is sigma 1. So now let me plug it in. So what do I have? I want A V1 over length of V1. And I'm hoping to get that answer. Well, what's the denominator here? The length of V1 is 1. No big deal there. That's 1. What's the length of the top one? So is A V1. Now what is A V1? If V1 is the first singular, first right singular vector, then A V1 is sigma 1 times U1. Remember the singular vector deals were A V equals sigma U. A VK equals sigma K UK. Remember, so they're not eigenvectors. They're singular vectors. So that is A V1 is the length of sigma 1 U1. And it's divided by 1. And of course, U1 is also a unit vector. So I just get sigma 1. OK. So that's another way to say that you can find sigma 1 by solving this maximum problem. And you get that sigma 1. OK. And I could get other norms, matrix norms, by maximizing that blow up factor in that vector norm. I won't do that now, just to keep control of what we've got. Now what was the next matrix norm that came in last time? Very, very important one for deep learning and neural nets. Somehow it's a little simpler than this guy. And what was that matrix norm? What letter whose name goes here? Frobenius. So capital F for Frobenius. And what was that? That was the square root of the sum of the all the add up, add all the Aij squareds for all over the matrix, and then takes a square root. And then somebody asked a good question after class on Wednesday. What has that got to do with the sigmas? Because my point was that these norms are the guys that go with the sigmas, that have nice formulas for the sigmas. And here it is. It's the square root of the sum of the squares of all the sigmas. So let me write Frobenius again. But this notation with an F is now pretty standard. And we should be able to see why that number is the same as that number. Yeah, I could give you a reason, or I couldn't put it on the problem set. I think it's better on the problem set. Because first of all, I'll get off the hook right away. And secondly, this connection between in Frobenius, that's like a beautiful fact about Frobenius norm that you add up all the sigma squareds. This is m times n of them, because it's all a filled matrix. Or you add up. So another way to say it is, we haven't written down the SVD today. A equal u sigma v transpose. And the point is that for the Frobenius norm, actually for all these norms, I can change u. It doesn't change the norm. So I can make u the identity. We all know it's an orthogonal matrix. And what I'm saying is, orthogonal matrix u doesn't change any of these particular norms. So suppose it was the identity, same here. That could be the identity without changing the norm. So we're down to the norm of the Frobenius. So what's the Frobenius norm of that guy? What's the Frobenius norm of that diagonal matrix? Well, you're supposed to add up the squares of all the numbers in the matrix. And what do you get? You get that. So that's why this is the same as this, because the orthogonal guy there and the orthogonal guy there make no difference in the norm. But that takes checking. But that's another way to see why the Frobenius norm gives this. And then finally, this was the nuclear norm. And actually, just before my lunch lecture on the subject of probability, I've had a learning morning. The lunch lecture was about this crazy way that humans behave, not us, but other humans, other actual. Well, no, I don't want to say that. Take that out of the tape. Anyway, that was that lecture. Well, before that was a lecture for an hour plus about deep learning by somebody who really, really has begun to understand what is happening inside. How does that gradient descent optimization algorithm pick out? What does it pick out as the thing it learns? This is going to be our goal in this course. We're not there yet. But his conjecture is that, yeah, so it's a conjecture. He doesn't have a proof. He's got proofs of some nice cases where things commute, but he hasn't got the whole thing yet. But it's pretty terrific work. So this was Professor Srebro who's in Chicago. So he just announced his conjecture. And his conjecture is that in a model case, the deep learning that we'll learn about with the gradient descent that we'll learn about to find the best weights, the point is that in a typical deep learning problem these days, there are many more weights than samples. And so there are a lot of possible minima. Many different weights give the same minimum loss because there are so many weights. The problem is like too many variables. But it turns out to be a very, very good thing. That's part of the success. And he believes that in a model situation that optimization by gradient descent picks out the weights that minimize the nuclear norm. So this would be a norm of a lot of weights. And he thinks that's where the system goes. We'll see this. This comes up in compressed sensing, as I mentioned last time. But now I have to remember what was the definition. Do you remember what the nuclear norm was? It's also, he often used, he used a little star instead of an n. I'll put that in the notes. Other people call it the trace norm. But I think this n kind of gives it a name, a notation you can remember. So let's call it the nuclear norm. Do you remember what that one was? It was, yeah, somebody saying it right. Add the sigmas. Yeah, just the sum of the sigmas, like the L1 norm. Yeah, in a way, so that's the idea, is that this is the natural L1 type of norm for matrices. It's the L1 norm of that sigma vector. This would be the L2 norm of the sigma vector. That would be the L infinity norm. Notice that the vector numbers, infinity 2 and 1, get changed around to when you look at the matrix guy. Well, so that's an exciting idea. And it remains to be proved. And people are experimenting to see, is it true? Yeah, so that's a big thing for the future. Yes, OK, so that's, today we've talked about norms, and this section of the notes will be all about norms. We've taken a big leap into comment about deep learning. And this is what I want to say the most. And I say it to every class I teach near the start of the semester. My feeling about what my job is is to teach you things, or to join with you in learning things, as has happened today. It's not to grade you. I don't spend any time, like losing sleep, should that person take a one-pointer epsilon penalty for turning it in four minutes late? Hell with that, right? We've got a lot to do here. And so anyway, we'll get on with the job. So homework three coming up, and we'll be using the notes that you already have posted in Stellar for those sections, eight and nine and so on. And we'll keep going on Monday. OK, see you Monday, and have a great weekend.