 So for the last few lectures, we've been talking about the extremal problem of forbidding a complete bipartite graph. So today, I want to move beyond the complete bipartite graph and look at other sparser bipartite graphs. So we'll be looking at what happens to the extremal problem if you forbid a sparse bipartite graph. So recall the Kovari-Sos-Turon theorem, which tells you that the extremal number for K st's is upper bounded by something on the order of n to the 2 minus 1 over s. So if I give you some bipartite H, we know that because it's a bipartite graph, it is always contained in some K st for some values of s and t. So you already automatically have an upper bound on the extremal number for this H from the Kovari-Sos-Turon theorem. But as you might expect, we might be very wasteful in this step. And the question is, are there situations where we can do much better than what is just given by applying Kovari-Sos-Turon? And today, I want to show you several examples where you can significantly improve the bound given by Kovari-Sos-Turon for various sparser bipartite graphs. The first result is the following theorem, which is initially due to Ferdi. And then a later different proof was given by a lone-curved leverage of Sudakov. And the latter proof is the one I want to present because it presents an important and intricate probabilistic technique, which is the main reason for showing you this theorem. But let me tell you the theorem first. Here, H will be a bipartite graph. And it has vertex bipartitions A and B, such that every vertex in A has degree at most r. So the bipartition is A and B, and the degree in A is at most r for every vertex on the left side in the set A. And I want to understand, is there some upper bound on the extremal number that does better than Kovari-Sos-Turon? And the theorem guarantees such a bound. So then there exists some constant depending on H, such that the extremal number is upper bounded by something to the order of n to the 2 minus 1 over r. Compare with Kovari-Sos-Turon. On one hand, if your H is the complete bipartite graph Kst, then this is the same bound as Kovari-Sos-Turon. On the other hand, you might have a lot more vertices in A and a lot more vertices in B. The hypothesis only requires that the degrees in A, max degrees at most r. So it could be a much bigger graph. So if you apply Kovari-Sos-Turon, you would get a much worse bound compared to what this theorem guarantees. This 1 over r is optimal. You cannot, in this given statement, you cannot improve this 1 over r, because we know that from the Kst example and the lower bounds I showed you last time, you cannot improve upon this 1 over r. So in this form, this theorem is best possible. All right. I want to show you a probabilistic technique for proving this theorem. And it is an important idea called dependent random choice. So let me first give you an informal interpretation of what's going on. So the idea is that if you have a graph G with many edges, then inside G, I can find a large subset of vertices, U, such that all small subsets of vertices in U have many common neighbors. Because I won't tell you what the small and many are just yet, so we'll see it through the proof of the theorem. But that's the idea. So I give you a graph. It's not too sparse. It's relatively dense. Then I should be able to find some subset that is fairly large so that, let's say, every pair of vertices in the subset has many common neighbors. So let me write down, or at least attempt to write down, a formal statement, dependent random choice. The statement of the theorem, as I will present, has a lot of parameters. But I don't want you to be scared off by the parameters. I won't even tell you what they are. But first, tell you what the statement of the conclusion is. And then we'll derive what is the dependencies on the parameters through the proof. The technique is much more important than the statement of the theorem itself. So I will leave some space here. The conclusion is that every graph with n vertices and at least alpha n squared over 2 edges contains a subset U of vertices. U is not too small. So the size of U is at least little u. And such that for every subset S of U with r elements, the set S has at least n common neighbors. So what's the idea here? I give you this graph. And I want you to produce the set U that has this property. How might you go about finding the set U? Let me give you an analogy. I suppose you have the friendship graph on MIT campus. And I want to select a large set, let's say 100 students, such that every pair of them, or maybe even just most pairs of them, have many common friends. Well, how might you go about doing that? Well, if you select 100 students at random, you are unlikely to achieve that outcome. They're going to be pretty dispersed across campus. But if you focus on some commonalities, so for example, you go to someone, some specific individual, and look at their circle of close friends, then it seems more likely you will be able to identify a group of people who are very well-connected in that they pair-wise have lots of common friends. And that's the idea here. We're going to make the random choice by picking that core individual. That's the random choice. But then make the subsequent dependent random choice by looking at the group of friends from that specific random individual, instead of choosing the 100 people uniformly at random, which is not going to work. So let's execute that strategy on this graph. Let me take T to be the random set. That's the core set. So let T be, for convenience, I'm going to choose with repetition. So instead of just choosing one person, I'm going to choose T vertices. So a list of T vertices chosen uniformly at random from the vertex set. So G is going to be my graph. So this graph is G. And what we are going to do is look at the set A, which is the set of common neighbors of T, the vertices that are adjacent to O of T. And that's the set I want to think about. So I want to basically argue that this set A has more or less the properties that we desire, maybe with some small number of blemishes, which we will fix by cleaning up. First, I want to guarantee that A has a big size. We are actually choosing a lot of vertices. So let's evaluate the size of A in expectation. By linearity of expectation, we need to compute the sum of the probabilities that individual vertices fall in this random A. For each particular vertex A, when for each particular vertex V, when is it in A? Well, it is in A if T is contained in the neighborhood of this V. So all of your chosen T's fall into the neighborhood. Otherwise, V is not going to be contained in A. So each individual probability can be computed easily by looking at the size, the degree of V, divided by the number of vertices raised to the power T. T, independent choices chosen with replacement. And by convexity, applied to the final expression, we find that it is at least this quantity here, where essentially we're taking the averages of the degrees. So this is by convexity. And finally, the graph has at least that many edges. So the final quantity there is at least N alpha to the T. Yes, question? AUDIENCE 2. So in our original list, we're allowed to have repeated vertices? YUFEI ZHAO. Yes, question is, in our original list, are we allowed to have repeated vertices? Yes, so it's a list. And we're choosing every element independently at random, allowing repetition. So it's sometimes called choosing with replacement. You choose a T, you throw it back, you choose another one. All right, the property that we're looking for is that for every S subset, for every r element subset of U, it has many common neighbors. So let's look at such an S. So for each S, which is an r element subset of the vertex set, so an r element subset of V, what is the probability that this set S is contained in A? So I give you the set S. It is contained in A if, well, let's think about how A is chosen. You want this to happen. And A is chosen as a common neighborhood of this T. So S is contained in A if and only if T is contained in the common neighborhood of S. OK? So X is fixed for now. T is random. So we draw the elements of T independently, uniformly, at random. Therefore, this probability is equal to the number of common neighbors of S as a fraction of the total number of vertices, this fraction raised to the power T. We want all S subsets of A, all r element subsets of this A, to have at least m common neighbors. But maybe we cannot get that. So let's figure out how many bad S's are there, how many S's do not satisfy this condition here. So let's call such a set S bad if it has fewer than m common neighbors. And from this equation, we see that for each fixed S that is an r element subset of vertices, it is bad with probability. Well, it is bad if it has few common neighbors. But if it has few common neighbors, then this probability is small. So it is bad with probability strictly less than m over n raised to the power T. So we chose this A in this dependent random way. And basically, we want that all subsets, all r element subsets have many common neighbors. But maybe we cannot get that at the first try. But we only have a small number of blemishes. So we can fix those blemishes by getting rid of the possible bad r subsets. And that's fine to do as long as there are not too many of them. And indeed, because each S is bad with small probability, the expected number of bad r element subsets of A is at most, well, I look over all possible r element subsets of vertices. Each one of them is bad with this probability here. So we have that bound. And the point now is that if this number is significantly smaller than the expected size of A, then I can clean up all the bad subsets by plucking out one vertex from each bad subset. So indeed, that is the case, because the expectation of the size of A minus, so let me call this quantity here, star. So the size of A minus star by what we have shown is at least N alpha to the t minus the quantity just now. And I want this number to be somewhat large. And now let me put that as the hypothesis of the theorem. So dependent random choice. So let U, N, R, M, T be positive integers, alpha positive real. And suppose N alpha to the t minus N choose R, M over N to the t is at least W. All right, that's where that inequality comes from. So if this is at least W, then what we can do is delete one vertex from each bad subset, each bad subset S. And after deleting, A then becomes some smaller set, A prime, with at least U elements in expectation. But then there exists U elements. So let me put it this way. We know that this is true in expectation. So thus, there exists some T such that that inequality is true without the expectation. So there exists some T such that this quantity is at least that. So now delete a vertex from each bad subset. We obtain this A prime with at least U elements. And we have gotten rid of all the possible bad subsets. So no bad R element subsets. And that finishes off the proof of dependent random choice. Just to recap, the idea is that if you have a large, if you have a dense enough graph, then I want to find the conclusions that you can find a fairly large subset of vertices so that every small subset, so every pair, every element, have many common neighbors. And the way you do this, instead of choosing your set at random, which is not going to work, you choose a small set of anchors. T, you think of that as the anchors. You choose a bunch of anchors. And then you look at their common neighborhoods and use that as a starting point. That will almost work. It might not work perfectly. But then you fix things up by removing the blemishes. So this is a very tricky probabilistic idea. It's also a very important one. It will allow us to prove the theorem over there about the extremal numbers of bounded degree graphs. Question? AUDIENCE 1 In the definition of bad, is that for any subset of B, or is it only for subsets of A? YUFEI ZHAO The question is, in the definition of bad, do I use this definition for all subsets of B, all element subsets, or just subsets of A? So I use it to mean all subsets of B. Because A is random. A is random. So the definition of bad does not depend on the randomness. It only depends on the original graph. AUDIENCE 2 Wait. How is it, how is the badness dependent on any probability? YUFEI ZHAO The badness does not. So the question is, how does the badness depend on any probability? Badness does not depend on the probability. But A is random. So the number of bad r element subsets of A is a random variable. So each individual, so you start with a graph. Some r element subsets are bad. Some are not. And now I choose this random A in this dependent random manner. And A might contain some bad subsets. I'm trying to calculate how many bad subsets this A has. Question? Yeah. I think you should be straighter, right, because an S is either bad or not bad. So you said, like, bad with probability. So each, OK, so your concern is each S is bad with probability. Ah, sorry. It's contained in B with probability. Fine. Yeah. Thank you. So each, so I define, great. So each bad subset. OK. So for each fixed bad subset, it is contained in A with probability. Thank you. This is, I hope this makes it clear. So the probability of the property of being bad is not random, but it being contained in A is random. Question? Can you tell me why S is the best subset of A and A is the best subset of A? OK, so question is, why is this true? So why are these two events the same? And you kind of have to steer the definition a bit. So T is chosen as the common neighborhoods. So you choose T at random and choose A to be the common neighborhood of T. And so how do you characterize subsets of A if every element is connected to all of T? Let's think about it. Any more questions? Yes? When we use this power, do we pick T? We pick T at random. T is uniform. So question is, how are we picking T? T is uniform at random. Ah, great. So question is, how do we pick the little t in the theorem statement? It depends on the application. It is a little bit weird, because in the statement of the theorem, the little t shows up in the inequality, but not in the conclusion. So you think of T as an auxiliary parameter. So the little t comes up in the proof, but not really in the conclusion. Any more questions? It's a tricky lemma. It's a tricky idea. So now let me use it to prove the statement over there. And here, it's not so hard. So it's mostly an application of this dependent random choice lemma. So for all h, as in the theorem, so let me prove this lemma. So for all h, as in the theorem, there exists a constant C such that every graph with at least C n to the 2 minus 1 over r edges contains a vertex U with vertex subset U with the size of U equal to b. So b comes from the vertex bipartition of h. It's a constant such that every r element subset in U has lots of common neighbors, has at least another constant, which is the number of vertices in h, that many common neighbors. Which, as you see, is a direct corollary of the dependent random choice lemma by setting in the right parameters, and indeed, by the dependent random choice lemma, where we choose this auxiliary variable T in the dependent random choice lemma to equal to r. So it suffices to check that there exists a C such that, well, plug it into that expression, that inequality up there. So n to C n to the minus 1 over r raised to the power r minus n choose r, and then this expression here. So I'm just plugging in the various graph parameters into the dependent random choice statement. And I want to show that you can find a constant C such that this inequality is true. And indeed, these exponents, they cancel out. So the first term is simply 2C raised to the r. And the second one, because, again, you notice that the exponents work out just fine, so it is a constant, at most a constant. So you can choose C big enough so that this is true. So it's a direct verification of the hypotheses of the lemma. And now we're ready to prove the theorem over there. The idea is, yes, question. AUDIENCE 2 Sorry. Why? How do you have size A plus B common neighbors? Isn't that like the entire graph? YUFEI ZHAO. So the question is, how do you have size of A plus B common neighbors? So H is fixed. And A and B are constants. A plus B is the size of the number of vertices of H. That's a constant. AUDIENCE 2 Oh, sorry. I thought. YUFEI ZHAO. So yeah, I'm talking about common neighbors not in H, but in the big graph G as n vertices. Now, I like questions. So this is tricky. It's a tricky argument. So please do ask questions if you're confused. And there are times when I may not have explained it very well, so please do ask questions. So let's prove the theorem. And now we're almost there. The idea is that we embed the vertices of B into the big graph G one by one. First, embed B into the vertices of G using U from the lemma, so the lemma that we just stated. And I claim that once you have done that, so the vertices of B, and now I need to embed the remaining vertices of A. And I can do this one by one. Because if I need to embed some vertex of A, A has at most R neighbors to B. But I've embedded B in such a way that they have a lot of common neighbors inside G. So I can always do it. So I can always embed the vertices of A one at a time in such a way that I even avoid collisions. I don't allow vertices to be embedded into the same place. And this is all using that B, the embedding of B, which is U, has many common neighbors in G. So once you put that in, and the rest, you just make one choice at a time. And you need to embed a second vertex. Well, you can find somewhere in their common neighborhood that allows you to do it. So you embed the vertices one at a time, and then you finish embedding the whole graph. Any questions? It's a tricky argument. So let's take a break, and I want you to think about it. Any questions? Yeah. Never mind. Yeah. Yeah. Sorry, again, how do you embed A without having collisions between the neighbors in the middle? All right, so the question is, how do you embed A without having any collisions? So I'm putting the vertices of B so that every r of them have many neighbors. And now I want to try to embed the vertices of A one by one in any order. Think about, you pick the first vertex. Where can it go? It has, let's say, adjacent to the first three vertices of B. So in the embedding, it has to go in the common neighborhood of those three vertices, which we know is large. So I put them anywhere. And I do the same for the second vertex. I do the same for the fourth vertex. So I just keep on going. Because the common neighborhood is large, it may be that some of the potential vertices I might embed is already used by the previous steps in the process. But because I always have at least A plus B common neighbors, I always have some possibilities that remain. Yes, question. So not like before the lemma, like the last line of that question is how does this bad subset deletion work. So you have this A, which is fairly large. And you know that there exists some instance of this randomness that produces for you a situation where A has very few bad R subsets. So then I take A, and I delete from A one vertex in each bad subset. I haven't changed the size of A very much. A is still quite large after this deletion. But now A has no bad subsets remaining, because I've gotten rid of one vertex from each one, from each bad subset. So very similar to what we've seen before, the random process for creating an H-free graph. You generate a random H-free graph, which has very few copies of H relative to the number of edges. And then you get rid of them by removing one edge from each copy of H. So with the theorem that we saw in the first part of the lecture, we saw how to improve on the bound of Kovari-Sos-Turon in some circumstances, namely one where the graph that you're forbidding, this H, essentially has bounded degree. So we stated something a bit stronger, namely, has bounded degree from one side. And that's a really general result. And now I want to look at some more specific situations where you might be able to improve further. So what are some nice bipartite graphs? One that comes up is kind of even cycles. So you have C4, C6, and so on. And you see C4 is the same as K2,2, which we already saw before. But even C6, the techniques so far allow us to obtain the theorem that we just saw, gives us a bound on C6 that's more or less the same as that of C4, namely, n to the 3 halves. So what's the truth for C6? So it turns out that you can do much better. So this is a theorem of Bondi and Simonovic that for all integers k, at least 2, there exists some constant C such that the extremal number of C2 sub k is at most on the order of n to the 1 plus 1 over k. So in particular, for six cycles, the upper bound is 4 thirds in the exponent, better than the 3 halves. So this is another class of graphs where there are some nice upper bounds. And you can ask, well, just like Kovari-Sos-Turon for complete bipartite graphs, do we know matching lower bound constructions? And what is known is that it is tight only for a small number of cases. And the others, we do not know whether they are tight. So this Bondi-Simonovic theorem, it is tight for k being 2, 3, or 5, and open for others. So there are constructions for C4-free, C6-free, and C10-free, but not for C8-free. That's an open problem. The proof of the Bondi-Simonovic theorem is slightly involved, but I want to show you a weaker result that already contains a lot of interesting ideas. So a weaker result is this, that for every integer k, at least 2, there exists a constant C such that every n-vertex graph G with at least C, so this is the correct, the same order of number of edges. So we know from Bondi-Simonovic it contains an even cycle of length exactly 2k. So we'll show something slightly weaker that contains an even cycle of length at most 2k, which, in other words, says that the extremal number, if you, so we haven't introduced this notation, but hopefully you can guess what it means, that if you forbid all of these cycles, then it is this quantity there. So we'll show this weaker result. All right, so let's do it. First, I want to show you a couple of easy preparatory lemmas. So first, so every graph G contains a subgraph with min degree at least half of the average degree of G. So you have a graph G. It has large average degree, has lots of edges, but maybe there are some small degree vertices. And it will be useful to know that the minimum degree is actually quite large as well. So it turns out by passing to a subgraph, you can guarantee that. How do you think we might prove this? I give you a graph. I know it has lots of edges, but maybe some vertices with small degree. Yep? AUDIENCE 1. Dependent random choice. YUFEI ZHAO. So you suggest dependent random choice. That's a heavy hammer for such a simple statement. So it turns out we can do something even simpler. AUDIENCE 2. Just throw out all those bad edges. YUFEI ZHAO. So we just throw out the min degree vertices, so throw out the small degree vertices. If the average degree is 2t, then the number of edges is number of vertices times t. And you see that removing a vertex of degree t, for at most t, cannot decrease average degree. So I have this process where I remove vertices that are less than half of average degree. And the average degree never goes down. So I keep on doing this. Average degree stays the same. It can go up, but it never goes down. And when I stop, I don't have any more small degree vertices to get rid of. Why does this process even terminate? Maybe we end up with the empty graph, which is not so useful. Yes? AUDIENCE 2. Is your average degree non-decreasing? YUFEI ZHAO. You said the average degree is non-decreasing. But how do I know the graph has at least some number of vertices when I stop? Yes? AUDIENCE 2. Can't you just go after it? Can't you have that degree at most? The average degree is 2t. YUFEI ZHAO. You must terminate because every graph, if you have way too small number of vertices, because, for example, if you just notice that every graph with at most 2t vertices has average degree less than 2t. So you'll never get below 2t vertices. You run out of room if you go too far. Because that's the first preparation lemma. The second one is that every graph G has a bipartite subgraph with at least half of the number of edges as the original graph. OK. This is a very nice and quick exercise in the probabilistic method. So we can color every vertex black and white uniformly random. And the expected number of black to white edges is exactly half of the original number of edges by linearity of expectations. So there are some instance with at least that many black-white edges, and that's a bipartite subgraph. OK. So now we can prove the theorem about even cycles, the weaker theorem, at least. I start with a graph which has a lot of edges. But by these two lemmas and changing constant somewhat, I can obtain a subgraph that's bipartite and has min degree quite large. So I lose almost a constant factor. And I get basically within a factor of 2, well, factor of 4 of the average degree. OK. So let me call this quantity delta, the min degree in this subgraph, this bipartite subgraph. Let's think about what happens to this graph if I start with an arbitrary vertex. So now I have a min degree condition. Really, all vertices are now kind of the same to me. So pick an arbitrary vertex and look at its neighborhood. It has at least delta edges coming out. So let me call the first vertex level 0 and the second set level 1. It's bipartite, so there are no edges within level 1. Let's expand out even further. OK. Can there be some collisions where two of these edges go to the same vertex? Well, if there were, then I find a C4. C4, so if I assume, so let's assume that there's no C4, C6, and so on C2, C2k for contradiction. So because there's no C4, all the endpoints of these paths of length 2 are distinct in level 2. And you keep going, or you keep expanding further, and so on, and all the way to level T. And all of these guys must be distinct as well, because otherwise, you find a cycle of length at most 2T. And so when you do this expansion, each step, you get distinct vertices. And you also have no edges inside each part, because you are looking in the bipartite setting. So how many vertices do you get at the end? Well, I have a min-degree condition. The min-degree condition tells me that I expand by a factor of at least delta minus 1 each time. So the number of vertices in here is at least delta minus 1 raised to the power T, so raised to the power k. Here's k. I expand all the way to the end. But you see, that number there is quite large. So in particular, if C is large enough, then it's bigger than N. And that would be a contradiction, because you only had N vertices in the graph to begin with. Therefore, the assumption that there are no cycles, even cycles of length at most 2k is incorrect. And that finishes the proof. Any questions? Yeah? AUDIENCE 2 Are the ideas for the full Bondi-Simonovitz theorem similar? YUFEI ZHAO So the question has to do with, in the original, in the full Bondi-Simonovitz theorem, so what do we need to do? Are they similar? So certainly, you want to do something like this. But then you also want to think about, if you do have short cycles, how can you bypass them? So there's a more careful analysis of what happens with shorter cycles. We will not get into that. Any more questions? All right, so the first theorem that we did in today's lecture has to do with what? One question? So why are all the vertices distinct for level k? Or that just defines that? So the question is, why are the vertices distinct for level k? So at level k, if you have some collapse, then it came from two different paths, therefore forming a C2k. OK. So now I want to revisit the first theorem that happened in today's lecture, namely, if you have this H bipartite A and B. So we saw the hypothesis was that if every vertex in A had bounded degree, degree at most r, then we got an upper bound on the extremal number that was n to the 2 minus 1 over r. In particular, for r equals to 2, suppose I have that situation there. Suppose that the degree for every vertex in A is at most 2. Then the first theorem guaranteed us that the extremal number is on the order at most n to the 3 halves, just like the extremal number for 4 cycles, for k22s. And of course, this statement is tight in the sense that if I change this 3 halves to any smaller number, then just taking H to be k22 violates it. So I cannot replace in this generality 3 halves by any smaller number, because we know that the extremal number for k22 is this order. But it's not the only obstruction. So if H is not k22, well, you can make some sillier examples too by taking k22 and add some more edges. So let's forbid H from having a k22 subgraph. So can you do better now? In this case, can you improve for the specific H that exponent? And we already saw one case where you can do this. So namely, in Bonny-Simonovits theorem for cycles, if you only apply this theorem here, you get 3 halves. But Bonny-Simonovits tells you a much better exponent. So let's explore this situation. And it turns out in a very recent theorem that was only proved in the last couple of years by David Conlon and Jun-Kyung Lee, they showed that for every H as above, there exists constants little c and big C. Such that the extremal number of H is upper bounded by something where I can decrease 3 halves to some even smaller number. So somehow, this 3 halves, now we understand, it's really because of the presence of k22. The graph has, if H has no k22, then some smaller number suffices. And I want to use the rest of today to explain how to prove that theorem there. Yes, question? AUDIENCE 2. The c is not independent of H. YUFEI ZHAO. That's right. So the question is, is c independent of H? So c depends on H. So c, they're dependent on H. Questions? Let me put this question in a slightly different formulation that is also equivalent. So in graph theory, there is a notion of a subdivision. And in particular, the one subdivision of graph H is this operation where you start with a graph, let's say this graph here, and you add a vertex to the middle of every edge of this graph. So initially, it's four vertices. Now you add a new vertex to every edge. So you subdivide every edge into a path of two edges. That's called a subdivision. So for today's lecture, let me denote subdivisions by a prime. In particular, if this is k4, then I will denote this graph here k4 prime. So for example, k3 prime, so that's a triangle, subdivided by c6, 6i. So observe that every H that comes up in this theorem here, there's a subgraph of some subdivision, some one subdivision of a clique. Because the vertices on the left in A of degree 2, so you think of them as midpoints of edges. But because you are k223, if you collapse those paths of length 2 to single edges, you do not end up with parallel edges. So it is the subgraph of this one subdivision of some graph, which then you can complete to a clique. OK, so this theorem here is equivalent, at least qualitatively, to the statement that for every p, there exists some constants, again, depending on p, such that the extremal number of the one subdivision of a clique is bounded by something where we can improve upon the exponent in the first theorem in today's lecture. So that theorem there. So these two theorems are equivalent because of this remark. Any questions so far about the statements? Yeah? AUDIENCE 1 In the remark, how do you deal with, like, it's vertices of degree less than 2? YUFEI ZHAO Question, in the remark, how do you deal with vertices with degree less than 2 and complete it to a vertex of degree 2? Add another edge. Add another edge to a new vertex. Any more questions? All right, so the proof I want to show you is due to Oliver Jenser. And for this clique subdivision theorem. And this proof produces the C sub t equals to 1 over 4t minus 6. So if you plug in t equals to 3, you find that the exponent here is actually tight for the 6th cycle. So it actually agrees with what we know. OK. So I want to show you some of the main ideas from this proof. Just like the proof that we saw for the Siemens cycles theorem, it will be helpful to start with some preparation. Start with a graph that, even though it has lots of edges, may have lots of vertices with high degree, lots of vertices with low degree. It's nice to clean it up somewhat. And so let me state a preparatory lemma, which we will not prove. But it's of a similar nature to this very easy lemma that we saw earlier, but with a bit more work. AUDIENCE 1 I think there, if you plot C, it doesn't represent there should be replaced by C t minus C. YUFEI ZHAO Originally, I put a C over here, but now it's OK. There exists a C t. OK. So the preparation is that we're going to pass through a large, almost regular, subgraph. So the lemma, so don't worry too much about the details. I'll tell you what the idea is. So for every alpha, there exists constants beta and k, such that for every C and N sufficiently large, every N-vertex graph G with lots of edges, so C N to the 1 plus alpha edges, has a subgraph G prime such that, so I want some properties. First, G prime has lots of vertices. So N to the beta, so still lots of vertices. You lose a polynomial in that. And two, it has still lots of edges relative to the number of vertices it has. Basically, changing the constants, if I start with N to the 1 plus alpha, I still have roughly number of vertices to the 1 plus alpha, number of edges. It is almost regular in the sense that the max degree of G does not differ from its minimum degree by more than a constant factor. So you don't have vertices that are too small degree. You don't have vertices that are too large degree. And finally, G prime is bipartite. And the two parts of this bipartition have sizes differing by a factor at most 2. So if you like, think of G as a regular bipartite graph. This is a preparation lemma. We'll just make our life a bit easier. So from now on, let's treat T as a constant in our asymptotic notation to simplify the notation. So you have this graph G. It's a bipartite graph. And for a pair of vertices on one side, A, so there are no edges in A. But for a pair of vertices, I say that this u, v is light. So it's not an edge, but I talk about these pairs. I say that it is light if the number of common neighbors between of u and v is at least 1 and less than T choose 2. So it has some common neighbors, but not too many. And then we say that this pair is heavy if the number of common neighbors is at least T choose 2. So if a pair u, v has some common neighbors, then it's either light or heavy. I claim that if G is a kT prime, so this is the one subdivision of kT, free bipartite graph with vertex bipartition A union B. So not A, but u union B. u will eventually be a subset of A, such that all the vertices on the left on u have degree at least delta, and u is not too small. So it's at least 4BT over delta. Don't worry about it for now. Think of delta, this is a min degree. So it is somewhat smaller than the average. It's basically the average degree of your graph. And B, think of as n. It's more or less the whole set of vertices. Then the conclusion is that there exists a u in a lot of light pairs in the set U. There exists a vertex in many light pairs. It's important that we assume that this graph G is kT prime free, because otherwise you could imagine a situation where essentially you have a complete bipartite graph and every pair of vertices is heavy. So you don't have any light pairs at all. So having kT prime free somehow allows us to find light pairs. So let's see the proof of this lemma. So we combine some nice ideas that we've seen earlier in the course, namely double counting and also Euser's-Turon's theorem. So first, let's do a double counting argument similar to the proof of the Kovari-Sos-Turon theorem, where let's count the number of k12s like that. So the number of k12s like that between U and B. So I claim one way to count this is to look through all the vertices on the right side, look at how many neighbors it has, and sum up the degrees choose 2. OK? So skipping some, well, OK, so let me I can tell you what comes out. So by convexity, we find that it is at least this quantity here. And then assuming the minimum degree condition, we find that this quantity is quite large. And so this is a calculation very similar to what we did for the proof of Kovari-Sos-Turon theorem. The low degree vertices in B do not contribute much to this sum. So this sum is large and sums over all vertices in B. But the low degree vertices in B, they contribute very little. Because if we sum over all the vertices of B with degree less than 2t, then each summed is at most 2t squared, which, again, by the assumption of delta, is less than half of the total sum. So the low degree vertices of B do not contribute very much to the sum. So let's look at the higher degree vertices. So the higher degree vertices, they contribute a substantial chunk. And the most important thing here is that among these vertices, there are no t mutually heavy. So not among these vertices, but in U. If you look at U, there are no t mutually heavy vertices in U. If you have t mutually heavy vertices in U, then what happens? So if you have t mutually heavy vertices, if you have, let's say, three vertices in U, they're mutually heavy, each one of them, because they're heavy, I can find many common neighbors. So I can build this path of length 2. I can build another path of length 2. And I don't run out of vertices, because they're all heavy. They all have at least t choose 2 common neighbors. So I can build this subdivision of kt. So there are no mutually heavy t, mutually heavy vertices in U. So where have we seen this before? So you have, think about the neighborhood of a vertex in B. Because it's inside a neighborhood, all the pairs are either heavy or light. And there are no t mutually heavy vertices. So then Turán's theorem tells us that there must be many light vertices, that there are many light vertices in this neighborhood. So the number of light pairs in the neighborhood of this V if has at least t neighbors, or else you run out of room, is at least, so if you think about what Turán's theorem says, is that the number of non-edges is at least this quantity here, which is at least a degree of B squared, degree of V. So Turán's theorem tells us that there cannot be so many heavy pairs inside the neighborhood of a vertex in B. So there must be many light pairs. And now we sum over all vertices in B. We obtain that U has a lot of light pairs. We might have over-counted a little bit, because each light pair is over-counted only by a bounded number of times, because it's light. So it's over-counted by less than k choose 2 times. So that's just a constant factor. And we're OK with that. So that's the conclusion for now. This lemma tells us that you have lots of light pairs in U. And what we're going to do is to keep on shrinking this U. So U is going to be a subset of A. Initially, let's let U be the entire set A. Tells us that there's one vertex in A with lots of light neighbors. Take that vertex, choose its neighborhood. Apply the lemma again. Find another vertex with lots of internal light neighbors. Keep on going. And then we build a large light clique. So that's the idea. So we'll find that. So we'll see that if this delta is bigger than basically the quantity claimed in the theorem, so t minus 2 over 2t minus 3, and sufficiently large C, then there exists the sequence U1, U2, U3, so on, all the way to Ut. And a sequence of vertices, V1, V2, Vt, so such that initially, I take A. And the idea is initially, I take V1 to be whatever comes out of that lemma. And I want the property that all of the Vi, Vj's are light. And two is that no three of these V's have a common neighbor. And I claim once you have these two properties, then you can find your clique subdivision. You find these t light vertices. So if you have V1, V2, V3, V4, you have these light vertices. And I can build a clique subdivision from these light vertices. Because they are light, they have at least one common neighbor for each pair. And just keep building them. So I build these common neighbors. Well, you should be somewhat worried that I end up using the same vertex twice. But of course, that should not be a worry if I guarantee that no three of them have a common neighbor. They cannot collapse. You cannot have two vertices end up being the same. Otherwise, I would violate property B. So these two properties alone allow you to build a K-T subdivision. But how do we find the sequence? So we build it iteratively using that lemma, you start with one vertex guaranteed by that lemma. You look at its neighborhood. You pick another vertex guaranteed by the lemma. You look at its neighborhood. And so on. And you build the sequence of light clique. Yeah? AUDIENCE 2. When you say neighborhood, you mean the light neighborhood? YUFEI ZHAO. The light neighborhood, yeah. So we're not in the graph anymore. Everything's inside A. Everything is inside A. So let me finish off the list of properties. And we're almost there. Third property I want is that when I do this operation, I do not reduce my space of possibilities by too much. Namely, that the size of U does not go down too substantially. And that's guaranteed by the lemma. And four is that basically this picture over here, Vi is light to O of ui plus 1. So I claim that you can find the sequence satisfying these properties. And the reason is that you repeatedly apply the lemma. So repeatedly apply the lemma. So the lemma doesn't address this part about triple vertices having common neighbors. But I claim that's actually not so hard to deal with. Because if you think about how many vertices, how many possibilities this restriction eliminates, so B only eliminates at each step t choose 2, because this is coming from the light restriction, times at most another t choose 2. And the first one comes from pairs of B1 through Bt. So eliminates at most this many times the max degree.