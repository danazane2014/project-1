 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. All right. So let's continue talking about maximum likelihood estimation in the context of generalized linear models. So in those generalized linear models, what we spend most of the past lectures working on is the conditional distribution of y given x. And we're going to assume that this follows some distribution in the exponential family. OK. And so what it means is that if I look at the density, say, or the PMF, but let's talk about densities to make things clear. We're going to assume that y given f has x as distribution. So x is now fixed because we're conditioning on it. And it has a density which is of this form, c of y i phi. So this c, again, we don't really need to think about it. This is something that's going to come up naturally as soon as you need a normalization factor. And so here, what it means, so if this is the distribution of y given xi, so that's the density of y i given xi is equal to little xi. So if it's the conditional distribution of y i given xi, it should depend on xi somehow. And it does not appear to depend on xi. And here, the model is going to be on theta i, which is just a function theta i of xi. And we're going to take a very specific one. It's going to be a function of a linear form of the xi. So really, we're going to take something which is of the form theta i, which is really just this theta. It does not depend on xi, of xi transpose some beta. So all these parts here, this is really some modeling assumptions that we're making once we've agreed on what distribution we want. So to do that, so our goal, of course, is going to try to understand what this beta is. There's one beta here. What's important is that this beta does not depend on i. So if I observe pairs xi, yi, let's say I observe n of them, i equals 1 to n, the hope is that as I accumulate more and more pairs of this form, where there's always the same parameter that links xi to yi, that's this parameter beta, then I should have a better and better estimation of this beta, because it's always the same. And that's essentially what couples all of our distribution. If I did not assume this, then I could have a different distribution for each pair xi given yi, and I would not be able to do any statistics. Nothing would average in the end. But here, I have the same beta, which means that I can hope to do statistics and average errors now. So I'm going to collect. So I'll come back to this. But as usual, in the linear regression model, we're going to collect all our observations, yi. So here, I'm going to assume that they're real valued and that my xi's takes value in rp, just like in the regression model. And I'm going to collect all my yi's into one big vector y in rn, and all my x's into one big matrix in rn times p, just like for the linear regression model. So again, what I'm interested in here is the conditional distribution of yi given xi. And I said this is this distribution. When we're talking about regression, I defined last time what the definition of a regression function was. It's just one particular aspect of this conditional distribution is the conditional expectation of yi given xi. And so this conditional expectation, I will denote it by, so if I talk about the conditional, so I'm going to call it, say, mu i, which is the conditional expectation of yi given xi equals some little xi, say. You can forget about this part if you find it confusing. It really doesn't matter. It's just so that this is just, this means that this is a function of little xi. But if I only had expectation of yi given big xi, this would be just a function of big xi. So it really doesn't change anything. It's just a matter of notation. So just forget about this part, but I'll just do it like that here. So this is just the conditional expectation of yi given xi. It just depends on xi, so it particularly depends on i. And so I will call it mu i. But I know that since I'm in a canonical exponential family, then I know that mu i is actually b prime of theta i. So there's a one-to-one link between the canonical parameter of my exponential family and the mean mu i, the conditional expectation. And the modeling assumption we're going to make is not directly, we remember that was the second aspect of the generalized linear model. We're not going to assume that theta i itself directly depends on xi. We're going to assume that mu i has a particular dependence on xi through the link function. So again, we're back to modeling. So we have a link function g. And we assume that mu i depends on xi as follows. g of mu i. And remember, all g does for us is really map the space in which mu i lives, which could be just the interval 0, 1, to the entire real line. And we're going to assume that this thing that lives in the real line is just xi transpose beta. I should maybe put a small one, xi transpose beta. So we're making, indeed, some modeling assumption. But compared to in the linear regression model, we only assume that mu i was xi transpose beta. So if you want to make a parallel between generalized linear models and linear model, the only difference is that g is not the identity necessarily in this case. And all that g does for us is to just make this thing compatible, that those two things on the left and the right of this equality live in the same space. So in a way, we're not making a much bigger leap of faith by assuming a linear model. The linear link is already here. We're just making things compatible. All right. And so this is going to be what? So it's always the same link function. So now if I want to go back to beta, because I'm going to want to express my likelihood. So if I were to express my likelihood from this, it would just be a function of theta. And so if I want to maximize my likelihood, I don't want to maximize it in theta. I want to maximize it in beta. So if I can write my density as a function of beta, then I will be able to write my likelihood as a function of beta and then talk about my maximum likelihood estimator. And so all I need to do is to just say, OK, how do I replace theta by what is? I know that theta is a function of beta. I wrote it here. So the question is, what is this function? And I actually have access to all of this. So what I know is that theta, so mu is b prime of theta, which means that theta i is b prime inverse of mu i. So that's what we got from this derivative of the log likelihood equal to 0. That gave us this guy, inverted. And now I know that mu i is g inverse of xi beta. So this composition of b prime inverse and g inverse is actually just the composition of g with b prime. Everybody's comfortable with this notation, the little circle? Any question about this? Just means that I first apply b prime. Well, actually, it's the inverse. But if I look at a function g composed with b prime, I first apply g b prime of x is just g of b prime of x. OK, and then I take the inverse of this function, which is first take g inverse and then take b prime inverse. So now I have everywhere I saw theta, now I see this function of beta. So I could technically plug that in. Of course, it's a little painful to have to write g circle beta prime all the time. So I'm going to give this guy a name. And so you're just going to define h, which is g b prime inverse, so that theta i is simply h of xi transpose beta. I could give it a name. But let's just call that the h function. So there's something which is nice about this h function is that f g is the canonical link. So what is the canonical link? So what is it canonical to? Canonical link is canonical to a particular distribution in the canonical exponential family. A canonical exponential family is completely characterized by the function b, which means that if I want to talk about the canonical link, all I need to tell you is how it depends on b. So what is g as a function of b? What? b inverse. b prime inverse, right? So this is g is equal to b prime inverse, which means that if g is composed with b prime, that means that this is just the identity. So h is the identity. So h of xi transpose beta is simply xi transpose beta. And it's true that the way we introduced the canonical link was just the function for which we modeled directly theta i as xi transpose beta, which we can read off here. So theta i is simply xi transpose beta. So now, for example, if I go back to my log likelihood, which I know. So if I go back to my log likelihood, so if I look log likelihood, so the log likelihood is sum of the log of the densities. So it's sum from i equal 1 to n of log of exponential yi theta i minus b theta i divided by phi plus c of yi phi. So this term does not depend on theta. So I have two things. First of all, the log and the exponential are going to cancel each other. And second, I actually know that theta is just a function of beta. And it has this form. Theta i is h of xi transpose beta. And that's my modeling assumption. So this is actually equal to the sum from i equal 1 to n of yi. And then here, I'm going to write h of xi transpose beta minus b of h of xi transpose beta divided by phi. And then I have, again, this function c of yi phi, which, again, won't matter. Because when I'm going to try to maximize this thing, this is just playing the role of a constant that's shifting the entire function. So in particular, the arg max is going to be exactly what it was. So this thing is really not going to matter for me. I'm keeping track of it. And actually, if you look here, it's gone. It's gone because it does not matter. So let's just pretend it's not here, because it won't matter when I'm trying to maximize the likelihood. Well, it's here, up to a constant term, it says. That's the constant term. Any question? All I'm doing here is replacing my likelihood as a function of theta i's. So if I had one theta i per observation, again, this would not help me very much. But if I assume that they're all linked together by saying that theta i is of the form xi transpose beta, or h of xi transpose beta, if I'm not using the canonical link, then I can hope to make some estimation. So again, if I have the canonical link, h is the identity, so I'm left only with yi times xi transpose beta, and then I have b of xi transpose beta, not b composed with h, because h is the identity, which is fairly simple, right? Why is it simple? Well, let's actually focus on this guy for one second. So let me write it down so we know what we're talking about. So we just showed that the log likelihood, when I use the canonical link so that h is equal to the identity, the log likelihood actually takes the form ln. And it depends on a bunch of stuff, but let's just make it depend only on the parameter we care about, which is beta. So this is of the form L of beta, and that's equal to what is the sum from i equal 1 to n of yi xi transpose beta minus, let me put the phi here, and then I'm going to have minus b of xi transpose beta. And phi, we know, is some known positive term, so again, optimizing a function plus some constant, we're optimizing a function times a constant. That's not going to change much either, so it won't really matter to think about whether this phi is here or not. But let's just think about what this function looks like. I'm trying to maximize a function. I'm trying to maximize the log likelihood. If it looked like this, that would be a serious problem. But we can do a basic back of the envelope guess of what the variations of this function is. This first term here is, as a function of beta, what kind of function is it? It's linear, right? This is just xi transpose beta. If I multiply beta by 2, I get twice. If I add something to beta, it just gets added, so it's a linear function of beta. And so this thing is both convex and concave. In the one-dimensional case, so think about p as being one-dimensional, so that beta is a one-dimensional thing. Those are just the function that look like this, right? Those are linear functions. They are both convex and concave. So this is not going to matter when it comes to the convexity of my overall function, because I'm just adding something which is just a line. And so if I started with convex, it's going to stay convex. If I started with concave, it's going to stay concave. And if I started with something which is both, it's going to stay both, meaning neither. It cannot be both. Yeah. So if you're neither convex or concave, I think this linear. This will not really matter. If I want to understand what my function looks like, I need to understand what b of xi transpose beta does. Again, the xi transpose beta, no impact. It's a linear function. In terms of convexity, it's not going to play any role. So I really need to understand what my function b looks like. What do we know about b again? So we know that b prime of theta is equal to mu, right? Well, the mean of a random variable in the canonical exponential family can be a positive or negative number. This really does not tell me anything. That can be really anything. However, if I look at the second derivative of b, I know that this is what? This is the variance of y divided by phi. That was my dispersion parameter. The variance was equal to phi times b prime prime. So we know that if theta is not degenerate, meaning that the density does not take value infinity at only one point, this thing is actually positive. And clearly, when you have something that looks like this, unless you have some crazy stuff happening with phi being equal to 0 or anything that's not normal, then you will see that you're not degenerate. So this thing is strictly positive. And we've said several times that if b prime prime is positive, then that means that's the derivative of b prime, meaning that b prime is increasing. And b prime is increasing is just the same thing as saying that b is convex. So that implies that b is strictly convex. And the strictly comes from the fact that this is a strict sign. Well, I should not do that, because now it's no longer. So it's just a strict sign, meaning that the function, this is not strictly convex, because it's linear. Strictly convex means there's always some curvature everywhere. So now I have this thing that's linear minus something that's convex. Something that's negative something convex is concave. So this thing is linear plus concave. So it is concave. So I know just by looking at this that ln of beta, which of course is something that lives in rp, but if I saw it living in r1, it would look like this. And if I saw living in r2, it would look like a dome, like this. And the fact that it's strict is also telling me that there's actually a unique maximizer. So there's a unique maximizer in xi transpose beta, but not in beta necessarily. We're going to need extra assumptions for this. So this is what I say here. The log likelihood is strictly concave. And so as the consequence under extra assumptions on the xi's, because of course if the xi's are all the same, so that if the entries of xi's, so if xi is equal to 1, 1, 1, 1, 1, then xi transpose beta is just the sum of the betas. And I will be of the beta i's, I will be strictly concave in those guys, but certainly not in the individual entries. So I need extra thing on my xi's so that this happens, just like we needed the matrix capital X in the linear regression case to be full rank, so we could actually identify what beta was. It's going to be exactly the same thing. So here, this is when we have this very specific parametrization. And the question is, but it might not be the case if we change the parameter beta into something else. So here, the fact that we use the canonical link and et cetera, everything actually works really to our advantage, so that everything becomes strictly concave and we know exactly what's happening. So I understand I went a bit fast on playing with convex and concave functions. This is not the purpose. I could spend a lecture telling you, oh, if I add two concave functions, then the result remains concave. If I had a concave and a strictly concave, then the result still remains strictly concave, and we could spend time proving this. This was just for you to get an intuition as to why this is correct, but we don't really have time to go into too much details. One thing you can do, a strictly concave function, if it's in one dimension, all I need to have is that the second derivative is strictly negative. That's a strictly concave function. That was the analytic definition we had for strict concavity. So if this was in one dimension, it would look like this. Yi times Xi times beta. Now, beta is just one number. And then I would have minus beta Xi times b. And this is all over phi. You take second derivatives. The fact that this is linear in beta, this is going to go away. And here, I'm just going to be left with minus. So if I take the second derivative with respect to beta, this is going to be equal to minus b prime prime Xi beta times Xi squared divided by phi. So this is clearly positive. If Xi is 0, this is degenerate, so I would not get it. Then I have the second derivative of b prime, which I know is positive because of the variance thing that I have here, divided by phi. And so that would all be fine. That's for one dimension. If I wanted to do this in higher dimensions, I would have to say that the Hessian is a positive definite matrix. And that's maybe a bit beyond what this course is. So in the rest of this chapter, I will do what I did not do when we talked about maximum likelihood. And what we're going to do is we're going to actually show how to do this maximization. So here, we know that the function is concave. But what it looks like specifically depends on what b is. And for different b's, I'm going to have different things to do. Just like when I was talking about maximum likelihood estimation, if I had a concave log-likelihood function, I could optimize it. But depending on what the function is, I would actually need some algorithms that's maybe working better on some functions than others. Now, here, I don't have random things. I have the b is the cumulant generating function of a canonical exponential family. And there is a way for me to leverage that. So not only is there the b part, but there's also the linear part. And if I start trying to use that, I'm actually going to be able to devise very specific optimization algorithms. And the way I'm going to be able to do this is by thinking of simple black box optimization, to which I can actually technically feed any function. But it's going to turn out that the iterations of these iterative algorithms are going to look very familiar when we just plug in the particular values of b of the log-likelihood that we have for this problem. And so the three methods we're going to talk about, going from more black box, meaning you can basically stuff it in any function, it's going to work, any concave function is going to work, all the way to this is working specifically for generalized linear models, are Newton-Raphson method. Who's already heard about the Newton-Raphson method? So some people actually learned this algorithm without even knowing the word algorithm. It's a function that, typically, it's supposed to be finding roots of functions, but finding the root of a function of a derivative is the same as finding the minimum of a function. So that's the first black box method. I mean, it's pretty old. And then there's something that's very specific to what we're doing, which is called, so this Newton-Raphson method is going to involve the Hessian of our log-likelihood. And since we know something about the Hessian for a particular problem, we're going to be able to move on to Fisher scoring. And the word Fisher here is actually exactly coming from Fisher information. So the Hessian is going to involve the Fisher information. And finally, we will talk about iteratively reweighted least squares. And that's not for any function. It's really when we're trying to use the fact that there is this linear dependence on the xi's. And this is essentially going to tell us, well, you can use least squares for linear regression. Here you can use least squares, but locally, and you have to iterate. And this last part is essentially a trick by statisticians to be able to solve the Newton-Raphson updates without actually having a dedicated software for this, but just being able to reuse some least squares software. So we've talked about this many times. I just want to make sure that we're all on the same page here. We have a function f. We're going to assume that it has two derivatives. And it's a function from Rm to R. So its first derivative is called gradient. That's just a vector that collects all the partial derivatives with respect to each of the coordinates. It's dimension m, of course. And the second derivative is an m by m matrix. It's called the Hessian. And on the i-th row and j-th column, you see the second partial derivative with respect to the i-th component and the j-th component. We've seen that several times. This is just multivariable calculus. But really, the point here is to maybe the notation is slightly different, because I want to keep track of f. So when I write gradient, I write nabla sub f. And when I write Hessian, I write h sub f. And as I said, if f is strictly concave, then hf of x is negative definite. What it means is that if I take any x in Rm, then x transpose hf, well, that's for any x0. x, this is actually strictly negative. That's what it means to be negative definite. So every time I do x transpose, so this is like a quadratic form. And I want it to be negative for all values of x0 and x, both of them. That's very strong, clearly. But for us, actually, this is what happens just because of the properties of b. Well, at least the fact that it's negative less than or equal to. If I want it to be strictly less, I need some properties on x. And then I will call the Hessian map the function that maps x to this matrix hf of x. So that's just the second derivative at x. Yeah? What do you mean smooth function? What is that to be? Where do I say smooth? Continuous function. Oh, yeah. I mean, you need to be able to apply Schwarz's lemma. Let's say two continuous derivatives, that's smooth. OK, so you don't want to use Schwarz's lemma. No, that's fine. OK, so how does the Newton-Raphson method work? Well, what it does is that it forms a quadratic approximation to your function. And that's the one it optimizes at every single point. And the reason is because we have a closed form solution to defining the minimum of a quadratic function. So if I give you a function that's of the form ax squared plus bx plus c, you know exactly a closed form for its minimum. But if I give you any function, or let's say, yeah, so here it's all about maximum. Sorry, I'm going to try to, if you're confused with me using the word minimum, just assume that it was the word maximum. So this is how it works. If I give you a function which is concave, and that's quadratic, so it's going to look like this. All right, so that's of the form ax squared, where a is negative, of course, plus bx plus c. Then you can solve your whatever. You can take the derivative of this guy, set it equal to 0, and you will have an exact equation into what the value of x is that realizes this maximum. If I give you any function that's concave, that's not clear, right? I mean, if I tell you the function that we have here is of the form ax minus b of x, then I'm just going to have something that inverts b prime. But how do I do it exactly? It's not clear. And so what we do is we do a quadratic approximation, which should be true approximately everywhere. So if I'm at this point here, I'm going to say, oh, I'm close to being that function. And if I'm at this point here, I'm going to be close to being that function. And for this function, I can actually optimize. And so if I'm not moving too far from one to the other, I should actually get something. So here's how the quadratic approximation works. I'm going to write the second order Taylor expansion. And so that's just going to be my quadratic approximation. It's going to say, oh, f of x, when x is close to some point x0, is going to be close to f of x0 plus the gradient of f at x0 transpose x minus x0. And then I'm going to have plus 1 half x minus x0 transpose hf at x0 x minus x transpose, right? x minus x0. So that's just my second order Taylor expansion, multivariate 1. And let's say x0 is this guy. Now, what I'm going to do is say, OK, if I wanted to set this derivative of this guy equal to 0, I would just have to solve, well, f prime of x equals 0, meaning that x has to be f prime inverse of 0. And really, apart from being some notation manipulation, this is really not helping me. Because I don't know what f prime inverse of 0 is in many instances. However, if f has a very specific form, which is something that depends on x in a very specific way, there's just a linear term and then a quadratic term, then I can actually do something. So let's forget about this approach. And rather than minimizing f, let's just minimize the right-hand side. Sorry, maximize. So maximize the right-hand side. And so how do I get this? Well, I just set the gradient equal to 0. So what is the gradient? The first term does not depend on x. So that means that this is going to be 0. Plus, what is the gradient of this thing, of the gradient of f at x0 transpose x minus x0? What is the gradient of this guy? So I have a function of the form b transpose x. What is the gradient of this thing? I'm sorry? I'm writing everything into column form, right? So it's just b. So here, what is b? Well, it's gradient of f at x0. And this term here, gradient of f at x0 transpose x0, is just a constant. This thing is going away as well. And then I'm looking at the derivative of this guy here. And this is like a quadratic term. It's like h times x minus x0 squared. So when I'm going to take the derivative, I'm going to have a factor 2 that's going to pop out and cancel this 1 half. And then I'm going to be left only with this part times this part. So that's plus hf x minus x0. So that's just the gradient. And I want it to be equal to 0. So I'm just going to solve this equal to 0. So that means that if I want to find the minimum, this is just going to be the x star that satisfies this. So that's actually equivalent to hf times x star is equal to hf x0 minus gradient f at x0. Now, this is a much easier thing to solve. What is this? This is just a system of linear equations. I just need to find the x star such that when I pre-multiply it by a matrix, I get this vector on the right-hand side. This is just something of the form Ax equals b. And I have many ways I can do this. I could do Gaussian elimination. Or I could use Spielmann's fast Laplacian solvers if I had some particular properties of h. I mean, there's huge activity in terms of how to solve those systems. But let's say I have some time. It's not a huge problem. I can actually just use linear algebra. And linear algebra just tells me that x star is equal to hf inverse times this guy, which those two guys are going to cancel. So this is actually equal to x0 minus hf inverse gradient f at x0. And that's just what's called a Newton iteration. I started at some x0. I'm at some x0 where I make my approximation. And it's telling me if starting from this x0, I wanted to fully optimize the quadratic approximation, I would just have to take the x star that's this guy. And then I could just use this guy as my x0 and do it again, and again, and again, and again. And those are called Newton iterations. And they're basically the workhorse of interior point methods, for example, a lot of optimization algorithms. And that's what you see here. x star is equal to x0 minus the inverse Hessian times the gradient. We briefly mentioned gradient descent. We briefly mentioned gradient descent at some point to optimize a convex function. And if I wanted to use gradient descent, again, h is a matrix. But if I wanted to think of h as being a scalar, would it be a positive or negative number? Yeah. Why? So that would be this. So I want to move against the gradient to do what? To minimize. But I'm maximizing here, right? Everything is maximized, right? So I know that h is actually negative definite. So it's a negative number. So you're making the same, you have the same confusion as I do. We're maximizing a concave function here. So h is negative. So this is something of the form x0 plus something times the gradient. And this is what your gradient ascent, rather than descent, would look like. And all it's saying, Newton is telling you, don't take the gradient for granted as a direction in which you want to go. It says, do a slight change of coordinates before you do this according to what your Hessian looks like. And those are called second-order methods that require knowing what the Hessian is. But those are actually much more powerful than the gradient descent, because they're using all of the local geometry of the problem. All of the local geometry of your function is completely encoded in this Hessian. And in particular, it implies that it tells you where to switch and not go slower in some places or go faster in other places. Now, this, in practice, for, say, modern large-scale machine learning problems, inverting this matrix H is extremely painful. It takes too much time. The matrix is too big. And computers cannot do it. And people resort to what's called pseudo-Newton methods, which essentially try to emulate what this guy is. And there's many ways you can do this. Some of them is by using gradients that you've collected in the past. Some of them is using the fact that just say, well, let's just pretend H is diagonal. There's a lot of things you can do to just play around this and not actually have to invert this matrix. OK? So once you have this, you start it from H0. It tells you which H star you can get as a maximizer of the local quadratic approximation to your function. You can actually just iterate that. So you start at some X0 somewhere. And then once you get to some Xk, you just do the iteration we just described, which is just find a k plus 1, which is the maximizer of the local quadratic approximation to your function at Xk, and repeat until convergence. OK, so if this was an optimization class, we would prove that convergence actually eventually happens for a strictly concave function. This is a SAS class. You're just going to have to trust me that this is the case. And it's globally convergent, meaning that you can start wherever you want. And it's going to work. And the four underminer conditions on F, in particular, those conditions are satisfied for the log-likelihood functions we have in mind. OK? And it converges at an extremely fast rate. Usually, it's quadratic conversions, which means that every time you make one step, you improve the accuracy of your solution by two digits. OK? If that's something you're vaguely interested in, I highly recommend that you take a class on nonlinear optimization. It's a fascinating topic. Unfortunately, we don't have much time. But it starts to be more and more intertwined with high-dimensional statistics and machine learning. I mean, it's an algorithms class, typically. But it's very much more principled. It's not a bunch of algorithms that solve a bunch of problems. There's basically one basic idea, which is if I have a convex function, I can actually minimize it. If I have a concave function, I can maximize it. And it evolves around similar things. So let's stare at this iterative step for a second, and pause, and let me know if you have any questions. OK, so of course, in a second, we will plug in for the log-likelihood. This is just a general thing for a general function, F. But in a second, F is going to be ln. OK, so if I wanted to implement that for real, I would have to compute the gradient of ln at a point xk. And I would have to compute the Hessian at a given point and invert it. So this is just the basic algorithm. And this, as you can tell, used in no place the fact that ln was the log-likelihood associated to some canonical exponential family in a generalized linear model. This never showed up. So can we use that somehow? Optimization for the longest time was about making your problems as general as possible, accumulating maybe in the interior point method theory and conic programming in the mid-90s. And now what optimization is doing is that it's very general. It says, OK, if I want to start to go fast, I need to exploit as much structure about my problem as I can. And the beauty is that as statisticians or machine learning people, we do have a bunch of very specific problems that we want optimizers to solve. And they can make things run much faster. But this did not require to wait until the 21st century. Problems with very specific structure arose already in this generalized linear model. So what do we know? Well, we know that this log-likelihood is really one thing that comes when we're trying to replace an expectation by an average and then doing something fancy. That was our statistical hammer. And remember, when we introduced likelihood maximization, we just said, what we really want to do is to minimize the KL. That's the thing we wanted to minimize, the KL divergence between two distributions, the true one and the one that's parameterized by some unknown theta. And we're trying to minimize that over theta. And we said, well, I don't know what this is because it's an expectation with respect to some unknown distribution. So let me just replace the expectation with respect to my unknown distribution by an average over my data points. And that's how we justified the existence of the log-likelihood maximization problem. But here, I might be able to compute this expectation at least partially where I need it. And what we're going to do is we're going to say, OK, since at a given point xk, say, let me call it here theta, I'm trying to find the inverse of the Hessian of my log likelihood. So if you look at the previous one, as I said, we're going to have to compute the Hessian h sub ln of xk and then invert it. But let's forget about the inversion step for a second. We have to compute the Hessian. This is because we're trying, this is the Hessian of the function we're trying to minimize. But if I could actually replace it, not by the function I'm trying to minimize, to maximize, sorry, the log-likelihood, but really by the function I wish I was actually minimizing, which is the KL, then that would be really nice. And what happens is that since I'm actually trying to find this at a given xk, I can always pretend that this xk that I have in my current iteration is the true one and compute my expectation with respect to that guy. And what happens is that I know that when I compute the expectation of the Hessian of the log-likelihood at a given theta, and when I take the expectation with respect to the same theta, what I get out is negative Fisher information. The Fisher information was defined in two ways. As the expectation of the square of the derivative, or negative the expectation of the second derivative of the log-likelihood. And so now I'm doing some sort of a leap of faith here, because this theta, there's no way the theta, which is the current xk, that's the current theta at which I'm actually doing this optimization, I'm actually pretending that this is the right one. But what's going to change by doing this is that it's going to make my life easier, because when I take expectations, I tend to have, we'll see that when we look at the Hessian, so the Hessian as essentially the derivative of, say, a product is going to be the sum of two terms. The derivative of u times v is u prime v plus uv prime. One of those two terms is actually going to have expectation 0. And that's going to make my life very easy when I take expectations, and basically, I'm going to just have one term that's going to go away. And so in particular, my formula is just by the virtue of taking this expectation before inverting the Hessian, is going to just shrink the size of my formulas by half. So let's see how this works. You don't have to believe me. Is there any question about this slide? You guys remember when we were doing maximum likelihood estimation and Fisher information and the KL divergence, et cetera? Yeah. AUDIENCE MEMBER 2. Why would you have to take the expectation before inverting the Hessian? PHILIPPE RIGOLLET. Because that's what we're really trying to minimize. AUDIENCE MEMBER 2. So you're talking about the expectation of the Hessian? PHILIPPE RIGOLLET. Yeah, so there's something you need to trust me with, which is that the expectation of h of ln is actually h of the expectation of ln. Yeah, it's true, right? Because taking derivative is a linear operator. And we actually used that several times when we said expectation of second derivative, well, actually expectation of partial of l with respect to theta is equal to 0. Remember we did that? It's basically what we used, right? AUDIENCE MEMBER 3. ln is the likelihood? PHILIPPE RIGOLLET. It's the log likelihood. AUDIENCE MEMBER 3. Oh, log likelihood is all 1. OK. When you do the Fisher information, is it likely to be log likelihood? PHILIPPE RIGOLLET. Yeah. AUDIENCE MEMBER 3. Why does ln exist? PHILIPPE RIGOLLET. So actually ln is typically not normalized. So I really should talk about ln over n. But let's see that. So if I have iid observations, that should be pretty obvious. So if I have iid, say, x1, xn with density f theta, and if I look at log f theta of xi, sum from i equal 1 to n, as I said, I need to actually have a 1 over n here. When I look at the expectation, they all have the same expectation. So this is actually, indeed, equal to negative kl plus a constant. And negative kl is because this, if I look at the expectation, so the expectation of this guy is just the expectation of one of them. So I just do expectation theta. Agree? Remember, the kl was expectation theta log f theta divided by f. So that's between p theta and p theta prime. And that's the true p. And let's call it f. p theta, so that's what showed up, which is, indeed, equal to minus expectation theta log f theta plus log of f, which is just a constant with respect to theta. It's just this thing that's up there. It doesn't matter. So this is what shows up here. And just the fact that I have this 1 over n doesn't change because they're iid. Now, when I have things that are not iid, because what I really had was y1, yn, and yi at density f theta i, which is just the conditional density given xi, then I could still write this. And now when I look at the expectation of this guy, what I'm going to be left with is just 1 over n sum from i equal 1 to n of the expectation of log f theta i of yi. And it's basically the same thing, except that I have a 1 over n expectation in front. And I didn't tell you this because I only showed you what the KL divergence was for between two distributions. But here, I'm telling you what the KL is between two products of distributions that are independent but not necessarily identically distributed. But that's what's going to show up, just because it's a product of things. So when you have the log, it's just going to be a sum. Other questions? All right. So what do we do here? Well, as I said, now we know that the expectation of h is negative Fisher information. So rather than putting h inverse in my iterates for Newton-Raphson, I'm just going to put the inverse Fisher information. And remember, I had a minus sign in front. So I'm just going to pick up a plus sign now, just because i is negative, the expectation of the Hessian. And this guy has essentially the same convergence properties. And it just happens that it's easier to compute i than h ln. And that's it. That's really why you want to do this. Now, you might say that, well, if I use more information, I should do better, right? But it's actually not necessarily true for several reasons. But let's say that one is probably the fact that I did not use more information. Every step, when I was computing this thing at xk, I actually pretended that at theta k, the true distribution was the one distributed according to theta k. And that was not true. This is only true when theta k becomes close to the true theta. And so in a way, what I gained, I lost again by making this thing. It's just really a matter of simple computation. So let's just see it on a particular example. Well, actually, on this example, it's not going to look much simpler. It's actually going to be the same. All right, so I'm going to have the Bernoulli example. So we know that Bernoulli belongs to the canonical exponential family. And essentially, all I need to tell you is what b is. And b of theta for Bernoulli is log 1 plus e theta. We computed that. And so when I look at my log likelihood, it's going to look like the sum from i equal 1 to n of yi of, OK, so here I'm going to actually use the canonical length. So it's going to be xi transpose beta minus log 1 plus exponential xi transpose beta. And phi for this guy is equal to 1. Is it clear for everyone what I did? So this, remember, the density, right, so that was really just, so the PMF was exponential y theta minus log 1 plus e theta. There was actually no normalization. That's just the density of a Bernoulli. And theta is actually log p over 1 minus p. And so that's what actually gives me what my, since p is the expectation, this is actually giving me also my canonical length, which is the logit length. We saw that last time. And so if I start taking the log of this guy and summing over n and replacing theta by xi transpose beta, which is what the canonical length tells me to do, I get this guy. Is that clear for everyone? If it's not, please redo this step on your own. So I want to maximize this function. Sorry, so I want to maximize this function over there on the first line as a function of beta. And so to do this, I want to use either Newton-Raphson or what I call Fisher scoring. So Fisher scoring is the second one when you replace the Hessian by negative Fisher information. So I replace these two things. And so I first take the gradient. So let's take the gradient of ln. So the gradient of ln is going to be, well, sum. So here, this is of the form Yi, which is a scalar, times a vector xi beta. That's what I erase from here. The derivative of b transpose x, the gradient of b transpose x is just b. So here I have just Yi xi. So that's of the form Yi, which is a scalar, times xi, which is a vector. Now, what about this guy? Well, here I have a function. So I'm going to have just the usual rule, the chain rule. So that's just going to be 1 over this guy. And then I need to find the Hessian of this thing. So the 1 is going away. And then I apply the chain rule again. So I get E of xi transpose beta, and then the Hessian of this thing, which is xi. OK? So my Hessian, I can actually make gradient, sorry. I can actually factor out all my xi's. And it's going to look like this. Hm. My gradient is a weighted average or a weighted sum of the xi's. This will always happen when you have a canonical, when you have a generalized linear model. And that's pretty clear. Where did the xi show up? Whether it's from this guy or that guy, the xi came from the fact that when I take the gradient of xi transpose beta, I have this vector xi that comes out. It's always going to be the thing that comes out. So I will always have something that looks like some sum with some weights here of the xi's. Now, when I look at the second derivative, so same thing. I'm just going to take the derivative of this guy. Since nothing depends on beta here or here, I'm just going to have to take the derivative of this thing. And so it's going to be equal. So if I look now at the Hessian ln as a function of beta, I'm going to have sum from i equal 1 to n of, well, yi. What is the derivative of yi with respect to beta? What? Yeah. 0. It doesn't depend on beta. I mean, its distribution does, but yi itself is just a number. So this is 0, so I'm going to get minus. And then I'm going to have, again, the chain rule that shows up. So I need to find the derivative of x over 1 plus x. What is the derivative of x over 1 plus x? I actually don't even know. So that gives me. So that's 1 over 1 plus x squared. So that's minus 1 divided by 1 plus e xi transpose beta squared times the derivative of the exponential, which is e xi transpose beta, and again, xi. And then I have this xi that shows up. But since I'm looking for a matrix, I'm going to have xi xi transpose. OK? So this is just, I know I'm going to need something that looks like a matrix in the end. And so one way you want to think about it is this is going to spit out an xi. There's already an xi here. So I'm going to have something that looks like xi, and I'm going to have to multiply by another vector xi. And I want it to form a matrix. And so what you need to do is to take an outer product. And that's it. All right. So now, well, as a result, the updating rule is this. Honestly, this is not a result of anything. I actually rewrote everything that I had before with a theta replaced by beta, because it's just painful to rewrite this entire thing. Put some big parentheses and put minus 1 here. OK? And then I would have to put the gradient, which is this thing here. So as you can imagine, this is not super nice. Actually, what's interesting is, at some point I mentioned those pseudo-Newton method. They're actually doing exactly this. They're saying, oh, I'm actually, at each iteration, I'm actually going to just take those guys. If I'm at iteration k, I'm actually just going to sum those guys up to k, rather than going all the way to n and look at every one. So you're just looking at your observations one at a time, based on where you were before. OK, so you have a matrix. You need to invert it. So if you want to be able to invert it, you need to make sure that the sum with those weights of xi outer xi, or xi xi transpose, is invertible. So that's a condition that you need to have. And well, you don't have to, because technically, in order to invert, you just need to solve the linear system. But that's actually guaranteed in most of the cases, if n is large enough. So everybody sees what we're doing here? So now, that's for the Newton-Raphson. If I wanted to actually do the Fisher scoring, all I would need to do is to replace the Hessian here by its expectation, when I pretend that the beta that I have at iteration k is the true one. What is the expectation of this thing? And when I say expectation, here I'm always talking about conditional expectation of y given x. The only distributions that matter, that have mattered in this entire chapter, are conditional expectation of y given x. The conditional expectation of this thing given x is what? It's itself. It does not depend on y. It only depends on the x's. So conditionally on x, this thing, as far as we're concerned, is completely deterministic. So it's actually equal to its expectation. And so in this particular example, there's no difference between Fisher scoring and Newton-Raphson. And the reason is because the gradient no longer depends on yi. Sorry, the Hessian no longer depends on yi. OK? So this slide is just repeating some stuff that I've said. OK. So I think this is probably, well, OK, let's go through this, actually. At some point, I said that Newton-Raphson, do you have a question? Yeah, when would the gradient, sorry, the Hessian ever depend on yi? Not when you have a canonical link. So the canonical link, there is no difference between? No. Yeah, so maybe I wanted you to figure that out for yourself. But OK, so yi times xi transpose beta. So essentially, when I have a general family, what it's referring to is that this is just b of xi transpose beta. So I'm going to take some derivatives, and there's going to be something complicated coming out of this. But I'm certainly not going to have some yi showing up. The only place where yi shows up is here. Now, if I take two derivatives, this thing is gone because it's linear. The first one is going to keep only this guy, and the second one is going to make it gone. The only way this actually shows up is when I have an h here. And if I have an h, then I can take second derivatives, and this thing is not going to be completely independent of beta. Sorry, yeah, this thing is still going to depend on beta, which is why i term is not going to disappear. I believe we'll see an example of that, or maybe I removed it. I'm not sure, actually. I think we will see an example of that. So let us do iteratively reweighted least squares, or IRLS, which I've actually recently learned is a term that, even though it was defined in the 50s, people still feel free to use to define, to call their new algorithms, which have nothing to do with this. This is really something where you actually do iteratively reweighted least squares, which means that every step, OK, let's just actually go through this quickly. What is going to be iteratively reweighted least squares? The way the steps that we had here showed up, let's say those guys, x star is this, is when we were actually solving this linear system. That was the linear system we were trying to solve. But solving a linear system can be done by just trying to minimize. If I have x, a, and b, it's the same as minimizing the norm of ax minus b squared over x. If I can actually find an x for which it's 0, it means that I've actually solved my problem. And so that means that this is actually, I can solve linear systems by solving least squares problems. And least squares problems are things that statisticians are comfortable solving. And so all I have to do is to rephrase this as a least squares problem. And I could just write it directly like this, but there's a way to streamline it a little bit. And that's actually by using weights. So I'll come in the weights in this, well, not today, actually, but very soon. So this is just a reminder of what we had. We have that yi given xi as a distribution distributed according to some distribution in the canonical exponential family. So that means that the log likelihood looks like this. Again, this does not matter to us. This is the form that matters. And we have a bunch of relationships that we actually spent some time computing. The first one is that mu is b prime of theta i. The second one is that if I take g of mu i, I get this systematic component, xi transpose beta. That's modeling. Now, if I look at the derivative of mu i with respect to theta i, this is the derivative of b prime of theta i with respect to theta i. So that's the second derivative. And I'm going to call it vi if phi is equal to 1. This is actually the variance. And then I have this function h, which allows me to bypass altogether the existence of this parameter mu, which says if I want to go from xi transpose beta all the way to theta i, I have to first do g inverse and then b prime inverse. If I stopped here, I would just have mu. OK? OK, so now what I'm going to do is I'm going to apply the chain rule. And I'm going to try to compute the derivative of my log likelihood with respect to beta. So again, the log likelihood is much nicer when I write it as a function of theta than a function of beta. But it's basically what we've been doing by hand. You can write it as a derivative with respect to theta first and then multiply by the derivative of theta with respect to beta. And we know that theta depends on beta as h of xi transpose beta. I mean, that's basically what we've been doing for the Bernoulli case. I mean, we use the chain rule without actually saying it. But this is going to be convenient to actually make it explicitly show up. So when I first take the derivative of my log likelihood with respect to theta, I'm going to use the fact that my canonical family is super simple. So what I have is that my log likelihood, Ln, is the sum from i equal 1 to n of Yi theta i minus b of theta i divided by phi plus some constant, which I will go away as soon as I'm going to take my first derivative. So if I take the derivative with respect to theta i of this guy, this is actually going to be equal to Yi minus b prime theta i divided by phi. And then I need to multiply by the derivative of theta i with respect to beta. And remember, theta is h of xi transpose beta. So the derivative of theta i with respect to beta j, this is equal to h prime of xi transpose beta. And then I have the derivative of this guy. Actually, let me just do the gradient of theta i at beta. That's what we did. I'm just calling theta i. I'm just thinking of theta i as being a function of theta. So what should I add here? It's just the vector xi, which is just the chain rule again. That's h prime, right? We don't see it, but there's a prime here as the derivative. We've done that without saying it explicitly. So now if I multiply those two things, I have this Yi minus b prime of theta i, which I call by its good name, which is mu i. b prime of theta i is the expectation of Yi conditionally on xi. And then I multiply by this thing here. Here, this thing is written coordinate by coordinate, but I can write it as a big vector when I stack them together. And so what I claim is that this thing here is of the form Yi minus mu. But here I put some tildes, because what I did is that I first divided this. I multiply everything by g prime of mu for each mu. So why not? So actually, on this slide, it will make no sense why I do this. I basically multiply by g prime on one side and divide by g prime on the other side. So what I write so far is that the gradient of ln with respect to beta is the sum from i equal 1 to n of Yi minus mu i, let's call it, divided by phi times h prime of xi transpose beta xi. So I just stacked everything that's here. And now I'm going to start calling things. The first thing I'm going to do is I'm going to divide. So this guy here, I'm going to push here. Now, this guy here, I'm actually going to multiply by g prime of mu i. And this guy, I'm going to divide by g prime of mu i. So there's really nothing that happened here. I just took g prime and multiply and divide it by g prime. Why do I do this? Well, that's actually going to be clear when we talk about iteratively reweighted least squares. But now, essentially, I have a new mu, a new y, which is, so this thing now is going to be y tilde minus mu tilde. So ii. Now, this guy here, I'm going to call wi. And I have the xi that's there, which means that now the thing that I have here, I can write as follows. Gradient ln of beta is equal to what? Well, I'm going to write it in matrix forms. So I have the sum over i of something multiplied by xi. So I'm going to write it as x transpose. Then I'm going to have this matrix w1, wn, and then 0 elsewhere. And then I'm going to have my y tilde minus mu. And remember, x is the matrix with, sorry, this should be a bit narrow. I have n and then p. And here I have my xij and this matrix on row i and column j. And this is just a matrix that has the wi's on the diagonal. And then I have y tilde minus mu. So this is just the matrix we're writing of this formula. So it's just saying that if I look at the sum of weighted things of my columns of xi, it's basically the same thing. When I'm going to multiply this by my matrix, I'm going to get exactly those terms, yi minus mu i tilde times wi. And then when I actually take this xi transpose times this guy, I'm really just getting the sum of the columns with the weights. Agreed? If I look at this thing here, this is a vector that has s-coordinate, wi times yi tilde minus mu i tilde. And I have n of them. So when I multiply x transpose by this guy, I'm just looking at a weighted sum of the columns of x transpose, which is a weighted sum of the rows of x, which are exactly my xi's. And that's this weighted sum of the xi's. So here, as I said, the fact that we decided to put this g prime of mu i here and g prime of mu i here, we could have not done this. We could have just said, this I call. I don't even forget about the tilde. I just call it yi minus mu i. And here, I just put everything I don't know into some wi. And so why do I do this? Well, it's because when I actually start looking at the Hessian, what's going to happen? Yeah, we'll do that next time. But let's just look quickly at the outcome of the computation of my Hessian. So I compute a bunch of second derivatives. And here, I have two terms. Well, he's gone. So I have two terms. And when I take the expectation now, it's going to actually change. This thing is actually going to depend on yi, because I have an h, which is not the identity. Oh, no, you're here. And so when I start looking at the expectation, so I look at the conditional expectation given xi. The first term here has a yi minus expectation. So when I take the conditional expectation, this is going to be 0. The first term is going away when I take the conditional expectation. But this was actually gone already if we had the canonical term, because the second derivative of h, when h is the identity, is 0. But if h is not the identity, h prime prime may not be 0. And so I need that part to remove that term. And so now, I work a little bit, and I get this term. That's not very surprising. In the second derivative, I see I have terms in b prime prime. I have term in h prime, but square. And then I have my xi, outer xi, xi, xi transpose, which we know we would see. So we'll go through those things next time. But what I wanted to show you is that now, once I compute this, I can actually show that if I look at this product that showed up, I had b prime prime times h prime squared. One of those terms is actually 1 over g prime. And so I can rewrite it as one of the h primes, because I had a square, divided by g prime. And now, I have this xi, xi transpose. So if I did not put the g prime in the w that I put here completely artificially, I would not be able to call this guy wi, which is exactly what it is from this board. And now that this guy is wi, I can actually write this thing here as x transpose wx. And that's why I really wanted my w to have this g prime of mu i in the denominator, because now, I can actually write a term that depends on w. Now, you might say, how do I reconcile those two things? What the hell are you doing? And what the hell I'm doing is essentially that I'm saying that if you write beta k according to the Fisher scoring iterations, you can actually write it as just this term here, which is of the form x transpose x inverse x transpose y, but where I actually squeezed in this w. And that's actually a weighted least squares, and it's applied to this particular guy. So we'll talk about those weighted least squares. But remember, least squares is of the form beta hat is x transpose x inverse x transpose y. And here, it's basically the same thing, except that I squeeze in some w after my x transpose. So that's how we're going to solve it. I don't want to go into the details now, mostly because we're running out of time. Are there any questions?