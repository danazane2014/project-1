 All right. So today, we're going to start a new chapter on graph limits. So graph limits is a relatively new subject in graph theory. So as the name suggests, we're looking at some kind of an analytic limit of graphs, which sounds kind of like a strange idea because you think of graphs as fundamentally discrete objects. But let me begin with an example to motivate, at least pure mathematical motivation for graph limits. There are several other ways you can motivate graph limits, especially coming from more applied perspectives. But let me stick with the following story. So suppose you lived in ancient Greeks and you only knew rational numbers. You didn't know about real numbers. But you understand perfectly rational numbers. And we wish to maximize, so we wish to minimize, the following polynomial, x cubed minus x, let's say, for x between 0 and 1. OK. So you can do this. And suppose also the Greeks knew calculus and take the derivative and all of that. So you find that, well, you have a problem. Because we know, so given our advanced state of mathematics, we know that the maximum, so the minimizer, is at x equals to 1 over root 3. But that number doesn't exist in the real numbers. So how might a civilization that only knew rational numbers express this answer? They could say the minimum occurs not in q. So there's not minimized in q, but not minimized by a single number, but by a sequence. And this is a sequence that more advanced civilization would know, a sequence that converges to 1 over root 3. But I can give you this sequence through some other means. And this is one of the ways of defining the complete set of real numbers, for instance. But you can define explicitly a sequence of real numbers that converges. OK, but of course, this is all quite cumbersome. If you have to actually write down the sequence of real numbers to express this answer, it would be much better if we knew the real numbers. And we do. And the real numbers, in some sense, I mean, in a very rigorous sense, is a completion of the rational numbers. That's the story that we're all familiar with. But now let's think about graphs, which are some kind of a discrete set of objects akin to the rational numbers. And the story now is among graphs, suppose I have a fixed p between 0 and 1. And the problem now is to minimize the 4-cycle density among graphs with density, with edge density p. So this is some kind of optimization problem. So I don't restrict the number of vertices. You can use as many vertices as you like. And I would like to minimize the 4-cycle density. Now, we saw a few lectures ago this inequality that tells us that, so we saw a few lectures ago, that this density is always at least p to the 4. So in the lecture on quasi-randomness, so we saw this inequality. And we also saw that this minimum is approached by a sequence of quasi-random graphs. And in some sense, so the answer is p to the 4. And there's not a specific graph. There's no one graph that minimizes this 4-cycle density. It's minimized by a sequence. And just like in the story with rational numbers and the real numbers, it would be nice if we didn't have to write out the answer in this cumbersome sequential way, but just have a single graphical-like object that depicts what the minimizer should be. And graph limits provides a language for us to do this. So one of the goals of graph limits is gives us a single object for this minimizer instead of taking a sequence. So roughly, that is the idea that you have a sequence of graphs. And I would like some analytic object to capture the behavior of the sequence in the limit. And these graph limits can be written, actually, in a fairly concrete form. And so now let me begin with some definitions. The main object that we'll look at is something called a graphon. So this merges the two words graph function. A graphon is, by definition, a symmetric measurable function often denoted by the letter w from the unit square to the 0, 1 interval. And here, being symmetric means that if you exchange the two argument variables, this function remains the same. So that's it. So that's the definition of a graphon. And these are the objects that will play the role of limits for sequences of graphs. And I will give you lots of examples in a second. So that's the definition. This is the form of the graphons that we'll be looking at mostly. But just to mention a few remarks that the domain can be instead any product of any square of a probability measure space. So instead of taking the 0, 1 interval, I could also use any probability measure space. So it's only slightly more general. So there are some general theorems in measure theory that tells us that most probability measure spaces, if they're nice enough, they are, in some sense, equivalent or can be captured by this interval. So I don't want to worry too much about all the measure theoretic technicalities. I think they're not so important for the discussion of graph limits. But there are some subtle issues like that just lurking behind. But I just don't want to really talk about them. So for the most part, we'll be looking at graphons of this form. And also, the instead of the domain, so the values, so instead of 0, 1 interval, you could also take a more general space, for example, the real numbers or even the complex numbers. I'm going to use the word graphon to reserve this word for when the values are between 0 and 1. And if it's in R, let me call this just a kernel, although that will not come up so much. So when I say graphon, I just mean the values between 0 and 1. Although, if you do look up papers in the literature, sometimes they don't use these words so consistently. So be careful what they mean by a graphon. All right, so that's the definition. But now let me give you some examples. How do we think of graphons, and what do they have to do with graphs? So if we start with a graph, I want to show you how to turn it into a graphon. So let's start with this graph, which you've seen before. This is the half graph. So from this graph, I can label the vertices. Vertices and form an adjacency matrix of this graph, where I label the rows and columns by the vertices and put in 0's and 1's according to whether the edges are adjacent. So that's the adjacency matrix. And now I want you to view this matrix as a black and white picture, so think one of these pixelated images, where I turn the 1's into black boxes, where, of course, on the blackboard, black is white and white is black. So I turn the 1's into black boxes, and I leave the 0's as empty white space. So I get this image. And I think of this image as a function. And this is a function going from 0, 1 squared to 0, 1 interval, taking only 0 and 1 values. OK? So that's a function on the square. But now, so this is a single graph. So for any specific graph, I can turn it into a graphon like this. But now imagine you have a sequence of graphs. In particular, consider a sequence of half graphs. So here is H3. So Hn is the general half graph. And you can imagine that as n gets large, this picture looks like instead of the staircase, you just have a straight line connecting the two ends. And indeed, this function here, this graphon, is the limit of the sequence of half graphs as n goes to infinity. So one way you can think about graphons is you have a sequence of graphs. You look at their adjacency matrix. You view it as a picture, a pixelated image, black and white, according to the 0's and 1's in the adjacency matrix. And as you take a sequence, you make your eyes a little bit blurry. And then you think about what the sequence of images converges to. So the resulting limit is the limit of this sequence of graphs. So that's an informal explanation. So I haven't done anything precisely. And in fact, one needs to be somewhat careful with this depiction. Because let me give you another example. Suppose I have a sequence of random or quasi-random graphs with edge density 1 half. So what does this look like? Again, I have this picture here. And I have a lot of 1 half of the pixels are black, and the other half pixels are white. And you can think from far away, I cannot distinguish necessarily which ones are black and which ones are white. And in the limit, it looks like a grayscale image, with the grayscale being 1 half density. And indeed, it converges to the constant function 1 half. So the limits represented by this problem up here is the constant graphon with constant value p. But now let me give you a different example. Consider a checkerboard. OK, so here's a checkerboard where I color the squares in this alternating black and white manner according to a usual checkerboard. And as the number of squares goes to infinity, what should this converge to? By the story I just told you, you might think that if you zoom out, everything looks like density 1 half. And so you might guess that the image, the limit, is the 1 half constant. But what is this graph? It's a complete bipartite graph. It is a complete bipartite graph between the odd and even rows. And there's a different way to draw the complete bipartite graph, namely that picture just by permuting the rows and columns. And it's much more reasonable that this is the limit of the sequence of complete bipartite graphs with equal parts. So one needs to be very careful. So it's not necessarily an intuitive definition. I mean, the idea that you just squint your eyes and think about what the image becomes, that works fine for intuition for some examples, but not for others. So we do really need to be careful in giving a precise definition. And here, the rearrangement of rows and columns needs to be taken care of. OK, so let me be more precise. Starting with a graph G, I can label the vertices by 1 through N. I can denote by W sub G this function, this graphon, obtained by the following procedure. First, you partition the interval into intervals of length exactly 1 over N. And you set W of x, y to be basically what happened in the procedure above. If x and y lie in the box I sub i cross I sub j, then I put in 1 if i is adjacent to j and 0 otherwise. OK, so this picture where we obtained by taking the adjacency matrix and transforming it into a pixelated image. What are some of the things that we would like to do with graph limits or graphs in general? Yeah? AUDIENCE 2 Is the range of the 0, 1 squared or 0, 1? Thank you, the range is 0, 1. Good. So here are some quantities we're interested in when considering graph limits. So given two graphs, G and H, we say that a graph homomorphism between from G to H is a map of their vertex sets such that the edges are preserved. So you have, so whenever u, v is an edge of H, the image vertices get mapped to an edge of G. And we are interested in the number of graph homomorphisms. So often, I use uppercase to denote a set. So the set of homomorphisms G to H and lowercase to denote the number. So for example, the number of homomorphisms from a single vertex, so a single vertex with no edge to a graph G, that's just the, what is this quantity? So it's the number of vertices of G. What about homomorphisms from an edge to G? Yeah. Not quite the number of edges, but twice the number of edges. What about the number of homomorphisms from a triangle to G? OK, so that's OK. You get the idea. The 6 times the number of triangles. Let me ask a slightly more interesting question. What about the number of homomorphisms from H to a triangle? What's a different name for this quantity here? It's the number of proper 3-colorings. So it's the number of proper 3-colorings. So the number of proper colorings of H with three labeled colors, red, green, and blue. So think about the three vertices, that's red, green, and blue. And whichever vertex of H get mapped to red, color that vertex red. So you see that there's a one-to-one correspondence between such homomorphisms and proper colorings. So many important graph parameters, graph quantities, can be encoded in terms of graph homomorphisms. And these are the ones that we're going to be looking at most of the time. When we're thinking about very large graphs, often it's not the number of homomorphisms that concern us, but the density of homomorphisms. And the difference between homomorphisms, on one hand, and subgraphs is that homomorphisms are not quite the same as subgraphs, other than this multiplicity, because you might have non-injective homomorphisms. But these non-injective homomorphisms do not end up contributing very much, because they only have n to the number of vertices of H minus 1 on that order, where I think of n as the number of vertices of G. So n is supposed to be large. So in terms of graph limits, when n gets large, I don't need to distinguish so much between homomorphisms and subgraphs. So we define the homomorphism density denoted by the letter T from H to G by defining it to be the fraction of all vertex maps that are homomorphisms. So this is also equivalent to be defined as the probability that a uniform random map from the vertex set of H to the vertex set of G is a homomorphism from H to G. So it's a graph homomorphism. And this quantity turns out to be quite important. So we're going to be seeing this a lot. And because of this remark over here, in the limit, this quantity on graph homomorphism densities, in the limit as the number of vertices, G goes to infinity and H fixed, the homomorphism densities approaches the same limit as subgraph densities. So you should regard these two quantities as basically the same thing. Any questions so far? All right, so all of these quantities so far defined are for, OK, so everything's defined so far for graphs, so what happens between graphs and graphs. So what about for graphons? Now, I'll give you this limit object, this analytic object. I can still define densities by integrals now. So suppose I start with a symmetric measurable function, so W. For example, a graphon, but I can let my range be even more generous. Starting with such a function, I define the graph homomorphism density from a fixed graph H to this graphon, or kernel more generally, to be the following integral, where I'm, before writing down the full form, let me first give you an example. I think it will be more helpful. So if I'm looking at a triangle going to W, what I would like is an integral that captures the triangle density. So this quantity here, if I let x, y, and z vary over 0 and 1, 0 through 1, independently and uniformly, then this quantity here captures the triangle density in W. In fact, and I'll state this more precisely in a second, if you look at the translation from graph to graphon and combine that translation with this definition here, you recover the triangle density. More generally, for H instead of a triangle, the H density in a graphon is defined to be the integral of, instead of this product here, I take a product corresponding to the graph structure of H with one factor for each edge of H, and the variables go over the vertex set of H. So this is the definition of homomorphism densities, not for graphs, but for symmetric measurable functions, in particular for graphons. And we define it this way because, and we use the same symbols, because these two definitions agree. If you start with a graph and look at the H density in G, then this quantity here is equal to the H density in the graphon associated to the graph G constructed as we did just now. So make sure you understand why this is true and why we defined densities this way. Any questions so far? So we've given the definition of graph homomorphism density, and we've defined these objects, these graphons. And I mentioned even something about the idea of a limit. But in what sense can we have a limit of graphs? Here. So here's an important definition on the convergence of graphs. So in what sense can we say that a sequence of graphs converge? So we say that a sequence of graphs G sub n, graphs or graphons, so these two definitions are interchangeable for what I'm about to say regarding limits. So graphons, in which case I'm going to denote them by W sub n. So we say the sequence is convergent if the sequence of subgraph densities, of course, if you're looking at graphons, then you should look at the graphon, the subgraph density in, homomorphism density in graphons. If this sequence converges as n goes to infinity for every graph H. So that's the definition of what it means for a sequence of graphs to converge, which so far looks actually quite different from what we discussed intuitively. But I will state some theorems towards the end of this lecture explaining what the connections are. So intuitively, what I've said earlier is that you have a sequence of graphs that are convergent if you have some vague notion of one image morphing into, a sequence of images morphing into this final image. Still hold that thought in your mind, but that's not a rigorous definition yet. The definition we will use for convergence is if all the subgraph, all the homomorphism densities, or equivalently, subgraph densities, they converge. So this is a definition. It's not required. So this is basically rigorous as stated. Just as a remark, it's not required that the number of vertices goes to infinity, although you really should think that is the case. So just put it out there. I can have a sequence of constant graphs, and they will still be convergent, and that's still OK. But you should think of the number of vertices going to infinity. Yeah? AUDIENCE 2 What is f in the definition? YUFEI ZHAO? Thank you. OK, any other questions? All right. So there are some questions I would like to discuss, and this will occupy the next few lectures in terms of proving the following statements. One is, do you always have graph limits? If you have a convergent sequence of graphs, do they always approach a limit? Just because something is convergent doesn't mean you can represent the limit necessarily. So it turns out the answer is yes. It turns out that this makes it a good theory, a good, useful theory, an easy theory to use, that there's always a limit object whenever you have convergence. And the other question is, well, we've described intuitively one notion of convergence and also defined more rigorously another definition of convergence. Are these two notions compatible? And what does this even mean, this idea of image becoming closer and closer to a final image? What does that even mean? So these are some of the questions I would like to address. So in the next few things I would like to discuss is, first, I want to give you a definition of a distance between two graphons or two graphs. If I give you two graphs, how similar or dissimilar are they so that we have this metric? And then we can talk about convergence in metric spaces. OK, so let's take a quick break. So given this notion of convergence, I would like to define a notion of distance between graphs so that convergence corresponds to convergence in the metric space sense of distance going to 0. So how can we define distance? First, let me tell you that there's a trivial way. So there's a way in which you look at that definition and produce a distance out. And here's what you can do. I can convert that definition to a metric by setting the distance between two graphs, g and g prime, to be the following quantity obtained by, well, what would I like to do? I would like to say the distance goes to 0 if and only if the homomorphism densities, they're all close to each other. So I can sum up all the homomorphism densities and look at their differences between g and g prime. And I simply enumerate the list of all possible graphs. I want to be just slightly more careful with this definition here because I want something which, OK, so when I write this, this number might be infinite for all pairs g and g prime. So if I just add a scaling factor here, then, OK. So this is some distance. So this is some distance. And you see that it matches the definition up there. But it's completely useless. It might as well not say anything because it's tautologically the same as what's happened up there. And if I give you two graphs, it doesn't really tell you all that much information except to encapsulate that definition into a single number. OK, great. So I'm just, the point of this is just to tell you that there's always a trivial way to define distance. But we want some more interesting ways. So what can we do? So here's an attempt, which is that of an added distance. So we've seen this before when we discussed removal lemmas. The added distance is the number of edges you need to change to go from one graph to the other graph. And this seems like a pretty reasonable thing to do. And it is an important quantity for many applications. But it turns out not the right one for our application. And here's the reason. So this is why the added distance is, OK, so by added distance, I mean 1 over the number of vertex squared times the number of edge changes needed. So there's normalization, so that distance is always between 0 and 1. But this is not a very good notion for the following reason. If I take two copies of the Erdos-Renyi random graph, GN 1 half, what do you think is the added distance between two such random graphs? How many edges? Yeah? AUDIENCE 2 I think roughly 1 half. There's a number of edges, so there's a 1 top half probably that will both be there or not be there in both cases. YUFEI ZHAO OK. So yeah, so let me try to rephrase what you're saying. So suppose I have this G and G prime both sitting on top of the vertex set N. So if I'm not allowed to rearrange the vertices, how many edge changes do I need to go from one to the other? I need about 1 half. So 1 half the time, I'm going to have a wrong edge there. Now, you can make this number just slightly smaller by permuting the vertices, but actually you will not improve that much. It's still going to be roughly that added distance, which is quite large. I mean, this is almost as large as you can possibly get between two arbitrary graphs. So if we want to say that random graphs, they approach a limit, a single limit, then this is not a very good notion because they are quite far apart for every N. So this is the reason why the more obvious suggestion of an added distance might not be such a great idea. So what should we use instead? So we should take inspiration from what we discussed in quasi-randomness. Yes, question? AUDIENCE 2 Is the added distance only for two graphs with the same vertex set? YUFEI ZHAO OK, so the question is, is the added distance only for two graphs with the same vertex set? Let's say yes. So we'll see later on. You can also compare graphs with different number of vertices. So hold on to that thought. So I would like to come up with a notion of distance between graphs that is inspired by our discussion of quasi-randomness earlier. So think about the discussion of quasi-randomness, quasi-random graphs. In what sense can g be close to a constant, let's say, p? So this was this Chung-Graham-Wilson theorem that we proved a few lectures ago. So in what sense can g be close to p? And one of those definitions was discrepancy. And discrepancy says that if the following quantity is small for all subsets x and y, which are subsets of vertices of g. So you remember, all of you remember this part, the discrepancy hypothesis for quasi-randomness. And this is the kind of definition that we would like to describe when two graphs are similar to each other, when they are close in this discrepancy sense. So now, instead of a graph and a number, what if now I have two graphs? I give you two graphs, g and g prime. And what I would like to say is that if, for now, so if they have the same vertex set, I want to say that they are close if I have that the number of edges between x and y in g is very close to the number of edges between x and y in g prime. And I normalize by the number of vertices squared, and this number of vertices. And I would like to find out the worst possible scenario, so over all x and y subsets of the vertex set. If this quantity is small, then I would like to say that g and g prime are close to each other. So this is inspired by this discrepancy notion. Can you see anything wrong with this definition here? Yeah? AUDIENCE MEMBER 2. Permutations. YUFEI ZHAO. Ah, so permutations of vertices. So just like in the checkerboard example we saw earlier, you have two graphs. And if they are indeed labeled graphs in the same labeled vertex set, then this is the definition more or less we will use. I will define it more precisely in a second. But if they are unlabeled vertices, we need to possibly optimize over permutations, over rearrangements of vertices, which actually turns out to be quite subtle. So I'm going to give precise definitions in a second. But this one here, yeah, so think about permuting vertices, but it's actually a bit more subtle than that. All right, so here are some actual definitions. I'm going to define this quantity called a cut norm. So this chapter is all going to be somewhat functional analytic in nature. So get used to the analytic language. So the cut norm of W is defined to be the following quantity, denoted by this norm with a box in the subscript, which is defined to be if I look at this W and I integrate it over a box, and I would like to maximize this quantity here over choices of boxes S and T. They are subsets of the interval, measurable subsets. Subsets. So choose your overall possible choices of measurable subsets S and T. If I integrate W over S cross T, what is the furthest I can get from 0? So this is the definition of cut norm. And you can already see that it has some relations to what we've discussed up there. But while we're talking about norms, let me just mention a few other norms that might come up later on when we discuss graph limits. So there will be a lot of norms throughout. So in particular, the LP norm is going to play a frequent role. So the LP norm is defined by looking at the p-th norm of the absolute value, integrate it, and then raise to 1 over p. And so the infinity norm, the L infinity norm, so this is almost but not quite the same as the sup. So almost the same as the supremum, but not quite because I need to ignore subsets of measure 0. So I can write down a formal definition in a second. But I need to, if I change W on the subset of measure 0, I shouldn't change any of these norms. And so the one way to define this essential supremum, it's called an essential sup, is that it is the largest. So it is the smallest lambda such that, so smallest number m, such that the measure of the set taking value bigger than m the set has measure 0. So it's the threshold above which you, it has measure 0. And the L2 norm will play a particularly special role. And for the L2 norm, you're really in a Hilbert space, in which case we are going to have inner products. And we denote inner products using these brackets. So everything's real. I don't have to worry about complex conjugates. So comparing with the discussion up there, we see that a sequence of, so sequence GN of quasi-random graphs has property that the associated graphons converge to p in the cut norm. For quasi-random graphs, there's no issue of having to do with permutations, because the target is invariant upon permutations. But if I give you two different graphs, then I need to think about their permutations. And to study permutations of vertices, the right way to do this is to consider measure-preserving transformations. So we say that phi from the interval to the interval is measure-preserving. So first of all, it has to be a measurable map. And everything I'm going to talk about are measurable. So sometimes I will even omit mentioning it. So it is measure-preserving if, for all measurable subsets A of this interval, one has that the pullback of A has the same measure as A itself. Let me give you an example. So you have to be also slightly careful with this definition. If you think about the push forward, that's false. It has to be the pullback. So for example, in the map, which sends, so an easy example, the map which sends x to x plus 1 half. So think about the circle as your space. And here, I'm just rotating the circle by 1 half rotation. So it's obviously measure-preserving. I'm not changing any measures. Slightly more interesting example, quite a bit more interesting example, is setting x to 2x. This is also measure-preserving. And you might be puzzled for a second why it's measure-preserving, because it sounds like it's dilating everything by a factor of 2. But if you look at the definition, so here's, again, mod 1. If you look at the definition, if you look at, let's say, a subset A, which is like, so what should I take? OK, so for example, so if that is my A, so what's the inverse of A? So it's this set. So the measure is preserved upon this pullback. So if you push forward, then you might dilate by a factor of 2. But when you pull back, the measure gets preserved. So these measure-preserving transformations are going to play the role of permutations of vertices. So it turns out that these things are actually quite subtle, technically. And I'm going to, as much as I can, ignore some of the measure-derivative technicalities. But they are quite subtle. So for example, so now let me give you a definition for the distance between two graphons. I write, starting with a symmetric measurable function w, so I write w superscript phi to denote the function obtained by as follows. So I think of this as relabeling the vertices of a graph. And now I define this distance. So this is going to be called the cut distance between two symmetric measurable functions, u and w, to be the infimum over all measure-preserving bijections. So this is the definition for the distance between two graphons. Take the optimal, well, in my case, the question does it, I'm looking at inf. So I haven't told you yet whether you can take a single one. And it turns out that's a subtle issue. And generally, it doesn't exist. But I look over all measure-preserving bijections phi. And I look at the distance between w and w phi, optimized over the best possible measure-preserving bijection. So this inf is really an inf. It's not always obtained. Actually, this example here is a great example for, you can create an example for why inf is not always obtained from the discussion over here. For example, if u is the function x times y, so there's a graphon xy, and w is u phi, where phi is the map that sends x to 2x, then in your mind, you should think of these two as really the same graphons. You're applying the measure-preserving transformation. It's like doing a permutation. But because phi is not bijective, you cannot put in phi here to get these two things to be the same. So there's some subtleties. So this is really an example to just highlight. There's some subtleties here, which I'm going to try to ignore as much as possible. But I'll always give you correct definitions. Any questions? Yeah? AUDIENCE 1. So do we expect the cut distance between these two to be 0 and the answer is 1? YUFEI ZHAO. So the question, do we expect the cut distance between these two to be 0? And the answer is yes. So we do expect them to be 0. And they are 0. They are equal to 0. I mean, let me just tell you one, something that is known. And this is one of those statements that has a lot of measure-derivative technicalities. For all graphons U and W, it turns out that there exist measure-preserving maps. So not necessarily bijections, but measure-preserving maps from 0, 1 interval to itself, such that the distance between U and W, the cut distance, is obtained by the cut norm of the difference between U phi and W psi. So don't worry about it. OK. OK. OK, so so far, we've defined this notion of a cut distance between two graphons. But now I'll give you two graphs. So what should you do for two graphs? Well, I can. AUDIENCE 1. Take the graphons associated with them. YUFEI ZHAO. OK, great. So take the graphon associated with these graphs and consider their cut distance. So for graphs G and G prime, and potentially even different number of vertices, I can define the distance, the cut distance, between these two graphs to be the distance between the associated graphons. And similarly, if I have a graph and a graphon, I can also compare their distance. So what does this actually mean? All right, so if I give you two graphs, even with the same number of vertices, it's not quite the same thing as a permutation of vertices. It's a bit more subtle. Why is it more subtle than just permuting the vertices? So here we're using measure-preserving transformations, which doesn't see your atomic vertices. So it might split up your vertices. So you might take a vertex and then chop it in half and send one half somewhere and another half somewhere else. These guys, they don't care about your vertices anymore. So it's not quite the same as permuting vertices. But it's some kind of, so you allow some kind of splitting and rearrangement and overlays. So you can write out this distance in this format, find out the best way to split an overlay and to rearrange that way. But it's much cleaner to define it in terms of graphons. Yes? AUDIENCE 1. Why do we take bijections up there and not? YUFEI ZHAO. OK, question is, is there why we take bijections up there? No. So up there, if I wrote instead measure-preserving maps, it's still a correct definition. And it's the same definition. And the fact that these two are equivalent goes through some measure theory, which I will not, I do not want to indulge in. OK. Great. But the moral of the story is you take two graphons and rearrange the vertices in some way, in the best way, overlay them on top of each other, and take the difference, and look at the cut norm. So that's the distance. All right. So I want to finish by stating the main theorems that form graph limit theory. And these address the questions I mentioned right before the break. So do there exist limits? And do these two different notions of one having to do with distance and another having to do with homomorphism densities, how do they relate to each other? Are they consistent? So the first theorem, theorem 1, has to do with the equivalence of convergence, namely, that if you have a sequence of graphs or graphons, the sequence is convergent in the sense up there, if and only if they are convergent in the sense of in this metric space. So remember what convergence means in the metric space is that of a Cauchy sequence. So if and only if it is a Cauchy sequence with respect to this cut distance. So just maybe for many of you, it's been a while since you took 18.100. So let me remind you, a Cauchy sequence, in this case, it means that if I look at the distance between two graphs, if I look far enough out, then I can contain the rest of the sequence in an arbitrarily small ball. So the sup positive m of this guy here goes to 0 as n goes to infinity. Because we don't know yet whether the limit exists. So I can't talk about them getting closer and closer to a limit. But they mutually get closer to each other. So theorem 1 tells us that these two notions, one having to do with homomorphism densities, is consistent and, in fact, equivalent to the appropriate notion in the metric space. So now we, OK, so let's use a symbol. So we say that g sub n converges to w. Or in the case of a sequence of graphons, so we can do that as well. So if, so here we say that g sub n converges to w. If whenever you look at the f density in g sub n, this sequence converges to the corresponding f density in w for every f. And similarly, if you have a graphon instead of a graph. So that definition was just whether a sequence is convergent. Here it converges to this graphon w. And the question is, if you give me a convergent sequence, is there a limit? Does it converge to some limit? And the answer is yes. And that's the second theorem, which tells us the existence of the limit object. So the statement is that every convergent sequence of graphs or graphons has a limit graphon. So now I want you to imagine this space of graphons. So I have this space containing all the graphons. And let me denote this space by this curly w 0. So this 0 is, don't worry about it. It's more just convention. But let me also put a tilde on top for the following reason. Let this be the space of graphons where we identify graphons with distance 0. So then this space combined with this metric is a metric space. It is the space of graphons. So the third theorem is that it's the compactness of the space of graphons, namely that this space is compact. Because we're in the metric space, compactness in the usual sense of every open cover has a finite sub-cover is equivalent to a slightly more intuitive notion of sequential compactness. Every sequence has a convergent subsequence. And then it's also you have a limit. So it converges to some limit. So how should you think of theorem 3? So it's about compactness and some topological notion. But intuitively, you should think of compactness as saying in the English word, the English meaning of the word compact is small. You should think of this space as being quite small, which is rather counterintuitive because we're looking at the space of graphons, certainly at least as large as the space of graphs, but in really all functions from the square to the interval. It seems like a pretty large space. But this theorem here says that, in fact, that space is quite small. And where have we also seen that philosophy before? So in Szemeredi's graph regularity lemma, the underlying philosophy there is that even though the possibilities, the space of possibilities for graph is quite large, once you apply Szemeredi's regularity lemma and once you are OK with some epsilon approximations, there's only a small description, this bounded description of a graph. And you can work with that description. And these two philosophies, it's no coincidence that they are consistent with each other because we will use Szemeredi's regularity lemma to prove this compactness. In fact, we will use a slightly weaker version of Szemeredi's regularity lemma to prove compactness. And then you'll see that from the compactness, one can use properties of the compactness to boost to a stronger version of regularity. But the underlying philosophy here is that this compactness is, in some sense, a qualitative reformulation, analytic reformulation of Szemeredi's graph regularity lemma. So this topic, this graph limits, which we'll explore for the next few lectures, including giving a proof of all three of these main theorems, nicely encapsulates the past couple of topics we've done. So on one hand, Szemeredi's regularity lemma, or some version of that, will be used in proving the existence of the limit and also the compactness. And also, it's philosophically, in some sense, related, very much equivalent, in some sense, and related to these notions. It is also related to quasi-randomness, in particular, quasi-random graphs that we did a few lectures ago. Well, in quasi-random graphs, we are really looking at the constant graphon in this language. And now, we expand our horizons. And instead of just looking at the constant graphon, we can now consider arbitrary graphons. They're this model for a very large graph. Any questions? Yeah. AUDIENCE 1 Can we prove theorem 3 analytically and deduce the regularity lemma? YUFEI ZHAO Can we prove theorem 3 analytically and deduce the regularity lemma? So you'll see once you see the proof. Depends on what you mean, but roughly, the answer is yes. But there's a very important caveat. It's that because we're using compactness, any argument involving compactness gives no quantitative bounds. So you will have a proof of the Szemeredi regularity lemma that tells you there is a bound for each epsilon, but it doesn't tell you what the bound is. AUDIENCE 1 Doesn't theorem 3 imply theorem 1? Because every code is doing this thing. YUFEI ZHAO Does code theorem 3 imply theorem 1? And the answer is no, because in theorem 1, the notion of convergence is about homomorphism densities. So theorem 1 is about these two different notions of convergence and that they are equivalent to each other. Theorem 3 is just about the metric. It's about the cut metric. So theorem 1 is the point of theorem 1 is that you have these two notions of convergence, one having to do with subgraph densities and the other having to do with a cut distance. And in fact, they are equivalent notions. OK, so all great questions. Any others? Yeah. AUDIENCE 2 For the f, is f a graphon? Does the f and lg n converge to w? Is f a graphon? YUFEI ZHAO Yeah, so the question is, is f a graph or a graphon? f is always a graph. So in TFW, I do not define this quantity for graphon f. So this quantity here, I've only allowed a second argument to be a graphon. The first argument is not allowed to be a graphon. It doesn't make sense. Yeah. AUDIENCE 3 Doesn't theorem 1 and theorem 2 together imply theorem 3? YUFEI ZHAO OK, question is, doesn't theorem 1 and theorem 2 together imply theorem 3? So first of all, theorem 1 is really, it's not about compactness. So it's really about the equivalence of two different notions of convergence. It's like you have two different metrics. I'm showing that these two metrics are equivalent to each other. Theorem 2 and theorem 3 are quite intimately related. So theorem 2 is about, I mean, theorem 2, OK, so they're quite related, but they're not quite the same. So let me just give you the real line analogy, going back to what we said in the beginning. So theorem 2 is kind of like saying that the real numbers is complete. Every convergence sequence has a limit. So whereas theorem 3 is more than that. It's also bounded in some sense. But here, if there's no notion of bounded, it's compact. But the main, I mean, you should think of these two are very much related to each other. But here, it's, but they're not equivalent. Anything else? OK, great. So that's all for today.