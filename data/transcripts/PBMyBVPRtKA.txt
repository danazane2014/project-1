 OK. So the name of the game today is compactness. And so last time, I recalled that the definition of a subset of a metric space X is compact if every sequence x in k, so every sequence of elements in k, has a subsequence converging in k, has a subsequence which converges to an element in k. And so, for example, by the Heine-Borel theorem, closed and bounded subsets of Rn or Cn are compact. This is from 18100. And in general, closed and bounded subsets of a Hilbert space are not necessarily compact. Last time, we looked at the closed unit ball in a Hilbert space or, let's say, in little l2. This is not compact because you can take the orthonormal basis vectors, which are the sequences that have 0 except for 1 and, say, the nth spot. These are all of unit length. And the sequence consisting of these basis vectors, which are sequences. So this sequence of sequences does not converge in little l2 or have a subsequence which converges in little l2. So we were looking for an additional condition that would ensure compactness. And as a warm-up to this, last time we proved the following that so we're always working in a Hilbert space. It may be separable or not, depending on if I say it is or not. That if we take a convergent sequence in H, then the conclusion is, one, that the set K consisting of all the elements of the sequence and the limit is compact. And the set K has what are called equi-small tails with respect to any orthonormal subset of H. If E sub K is a countable orthonormal subset of H, then the following holds. This condition we called K having equi-small tails, then for every epsilon positive, there exists a natural number n such that for all, let's call it, let me use a different letter V tilde in K, so either an element of the sequence or V, we have that the sum over K bigger than n squared is less than epsilon squared. So what this says in this condition we called K has equi-small tails with respect to this orthonormal subset. So of course, for any fixed V, because this entire sum, forget just K over n, is bounded by the norm of this V tilde squared by Bessel's inequality, we can always choose an n for any individual V tilde. Now equi-small tails means that I can choose this n independent of the element in K. So if you like uniformly small tails is another way you could think about it. But the terminology equi has been used instead. But another way is to think we can uniformly make the tails with respect to an orthonormal sequence or orthonormal subset of H small. Now what we're going to prove is that, in fact, this condition here of having equi-small tails with respect to an orthonormal basis on top of the set being closed and bounded suffice to prove that the subset is compact, and that's the following theorem. So let H be a separable Hilbert space. And let EK be an orthonormal basis of H. So I said suffice, but not just suffice. It's also necessary, as we'll show. When K, subset of H, is compact if and only if K is closed, bounded, and has equi-small tails with respect to this orthonormal basis. So in particular, for any orthonormal basis. So this is a complete description of compact subsets of a Hilbert space. It's those subsets which are closed, bounded, and have equi-small tails with respect to one orthonormal basis. And what this theorem also says is that if it has equi-small tails with respect to one orthonormal basis and is closed and bounded, then the same holds with respect to any orthonormal basis. So let's prove. So it's a two-way street. One well, it doesn't make sense to say one street is shorter. One side of the street is shorter. Maybe wider, easier to use. But let's prove that if K is compact, then it implies all of this. So suppose K is compact and K is closed and bounded by general metric space theory. So look this up in the 18.100s material or intro to analysis material. So this also gives us an opportunity to understand this equi-small tail condition as well by having to negate it. So we're also going to prove that K has equi-small tails by contradiction. So suppose K does not have equi-small tails with respect to this basis or this orthonormal basis. Then, what's that mean? So this is a for all there exists in statement. So then there exists some bad epsilon so that for all n natural number, there exists a u n in K such that so these n's are increasing such that u, so we have the negation of the condition that satisfied K bigger than n of u n e K squared is bigger than or equal to this epsilon 0 squared. Now this, so note that this is a sequence in this compact set K. And so by the defining property of a compact set, it must have a convergent subsequence, which implies there exists subsequence, which you usually denote u sub n, capital N sub m, some other letter, but I'll just call it to make it a little connection back to what we wrote a minute ago v sub n. So this is a subsequence of u sub capital N and v in K such that v sub n converges to v. Now then for all n natural number, we have that sum over K bigger than n of v n e K squared is bigger than epsilon 0 squared, right? Because this is a subsequence of the original sequence, which satisfied this condition. All right, now this would actually be capital N sub n, but if I throw in some more, capital N sub n would be increasing sequence of numbers, and it's always bigger than n, so I can replace that capital N sub n with this. All right, so this is fine. And that says the subset consisting of the elements v sub n in a natural number union v does not have equi-small tails, which is a contradiction to the theorem we proved last time. This contradicts the previous theorem that I stated and we proved last time, right? So any convergent sequence along with its limit is a compact set and has equi-small tails. So that's a contradiction. And what was the assumption that brought us to this road forking itself is the assumption that K is not compact. So therefore, K has equi-small tails with respect to this orthonormal basis. So that's one direction, that if K is compact, then it has to have equi-small tails with respect to the orthonormal basis. We didn't use anything about it being a basis, so this also says that if you have a compact set, then with respect to any orthonormal subset, it has equi-small tails with respect to that orthonormal subset. So now let's use this condition that K is closed and bounded and has equi-small tails to prove that K is compact. So suppose K is closed, bounded, and has equi-small tails with respect to this orthonormal basis. Now let u n be a sequence in K. So we want to show that it has a subsequence which converges to an element in K. Now K is closed, which means that if I take any sequence which converges in K, or any sequence of elements in K that converges, the limit has to be in K. That's the definition of a set being closed. So I just need to show that this sequence, in fact, has a convergent subsequence. That's all I have to show. So since K is closed, we just need to show u n has a convergent subsequence. OK, so the idea is to use what we know about bounded subsets of complex numbers. So we know about bounded subsets of complex numbers. If I have a bounded sequence, I shouldn't say subsets, but if I have a bounded sequence of complex numbers, it has a convergent subsequence. This is a Bolzano-Weierstrass. Usually it's stated in terms of subsets of R. But sequences of complex numbers converge if and only if the real and imaginary part converge, and are bounded if and only if the real and imaginary part, which are sequences of real numbers, are bounded. So Bolzano-Weierstrass immediately implies a similar statement for complex numbers, that every bounded sequence of complex numbers has a convergent subsequence. OK, so how would we use that? What we're going to do is we're going to expand u n's, and this is where we'll use that this is an orthonormal basis, in terms of this orthonormal basis. So for each K, we'll have a sequence of complex numbers given by the K-th entry of this sequence. And that will be a bounded sequence of complex numbers, because we're assuming K is bounded, and therefore this sequence is bounded in K. So what we're going to end up having is, entry by entry, a bounded sequence of complex numbers, where these complex numbers represent the K-th entry of this sequence of vectors. And we're going to then take a subsequence that converges for the first entry. We're going to take a subsequence of that subsequence to get a subsequence which converges for the first and second entry, and then so on and so on, a diagonalization argument. And that subsequence will show converges. Now, it's not that simple, because if you can do something 10 times, it doesn't mean you can get some sort of uniform control. We just have control for any finite number of entries. What saves us in the end is this equi-small-tail condition, which allows us to basically throw away infinitely many entries. What this says is that you can always choose an end so that the tail end of the entries don't matter. And so that, with having control over the first large block of entries, will be enough to conclude. So that was the roadmap. If you didn't follow the roadmap, fine. Hopefully, you follow the proof. But it's a diagonal argument, and those are relatively easy to understand, somewhat difficult to write down. So I will do my best. But OK, so since K is bounded, that implies there exists a C, non-negative, such that for all n, norm of u sub n is less than or equal to C. The C is independent of n. It just depends on the set K. And therefore, this says for all K and n, I should say, for all K, for all n, if I look at the Kth, let me call this the Fourier coefficient in this basis of these vector, in this orthonormal basis of EK. If I look at the Kth Fourier coefficient, this is bounded by Cauchy-Schwarz, u n, EK. And these are orthonormal, so this has unit length. This is bounded by C. So for each fixed K, this is a sequence in n of complex numbers, which is bounded. The sequence consisting of the Kth Fourier coefficient of the elements u sub n, so n is the thing that's changing, is a bounded sequence of complex numbers. So now this is where we're going to start choosing subsequences of subsequences so that we get convergence along the entries. So since u sub is bounded, this implies that there exists, by Bolzano-Weierstrass, a subsequence, which I'll denote u sub n sub 1 of K. Let me not use K. Let's use, what do I use here? J. E1, so 1, 1, in which converges in C. OK. All right. OK, so I now have a subsequence of the original sequence of the u n's so that the first entry converges here. This sequence converges. Now I'm going to take, OK, so since u n1 of J E2, this is still a bounded sequence by what I showed here. So bounded sequence, this implies there exists a subsequence. Now I'll call this subsequence u n2 of J E2 J. So n2 of J, this is a sequence of integers that is a subset of these guys that's increasing, of u n1 of J, so I should say, which converges. OK, so now note this sequence converges, but the n2 of J, so this is a subsequence of integers. So this is a subsequence of integers of the n1 of J's. So since the n2 of J's are a subsequence of the n1 of J's, we get that limit J goes to infinity of u n1 of J, I should say n2 of J, E1. So this is a subsequence of this sequence. So the n2 of J's is just a subsequence of the n1 of J's. So since this sequence in J converges, this subsequence also converges. But now we've chosen the two so that we also have not just convergence along the first entry, but convergence along the second entry. But now you see what, so this is how the argument goes, that now I will take a subsequence of the n2 of J's so that the subsequence converges when I pair it with E3, and then I take a subsequence of that subsequence so that I get E4 and so on and so on. And all of these subsequences are subsequences of, or the subsequences of integers are subsequences of the previous set of integers, which are, again, a subsequence of the original integers n. So then what do we get? We get that for all l, there exists a subsequence of integers in l of J of the previous set of integers in, let's say, l minus 1 of J, such that for all k between 1 and l, limit as J goes to infinity of u in l of J, Bk exists. OK? So I hope this is clear. So you just take a subsequence of the original sequence so that the first entry converges, take a subsequence of that so that the second entry converges, and so on and so on. So then you have this kind of nested subsequences so that at each fixed l, you have convergence of the first l entries. And so what you now do is that you pick, so let me write, or pick Vl to be the diagonal along this sequence. So u in l of l for l equals 1, 2, 3, and so on. Then what we get is since the l-th subsequence converges, or the k-th entry for 1 between, OK, so let me back up and say this one more time. Since for fixed l here, I have convergence of the first l entries, by picking along the diagonal, I get a subsequence of u in such that for all k, I have convergence for the k-th entry as l goes to infinity. Converges. Converges. OK? All right, so I can take subsequences of subsequences and obtain a subsequence in the end so that for this subsequence of the original sequence u n's, I have convergence of the entries, entry by entry, or I have convergence of the Fourier coefficients as l goes to infinity. And now we're going to use this equi-small-tails to prove that the sequence v sub l converges. We're just actually going to show it's Cauchy. Since H is a Hilbert space, we then conclude that it must converge. So claim v l is Cauchy. And then that will conclude the proof, because then it must converge in H. And by what I said at the beginning, since this is a sequence of elements in k, which is a closed set, the limit has to be in k. So we just need to show it's Cauchy, and then the proof is done. So let epsilon be positive. So since k has equi-small-tails, let's go over here. There exists a natural number, capital N, so that for all l, we have that the sum of squares k bigger than N of the tails, v l v k squared, is less than epsilon squared over 16. Why 16? Because I want everything to come out correct in the end. So since the N sequences given by the first N entries of the sequence v l, so since the N sequences v l v 1, l up to v l v N, l. So these are only, so this is the bit where I said, equi-small-tails allows us to throw away kind of a large or the tail end of these entries and just control over the first finitely many entries gives us the condition or the convergence that we want. This is where that's happening. So since these converge, there exists a natural number m, so that for all l m bigger than capital M, I have that the sum n equals 1 to capital N of v l e k minus v m e k squared is less than epsilon squared over 4. So each of these are convergent sequences, so each of these are Cauchy sequences. So for each one, I can find a capital M sub 1 so that just one of these entries is very small for l and m bigger than that capital M sub 1. And then I can do the next one so that for, so this should be k. I don't know what that looks like, but k. So that for k equals 2, it's very small, and then up to k equals N. And then I choose capital M to be the maximum over all those m sub n's. So I can always choose this because, again, there's only finitely many conditions. I have to verify. So now I claim this capital M works. Then for all l m bigger than or equal to m, if we look at the norm of v l minus v m, so this is equal to sum, should say, k equals 1 to N of v l minus v m. So by k equals k bigger than N v l minus v m e k 1 1. So here I'm using the fact that we have an orthonormal basis. Then in that case, the norm is equal to the sum of the squares of the Fourier coefficients. Bessel's inequality would say this is less than or equal to this side and wouldn't give us any control we want over the actual norm. But because we're working in an orthonormal basis, the norm of that is equal to the sum of squares, take the square root. And so now the square root of a plus b, that's always less than or equal to the square root of a plus the square root of b. So that's less than k equals 1 to N of v l minus v m squared 1 half plus k bigger than N v l e k. So I'm just going to write this out a little differently. v m e k 1 half. Now this part we know is less than, by how we've chosen capital M, this finite part is small. It's less than epsilon over 2. So that's less than epsilon over 2 plus, now we use the triangle inequality now for little l 2. I can think of this as the sum in little l 2 of, or I can think of this as the norm in little l 2 of this sequence in k minus this sequence in k. So this is less than or equal to k bigger than N v l e k 1 half plus some k bigger than v m e k 1 half. All right, we're almost done. So this is less than, this square part is less than epsilon squared over 16. So taking the square root of that, I get epsilon over 4 by how we chose N. So this is based on how we chose N, plus which came from the actually small tails part. And this part also, because of N, is less than epsilon over 4 equals epsilon. So we've now shown the claim that the subsequence v sub l is Cauchy, and therefore it converges since H is complete. OK, now, so for example, one can readily verify now that the following subset, let k be the set of all sequences in little l 2 with the property that a k is less than or equal to 2 to the minus k. So this is not a subspace. It's a subset. It's compact. This subset is known as the Hilbert cube. All right, now, maybe it's a bit unwieldy that our condition for being compact is phrased in terms of an orthonormal basis. Maybe that's not so simple to verify, or definitely not necessarily kind of, what's the word I'm looking for? Canonical, I guess, that's the word I'm looking for, in the sense that we have to make a choice to verify compactness. But a different way of characterizing compact subsets of a Hilbert space is the following. Now, I'm not going to give the proof of this. You can look it up in Melrose's notes. Maybe I'll say a word about why you should believe or think it's true. But again, it involves a diagonal argument, which that was tough enough to do here, or at least painful enough to write out. And so I don't want to do it again. So we have the following that, and this theorem also holds in a non-separable Hilbert space, basically by a trick of reducing it to a separable Hilbert space. But that's OK. A subset K of a Hilbert space H is compact if and only if K is closed, bounded. And the following condition holds, which you can think of as K can be approximated by finite dimensional subspaces. OK? And for all epsilon positive, there exists a finite dimensional subspace W in K such that for all u in K, if I look at the distance from u to this finite dimensional subspace, which I think I said at one point, maybe I didn't, but a finite dimensional subspace is always a closed subset subspace in H. A finite dimensional subspace is always closed. But if I look at the distance from this point to this subspace, this finite dimensional subspace, this is less than epsilon. So a subset in a Hilbert space is compact if and only if it's closed, bounded, and if you like, can be approximated by compact subsets of finite dimensional subspaces. OK, another way to think about it. Now, why is this kind of believable in the first place? Well, what this, where is it? What the equismal tail condition says is basically that all elements in K can be approximated by, if you let epsilon be positive, can be approximated by the subspace consisting of the span of the first one up to n basis vectors or orthonormal basis vectors. That's what this is. For all epsilon, the tails are small. Another way of saying that is that for all epsilon, you can approximate any element in the set K in the span of the first n basis vectors. So K has to be close to a finite dimensional subspace. But one can go the opposite direction as well, showing that finite dimensional subspace, being able to be approximated by those guys implies that K is compact. And it's another kind of diagonal argument that I just don't want to write out and rather just start using these conditions instead to start saying things about when we can solve certain equations and certain interesting operators that arise quite naturally. So let's start talking about certain classes of operators. And we should start with the simplest, which are finite rank operators. So the only linear operators you came into this class knowing, probably, if you didn't and you were more sophisticated, that's good, but were matrices. Now, a matrix is just a linear transformation defined in terms of an orthonormal, or not orthonormal basis, but a basis in both the domain and target. If we pick the target to be the domain, we just fix a basis. And then we can express a linear transformation as an array of numbers. Finite rank operators are the generalization of what you think of matrices now to Hilbert spaces. And we'll see that in just a minute. So and what follows H is a Hilbert space. And instead of writing the space of bounded linear operators BHH, I'm just going to concatenate this by dropping one of those H's. So the space of bounded linear operators from H to itself will be denoted by B of H. So finite rank operators, these are, like I said, these will be the analog of matrices. But let me give a sort of invariant definition first. So a bounded linear operator is a finite rank operator if the range of T, which is a subspace of H, is finite dimensional. So for example, let me give you maybe a simple, well, of course, the zero operator is a simple one. If H is a finite dimensional Hilbert space, i.e. Cn, then every bounded linear operator is a finite rank operator because the range is always contained in Cn, which is finite dimensional. For example, on little l2, if I define T a to be now the sequence given by a1 over 1, a2 over 2, up to a n over n for some n, and then 0 afterwards, and this is for a equals sequence in little l2, then T is a finite rank operator because the range of this operator, so here n is a fixed number, acting on sequences in little l2 just spits out the first, you know, let's make this even more explicit. Let's say that's 5. This is a finite rank operator because it's contained in the subspace consisting of those sequences which are 0 after the fifth entry. And that's finite dimensional. A basis is given by the sequence consisting of 0's with a 1 here, 0's with a 1 here, 0's with a 1 here, and so on. So this is a finite rank operator. And so let me just put over here that as far as notation goes, we write T is in R, H, R for rank, finite rank. All right, so R of H, this is the set of all finite rank operators. Now, it's not just a set. It's easy to see that this is a subspace of the Banach space consisting of all bounded linear operators from H to itself. Why is that? Well, OK, so if I take a operator that has finite dimensional range and multiply it by a scalar, then the range of the scalar multiple of that operator is equal to the range of the original operator unless it's the scalar 0. So that will be finite dimensional. On the other hand, if I take two finite rank operators and I consider their sum, the range of the sum is going to be contained in the direct sum of the ranges. And the direct sum of two finite dimensional subspaces is, again, finite dimensional. So if one has dimension 5 and the other has dimension 6, then the direct sum of these two will be of dimension 11. So that is me talking you through the proof that this is a subspace. So take a minute to write it down. But now let me prove that these finite rank operators really are essentially matrices. OK, so I have the following theorem characterizing finite rank operators. So T is a finite rank operator if and only if there exists an orthonormal, finite orthonormal subset, EK, K equals 1 to some integer l, and complex numbers Cij, ij equals 1 to l. So this is a subset of complex numbers such that if I want to compute what T applied to u is, it's just going to be some i equals j, ij goes from 1 to l of these numbers Cij times the jth entry of u times Ei. So in other words, T corresponds to a matrix where the coefficients are these C's, basically, acting on this finite dimensional subspace of EK's. OK, so let's give the proof of this. One direction is immediate. If T has such a representation, then it has finite rank, because then the image is contained in the span of the Ei's for i going from 1 to l. And that's a finite dimensional subspace. So this is clear, that this condition implies that T is a finite rank operator. So let's go the opposite direction. So since the range of T is finite dimensional, we can find an orthonormal basis. Call it EK, k equals 1 to n, such that T applied to u is equal to, so it's an element in the span of, so this spans the range. So I can write this as some numbers times the EK's. And what are these numbers? Well, these numbers, since these are orthonormal, have to be T applied to u inner product EK. So Tu, so the range of span by these orthonormal vectors and anything in the span of this orthonormal vectors has to be written as that thing, inner product EK times EK. But let me now make an observation that this thing right here, we can write as using the adjoint as u inner product T star EK. The adjoint operator, so T star, which goes from H to H, remember satisfies this property that given any two vectors, A or u and v, T applied to u inner product v is equal to u inner product T star v. That's the defining property of the adjoint. So I can write it like this, which is k equals 1 to n of u inner product, call this vk, EK bar, where here vk is simply defined to be T star EK. Now, let E1 up to EL be the orthonormal subset of H obtained by applying the Gram-Schmidt process to vectors E1, E2, up to EL, v1, up to vL. So applying it to this subset, I keep the E1 up to ELs, and then I might pick up some more from the some more orthonormal vectors that are orthonormal to these guys when I hit the v1's up to vL. Or I should have gone to n, I'm sorry. So not l here, but n. OK. OK. So I obtain an orthonormal subset that spans the same span of these guys. And so what does that mean? Then there exists numbers AKJ, BKJ, let's call it I, such that EK bar is in the span of these orthonormal vectors. So I can write it as AKI EI, K equals 1 to L. No, no, no. Sum's going the wrong way. I equals 1 to L. And also the same for the Vs. Sum, yeah, VK equals J equals 1 to L, BKJ, EJ. So these are obtained by the Gram-Schmidt process applied to this collection of vectors. And therefore, I can write each one of these vectors as a linear combination of these orthonormal vectors. So that's all I'm saying here. Now we just plug this in to this computation. And we have TU, which we computed was equal to sum from K equals 1 to n of U inner product VK, EK bar. This is equal to, now if I stick in these sums here, I get sum IJ equals 1 to L of sum K equals 1 to n of AKI, BKJ, complex conjugate. And now U inner product EJ. This is coming from when I stick in the VKs, EI. And this is coming from when I stick in what I have for the EK bar. And this number right here is my number CIJ. OK? And that's the end. So finite rank operators are really just matrices. You can just compute these numbers and they completely characterize the linear operator. So in particular, the kernel, the null space of T, is the subspace that's orthogonal to the E1 up to EL, or at least includes that. OK. Now from this, we can conclude a nice property about finite rank operators. Namely, the first is that if T is a finite rank operator, then its adjoint is also a finite rank operator. OK? And if T is a finite rank operator, and A and B are just bounded linear operators on H, then A times T times B, this is also a finite rank operator. OK? In fancy, or in short form, you would say that this is a star closed two-sided ideal in the space of bounded linear operators. Or it's closed undertaking adjoints and closed undertaking the two-sided multiplication by bounded linear operators. OK? So two I'll leave as an exercise. You know, what's the point? The T has finite dimensional range. A then acts on that finite dimensional range. And therefore, its range, the range of A hitting T, must then be finite dimensional. And what you do with B really doesn't matter. OK? So two is an exercise. So let's prove one. One we'll prove using this characterization of finite rank operators. So we're assuming T is finite rank. So we can write T as some constant cij, u inner product ej, ei, u and h. So we can express it in that way, where these are just some fixed constants not depending on u. And now let's compute how T star acts on vectors by using this defining property of the adjoint. OK? Then u T star v, what we're going to do is we're going to come up with an expression for this as u inner product something, from which we can conclude that T star v has to equal that something for all v. Now, this is equal to, by the defining property of the adjoint T applied to u, v. So this is equal to i and j will be understood to go from 1 to l. So this is just a finite sum. cij, u, ej, inner product v. Now, taking the inner product is linear in the first entry. So this is equal to sum ij. Again, this is a finite sum, ij, u, ej, inner product ei, v. And now I'm just going to switch what I'm summing. This I can write as now u inner product sum ij, cij bar. So if I want to, this I can write as bar, bar. But ei, v, complex conjugate, ej. We can check this is correct because, OK, so u will then inner product with each of the ej's. And these constants, when they come out of the inner product, get hit by a complex conjugate, which turns them back into cij and ei, v. Now, the complex conjugate of the inner product is just the same as if I flip the entry. So this is equal to cij times v, ei, ej. Now, this holds for all u and v. So what have I shown? I've shown that u t star v minus this finite sum. So I'll actually put it in here. cij v ei ej. This equals 0 for all u, for all v. Now, for a fixed v, this quantity here is orthogonal to everything in H. So it has to be 0. And therefore, I conclude that t star v has to equal sum from ij equals 1 to L. The complex conjugates of cij v inner product ei, ej for all v and H. And that proves that t star is a finite rank. And also gives you the expression for the coefficients in terms of the old ones. I could write this now, if I wanted to write it in terms of the i being in front of the basis vector that I'm having here. I write it as sum ij. So we see that the matrix corresponding to the adjoint is what we computed last class, basically, is the complex conjugate of the transpose of this matrix, of the original matrix cij. All right, so that proves adjoints are also finite rank. Now, so taking the adjoint of a finite rank operator leaves you in the space of finite rank operators. If you take finite rank operators and compose them on the left and on the right with bounded operators, you remain finite rank. The subset of finite rank operators also form a subspace in the Banach space of bounded linear operators on a Hilbert space. So the space of bounded linear operators on a Hilbert space naturally come with a norm as well, the operator norm. So the next obvious question is, is the subspace of finite rank operators, is this closed? Closed, meaning not as it closed under taking linear combinations. Of course, that holds for every subspace. But if I take a sequence of finite rank operators that are converging in operator norm to something, then is the limit also a finite rank operator? So that's the definition of being a closed subspace, or let me say subset, so that we're now asking about this in a metric sense. Is it a closed subset in the space of bounded linear operators? Now, the answer to this is no. Why? Let's, for example, take Tn. So this will be a sequence of operators from little l2 to little l2. And for an element a in little l2, this should give me another sequence in little l2. And it'll be the sequence given by a1 over 1, a2 over 2, up to an over n, and then 0s after that. So T1, the operator T1, takes a sequence in little l2 and just spits out the sequence that has the first entry divided by 1 and 0s followed after that. That's the operator T1. T2 takes the sequence in little l2 and spits out the first entry divided by 1, the second entry divided by 2. And 0s after that, T3, so on. Now, this is just a picture. This is formal. This is not meant to mean anything. If I were to express this as an infinite matrix times an infinite vector or infinite length vector, where the infinite length is the sequence a k's, this is 1, 1 half, 1 third, 1 over n, 0, 0, 0, 0. This infinite matrix times a1, a2, a3, and so on. So I'm just multiplying a1 by 1, a2 by half, a3 by a third, and then up to an over n, multiplying an by 1 over n. And then the rest of the entries get spat out to give me 0. So these are the finite rank operators. And you can check that Tn minus T in operator norm goes to 0, where what is T? T is the operator that takes a and spits out. So a will be an element in little l2. Now, as n goes to infinity, you can guess what's going to happen is I end up multiplying everything by 1 over wherever entry I'm at, a1 over 1, a2 over 2, a3 over 3, a4 over 4, and so on and on. So as an exercise, you can show that the operator norm of T minus Tn, this is less than or equal to 1 over n plus 1, something like that. OK. Now, this operator here does not have finite rank, because then I can find infinitely many linearly independent vectors in the range. For example, T of e1, where this here is the basis vector, gives me the first basis vector in little l2. If this is the second basis vector in little l2, this is just and so on. So if I applied T to en, this is 1 over n, 0, 0, and so on. This is in the nth spot. So it just takes each basis vector. And so if you like, this is e1. This is 1 half e2. This is 1 over n en, and multiplies each of these basis vectors by 1 over the integer that's not delineating, denoting them, sorry. All right. It's the end of the day, so some of my thinking has slowed down. OK, so finite rank operators is not a closed subset of the space of bounded linear operators. So what's the closure? And finite rank operators, we like to think we know how to solve equations for, right? I mean, just based on this expression for a finite rank operator, well, if we want to solve Tu equals, say, v, well, first we know v has to be in the range of, has to be in the range of, or in the span of these basis vectors that appear here. And now once we've restricted to that, then we can solve the equation by looking at the properties of this matrix. And the null space will be not only the null space that sits inside the span of these basis vectors, but it will also include the orthogonal complement in h of these basis vectors. So solving equations involving finite rank operators ought to be fairly straightforward if we remember linear algebra. Unfortunately, they're not closed upon taking subsequential limits. But we can identify the closure of, so now the question is, what is the closure of finite rank operators in the space of bounded linear operators on h? So something in the closure has to be a limit of finite rank operators. And since we know how to solve equations involving finite rank operators, hopefully we know how to solve equations with these operators as well. Is there a simple way we can characterize, or not simple, but at least a more useful way to characterize them than just being the limit of finite rank operators? And this is, so the answer to this question is, so I won't even write. OK, I'll write answer is what are called compact operators, so the subspace of compact operators. So let me make a definition. So K, a bounded linear operator, is compact operator if the closure of the image of the closed unit ball in h, so the closure of the image by K, the closure of the image of the closed unit ball is compact. And I can also write this as a closure of the subset KU, normal of U less than or equal to 1. So I won't have time to prove this theorem in this class, but we'll do it next time. And we'll actually show that, in fact, this class of compact operators, which are defined in this way, this is a compact operator according to this definition if and only if K is in the closure of finite rank operators, meaning there exists a sequence of finite rank operators converging in operator norm to K. And again, why are we interested in compact operators? Well, we know how to solve finite rank or equations involving finite rank operators. We just look at the subspace that contains the range. And then we look at the matrix that appears there in that decomposition. Compact operators, they are something completely new when we move from finite dimensional linear algebra now to functional analysis because these are not exactly necessarily equal to finite rank operators. I mean, we just did this example a minute ago of this operator given here by where I take the entry of the sequence and divide it by the place that it occurred in the sequence for a sequence given in L2. So if you believe the theorem, this is a finite rank. I mean, not finite. This is a compact operator. It's the limit of the sequence of finite rank operators. Perhaps you want to take a try at trying to prove this is a compact operator directly from the definition. But or not, you can wait till next time and just use this theorem. But many other operators, many interesting operators, for example, inverses of differential operators, in fact, turn out to be compact operators. That's also why we're interested in them. So we got these by taking the closure of finite rank operators. And then after, so now we have this kind of interesting subspace of operators that we hope to be able to solve equations involving. And that'll be basically what takes up the remainder of our time in this course. So just staring at this definition, you can kind of see that over it. Well, you can see that if k is a finite rank operator, then it's a compact operator. So just to, we're not going to prove this theorem like I said, but just to see or do a sanity check that finite rank operators ought to be compact operators. Well, a finite rank operator, the image of this closed unit ball by a finite rank operator, that's going to be a bounded subset of a finite dimensional subspace. Right? Any bounded operator takes bounded sets to bounded sets. So if it's finite rank, it takes this bounded set to a bounded subset of a finite dimensional subspace of h. And then I take the closure, I get a bounded and closed subset of a finite dimensional subspace. And we know by Heine-Borel that closed and bounded subsets of finite dimensional subspaces are compact. So that shows why at the very minimum, you should believe that finite rank operators are compact. OK, so we'll stop there.