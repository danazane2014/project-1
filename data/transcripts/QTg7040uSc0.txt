 So, today, we're going to apply functional analysis to a simple Dirichlet problem, i.e., an ordinary differential equation, on a line with conditions at the boundary. Typically, when you encounter ODEs for the first time, say, in your ordinary differential equations class, you always have an equation you want to solve, and then you specify maybe the function at a point and its first derivative, if it's a second-order derivative or just a second-order differential equation or just a function evaluated at that point. But now, in this Dirichlet problem, you're going to be specifying the function at two endpoints. So, what's the problem we're going to look at? Let V be a continuous real-valued function, and we will consider what we'll call the Dirichlet problem, minus u double prime of x plus u of x equals f of x, x in 0, 1. And with boundary conditions, u of 0 equals 0, u of 1 equals 0. So, what do we want to do with this? So, the question is, given, so what's the input? You may think of f as being kind of a force. So, you're given a force, and you would like to compute the solution or show there exists a solution on the interval 0, 1, satisfying the equation. And we'll say a classical solution, meaning it's twice continuously differentiable, so this equation makes sense at every x. So, given f, a continuous function, does there exist a unique solution, u of x, and c2, 0, 1. So, the 2, c2 meaning two continuous derivatives. Okay, c2 means two continuous derivatives. So, does there exist a unique solution to the Dirichlet problem here? Okay, does there exist a unique solution to this ODE with these boundary conditions at 0 and 1? And the purpose of this section is to apply functional analysis that we've done to say the answer is yes if b is non-negative. Okay? If b is not, if b is not necessarily non-negative, then the answer is it depends on f. Okay? All right, but given any f, does there exist a unique solution to the Dirichlet problem when b is not negative? The answer is yes. Okay, and this is what we're going to prove. All right? This is the main goal of this section. All right? So, and I'll state, you know, kind of this answer as two theorems, one saying that there exists a unique solution and the other will be, or I should say that a solution to this is unique. And then the second part, the more involved part will be that there does exist at least one solution to this problem, assuming b is non-negative. Okay? Okay, so the first thing we'll prove is that any solution to the Dirichlet problem is unique. Okay? f is a continuous function on 0, 1. u1, u2 are twice continuously differentiable functions that solve, that satisfy the Dirichlet problem. I should say we are now working under the assumption that b is non-negative, so I should also put that in here. Suppose b is non-negative. We won't always have to assume b is non-negative in some of these theorems that we'll prove about this problem or about certain operators, so it would be good if I specify when I'm assuming b is non-negative. So, suppose b is non-negative and f is a continuous function, and I have two solutions to the Dirichlet problem, then they must be the same. Okay? Now, the proof, I think I've given this as an assignment, as a problem in an assignment in past classes, but when in doubt, integrate by parts. Okay? So, by subtracting, let mu be 1 minus mu2, then u is a twice continuously differentiable function which satisfies u of x plus v of x times u of x equals 0, and u of 0 equals u of 1 equals u of 1. Okay? Now, what we'd like to conclude is that u is 0, and how we're going to do this is we're going to integrate by parts, so then, or multiply this equation by something and integrate by parts. We have 0 is equal to the integral from 0 to 1 of minus mu double prime of x plus v of x, u of x times the complex conjugate of u of x dx. Now, this is equal to the integral from 0 to 1, u prime of x times u of x plus, the integral from 0 to 1, v of x times the value of u of x squared. Okay? And now, we integrate by parts. So, integration by parts says that if I have a derivative on a function, I can move it to the other function, and then plus boundary terms, and changing this minus sign to a plus sign. So, this is equal to u prime of x, u bar of x, evaluated at 0 of 1, minus, with this minus gives me a plus, integral from 0 to 1, u prime of x, derivative of complex conjugate of u, dx plus u of x, u of x squared dx. Okay? Now, we use the boundary conditions that at 0 and at 1, u of x is 0, so its complex conjugate is 0, so these go away. And what I'm left with is integral from 0 to 1, and I'm not going to keep writing the x part. So, integral from 0 to 1 of u prime squared, plus integral from 0 to 1, v times u prime, or times the magnitude of u squared. Now, since v is non-negative, v times the absolute value of u squared is non-negative, so this is certainly bigger than 0, 1, u prime squared. Okay? And remember, what I started off with was 0. So, I showed 0 is bigger than or equal to this non-negative quantity. Now, u is twice continuously differentiable, so that u prime, that's a continuous function, non-negative continuous function, so I get that, since this equals 0, I get that u prime is identically 0, which implies u is constant, and because u is constant, and u of 0 equals 0, I get that u is identically 0, 0 prime, which is what I wanted to show. u equals 0 means u1 equals u2. So, that's uniqueness. That's half of what we wanted to show, which was if v is non-negative, then there exists a unique solution to the Dirichlet problem, given any f, any input on the right-hand side. Now, we're going to turn to unique existence, which is a more interesting and harder part. So, now we're going on to existence. And, you know, if you don't know how to do something, try and do something easier. So, let's start off with the v equals 0 case. Okay, so we'll look at... Okay. So, we have the following theorem that says we can uniquely solve the Dirichlet problem when v equals 0. That just means we want to solve minus u double prime equals f of x with these boundary conditions, and we can actually write down explicitly the solution in terms of f as a compact self-adjoint operator, which I've alluded to maybe a couple of points during the class. Let k, x, y be the function given by x minus 1 times y, where if y is less than or equal to x, it's less than or equal to 1. And now switch. Okay, so it's piecewise defined across the diagonal, and it's continuous across the diagonal when y equals x. I get the same thing. So, this thing is, in fact, is a continuous function on 0, 1, cross 0, 1. Okay. We have this function here. Define an operator A f of x to be integral k of x, y, f of y, u y. And A is a bounded linear operator on L2. And, in fact, it's a self-adjoint compact operator. Compact self-adjoint operator. And, basically, A times f is the inverse of u double prime, where A times f solves the Dirichlet problem. f is c 0, 1. And u equals A f is the unique solution to the Dirichlet problem with v equals 0. I'm not writing out the argument of x, but it should be understood u of 0 equals u of 1 equals 0. Okay, so, with v equals 0, we can write down the explicit solution in terms of this, what's called an integral operator, because you take a function, multiply it by another function, and integrate. This thing here is usually referred to as the Green's function for this differential operator. And what this theorem says is that the solution to your differential equation, your Dirichlet problem, is given by an integral operator, which is a compact self-adjoint operator on L2. So, it shouldn't come too much of a surprise that the solution to a Dirichlet problem, u, can be written as an integral operator, right? Because by the fundamental theorem of calculus, integration and differentiation are inverses of each other. Okay, so... Okay, so, let's let c be the sup over 0, 1, plus 0, 1 of the absolute value of k. So, I'm not writing x, y, but, so, this is the supremum of k over the square of 0, 1, cross 0, 1, which is finite, since k is continuous. And by the Cauchy-Schwarz inequality, we get that a times f of x, a f of x is equal to the integral from 0, 1, a x, y, f of y, dy. It's less than or equal to, now, if I bring the absolute value inside, and then bound k by c, my name's Casey, is bounded by c times the integral from 0, 1, of f of y, dy. And now I apply Cauchy-Schwarz to this. This I can think of as this quantity times 1, so this is less than or equal to c times integral 0, 1, 1 squared, raised to the 1 half power. At certain points, I will stop writing of x, dx, or of y, dy, and the meaning should be clear, integral 0, 1, s squared, 1 half, and I get c times the L2 norm of f. So what have I done? I've bounded for every x in 0, 1, a f of x by constant times f in L2. I also have... If I look at a f of x minus a f of z, so I can just, again, by applying... bringing the absolute value inside the integral, bound it, and so the difference here is going to be k of x, y, minus k of x, z, times f of y, and integrate it. So what I get is that this is less than or equal to sum y, 0, 1, k of x, y, minus k of z, y, times the L2 norm of f. So I'm not going to give all the details because I've actually assigned this problem before that if k is a continuous function, then this is a bounded compact operator on L2. So maybe you haven't done the exercises yet, but in any case, I'll just sketch out from here that just based on these two estimates, and the Arzela-Skoli theorem, which tells you when... which gives you sufficient conditions that a sequence of continuous functions has a convergent subsequence in the space of continuous functions, you can conclude that A is a compact operator on L2. Okay? Okay, now, so that shows A is a compact operator on L2. y is itself a joint. f, g, the continuous functions on 0, 1. Then if I look at A times f paired with g, this is in the L2 pairing, this is equal to a number of 0, 1, integral 0, 1, k of x, y, f of y, dy, v of x, dx. Now, I have a double integral involving nothing but continuous functions, so I can apply Fabini's theorem to interchange the integration, right? So this becomes 0, 1, integral 0, 1. I can write it this way, f of y. So I haven't done anything yet, I'm just distributing, bringing this gx inside, and stating that this integral is equal to this double integral, dy, dx. I guess I'm using Fubini's theorem there. And Fubini's theorem says, if you're integrating continuous functions over, you know, a box, then it doesn't matter if you integrate dy, dx first, or dx, dy, or dy first or dx first. So then I can just undo this and, oh, the pairing should have a complex conjugate over g. And so if I just integrate dx first and kind of undo things, I can write this as the integral of f of y times the integral from 0 to 1 of k of x, y, v of x, dx. Now, complex conjugate over k, all of this complex conjugate, dy. Okay? Again, you can check that this is equal to this previous thing by Fubini, because then this iterated integral becomes just the integral dx, dy, which again, by Fubini, doesn't matter what the order is. And the complex conjugate hits this complex conjugate, I get back k. This complex conjugate hits g, and I get that. Okay? So this says that af paired with g is equal to f paired with b times g, where b, g of x now, okay, so I got out a function of y integrating x. So let me, if I switch the variables, the dummy variables, I can write this as k, y, x, complex conjugate, dy, dy. Okay? But now, the thing to note is that k, this, what is k? So k is this function here. It's a real value function, okay? So the complex conjugate just doesn't matter, right? It's real value, so the complex conjugate is equal to the original function. And it's symmetric in x and y. If I switch x and y, I get k, x, y back. Right? So k of y, x is equal to k of x, y. So this is equal to 0, 1, k of x, y, g of y, dy. But this is just by definition equal to a times g. So we've proven that af, g equals af paired with ag. Now for all f, g, which are continuous, which, remember, is a subset of 0, 1. But not just a subset, it's a dense subset. Since continuous functions on 0, 1 are dense in L2, and the density argument implies that this relation has to hold for every f and g in L2. It implies that af, g equals f, ag, not just for all continuous functions, but for all functions in L2. That proves that A is self-adjoint. All right. And so the last part, which is verifying that, in fact, defining u to be af gives you a twice-continuously differentiable function, which satisfies the Dirichlet problem with B equals 0. It just follows from direct computation. So f is in 0, 1. Then if I define u of x to be af of x, and I actually write out the integral over the various domains on how it's defined, this is first I pick up an integral 0 to x of x minus 1 times y of y dy, plus x times 0, 1 of y minus 1 dy. And now I can just apply the fundamental theorem of calculus to show that, indeed, by fundamental theorem of calculus, that u is twice-continuously differentiable, and u minus u double prime gives me half. And u is given by af. This is the unique solution to that problem, because we've already proven that when v is non-negative, there exists only one solution to the Dirichlet problem. So in the case v equals 0, we can write down an explicit, or the explicit solution in terms of this integral operator, which on L2 is a self-adjoint compact operator. All right, so to solve the Dirichlet problem, so now what's the plan for v not equal to 0? So the plan is this, that if I have minus u double prime plus v times u, so this is just kind of formal stuff, and then we'll actually prove rigorous statements and prove that there exists a solution in the end, we already have uniqueness, like I said. How do we solve this differential equation with, again, the boundary conditions, which I'm not going to write down, so let me just write it down here. So that implies that minus u double prime equals f minus v u, right? And so, equals f minus v u. Now think of this as just a fixed function g, and therefore by this uniqueness, or this existence, uniqueness result that we have for the v equals 0 case, so think of this as g. So now I have minus u double prime equals g. So this implies that u equals a applied to f minus v u, right? So the unique solution to minus u double prime is equal to a applied to g. g is f minus v u, so I have this, right? So I get, this is if and only if here, which, if I distribute this through, is the same as saying the identity plus the operator given by a applied to multiplication by b, so when I write b here I mean multiplication by b, applied to u, equals a applied to f, right? And now this is good because we've kind of gotten rid of this differentiation, and now we're talking about solving an equation that involves bounded operators, right? The identity is a bounded operator, multiplication by a continuous function is a bounded operator on L2, and a is a compact self-adjoint operator on L2, so now we're solving an equation involving bounded operators on the Hilbert space. What would make it even better is if this thing was, so we already know it's a compact operator because a is compact, if we knew this was self-adjoint, but that doesn't necessarily hold because the adjoint of a, b will be b times a, so these don't exactly commute, but we can get around that and reduce ourselves to studying an equation that involves compact self-adjoint operators by a nice little trick. So, write u as, I'll say a to the 1 half, b, a to the 1 half, meaning its square gives me a, the fact that such a thing exists is not clear right now, but we will show it in fact exists. Again, formal stuff, so write u as a to the 1 half applied to b, where now b is the thing we solve for. And if we stick this into this equation here, if a, we pull out an a to the 1 half, applied to i plus a to the 1 half, b, a to the 1 half, applied to b equals a, f. So this should be little b, not this capital B. And therefore, if I can believe I can kind of cancel 1 half powers, I reduce myself to studying the equation a to the 1 half, b, times a to the 1 half, applied to little b equals a to the 1 half, f. Remember, f is given, so whatever this thing is on the right side, we'll know that ahead of time. So our problem is to solve this equation here. Now, what's the great thing? The great thing is that because a is compact, self-adjoint, and in fact a non-negative operator, a to the 1 half exists, is also a compact operator, and is also self-adjoint. So then we have this self-adjoint operator on both sides of b, both of them are compact, so this whole thing will be a compact, self-adjoint operator. So if we want to be able to solve for b, i.e. invert this thing on the left side, this is just a compact self-adjoint operator plus the identity, which you can think of as a compact self-adjoint operator, minus 1 times the identity. So now, that's an equation we don't have to solve. We have to find an alternative that says we can invert this operator, only if this entire operator doesn't have a null space, or doesn't have a non-trivial null space. So that's the plan, is we will reduce ourselves to studying, or we will prove that we can invert this operator here. First, we have to prove that we can find such a a to the 1 half, meaning an operator that's square gives me a, prove the properties we need, and then also show that this operator is invertible, define b as, you know, inverse of this times this, u as a to the 1 half times b, and conclude that u solves our problem. So that's where we're headed. To get this plan off the ground, we need to show that we can come up with such a compact self-adjoint operator, whose square gives me a. As a first step in this direction, we are going to compute the spectrum of the operator a, which again is the inverse of this Dirichlet problem. It's the unique function which twice is the unique, so when f is continuous, it's the unique twice-continuously differentiable function that's second derivative gives me f and a zero at the endpoints, but in general it's this integral operator. So, first theorem I want to prove is that null space of A is the zero vector, or the zero function, and so it has no non-trivial null space, and the, or the normal eigenvectors for A are given by u k of x equals square root of 2 sine a pi of x, here a is a natural number, with associated eigenvalues lambda k equals 1 over k squared pi squared. So, let me just make a brief remark. We have, via the spectral theorem that we proved last time, that for a compact self-adjoint operator, we can find, or that the eigenvectors form an orthonormal basis for the range of the null space of A, and then to complete it to an orthonormal basis of all of the Hilbert space, in this case L2, we just need to take an orthonormal basis of the null space of A. But the null space of A is the zero vector, so by the spectral theorem, for compact self-adjoint operators, we get that square root of 2 sine a pi x, with associated points in infinity, so this is an orthonormal basis for L2, or, so we use the spectral theorem to conclude that this is an orthonormal basis for L2. You can also prove it directly using what we know about e to the i n x being an orthonormal basis for minus pi to pi, just by rescaling e to the i n pi x as an orthonormal basis for L2 to minus 1 to 1. Now we have functions here defined on 0, 1, which we can extend by odd parity to minus 1 to 1. And the only parts of the e to the i n pi x that, in the expansion for an odd function that come out, only involve the sign, these guys. So that's, without knowing these are the eigenfunctions or eigenvectors for this operator A, you can also conclude that this is an orthonormal basis for L2 of 0, 1. Alright, so, proof of this theorem. So what we're going to do to prove that the null space of A is trivial is we will show, we'll go about it a couple of different ways, we'll show that the range of A is in fact dense in L2. So the first thing we're showing is that the null space is trivial, so let u, we'll show that the range of A closure equals all of L2. Remember this is equal to the orthogonal complement of the null space, so if the orthogonal complement is the entire space, then that means the null space of A is just a trivial vector. So I need to be able to show that a dense subspace of L2 can be solved for by A. Okay, so let u be polynomial on 0, 1, and f equals minus u double prime. Okay? Now by the previous theorem, I should say, I can have a part with u of 0 equals u of 1 equals 0. Then by the previous theorem, A applies to f. This is the unique solution to the Dirichlet problem with that function v being 0. Remember u, so f, i.e., write it this way, i.e., f applies to u double prime should give me f, and A f of 0 equals A f of 1 equals 0. But how do we define f? f is minus u double prime. Use a polynomial on 0, 1 that's 0 at the endpoints, and therefore I conclude that A f equals u. Okay? I hope this is clear. Now since set of polynomials on 0, 1 vanishing at x equals 0 and 1 are dense in L2, why is this? This is because we know that continuous functions that are vanishing at the two endpoints are dense in L2. Right? And by the Weierstrass approximation theorem, every continuous function on 0, 1 can be approximated by a polynomial. And it's not too difficult to convince yourself that if that's the case, then I can also approximate every continuous function that's vanishing at the two endpoints by a polynomial that's vanishing at the two endpoints. Okay? So since I can approximate every continuous function on 0, 1 vanishing at the endpoints by a polynomial vanishing at the endpoints, then those polynomials vanishing at the endpoints are dense in L2 of 0, 1. I'll just say here that this follows from, like I said, the density of 0, 1. I'll put a 0 here, meaning it's 0 at the two endpoints. And Weierstrass approximation theorem. Okay, so we've been able to solve for every u that's dense in L2, right? So every polynomial that's 0 at the endpoints is in the range of A. And therefore, the range of A contains a dense subset of L2 of 0, 1. And therefore, the closure has to be polynomial 1. And then since null space of A is equal to the orthogonal complement of the null space of A is equal to the range, the closure of the range, then this equals L2 of 0, 1. I conclude that the null space of A is just a trivial vector. So A has no null space. So, as said by the spectral theorem, an orthonormal basis for L2 of 0, 1 is given by the eigenvectors of A. So now we'll prove the eigenvectors are given by this form. Now, I'll just give you a brief sketch of this. So let's solve for the eigenvalues and eigenvectors. Suppose lambda does not equal 0, u is an element of L2 that has unit length or unit norm, and A applied to u equals lambda times u. Now, then... Okay, then u equals 1 over lambda times A u, which is fine because lambda is non-zero, so I can divide by that. Now, I didn't say this when I was discussing... Let's see if we still have it up there. We can talk about it. Now, for any function in L2, by this estimate here, this also proves that A applied to f is a continuous function. So this also shows that A applied to f is in fact a continuous function. I hope that's clear, right? Why is this so? I need to make this thing on the left-hand side small if x and z are close together. Now, that only depends on some number, the L2 norm of f times this quantity here. K is a continuous function on 0, 1 cross 0, 1. So as long as I make x and z close, then x, y and z, y will be close. Since K is continuous, this quantity here will be small. And therefore, the thing that's smaller than that will be small. So that's why if I take a function which is in L2 and hit it by A, I get a continuous function. So u equals 1 over lambda times this continuous function implies that u is continuous. But now we're going to feed that back in because if u is continuous, then A applied to u is twice continuously differentiable, which implies that u equals 1 over lambda A u is twice continuously differentiable. This is what's called a bootstrap argument, I guess, or is that the right word? Anyways, that doesn't sound right. In any case. Okay, so we conclude that an eigenfunction of A is twice continuously differentiable. Now u is a twice continuously differentiable function, which is equal to 1 over lambda times A u. So A times u is, so that, okay, so another way to write this as A times u over lambda because A is a linear operator. Now, A applied to something is a unique function whose second derivatives times minus 1 gives me that thing inside. So I conclude that minus u double prime equals 1 over lambda applied to u, and along with the boundary conditions, u of 0 equals u of 1 equals 0. But now I know how to solve this equation. It's a superposition of two, has to be a superposition of two functions. I get that. So u of x must be equal to A times sine 1 over square root of lambda times x plus B times cosine 1 over square root of lambda times x. And now since u of 0 has to give me 0, that tells me that B equals 0, which tells me u of x equals A times sine 1 over lambda times x. And since A, since u has unit length with A not equal to 0. Now u of 1 equals 0 implies that 1 over square root of lambda has to be an integer multiple of pi. If u of x has to be non-zero. Therefore, u of x is equal to A times sine A pi x for some k, and the fact that we get square root of 2 comes from the normalization condition. That u of x equals square root of 2 sine k pi x for some k, natural number. So this, so square root of 2 times sine k pi x as k varies over the natural numbers gives me an orthonormal basis for L2. That consists of eigenvectors of A. The eigenvalues are 1 over k squared pi squared. So I can think of A as simply multiplying each eigenvector by 1 over k squared pi squared. And if I want to define A to the 1 half, which is what I wanted to while I'm doing all this, then we'll define an operator so that it takes something in this orthonormal basis and simply multiplies by 1 over k pi, which is half of what A would do, or a half power of what A would do. This is also how one could define what's called a functional calculus for self-adjoint compact operators, which you can then extend to self-adjoint operators as well. So let's make this a definition. So if f is in L2, 0, 1 with f given by this Fourier expansion with ck given by integral 0 to 1, f of x square root of 2 sine k pi x dx. And we define an operator which we call A to the 1 half, although I haven't, I'm not saying that it's, you know, it's square as A just yet. We define linear operator A by A f of x equals sum from k equals 1 to infinity of 1 over k pi ck square root of 2 sine k pi x. So right now I just have this expression for given a function in L2 with this expansion in terms of this orthonormal basis. I do this to the coefficients, I take the coefficients and multiply them by 1 over k pi. And my claim is that A to the 1 half is a compact self-adjoint operator on L2. And this notation is not just for show. I take A to the 1 half and compose it with itself, meaning I square it, I get the operator A, which remember was defined as this integral operator involving k, right? If you like, though, so let me just... Remember A, when it hits each of these sine k pi x's, spits out 1 over k squared times pi squared, right? So A applied to f would be this thing multiplied by 1 over k squared pi squared. And so it makes sense that A to the 1 half should be something that when I apply it again, I get back A, since I have 1 over k squared pi squared when I hit with A, if I multiply the coefficients by 1 over k pi, then this should give me what I want. And now it's just a process of confirming these facts that we need. You may think of A as this integral operator. You can think of A as a solution operator for this Dirichlet problem. Or you may simply think of it as I take a function f, expand it in terms of sine k pi x, which is an orthonormal basis for L2 of 0, 1, and I simply multiply by 1 over k squared pi squared. How does this jive with what I just said about A also being the solution operator? Well, if I have A applied to this quantity, then I have sine k pi x over k squared pi squared. Now let's say I take two derivatives of that, then I get k squared pi squared over k squared pi squared. I get 1, so I get back out f, right, with a minus sign. So all of those things jive. All right, so proof of this theorem. So let's take two functions in L2, expand it in this basis. And, okay, so first thing I would like to show is that this operator is bounded. Linear operator, so I want the L2 norm of A to the 1 half f squared. So this is the L2 norm squared of the function with coefficients given by k pi square root of 2 sine k pi x squared. And now, so the L2 norm of this function, this function given by this Fourier expansion here, is just the sum of the squares of the coefficients appearing in front of the square root of 2 times sine k pi x. So this is equal to, this is by Parseval's identity, equals c k squared over k squared pi squared. And 1 over k squared pi squared as k goes from 1 to infinity is bounded by 1 over pi squared equals 1 to infinity of c k squared equals 1 over pi squared. Again, by Parseval's identity or completeness of this eigenbasis gives me back the L2 norm of that. So that proves boundedness. What about self-adjointness? I take A to the 1 half f, but I think it's inner product with g. Again, using how these things are defined and the fact that the inner product between a function, two functions, is given by just the little L2 pairing of the coefficients that appear in front of the square root of 2 sine k pi x. So this is c k over k times pi d k. And now we just move this over here. And these are real numbers, so I can move them over on the d without taking their complex conjugate. So that's equal to inner product A to the 1 half g. So that proves that A is self-adjoint. And now finally, we show that A to the 1 half squared gives me A. So, let f be... Oh, okay, so I already wrote that part, I don't need to write it again. If I look at A to the 1 half f, this is by definition equal to A to the 1 half, applied to now the function given in this eigenbasis by, or this orthonormal basis by, c k over k pi square root of 2 sine k pi x. Again, how do I apply A to the 1 half? I take the coefficients in front of the basis function and divide by k pi, so then k equals 1 to infinity of c k over k squared pi squared square root of 2 sine k pi x. Let's leave it here. Now, going from here to what we have next, since each of these is an eigenfunction of A, with eigenvalue 1 over k squared pi squared, this is also equal to A applied to function sine, square root of 2, sine k pi x. And now it's perfectly legitimate to pull this A out of the infinite sum, because this is a basis for L2, or really by... in any case, I can pull the A out and I get square root of 2 sine k pi x. Why can I do this? It's because as... so if I put a finite in here, then sum from k equals 1 to n converges in L2 norm to this quantity here with an infinity. So since A is a bounded linear operator, A applied to the finite sum is equal to A applied to... or the limit as n goes to infinity of A applied to the finite sum is equal to A applied to the infinite sum. Now, A applied to the finite sum is given by this, letting n go to infinity again. This we can take the A in and out. But this was just the expansion of F, so this is equal to A F. So we have proven that A to the 1 half squared, meaning A to the 1 half applied to A to the 1 half of F gives me back A F. Now, let me give a brief sketch on why this is a compact operator. We've shown it's a bounded self-adjoint operator whose square gives me A. So... So a compact operator will show that the image of the unit ball has equi-small tails. I will just say here it's... suffices to show that A applied to F has equi-small tails. The fact that this has equi-small tails then implies that the closure also has equi-small tails, and therefore by this theorem we proved about characterizing compact subsets of Hilbert spaces, we conclude that the closure of this set is compact, and therefore A to the 1 half is a compact operator. So, let epsilon be positive, choose n as a number, so that 1 over n squared is less than epsilon squared. We can do that because 1 over capital N squared goes to 0 as capital N goes to infinity, so I can always find such an n. And now claim that this n is the one that works for showing this set has equi-small tails. Let that be... L2 with norm less than or equal to 1. And again, if I look at the sum of the Fourier coefficients given in this basis of square root 2 sine k pi x, A to the 1 half F. This is equal to, by definition of how A operates, this is equal to the coefficient of F squared over k squared pi squared, and this is less than or equal to... k is bigger than n, so this is 1 over... less than or equal to 1 over n squared with a pi squared, but that's something like 9, so I can leave it and still be less than or equal to some k. I can now take this to be all ck's, and this is equal to 1 over n squared 2 squared, which is less than or equal to 1 over n squared, and then we've chosen n, so that 1 over n squared is less than epsilon squared, so that's less than epsilon squared. Therefore showing that this set has equismall tails, and therefore the closure of this set is compact. So we have this A to the 1 half operator, which we needed to carry out this plan that we sketched at the beginning, solving the Dirichlet problem, so now we just kind of need to check that that operator I had before, which is A to the 1 half composed with multiplication by B, composed with A to the 1 half is a compact self-adjoint operator, so first let me just state a theorem about multiplying by a continuous function, let B be a real value continuous function, define m sub B as a multiplication f of x to be just B of x f of x for every L2, then this is a boundless linear operator on L2 and its self-adjoint. So I'm actually going to leave this as an exercise, because it's not too difficult to prove, again just multiplying by this function which is continuous and bounded gives you boundedness automatically, and the fact that it's real value will give you the self-adjoints. So from this, we can conclude the following, A to the 1 half of B continuous function is a real value, and the operator T equals A to the 1 half composed with multiplication by B, composed with A to the 1 half satisfies the following, one is T is a self-adjoint compact operator on L2, and one extra condition is T is bounded, so in fact T is a bounded operator from L2 to continuous functions, meaning if I take an L2 function and stick it into this function, I get out a continuous function in a bounded way. So one follows immediately from what we've proven already, in this exercise of the theorem that I gave. Multiplication by B is a self-adjoint operator, A to the 1 half is a self-adjoint operator, so when I take the adjoint of this quantity, I will get the adjoint of A to the 1 half in front, adjoint of MV, adjoint of A to the 1 half, they all equal each other themselves, so I get out something self-adjoint. This is the composition of, if you like, a bounded operator with a compact operator, so I get a compact operator. So that proves it's a self-adjoint compact operator on L2. Why is this a bounded operator from L2 to continuous functions? Well, it suffices to show that this A to the 1 half is a bounded operator from L2 to continuous functions. Why is that? Because if I take an L2 function and feed it into this operator T, then A to the 1 half will be a continuous function, multiplication by B will be a continuous function again, because multiplication by B is continuous, so B is continuous, and then A to the 1 half applied to a continuous function again is a continuous function. So it suffices to show that A to the 1 half is a bounded operator from L2 to continuous functions. Let f be given in the Fourier expansion in terms of the sine k pi x's. If A to the 1 half and f of x, this is equal to k pi square root of 2 sine k pi x. And now to show that A to the 1 half applied to f is a continuous function, we'll apply the Bayer-Strauss index. So this is an infinite sum of functions, of continuous functions. So to apply the Bayer-Strauss index, I have to say that I can bound this thing by something which is summable. So I have this, and then I also, if I take the absolute value of that's less than or equal to c k over k pi square root of 2, which is less than or equal to d k over k, and I claim this is summable. If I sum from k equal to 1 to infinity, c k over k, by Cauchy-Schwarz, this is less than or equal to sum over k, 1 over k squared, 1 half, sum k, c k squared, raised to the 1 half. And remember this is just the L2 norm of f, right? Because the sine k pi x's are in L2 basis. So that equals something like pi squared over 6. Let's say it is this thing here times L2 norm. Okay, so each of these continuous functions in this infinite sum is bounded by a constant, and those constants are summable by this computation. So thus, that implies that a to the 1 half f is a continuous function by Weierstrass's M-test. Not only that, this computation we did shows that a to the 1 half applied to f of x is less than or equal to pi squared over 6, raised to the 1 half, times the L2 norm. So it's bounded linear operator from L2 to the space of continuous functions. Okay, so where are we? We have all the pieces we need in place to solve our problem. Rift like problem. All the ingredients are ready, we just need to cook them. So I was mentioning the Weierstrass M-test, that should have been covered in 18100. That is an infinite sum of continuous functions, and each of those continuous functions is bounded by a constant, which is, those constants are summable, then you get a continuous function out in the end. Okay, so now, the theorem that concludes the existence part of the Dirichlet problem is that we see 0,1, continuous function be non-negative, so it's real value is non-negative, and let f be a continuous function. Then, there exists a unique twice-continuously differentiable function, on 0,1, solving the Dirichlet problem, minus v double prime, plus v multiplied by u, equals f on 0,1, and the boundary conditions u of 0 equals u of 1 equals 0. Okay? Alright, so, again, the plan was to define u as a to the 1 half applied to, so, I'll just recall for you, the plan was to define u to be a to the 1 half, i plus a to the 1 half, v, a to the 1 half, inverse, a to the 1 half applied to f. Now, we just need to say why this thing exists, right? Why is this operator in the middle invertible? And then we'll get what we need. Proved by the Fredholm alternative, right? So, by the previous, so let me not skip ahead by it, previous theorem, this operator, m, v, sandwiched between a to the 1 half is a self-adjoint compact operator, and therefore, by the Fredholm alternative, the inverse exists, if and only if the null space of the operator is trivial. Now, suppose we have something in the null space, and we'll show that it has to be 0. Then, if I pair this with, so g is an element in L2, if I pair it with g, so I get that 0 is equal to i plus a to the 1 half, applied to g, product g. Now, carrying this g through, and it'll get inner product with itself, so I get g squared plus a to the 1 half, multiplication by b of a to the 1 half, applied to g, inner product g. Now, since a to the 1 half is self-adjoint, I can move it over to the second entry. And let me just write this out, this is equal to g L2 plus the inner rule from 0 to 1, b times a to the 1 half g times, so this is all multiplication, pointwise multiplication, times a to the 1 half g complex conjugate, dx. Which just gives me a to the 1 half g times 1 squared. Okay, now, b is non-negative, so this quantity here is non-negative, so this is bigger than or equal to g squared, L2 norm of g, and we started out with 0. So, we get that g is 0, and therefore the null space, i plus this self-adjoint compact operator, is trivial, and therefore this compact, and therefore i plus this compact self-adjoint operator is invertible. So, I'm going to turn it. So, this inverse exists, and I define u to be, what, a to the 1 half. Okay, say it this way, I'll define v to be i plus a to the 1 half multiplication by b, a to the 1 half, inverse applied to a to the 1 half f, and u to be a to the 1 half of b. So then what do I get? Then, u plus a, applied to multiplication by v of, so I say a, v, u, this is equal to, by definition, a to the 1 half, b, plus a to the 1 half, simply because a is equal to a to the 1 half squared, applied to b, so I get a to the 1 half, i plus a to the 1 half, b, a to the 1 half, applied to b, and now b is given by this thing, so I just get, when it hits that, I just get back a to the 1 half f, multiplied by a to the 1 half, you get a f.