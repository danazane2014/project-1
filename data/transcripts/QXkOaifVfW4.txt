 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So again, before we start, there's a survey online. If you haven't done so, I would guess at least one of you has not. Some of you have entered their answers and their thoughts, and I really appreciate this. It's actually very helpful. So it seems that the course is going fairly well from what I've read so far. So if you don't think this is the case, please enter your opinion and tell us that we can make it better. One of the things that was said is that I speak too fast, which is absolutely true. I just can't help it. I get so excited. But I will really do my best. I will try to. So just, you know, I think I always start OK. I just end not so well. So last time, we talked about this chi-squared distribution, which is just another distribution that's so common that it deserves its own name. And this is something that arises when we sum the squares of independent standard Gaussian random variables. And in particular, why is that relevant is because if I look at the sample variance, then it is a chi-squared distribution. And the parameter that shows up is also known as the degrees of freedom is the number of observations minus 1. And so as I said, this chi-squared distribution has an explicit probability density function. And I tried to draw it. And one of the comments was also about my handwriting. So I will actually not rely on it for detailed things. So this is what the chi-squared with one degree of freedom looked like. And really, what this is is just the distribution of the square of a standard Gaussian. I'm summing only one. So that's what it is. Then when I go to 2, this is what it is, 3, 4, 5, 6, and 10. And as I move, you can see this thing is becoming flatter and flatter and pushing to the right. And that's because I'm summing more and more squares. And in expectation, we just get 1 every time. So it really means that the mass is moving to infinity. In particular, a chi-squared distribution with n degrees of freedom is going to infinity as n goes to infinity. Another distribution that I asked you to think about, anybody looked around about the student t-distribution, what the history of this thing was? So I'll tell you a little bit. Maybe I understand if you have time. So the t-distribution is another common distribution that is so common that it will be used and will have its table of quintiles that are drawn at the back of the book. Now, remember, when I mentioned the Gaussian, I said, well, there are several values for alpha that we're interested in. And so I wanted to draw a table for the Gaussian. We had something that looked like this. And I said, well, q alpha over 2. We get alpha over 2 to the right of this number. And we said that there's a table for this thing for common values of theta. Well, if you try to envision what this table will look like, it's actually a pretty sad table, because it's basically one list of number. Why would I call it a table? Because all I need to tell you is something that looks like this. If I tell you this is alpha and this is q alpha over 2, and then I say, OK, basically the three alphas that I told you I care about are something like 1%, 5%, and 10%. And my table will just give me q alpha over 2. So that's alpha, and that's q alpha over 2. And that's going to tell me that I don't remember this one, but this guy is 1.96. This guy is something like 2.45. I think this one is like 1.65, maybe. And maybe you can be a little finer, but it's not going to be an entire page at the back of the book. And the reason is because I only need to draw these things for the one standard Gaussian, when the parameters are 0 for the mean and 1 for the variance. Now, if I'm actually doing this for the chi-squared, I basically have to give you one table per values of the degrees of freedom, because those things are different. There's no way I can take, right, for Gaussians, if you give me a different mean, I can subtract it and make it back to be a standard Gaussian. For the chi-squared, there's no such thing. There's no thing that just takes the chi-squared with d degrees of freedom, nd, and turns it into, say, a chi-squared with 1 degree of freedom. This just does not happen. So there's no way to say the word is standardized, make it a standard chi-squared. There's no such thing as a standard chi-squared. So what it means is that I'm going to need one row like that for each value of the number of degrees of freedom. So that will certainly fill a page at the back of a book, OK, even maybe more. I mean, I need one per sample size. So if I want to go from sample size 1 to 1,000, I need 1,000 rows. OK, so now the student distribution is one that arises where it looks very much like the Gaussian distribution. And there's a very simple reason for that, is that I take a standard Gaussian and I divide it by something. That's how I get the student. What do I divide it with? Well, I take an independent chi-squared. I'm going to call it v. And I want it to be independent from z. And I'm going to divide z by root v over d. So I start with a chi-squared v. So this guy is chi-squared d. I start with z, which is n0,1. I'm going to assume that those guys are independent. And my t distribution, I'm going to write it T. Capital T is z divided by the square root of v over d. Why would I want to do this? Well, because this is exactly what happens when I divide not by the true variance, a Gaussian, but by its empirical variance. So let's see why in a second. So I know that if you give me some random variable, let's call it x, which is n mu sigma squared, then I can do this, x minus mu divided by sigma. I'm going to call this thing z, because this thing actually has some standard Gaussian distribution. I have standardized x into something that I can read the quintiles of at the back of the book. So that's this process that I want to do. Now, to be able to do this, I need to know what mu is. And I need to know what sigma is. Otherwise, I'm not going to be able to make this operation. Mu, I can sort of get away with, because remember, when we're doing confidence intervals, we're actually solving for mu. So it was good that mu was there. When we're doing hypothesis testing, we're actually plugging in here the mu that shows up in h0. So that was good. We had this thing. Think of mu as being p, for example. But this guy here, we don't necessarily know what it is. I just had to tell you for the entire first chapter, I said assume you have Gaussian random variables and that you know what the variance is. And the reason why I said assume you know it, and I said sometimes you can read it on the side of the box of measuring equipment in the lab. So that was sort of just the way I justified it. But the real reason why I did this is because I would not be able to perform this operation if I actually did not know what sigma was. But from data, we know that we can form this estimator Sn, which is 1 over n sum from i equal 1 to n of xi minus x bar squared. And this thing is approximately equal to sigma squared. That's the sample variance, and it's actually a good estimator, just by the law of large number, actually. This thing, by the law of large number, as n goes to infinity, well, let's say in probability, it goes to sigma squared by the law of large number. So it's a consistent estimator of sigma squared. So now what I want to do is to be able to use this estimator rather than using sigma. And the way I'm going to do it is I'm going to say, OK, what I want to form is x minus mu divided by Sn this time. I don't know what the distribution of this guy is. Sorry, it's square root of Sn. This is sigma squared. So this is what I would take. And I could think of Slutsky, maybe, something like this that would tell me, well, just use that and pretend it's a Gaussian. And we'll see how, actually, it's valid to do that, because Slutsky tells us it is valid to do that. But what we can also do is to say, well, this is actually equal to x minus mu divided by sigma, which I know what the distribution of this guy is. And then what I'm going to do is I'm going to just cancel this effect, sigma over square root Sn. So I didn't change anything. I just put the sigma here. So now what I know is that this is some z, and it has some standard Gaussian distribution. What is this guy? Well, I know that Sn, we wrote this here. Maybe I shouldn't have put those pictures, because now I'm keep on skipping before and after. We know that Sn times n divided by sigma squared is actually k squared n minus 1. So what do I have here? I have that k squared. So here I have something that looks like 1 over square root of Sn divided by sigma squared. This is what this guy is. If I just do some rewriting, and maybe I actually want to make my life a little easier, I'm actually going to plug in my n here. And so I'm going to have to multiply by square root of n here. OK, everybody's with me? So now what I end up with is something that looks like this, where I have, OK, here I started with x. I should really start with xn bar minus mu times square root of n. That's what the central limit theorem would tell me. I need to work with the average rather than just one observation. So if I start with this, then I pick up a square root of n here. OK? So if I had the sigma here, I would know that this thing is actually xn bar minus mu divided by sigma times square root of n would be a standard Gaussian. So if I put xn bar here, I really need to put this thing that goes around the xn bar. Sorry about that. OK? That's just my central limit theorem that says, if I average, then my variance is shrunk by a factor 1 over n. Now, I can still do this, right? That was still fine. And now I say that this thing is basically this guy. OK? Now, so what I know is that this thing is a chi squared with n minus 1 degrees of freedom. So this guy here is chi squared with n minus 1 degrees of freedom. Let me call this thing v in the spirit of what was used there and in the spirit of what is written here. So this guy was called v. So I'm going to call this v. So what I can write is that square root of n xn bar minus mu divided by square root of Sn is equal to z times square root of n divided by square root of v. OK? Everybody's with me here? OK. Which I can rewrite as z times square root of v divided by n. And if you look at what the definition of this thing is, I'm almost there. What is the only thing that's wrong here? Is this a student distribution, right? So there's two things. The first one was that they should be independent. And they actually are independent. That's what Cochrane's theorem tells me. And you just have to count on me for this. I told you already that Sn was independent of xn bar. So those two guys are independent, which implies that the numerator and the denominator here are independent. That's what Cochrane's theorem tells us. But is this exactly what I should be seeing if I wanted to have my sample variance, if I want to write this? Is this actually the definition of a student distribution? Yes. No. OK. So see, we see z divided by square root of v over d. That looks pretty much like this, except there's a small discrepancy. What is the discrepancy? There's just this square root of n minus 1 thing. So here, v has n minus 1 degrees of freedom. And in the definition, if v has d degrees of freedom, I divide it by d, but not by d minus 1, or not by d plus 1, actually, in this case. So I have this extra thing. Well, there's two ways I can address this. The first one is by saying, well, this is actually equal to z over square root of v divided by n minus 1 times square root of n over n minus 1. I can always do that and say, for n large enough, this thing is actually going to be pretty small, or I can take account for it. Or I can actually just compute it. For any n you give me, I can compute this number. And so rather than having a t distribution, I'm going to have a t distribution times this deterministic number, which is just a function of my number of observations. But what I actually want to do instead is probably use a slightly different normalization, which is just to say, well, why do I have to define Sn to be divided by n? Actually, this is a biased estimator. And if I want it to be unbiased, I can actually just put an n minus 1 here. You can check that. You can expand this thing and compute the expectation. You will see that it's actually not sigma squared, but n over n minus 1 sigma squared. So you can actually just make it unbiased. Let's call this guy tilde. And now when I put this tilde here, what I actually get is S tilde here and S tilde here. I need actually to have n minus 1 here to have this S tilde be a chi-square distribution. Yes? So what's the significance of defining this way? So what we would mean? Yeah, so basically, this is what the story did. So the story was, well, rather than using always the central limit theorem and just pretending that my Sn is actually the true sigma squared, since this is something that I'm going to do a lot, I might as well just compute the distribution, like the quantiles for this particular distribution, which clearly does not depend on any unknown parameter. D is the only parameter that shows up here, and it's completely characterized by the number of observations that you have, which you definitely know. And so people said, let's just be slightly more accurate. And in a second, I'll show you how the distribution of the t. So we know that if the sample size is large enough, this should not have any difference with the Gaussian distribution. I mean, those two things should be the same, because we've actually not paid attention to this discrepancy by using empirical variance rather than true so far. And so we'll see what the difference is. And this difference actually manifests itself only in small sample sizes. So those are things that matter mostly if you have less than, say, 50 observations, then you might want to be slightly more precise and use t-distribution rather than Gaussian. So this is just a matter of being slightly more precise. If you have more than 50 observations, just drop everything and just pretend that this is the true one. Any other question? So now I have this thing. So I'm on my way to changing this guy. So here now I have not root n, but root n minus 1. Yes. And is that true? So I have a z. So this guy here is s. Yeah. Where did I get my root n from in the first place? Yeah, because I wanted this guy. And so now what I'm left with is xn minus mu divided by Sn tilde, which is the new one, which is now indeed of the form zv root n minus 1, which now I can write it as zvn minus 1. And here I know that I have. So now I have exactly what I want. And so this guy is n0,1. And this guy is chi squared with n minus 1 degrees of freedom. And so now I'm back to what I want. So rather than using Sn to be the empirical variance where I just divide my normalization is by n, if I use n minus 1, I'm perfect. Of course, I can still use n and do this multiplying by root n minus 1 over n at the end, but it just doesn't make as much sense. So what's the story? Everybody's fine with what this tn distribution is doing and why this last line is correct? So that's just basically because it's of the form of it's been defined so that this is actually happening. That was your question, and that's really what happened. So who is this student t distribution? Where does the name come from? Well, it does not come from Mr. T. And if you know who Mr. T was, you're probably too young for that. He was our hero in the 80s. And it comes from this guy. His name is Sean William Gossett, 1808. So that was back in the day. And this guy actually worked at the Guinness Brewery in Dublin, Ireland. And Mr. Guinness back then was a bit of a fascist, and he didn't want him to actually publish papers. And so what he had to do is to use a fake name to do that. And he was not very creative, and he used the name student, because I guess he was a student of life. And so here's the guy who actually, so back in 1808, it was actually not difficult to put your name or your pen name on a distribution. So what does this thing look like, and actually, how does it compare to the standard normal distribution? Do you think it's going to have heavier or lighter tails compared to the Gaussian distribution? Yeah, right, because I have extra uncertainty in the denominator. So it's actually going to make things wiggle a little wider. So let's start with a reference, which is the standard normal distribution. So that's my usual bell-shaped curve. And this is actually the t distribution with 50 degrees of freedom. So right now, that's probably where you should just stand up and leave, because you're like, why are you wasting our time? They're actually pretty much the same thing. And it is true. If you have 50 observations, both the central limit theorem, so here, OK, one of the things that you need to know is that if I want to talk about t distribution for, say, eight observations, I need those observations to be Gaussian for real. There's no central limit theorem happening at eight observations. But really, what this is telling me is not that the central limit theorem kicks in. It's telling me that what are the asymptotics that kick in? The law of large numbers, right? This is exactly this guy that's here. When I write this statement, what this picture is really telling us is that for n is equal to 50, I'm at the limit already almost. There's virtually no difference between using the left-hand side or using sigma squared. And now I start reducing. 40, I'm still pretty good. We can start seeing that this thing is actually losing some mass on top of this, because it's actually pushing it to the left and to the right in the tails. And then we keep going, keep going, keep going. So that's at 10. Even at 10, there's not much of a difference, right? And so you can start seeing difference when you're at 5, for example. You can see the tails become heavier. And the effect of this is that when I'm going to build, for example, a confidence interval, to put the same amount of mass to the right of some number, let's say I'm going to look at this q alpha over 2, I'm going to have to go much farther, which is going to result in much wider confidence intervals. 2, 4, 3, 2, 1. So that's the t1. Obviously, that's the worst. And well, if you ever use the t1 distribution, please ask yourself, why in the world are you doing statistics based on one observation? And OK, but that's basically what it is. So now that we have this t distribution, we can define more sophisticated tests and just take your favorite estimator and see if it's far from the value you're currently testing, right? That was our rationale to build a test before. And the first test that's non-trivial is a test that exploits the fact that the maximum likelihood estimator, under some technical condition, has a limit distribution which is a Gaussian with mean 0 when properly centered and a covariance matrix or given by the Fisher information matrix. Remember this Fisher information matrix? I'm totally, OK. And so this is the setup that we have. So we have, again, an iid sample. Now I'm going to assume that I have a d-dimensional parameter space, theta. And that's why I talk about Fisher information matrix and not just Fisher information, say, number. And I'm going to consider two hypotheses. So here I'm going to have h0 theta is equal to theta 0, h1 theta is not equal to theta 0. And this is basically what we thought when we said, are we testing if a coin is fair or unfair? So fair was p equals 1 half, and fair was p different from 1 half. And here I'm just making my life a bit easier. So now I have this maximum likelihood estimator that I can construct because, let's say, I know what p theta is. And so I can build a maximum likelihood estimator. And I'm going to assume that these technical conditions that ensure that this maximum likelihood properly standardized converges to some Gaussian are actually satisfying. So this thing is actually true. So the theorem, the way I stated it, if you're a little puzzled, this is not the way I stated it in the first time. The way we stated it was that theta hat MLE minus theta 0. So here I'm going to place myself under the null hypothesis. So here I'm going to say under H0. And honestly, if you have any exercise on tests, that's the way that it should start. What is the distribution under H0? Because otherwise, you don't know what this guy should be. So you have this. And what we showed is that this thing was going in distribution as n goes to infinity to some normal with mean 0 and covariance matrix, which was I of theta, which was here for the true parameter. But here I'm under H0. So there's only one true parameter, which is theta 0. Those were our conditions for the limiting central limit theorem for, I mean, it's not really central limit theorem, but limited theorem for the maximum likelihood estimator. Everybody remembers that part? The line before said under technical conditions, I guess. So now it's not really stated in the same way. If you look at what's on the slide, here I don't have the Fisher information matrix, but I really have the identity of Rd. How do I turn a, if I have a random variable X, which has some covariance matrix sigma, how do I turn this thing into something that has covariance matrix identity? So if this was a sigma squared, well, the thing I would do would be divide by sigma, and then I would have a 1, which is also known as the identity matrix of R1. Now, what is this? This was root of sigma squared, right? So what I'm looking for is the equivalent of taking sigma and sort of dividing by the square root of sigma, which obviously those are matrices I'm certainly not allowed to do. And so what I'm going to do is I'm actually going to do the following, sigma 1 over square root of sigma squared can be written as sigma to the negative 1 half. And this is actually the same thing here. So I'm going to write it as sigma to the negative 1 half, and now this guy is actually well-defined. This is just the, so this is a positive symmetric matrix, and you can actually define its square root by just, well, taking the square root of its eigenvalues, for example. And so you get sigma 1 half equals and follows n0 identity. And in general, this guy has, I'm going to see something that looks like sigma 1 half, negative 1 half, sigma, sigma negative 1 half. And that's all these things. I have minus 1 half plus 1 minus 1 half. This whole thing collapsed to 0, and it's actually the identity. So that's the actual rule. So if you're not familiar with this, this is basic multivariate Gaussian distribution computations. Take a look at it. If you feel like you don't need to look at it, but you know the basic maneuver, it's fine as well. We're not going to go much deeper into that, but those are part of the thing that are standard manipulations about standard Gaussian vectors, because obviously, standard Gaussian vectors arise from this theorem a lot. OK, so now I pre-multiply by sigma to the minus 1 half. Now, of course, I'm doing all of this in the asymptotics, and so I have this effect. So if I pre-multiply everything by sigma to the 1 half, sigma being the Fisher information matrix at theta 0, then this implies this is actually equivalent to saying that square root of n. So now i of theta 0 plays the role of sigma times theta hat MLE minus theta 0 goes in distribution as n goes to infinity to some standard Gaussian, multivariate standard Gaussian, n0 identity of our d. And here, to make sure that we're talking about a multivariate distribution, I can put a d here. So just so we know we're talking about the multivariate, though it's pretty clear from the context since the covariance matrix is actually a matrix and not a number. Michael. AUDIENCE 1. Isn't it the first two are the same? PROFESSOR 1. Oh, yeah, right. Thanks. Yeah. So yeah, you're right. So that's a minus, and that's a plus. OK, thanks. So yeah. Anybody has a way to remembering whether it's inverse Fisher information or Fisher information as a variance, rather than just learning it? It is called information, right? So it's really telling me how much information I have. So when my variance increases, I'm getting less and less information. And so this thing should actually be 1 over a variance. The notion of information is 1 over a notion of variance. OK? So now I just wrote this guy like this. And the reason why I did this is because now everything on the right-hand side does not depend on any unknown parameter, right? There's 0 and identity. Those two things are just absolute numbers or absolute quantities, which means that this thing, I call this quantity here. What was the name that I used? Started with a P. Pivotal, right? So this is a pivotal quantity, meaning that its distribution, at least its asymptotic distribution, does not depend on any unknown parameter. Moreover, it is indeed a statistic, right? Because I can actually compute it. I know theta 0, and I know theta hat MLE. One thing that I did, and you should actually complain about this, is on the board, I actually used i of theta 0. And on the slides, it says i of theta hat. And it's exactly the same thing that we did before. Do I want to use the variance as a way for me to check whether I'm under the right assumption or not? Or do I actually want to leave that part and just plug in the theta hat MLE, which should go to the true one eventually? Or do I actually want to just plug in the theta 0? So this is exactly playing the same role as whether I wanted to see square root of xn bar 1 minus xn bar in the denominator of my test statistic for P, or if I wanted to see square root of 0.5, 1 minus 0.5 when I was testing if P was equal to 0.5. So this is really a choice that's left up to you. And that's something you can really choose the two. And as we said, maybe this guy is slightly more precise. But it's not going to extend to the case where theta 0 is not reduced to one single number. Any question? So now we have our pivotal distribution. So from there, this is going to be my test statistic. I'm going to use this as a test statistic and declare that if this thing is too large in absolute value, because this is really a way to quantify how far theta hat is from theta 0, and since theta hat should be close to the true one, when this thing is large in absolute value, it means that the true theta should be far from theta 0. So this is my new test statistic. Now, I said it should be far, but this is a vector. So if I want a vector to be far, two vectors to be far, I measure their norm. And so I'm going to form the Euclidean norm of this guy. So if I look at the Euclidean norm of n, and Euclidean norm is the one you know, I'm going to take it square, because it's actually, let me not put a 2 here. So that's just the Euclidean norm. And so the norm of a vector x is just x transpose x. And this light transpose is denoted by prime. Well, that's hard to say, put primes in quotes. That's a statistics standard that people do. They put prime for transpose. Everybody knows what the transpose is? So I just make it flat, and I do it like this. And then that means that's actually equal to the sum of the coordinates xi squared. And that's what you know as a norm. But here, I'm just writing it in terms of vectors. And so when I want to write this, this is equivalent. This is equal to, well, the square root of n is going to pick up the square. So I get square root of n times square root of n, so n. This guy is just 1 half. So 1 half times 1 half is going to give me 1. And so I get theta hat MLE minus theta. And then I have E of theta 0. And then I get theta hat MLE minus theta 0. And so this thing is my, by definition, I'm going to say that this is my test statistic, Tn. And now I'm going to have a test that rejects if Tn is large, because Tn is really measuring the distance between theta hat and theta 0. So my test now is going to be psi, which rejects. So this is 1 if Tn is larger than some threshold T. And how do I pick this T? Well, by controlling my type I error. Sorry, this C, by controlling my type I error. So to choose C, what we have to check is that P under theta 0, so here it's theta 0, that I reject so that psi is equal to 1. I want this to be equal to alpha. That's how I maximize my type I error under the budget that's actually given to me, which is alpha. So that's actually equivalent to checking whether P0 of Tn is larger than C. And so if I want to compute, if I want to find this C, all I need to know is what is the distribution of Tn when theta is equal to theta 0. Whatever this distribution is, maybe it has some weird density like this. Whatever this distribution is, I'm just going to be able to pick this number. And I'm going to take this quintile alpha, here alpha, and I'm going to reject if I'm larger than alpha, whatever this guy is. So to be able to do that, I need to know what is the distribution of Tn under when theta 0 is equal to theta, when theta is equal to theta 0. What is this distribution? What is Tn? It's the norm squared of this vector. What is this vector? What is the asymptotic distribution of this vector? Yes? Just look one board up. What is the asymptotic distribution of this vector for which we're taking the norm squared? It's right here. It's a standard Gaussian multivariate. So when I look at the norm squared, so if z is a standard Gaussian multivariate, then the norm of z squared, by definition of the norm squared, is the sum of the zi squared. That's just the definition of the norm. But what is this distribution? That's a chi-square, right? Because those guys are all of variance 1. That's what the diagonal tells me. All only ones. And they're independent because they have only zeros outside of the diagonal. So that's really, this follows some chi-square distribution. How many degrees of freedom? Well, the number of them that I sum, d. So now I have found the distribution of Tn under this guy. And that's true because here, this is true under h0, right? If I was not under h0, again, I would need to take another guy here. How did I use the fact that theta is equal to theta 0 when I centered by theta 0? And that was very important. So now what I know is that this is really equal. Why did I put 0 here? So this here is actually equal. So in the end, I need c such that the probability, and here I'm not going to put a theta 0. I'm just talking about the probability of the random variable that I'm going to put in there. It's a chi-square with d degrees of freedom that exceeds c is equal to alpha. I just replaced the fact that this guy, Tn, under this distribution was just a chi-square. And this distribution here is just really referring to the distribution of the chi-square. There's no parameters here. And now that means that I look at my chi-square distribution. We know it sort of looks like this. And I'm going to pick some alpha here. And I need to read this number, q alpha. And so here, what I need to do is to pick this q alpha here for c. So take c to be q alpha, the quantile of order 1 minus alpha of a chi-square distribution with this d degree of freedom. And why do I say 1 minus alpha? Because again, the quantiles are usually referring to the area that's to the left of them by a convention. However, in statistics, we only care about the right tail, usually. So it's not very convenient for us. And that's why rather than calling this guy q sub 1 minus alpha all the time, I write it q alpha. So now you have this q alpha, which is the 1 minus alpha quantile or quantile of order 1 minus alpha of chi-square d. And so now I need to use a table for each d. This thing is going to take a different value. And this is why I cannot just spit out a number to you like I spit out 1.96. Because if I were able to do that, that would mean that I would remember an entire column of this table for each possible value of d. And that I just don't know. So you need just to look at tables. And this is what it will tell you. Often, software will do that to you. You don't have to search through tables. And so just as a remark is that this test, Waltz test, is also valid when I have this sort of other alternatives that I could see quite a lot, if I actually have what's called a one-sided alternative. By the way, this is called Waltz test. So taking Tn to be this thing, so this is Waltz test. Abraham Waltz was a famous statistician in the early 20th century. Yeah, who actually was at Columbia for quite some time. And that was actually at the time where India was getting very, where statistics were getting very popular in India. And so he was actually traveling all over India in some dinky planes. And one of them crashed. And that's how he died. Pretty young. But he actually has, there's a huge school of statistics now in India thanks to him. There's the Indian Statistical Institute, which is actually a pretty big thing and trains the best statisticians. So this is called Waltz test. And it's actually a pretty popular test. Let's just look back a sec. So you can do the other alternatives, as I said. And for the other alternatives, you can actually do this trick where you put theta 0 as well. As long as you take the theta 0, that's the closest to the alternative. You just basically take the one that's the least favorable to you. OK, to the alternative, I mean. OK, so what is this thing doing? If you did not know anything about statistics, and I told you here's a vector that's the MLE vector, theta hat MLE. And so let's say this theta hat MLE takes the values, say. OK, so let's say theta hat MLE takes values, say, 1.2, 0.9, and 2.1. And I'm testing h0 theta is equal to 1.12 versus theta is not equal to the same number. That's what I'm testing. So you compute this thing, and you find this. If you don't know any statistic, what are you going to do? You're just going to check if this guy is close to that guy. And probably what you're going to do is compute something that looks like the norm squared between those guys, so the sum. So you're going to do 1.2 minus 1 squared plus 0.9 minus 1 squared plus 2.1 minus 2 squared, and check if this number is large or not. Maybe you're going to apply some stats to try to understand how those things are. But this is basically what you're going to want to do. What Wald's test is telling you is that this average is actually not what you should be doing. It's telling you that you should have some sort of a weighted average. Actually, it would be a weighted average if I was guaranteed that my Fisher information matrix was diagonal. If my Fisher information matrix is diagonal, looking at this number minus this guy transpose i, and then this guy minus this, that would look like I have some weight here, some weight here, and some weight here. But if my matrix, sorry, it's only 3. So if it has non-zero numbers on all of its nine entries, then what I'm going to see is weird cross terms. If I look at some number pre-multiplied by some vector pre-multiplying this thing and post-multiplying this thing, so if I look at something that looks like this, x transpose i of theta 0, x transpose, think of x as being theta hat MLE minus theta. So if I look at what this guy looks like, it's basically a sum over i and j of xi xj i theta 0 ij. And so if none of those things are 0, you're not going to see a sum of three terms that are squares, but you're going to see a sum of nine cross products. And it's just weird. This is not something standard. So what is Walt's test doing for you? Well, it's saying, I'm actually going to look at all the directions all at once. Some of those directions are going to have more or less variance, i.e. less or more information. And so for those guys, I'm actually going to use a different weight. So what you're really doing is putting a weight on all directions of the space at once. So what this Walt's test is doing by squeezing in the Fisher information matrix is placing your problem into the right geometry. It's a geometry that's distorted in some where balls become ellipses that are distorted in some directions and shrunk in others, or depending on if you have more variance or less variance in those directions. Those directions don't have to be aligned with the axis of your coordinate system. And if they were, then that would mean you would have a diagonal information matrix. But they might not be. And so there's this weird geometry that shows up. There's actually an entire field, admittedly a bit dormant these days, that's called information geometry. And that's really doing differential geometry on spaces that are defined by Fisher information matrices. And so you can do some pretty hardcore, something that I certainly cannot do, differential geometry just by playing around statistical models and trying to understand what the geometry of those models are. What does it mean for two points to be close in some curved space? So that's basically the idea. So this thing is basically curving your space. So again, I think I always feel satisfied when my estimator or my test does not involve just computing an average and checking if it's big or not. And that's not what we're doing here. We know that this theta hat MLE can be complicated, CF problem set 2, I believe. And we know that this Fisher information matrix can also be pretty complicated. So here, your test is not going to be trivial at all. And that required understanding the mathematics behind it. I mean, it all built upon this theorem that I just erased, I believe, which was that this guy here inside this norm was actually converging to some standard Gaussian. OK. So there's another test that you can actually use. So Wald's test is one option. And there's another option. And just like maximum likelihood estimation and method of moments would sometime agree and sometime disagree, those guys are going to sometime agree and sometime disagree. And this test is called the likelihood ratio test. So let's parse those words, likelihood ratio test. So at some point, I'm going to have to take the likelihood of something divided by the likelihood of some other thing and then work with this. And this test is just saying the following. Here's the simplest principle you can think of. If you actually don't even have to understand the notion of likelihood in the context of statistics, you just have to understand the meaning of the word likelihood. This test is just saying if I want to test h0, theta is equal to theta 0 versus theta is equal to theta 1, all I have to look at is whether theta 0 is more or less likely than theta 1. And I have an exact number that spits out given a theta 0 or theta 1. And given data, I can put in this function called the likelihood, and they tell me exactly how likely those things are. And so all I have to check is whether one is more likely than the other. And so what I can do is form the likelihood of theta, say, 1 divided by the likelihood of theta 0 and check if this thing is larger than 1. That would mean that this guy is more likely than that guy. That's a natural way to proceed. Now, there's one caveat here, which is that when I do hypothesis testing, I have this asymmetry between h0 and h1. And so I still need to be able to control what my probability of type I error is. And here, I basically have no knob. This is something, if you give me data and theta 0 and theta 1, I can compute to you and spit out the yes, no answer. But I have no way of controlling the type I and type II error. So what we do is that we replace this 1 by some number c, and then we calibrate c in such a way that the type I error is exactly at level alpha. So for example, if I want to make sure that my type I error is always 0, all I have to do is to say that this guy is actually never more likely than that guy, meaning never reject. And so if I let c go to infinity, then this is actually going to make my type I error go to 0. But if I let c go to negative infinity, then I'm going to have something that's I'm going to have my I'm always going to conclude that h1 is the right one. So I have this trade-off, and I can turn this knob by changing the values of c and get different results. And I'm going to be interested in the one that maximizes my chances of rejecting the null hypothesis while staying under my alpha budget of type I error. So this is nice when I have two very simple hypotheses. But actually, to be fair, we've actually not seen any test that corresponds to a real-life example where theta 0 was of the form am I equal to, say, 0.5 or am I equal to 0.41. We actually sort of suspected that if somebody asked you to perform this test, they're seen the data before, and they're sort of cheating. So it's typically something am I equal to 0.5 or not equal to 0.5, or am I equal to 0.5 or larger than 0.5. But it's very rare that you actually get only two points to test. Am I this guy or that guy? Now, I could go on. There's actually a nice mathematical theory, something called the Neyman-Pearson lemma that actually tells me that this test, the likelihood ratio test, is the test given the constraint of type I error that will have the smallest type II error. So this is like the ultimate test. No one should ever use anything different. And we could go on and do this. But in a way, it's completely irrelevant to practice because you will never encounter such tests. And I actually find students that they took my class as sophomores, and then they're still around a couple of years later. They're doing something. And they're like, I have this testing problem, and I want to use likelihood ratio test, the Neyman-Pearson one, but I just can't because it just never occurs. I mean, it just does not happen. So here, rather than going into details, let's just look at what building on this principle we can actually make a test that work. So now, for simplicity, I'm going to assume that my alternative, so now I still have a two-dimensional vector theta. And what I'm going to assume is that the null hypothesis is actually only testing if the last coefficients from r plus 1 to d are fixed numbers. So in this example where I have theta was equal, so if I have d equals 3, here is an example. h1, h0 is theta 2 equals 1, and theta 3 equals 2. That's my h0. But I say I don't actually care about what theta 1 is going to be. So that's my null hypothesis. I'm not going to specify right now what the alternative is. That's what the null is. In particular, this null is actually not of this form. It's not restricting it to one point. It's actually restricting it to an infinite amount of points. Those are all the vectors of the form theta 1, 1, 2, for all theta 1 in, say, r. That's a lot of vectors, and so it's certainly not like it's equal to one specific vector. So now what I'm going to do is I'm actually going to look at the maximum likelihood estimator, and I'm going to say, well, the maximum likelihood estimator, regardless of anything, is going to be close to reality. Now, if you actually tell me ahead of time that the true parameter is of this form, I'm not going to maximize over all three coordinates of theta. I'm just going to say, well, I might as well just set the second one to 1, the third one to 2, and just optimize over this guy. So effectively, I can say, if you're telling me that this is the reality, I can compute a constrained maximum likelihood estimator, which is constrained to look like what you think reality is. So this is what the maximum likelihood estimator is. That's the one that's maximizing, say, here, the log likelihood over the entire space of candidate vectors, of candidate parameters. But this partial one, this is the constrained MLE, that's the one that's actually not maximizing over all thetas, but only over the thetas that are plausible in the denied hypothesis. So in particular, what can I say if I look at ln of this constrained thing, theta hat n, c, compared to ln theta hat, let's say, n MLE? So we know which one we're. Which one is bigger? The first one is bigger. So what's the, so why? AUDIENCE 2. Because we are maximizing over a larger space. PHILIPPE RIGOLLET. So the second one is maximized over a larger space? AUDIENCE 2. PHILIPPE RIGOLLET. So the first one, so I have this all of theta, which are all the parameters I can take. And let's say theta 0 is this guy. I'm maximizing a function over all this thing. So if the true maximum is this here, then the two things are equal. But if the maximum is on this side, then the one on the right is actually going to be larger. I'm maximizing over a bigger space. So this guy has to be less than this guy. So maybe it's not easy to see. Let's say that this is theta. And this is theta 0. And now I have a function. The maximum over theta 0 is this guy here. But the maximum over the entire space is here. So the maximum over a larger space has to be larger than the maximum over a smaller space. It can be equal, but the one on the bigger space can be bigger. However, if my true theta actually did belong to theta 0, if h0 was true, what would happen? Well, if theta 0 is true, then theta is in theta 0. And since the maximum likelihood should be close to theta, it should be the case that those two things should be pretty similar. I should be in a case, not in this kind of thing, but more in this kind of position, where the true maximum is actually attained at theta 0. And in this case, they're actually of the same size, those two things. If it's not true, then I'm going to see a discrepancy between the two guys. So my test is going to be built on this intuition that if h0 is true, the values of the likelihood at theta hat MLE and at the constrained MLE should be pretty much the same. But if it's not true, then the likelihood of the MLE should be much larger than the MLE of the constrained MLE, than the likelihood of the constrained MLE. And this is exactly what this test is doing. So that's the likelihood ratio test. So rather than looking at the ratio of the likelihoods, we look at the difference of the log likelihoods, which is really the same thing. And there's some weird normalization factor 2 that shows up here. And this is what we get. So if I look at the likelihood ratio test, so it's looking at 2 times ln of theta hat MLE minus ln of theta hat MLE constrained. And this is actually the test statistic. So we've actually decided that this test statistic is what? It's non-negative, right? We've also decided that it should be close to 0 if h0 is true. And of course, then maybe far from 0 if h0 is not true. So what should be the natural test based on Tn? Let me just check that it's already there. So the natural test is something that looks like indicator that Tn is larger than c. And you should say, well, again, we just did that. And it is basically the same thing that we just did. Agreed? But the Tn now is different. The Tn is the difference of log likelihoods, whereas before the Tn was this theta hat minus theta 0 transpose identity of Fisher information matrix theta hat minus theta 0. And there's no reason why this guy should be of the same form. Now, if I have a Gaussian model, you can check that those two things are actually exactly the same. But otherwise, they don't have any reason to be. And now what's happening is that under some technical conditions, if h0 is true, so now what happens is that if I want to calibrate c, what I need to do is to look at what is the value of what is the c such that this guy is equal to alpha. And that's for the distribution of T under the null. Right? But there's not only one, right? The null hypothesis here was actually just a family of things. It was not just one vector. It was an entire family of vectors, just like in this example. So if I want my type I error to be constrained over the entire space, what I need to make sure of is that the maximum overall theta in theta 0 is actually equal to alpha. Agreed? Yeah? AUDIENCE 2 So h0 is like that. So what is h1? PHILIPPE RIGOLLETT-LAMARCYS So not equal. In this case, it's going to be not equal. I mean, it can really be anything you want. It's just you're going to have a different type to error. Because here, we're sort of like stuck in a corner. We've built this T. It has to be small under the null. And whatever not the null is, we just hope that it's going to be large. So even if I tell you what the alternative is, you're not going to change anything about the procedure. OK, so here, q alpha. So what I need to know is that if h0 is true, then Tn, in this case, actually converges to some chi-square distribution. And now here, the number of degrees of freedom is kind of weird, right? But actually, what it should tell you is, oh, finally, I know what you call this parameter, degrees of freedom, rather than dimension or just the parameter. It's because here, what we did is we actually pinned down everything but r. So sorry, we pinned down everything but r coordinates of this thing, right? And so now I'm actually wondering why. Yeah, did I make a mistake here? I think this should be a chi-square with r degrees of freedom. Let me check and send you an update about this, because the number of degrees of freedom, if you talk to normal people, they will tell you that here, the number of degrees of freedom is r, right? This is what's allowed to move, and that's what's called a degrees of freedom. The rest is pinned down to being something. So here, this chi-square should be a chi-squared r, OK? And that's something you just have to believe me. Anybody guess what theorem is going to tell me this? Well, in some cases, it's going to be Cochrane's theorem, OK? It's just something that tells me that things work. Now here, I used a very specific form of the null alternative. And so for those of you who are sort of familiar with linear algebra, what I did here is I h0 consistent saying that theta belongs to a r-dimensional linear space, right? It's actually here, the r-dimensional linear space of vectors that have the first r-coordinates that can move and the last coordinates that are fixed to some number, OK? Actually, it's an affine space, because it doesn't necessarily go through 0. And so I have this affine space that has dimension r. And if I were to constrain it to any other r-dimensional space, that would be exactly the same thing. And so to do that, essentially what you need to do is to say, if I take any matrix that's invertible, let's call it u, and then I just say, I want h0 is going to be something like u is of the form u times theta. And now I look only at the coordinates r plus 1 to d, then I want to fix those guys to some numbers. So let's call them, I want to call them theta, so let's call them tau. So it's going to be tau r plus 1 all the way to tau d. OK, so this is not part of the requirements. But just so you know, it's really not a matter of keeping only some coordinates. Really, what matters is the dimension in the sense of linear subspaces of the problem. And that's what determines what your degrees of freedoms are. OK, so now that we know what the asymptotic distribution is under the null, then we know, basically, that we know how to control. We know which table we need to pick our q alpha from. And here again, the table is a chi-square table. But here, the number of degrees of freedom is this weird d minus r degrees of freedom thing. I just said it was r. I'm just checking, actually, if I'm. Yeah, it's r. It's definitely r. And so here, we've made tests. We're testing if our parameter theta was explicitly in some set or not. Was it explicitly? By explicitly, I mean we're saying, is theta like this or is theta not like this? Is theta equal to theta 0 or is theta 0 equal to theta 0? Are the last coordinates of theta equal to those fixed numbers or are they not? It was something I was stating directly about theta. But there's going to be some instances where you actually want to test something about a function of theta, not theta itself. For example, is the difference between the first coordinate of theta and the second coordinate of theta positive? That's definitely something you might want to test because maybe theta 1 is, OK, let me try to think of some good example. I don't know. Maybe theta 1 is your drawing accuracy with the right hand and theta 2 is the drawing accuracy with the left hand. And I'm actually collecting data on young children to be able to test early on whether they're going to be left-handed or right-handed, for example. And so I want to just compare those two with respect to each other. But I don't necessarily need to know what the absolute score for this handwriting skills are. So sometimes it's just interesting to look at the difference of things or maybe the sum, say, the combined effect. Maybe this is my two measurements of blood pressure. And I just want to talk about the average blood pressure. And so I can make a linear combination of those two. And so those things implicitly depend on theta. And so I can generically encapsulate them in some test of the form g of theta is equal to 0 versus g of theta is not equal to 0. And sometimes in the first test that we saw, g of theta was just the identity or maybe the identity minus 0.5. If g of theta is theta minus 0.5, that's exactly what we've been testing. g of theta is theta minus 0.5. And theta is p, the parameter of a coin. This is exactly of this form. So this is a simple one. But then there's more complicated ones we can think of. All right, so now how can I do this? Well, let's just follow our recipe. Everything we did was to try so we trace back. We were trying to build a test statistic which was pivotal. We wanted to have this thing that had nothing that depended on the parameter. And the only thing we had for that that we built even our chi-square test on is basically some form of central limit theorem. Maybe it's for the maximum likelihood estimator. Maybe it's for the average. But it's basically some form of asymptotic normality of the estimator. And that's what we started from every single time. So let's assume that I have this. And I'm going to talk very abstractly. Let's assume that I start with an estimator. Doesn't have to be the MLE. It doesn't have to be the average. But it's just something. And I know that I have an estimator such that this guy converges in distribution to some n0. And I have some covariance matrix theta. Maybe it's not the Fisher information. Maybe that's something that's not as good as the MLE, meaning that this is going to give me less information than the Fisher information, less accuracy. And now I can actually just say, OK, if I know this about theta, I can apply the multivariate delta method, which tells me that square root of n g of theta hat minus g of theta goes in distribution to some n0. And then the price to pay in one dimension was multiplying by the square of the derivative. And we know that in multivariate dimension, it's pre-multiplying by the gradient, post-multiplying by the gradient. So I'm going to write delta g of theta transpose sigma. Sorry, not delta, nabla g of theta. So gradient. And here, I assume that g takes values into rk. That's what's written here. g takes value from d to k. But think of k as being 1 for now. So the gradient is really just a vector and not a matrix. That's your usual gradient for real-valued functions. So effectively, if g takes values in dimension 1, what is the size of this matrix? I only ask trivial questions. Remember, that's rule number 1. It's 1 by 1, right? And you can check it, because on this side, those are just the difference between numbers. And it would be kind of weird if they had a covariance matrix at the end. I mean, this is a random variable, not a random vector. So I know that this thing happens. And now, if I basically divide by the square root of this thing, so for the board, I'm working with k is equal to 1, divided by square root of delta g of theta transpose sigma delta nabla, sorry, g of theta, then this thing should go to some standard normal random variable, standard normal distribution. I just divide it by square root of the variance here, which is your usual thing. Now, if you do not have a univariate thing, you do the same thing we did before, which is pre-multiply by the covariance matrix to the negative 1 half. So before, this role was played by the inverse Fisher information matrix. That's why we ended up having i of theta to the 1 half. And now, we just have this gamma, which is just this function that I wrote up there, and could be potentially k by k if g takes values into rk. Yes? AUDIENCE 2 So is the gradient of a vector just the gradient of each component? PHILIPPE RIGOLLETT-GABARITIERI Yeah, the gradient of a vector is just the vector with all the derivatives with respect to each component. Yes. So you know the word vector for derivatives, but not for vectors? I mean, the word gradient you use for one dimensional? AUDIENCE 2 One dimensional. PHILIPPE RIGOLLETT-GABARITIERI OK. Yes, derivative in one dimension. Now, of course, here, you notice there's something. I actually have a little caveat here. I want this to have rank k, right? I want this to be invertible. I want this matrix to be invertible. Even for the Fisher information matrix, I sort of needed to be invertible, even for the original theorem. That was part of my technical condition, just so that I can actually write Fisher information matrix inverse. And so here, you can make your life easy and just assume that it's true all the time, because I'm actually writing in a fairly abstract way. But in practice, we're going to have to check whether this is going to be true for specific distributions. And we'll see an example towards the end of the chapter, the multinomial, where it's actually not the case that the Fisher information matrix exists. The covariance matrix, the asymptotic covariance matrix, is not invertible. So it's not the inverse of the Fisher information matrix. Because to be the inverse of someone, you need to be invertible yourself. OK? And so now, what I can do is apply Slutsky, right? So here, what I needed to have is theta, the true theta. So what I can do is just put some theta hat in there. And then I'm just writing that. So that's the gamma of theta hat that I see there. And if theta is true, then g of theta is equal to 0, right? That's what we assumed. That was our theta 0. h 0 was that under h 0, g of theta is equal to 0. So the number I need to plug in here, I don't need to replace theta here. What I need to replace here is 0. Now, let's go back to what you were saying. Here, you could say, let me try to replace 0 here. But there's no such thing. There's no g here. It's only the gradient of g. So this thing that says replace theta by the theta 0 wherever you see it could not work here. If g was invertible, I could just say that theta is equal to g inverse of 0 in the null. And then I could plug in that value. But in general, it doesn't have to be invertible. And it might be a pain to invert g even. I mean, it's not clear how you can invert all functions like that. And so here, you just go with Sletsky. And you say, OK, I'm just going to put theta hat in there. But this guy, I know I need to check whether it's 0 or not. Same recipe we did for theta, except we do it for g of theta now. And now, I have my asymptotic thing. I want to check. I know this has a pivotal distribution. This might be a vector. So rather than looking at the matrix itself, I'm going to actually look at the norm. Rather than looking at the vectors, I'm going to look at their square norm. That gives me a chi-square. And I reject when my test statistic, which is the norm squared, exceeds the quintile of a chi-square. Same as before, just do it on your own. Before we part ways, I wanted to just mention one thing, which is, look at this thing. If g was of dimension 1, the Euclidean norm in dimension 1 is just the absolute value of the number, which means that when I'm actually computing this, this is actually, so I'm looking at the square. So it's the square of something. So it means that this is the square of a Gaussian. And it's true that, indeed, a chi-square 1 is just the square of a Gaussian. But OK, sure, this is the tautology. But let's look at this test now. This test was built using Wald's theory and some pretty heavy stuff. But now if I start looking at Tn, and I think of it as being just the absolute value of this quantity over there, squared, what I'm really doing is I'm looking at whether the square of some Gaussian exceeds the quintile of a chi-square of 1 degree of freedom, which means that this thing is actually equivalent, completely equivalent, to the test. So if k is equal to 1, this is completely equivalent to looking at the absolute value of something and check whether it's larger than, say, q alpha over 2. Well, then q alpha, well, that's q alpha over 2, so that the probability of this thing is actually equal to alpha. And that's exactly what we've been doing before when we introduced tests in the first place. We just took absolute values, said, well, it's the absolute value of a Gaussian in the limit, and so it's the same thing. So this is actually equivalent to the probability that the norm squared is larger. So that's the chi-square of some normal. And that's the q alpha of some chi-squared with 1 degree of freedom. Those are exactly the two same tests. So in one dimension, those things just collapse into being one little thing. And that's because there's no geometry in one dimension. It's just one dimension. Whereas if I'm in higher dimension, then things get distorted, and things can become weird.