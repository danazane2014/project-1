 OK, good. The final class in linear algebra at MIT this fall is to review the whole course. And, you know, the best way I know how to review is to take old exams and just think through the problems. So it'll be a three-hour exam next Thursday. Anybody who's got the --. I mean, nobody will be able to take an exam before Thursday. Anybody who needs to take it in some different way after Thursday should see me next Monday. I'll be in my office Monday. OK. Shall, may I just read out some problems and, I'll, let me bring the board down. And let's start. OK. Here's a question. This is a, this is about an, a three by n matrix. And we're given. So we're given, given A x equals one zero zero has no solution. And we're also given A x equals zero one zero has exactly one solution. OK. So you can probably anticipate my first question. What can you tell me about M? It's an M by n matrix of rank r, as always. What can you tell me about those three numbers? So what can you tell me about M? The number of rows, n, the number of columns, and r, the rank. OK. C. Do you want to tell me first what M is? How many rows in this matrix? Must be three, right? We can't tell what M is, but we can certainly tell that M is three. OK. And what do we know about, so what do these things tell us? Let's take them one at a time. When I discover that some equation has no solution, that there's some right-hand side with no answer, what does that tell me about the rank of the matrix? It's smaller than M, is that right? If there's no solution, that tells me that the rank, that the, that some rows of the matrix are combinations of other rows. Because if I had a pivot in every row, then I would certainly be able to solve the system. I would have particular solutions and all the good stuff. So if, if any time that there's a system that has with no solutions, that tells me that R must be below M. What about, the fact that if when there is a solution, there's only one? What does that tell me? Well, normally, there'd be one solution and then we could add in anything in the null space. So this is telling me the null space only has the zero vector in it. There's just that, there's just one solution, period. So what does that tell me? The null space has only the zero vector in it. That tells me, what does that tell me about the relation of R to N? So this is one solution only, that means the null space of the matrix must be just the zero vector. And what does that tell me about R and N? They're equal. The columns are independent. So I've got now R equal N and R less than M and now I, and I also know M is three. So those are really the facts I know. N equals R and those numbers are smaller than three. Sorry, yeah, yeah, R is smaller than M and N, of course, is also. So I guess this summarizes what we can tell. In fact, why not give me a matrix, because I would often ask for an example of such a matrix. Can you give me a matrix A that's an example, that shows this possibility, exactly that. That there's no solution with that right-hand side, but there's exactly one solution with this right-hand side. Anybody want to suggest a matrix that does that? Let's see. What do I, what vector do I want in, in the column space? I want zero one zero to be in the column space, because I'm, I'm able to solve for that. So let's put zero one zero in the column space. Actually, I could stop right there. That would be a matrix with M equal three, three rows, and N and R are both one, rank one, one column, and, of course, there's no solution to that one. So that's perfectly good as it is. Or if you kind of have a prejudice against matrices that only have one column, I'll, I'll accept a second column. So what could I include as a second column that would just be a different answer but equally good? I could put this vector in the column space, too, if I wanted. That would now be a case with R equal N equal two, but, of course, three, M is still three, and this vector is not in the column space. So you're, this is just like prompting us to remember all those things. Column space, null space, all that stuff. Now I probably asked a second question about this type of thing. Ah. Okay. Oh, I even asked, write down an example of a matrix that fits the description. Hm. I guess I haven't learned anything in, twenty-six years. Okay. Cross out all statements that are false about any matrix with these, so again, these are, this is the preliminary, these are the facts about my matrix. This is one example, but, of course, by having an example, it'll be easy to check some of these facts, or non-facts. Let me, let me write down some, some possible facts. So this is really true or false. The determinant, this is part one, the determinant of A transpose A is the same as the determinant of AA transpose. Is that true or not? Second one, A transpose A is invertible. Is invertible. Third possible fact, AA transpose is positive definite. So you see how on an exam question I try to connect the different parts of the course. So, well, I mean, the simplest way would be to try it with that matrix as a, as a, as a good example, but maybe we can answer, even directly. Let, let me take number two first. Because I'm, you know, I'm very, very fond of that matrix, A transpose A. And when is it invertible? When is the matrix A transpose A invertible? The great thing is that I can tell from the, from the rank of A that I don't have to multiply out A transpose A. A transpose A is invertible, well, if A has a null space, other than the zero vector, then it's no way it's going to be invertible. But the beauty is, if the null space of A is just the zero vector, so the fact, the, the key fact is this is invertible if R equals N, by which I mean independent columns of A, in A, in the matrix A. If R equals N, if the matrix A has independent columns, then this combination A transpose A is square and still that same null space, only the zero vector, independent columns, all good. And so what's the, what's the true false? Is it, is this middle one T or F for this, in this setup? Well, we discovered that, we discovered that, that R was N, from that second fact. So this is a true. That's a true. And of course, A transpose A in this example would probably be, what would A transpose A be for that matrix? Can you multiply A transpose A and see what it looks like for that matrix? What shape would it be? It'll be two by two. And what matrix will it be? The identity. So, checks out. OK, what about A A transpose? Well, depending on the shape of A, it could be good or not so good. It's always symmetric, it's always square, but what's the size now? This is three by N and this is N by three, so the result is three by three. Is it positive definite? I don't think so. False. If I multiply that by A transpose, A A transpose, what would the rank be? It would be the same as the rank of A, that's, that's, it would be just rank two. And if it's three by three and it's only rank two, it's certainly not positive definite. So what's, what could I say about A A transpose if I wanted to, like, say something true about it? It's true that it is positive semi-definite. If I made this semi-definite, it would always be true, always. But if I'm looking for positive definite, then I'm looking at the null space of whatever's here and in this case, it's got a null space. So A A, so we just figure it out here. A A transpose for that matrix will be three by three. If I multiplied A by A transpose, what would the first row be? All zeros, right? First row of A A transpose could only be all zeros, so it's probably a one there and a one there, something like that, but, I don't even know if that's right, but it's all zeros there, so it's certainly not positive definite. Let me not put up anything I'm not, don't check. What about this determinant? Oh, well, I guess that's a sort of tricky question. Is it true or false in this case? It's false, apparently, because A transpose A is invertible, we just got a true for this one and we got a false, we got a non-invertible one for this one, so actually, this one is false, number one. That surprises us, actually, because it's, I mean, why was it tricky? Because what is true about determinants? This would be true if those matrices were square. If I have two square matrices, A and any other matrix, B, could be A transpose, could be somebody else's matrix. Then it would be true that the determinant of B A would equal the determinant of A B. But if the matrices are not square, and it would actually be true that it would be, that would equal, that this would equal the determinant of A times the determinant of A transpose. We could even split up those two separate determinants, and of course those would be equal. But only when A is square. So that's a question that rests on the falseness, rests on the fact that the matrix isn't square in the first place. OK, good. Let's see. Oh, now, even asks more. Prove that A transpose y equals c, ha, god, it's, this question goes on and on. Now I ask you about A transpose y equals c. So I'm asking you about the equation, about the matrix A transpose. And I want you to prove that it has at least one solution, one solution for every c, every right-hand side c, and in fact, infinitely many solutions for every c. OK. Well, none of this is difficult, but it's been a little while, so we just have to think again. When I have a system of equations, this is, this matrix A transpose is now, instead of being three by n, it's n by three, it's n by m, of course. To show that a system has at least one solution, when does the system always solvable? When it has full row rank, when the rows are independent. Here we have n rows and that's the rank. So at least one solution because the number of rows, which is n, for the transpose is equal to r, the rank. This, this has, this A transpose has independent rows because A had independent columns, right? The original A had independent columns. When we transpose it, it has independent rows, so there's at least one solution. But now how do I even know that there are infinitely many solutions? Oh, I, I, what do I, I want to know something about the null space. What's the dimension of the null space of A transpose? So, so the answer has got to be the dimension of the null space of A transpose, what's the general fact if, if it's an, if I have, if A is an m by n matrix of rank r, what's the dimension of A transpose? The null space of A transpose? Do you remember that little fourth subspace that's tagging along down in our big picture? Its dimension was m-r. And that's bigger than zero. m is bigger than r. So there's a lot in that null space. So there's always one solution because n, this is speaking about A transpose. So for A transpose, the roles of m and n are reversed, of course. So I'm, keep, keep in mind that, that this board was about A transpose, so the roles, so it's the null space of A transpose, and there are m-r free variables. OK, that's, like, just some, review. Can I take another problem that's also sort of, suppose the matrix A has three columns, v1, v2, v3. Those are the columns of the matrix. All right. Question A. Solve. Ax equals v1 minus v2 plus v3. Tell me what x is. Well, there you're seeing the most, the one absolutely essential fact about matrix multiplication, how does it work when it, when we do it a column at a time. The very, very first day, way back in September, we did multiplication a column at a time. So what's, what's x? Just tell me. One minus one, one. Thanks. OK. Everybody's got that. OK? Then the next question is, suppose that combination is zero? Oh, yeah. OK. So question B says, part B says, suppose this thing is zero. Suppose that's zero. Then the solution is not unique. Suppose, no, so I want true or false. And a reason. Suppose this, this combination is zero. v1 minus v2 v3. Show that, what can, what does that tell me? So it's a separate question. Maybe I, I sort of saved time by writing it that way, but it's a separate, totally separate question. If I know, if I have a matrix and I know that column one minus column two plus column three is zero, what does that tell me about whether the solution is unique or not? Is there more than one solution? What's uniqueness about? Uniqueness is about, is there anything in the null space, right? The solution is unique when there's nobody in the null space except the zero vector. And if that's zero, then this guy would be in the null space. So, so then, so if this were zero, then, then this x is in the null space of A. So solutions are never unique. Because I could always add that to any solution and it, and Ax would, wouldn't change. So, so it's always that question. Is there somebody in the null space? Okay. Third, oh, now, here's a totally different question. Suppose those three vectors, v1, v2, v3, are orthonormal. So that's, I mean, they, this isn't going to happen for orthonormal vectors. Okay, so part c, forget part b. C. If v1, v2, v3 are orthonormal, orthonormal, so that I would usually have called them q1, q2, q3. Now, what combination, oh, here's a nice question, if I say so myself. What combination of v1 and v2 is closest to v3? What point on the plane of v1 and v2 is the closest point to v3 if these vectors are orthonormal? So let me, I'll start the sentence. Then the combination something times v1 plus something times v2 is closest, is the closest combination to v3. And what's the answer? What's the closest vector on that plane to v3? Zeros, right. We just imagine the x, y, z axis, v1, v2, v3 could be the standard basis, the x, y, z vectors, and of course, the point on the x, y plane that's closest to v3 on the z axis is zero. So if we're orthonormal, then the projection of v3 onto that plane is, it's perpendicular, it hits right at zero. OK, so that's like a quick, you know, easy question, but still brings it out. OK. Let me see what, shall I write down a, yeah, a Markov matrix, and I'll ask you for its eigenvalues. OK. Here's a Markov matrix, this, and tell me its eigenvalues. So here, I'll call the matrix A and I'll call the, this is point two, point four, point four, point four, point two, point four, point three, point three, point four. OK. Let's see. It helps out by, to notice that column one plus column two, what's interesting about column one plus column two? It's twice as much as column three. So column one plus column two equals two times column three. I put that in there, column one plus column two equals twice column three. That's an observation. OK, I want to, tell me the eigenvalues of the matrix. OK, tell me one eigenvalue? Zero. Because the matrix is singular. Tell me another eigenvalue? One, because it's a Markov matrix, the columns add to the all ones vector and, that will be a eigenvector of A transpose. And tell me the third eigenvalue? Let's see, to make the trace come out right, which is point eight, we need minus point two. OK. OK. And now, suppose I start the Markov process. Suppose I start with a U of zero, so I'm going to look at the powers of A applied to U of zero. This is U k. And there's my matrix, and I'm going to let U of zero be, this is going to be zero ten zero. And I want it, my question is, what does that approach? If U of zero is equal to this, if there's U of zero, shall I write it in? Maybe I'll just write in U of zero. A to the k, starting with ten people in state two, and every step follows the Markov rule, what's the, what's the, what, what does the solution look like after k steps? Let me just ask you that. And then what happens as k goes to infinity? This is a steady state question, right? I'm looking for the steady state. Actually, the question doesn't ask for the k step answer, it just jumps right away to infinity. But how would I express the solution after k steps? It would be some multiple of the first eigenvalue times the first eigen- to the k-th power times the first eigenvector plus some other multiple of the second eigenvalue times its eigenvector and some multiple of the third eigenvalue times its eigenvector. OK. Good. And these eigenvalues are zero, one, and minus point two. So what happens as k goes to infinity? The only thing that survives the steady state, so at u infinity, this is gone, this is gone, all that's left is c2 x2. So I better find x2. I've got to find that eigenvector to complete the answer. What's the eigenvector that corresponds to lambda equal one? That's the key eigenvector in any Markov process is that eigenvector. Lambda equal one is an eigenvalue, I need its eigenvector x2, and then I need to know how much of it is in the starting vector u0. OK. So how do I find that eigenvector? I guess I subtract one from the diagonal, right? So I have minus point eight, minus point eight, minus point six, and the rest, of course, is just still point four, point four, point four, point four, point three, point three, and hopefully that's a singular matrix. So I'm looking to solve A minus I equal the A minus I x equals zero. Let's see, can anybody spot the solution here? I don't know, I didn't make it easy for myself. What do you think there? Maybe those, since those, I'm just thinking aloud here, those first, let me guess, those first two entries might be, oh no, what do you think? Anybody see it? We could use elimination if we were desperate. Are we that desperate? Anybody, just call out if you see the vector here that's in that null space. There better be a vector in that null space or I'm quitting. OK. Well, I guess we could use elimination. Seems, I thought maybe somebody might see it from further away. You think, is there a chance that these guys are, could it be that these two are equal and this is whatever it takes, like something like three three two? Would that possibly work? I mean, that's great for this. No, it's not that great. Three three four, this is, deeper mathematics you're watching now. Three three four, that, is that, it works, don't mess with it. It works. Yeah, OK, it works, all right. And yes, OK, and, so that takes two, three three four. And, how much of that vector is in, is in the starting vector? Well, we could go through a complicated process, but what's the beauty of Markov things? That the total number of, the total population, the sum of these doesn't change. That the total number of people, they're moving around, but they don't get born or die, or get dead. So, there's ten of them at the start, so there's ten of them there, so c2 is actually one. Yeah. So that would be the correct solution. OK, that would be the U infinity. OK, so I used there in that process sort of the main facts about Markov matrices to, to get a jump on the, answer. OK. Let's see. OK, here's some, kind of quick, short questions. Maybe I, I'll move over to this board and leave that for the moment. I'm looking for two by two matrices. And I'll read out the property I want, and you give me an example or tell me there isn't such a matrix. All right. Here we go. First, so two by twos. First I want to project, I want the projection onto the line through A equals four minus three. So it's a one, one D, a one-dimensional projection matrix I'm looking for. And what's the formula for it? What's the formula for the projection matrix P onto a line through A? And then we'll just plug in this particular A. You remember that formula? There's an A and an A transpose and we can, normally we would have an A transpose A inverse in the middle, but here we just got numbers, so we just divide by it and then plug in A and we've got it. OK, so equals. You can put in the numbers. Trivial, right? OK. Number two, the matrix that has eigenvalues zero- and so this is a new problem, the matrix with eigenvalues zero and three and eigenvectors, well, let me write these down. Eigenvalue zero, eigenvector one two, eigenvalue three, eigenvector two one. I'm giving you the eigenvalues and eigenvectors instead of asking for them. Now I'm asking for the matrix. What's the matrix, then? What's A? Here was a formula, then we just put in some numbers. What's the formula here, into which we'll just put the given numbers? It's the S lambda S inverse, right? So it's S, which is this eigenvector matrix, it's the lambda, which is the eigenvalue matrix, it's the S inverse, whatever that turns out to be. Let me just leave it as inverse. That has to be it, right? Because if we went in the other direction, that matrix S would diagonalize A to produce lambda. So it's S lambda S inverse. Good. OK. Ready for number three. A real matrix that cannot be factored into A --. I'm looking for a matrix A that doesn't, that never could equal B transpose B for any B. A two by two matrix that could not be factored in the form B transpose B. So all you have to do is think, well, what does B transpose B look like, and then pick something different. What do you suggest? Let's see. What should we take for a matrix that could not be, could not have this form B transpose B? Well, what do we know about B transpose B? It's always symmetric. So just give me any non-symmetric matrix that couldn't possibly have that form. OK. And let me ask the fourth part of this question. A matrix that has orthogonal eigenvectors, but it's not symmetric. Tell me a matrix, how could a matrix, what matrices have orthogonal eigenvectors, but they're not symmetric matrices? What other matrices, tell me a different, other families of matrices that have orthogonal eigenvectors. We know symmetric matrices do, but others also. So I'm looking for orthogonal eigenvectors, and what do you suggest? The matrix could be skew-symmetric. It could be an orthogonal matrix. So I, it could be symmetric, but that was too easy, so I ruled that out. It could be skew-symmetric, like, like, one minus one, like that. Or it could be an orthogonal matrix, it could be an orthogonal matrix like cosine, sine, minus sine, cosine. All those matrices would have complex orthogonal eigenvectors. But they would be orthogonal, and so those examples are fine. OK. We can continue a little longer if you would like to, with, from this exam, from these exams. Least squares? OK, here's a least squares problem in which to make life quick, I've given the answer, it's like Jeopardy, right? I just give the answer and you give the question. OK. Oops, we've, sorry. I'll, let's see, can I stay over here for the, for the, next question. OK. Least squares. So I'm, I'm giving you the problem, one one one zero one two CD equals three four one, and that's B, of course. This is A x equal B. And the least squares solution, so may I put C hat D hat, to emphasize it's not the true solution, has lea- so the least squares solution, maybe I should, the hats really go here, is eleven thirds and minus one. Of course, you could have figured that out in no time. So this year I'll ask you to do it, probably. But suppose we're given the answer, then let's just remember what, what happened. What is the proj- OK, good question. What's the projection P of this vector onto the column space of that matrix? What's, so, shall I write that question down? What? What is P, the projection? Projection of B onto the column space of A is what? Hopefully that's what the least squares problem solved. What is it? It's, this was the best solution. This, it's eleven thirds times column one plus, or rather minus one times column two, right? That's what least squares did. It found the combination of the columns that was as close as possible to B. That's what least squares was doing. It found the projection. OK? Secondly, draw the straight line problem that corresponds to this system. So I guess that the straight line, straight line, fitting a straight line problem, we kind of recognize. We recognize these are the heights and these are the points. So at zero, one, two, the heights are three, and at, at t equal to one, the height is four, one, two, three, four, and at t equal to two, the height is one. So I'm trying to fit the best straight line through those points. God. Uh, I could fit a triangle very well, but, uh, where's the best, I don't even know which way the best straight line goes. Maybe it goes, oh, I do know how it goes, because there's the answer, yes. It has a height eleven thirds and it has slope minus one, so it's something like that. OK. Great. Now, finally, and this completes the course, find a vector, a different vector B, not all zeros, for which the least square solution would be zero. So I want you to find a different B so that the least square solution changes to all zeros. So tell me what I'm really looking for here. I'm looking for a B where the best combination of these two columns is the zero combination. So what kind of a vector B am I looking for? I'm looking for a vector B that's orthogonal to those columns. It's orthogonal to those columns. It's orthogonal to the column space, the best possible answer is zero. So a vector B that's orthogonal to those columns, let's see, maybe, maybe, uh, one of those, minus two of those, and one of those, that would be orthogonal to those columns, and the best vector would be zero zero. OK. So that's as many questions as I can do in an hour, but you get three hours. And, let me just say, as I've said by email, thanks very much for your patience as this series of lectures was videotaped, and, thanks for filling out these forms, maybe just leave them on the table up there as you go out, and, above all, thanks for taking the course. Thank you. Thanks.