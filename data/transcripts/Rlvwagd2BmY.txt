 All right, last time we started talking about pseudorandom graphs. And we considered this theorem of Chung, Graham, and Wilson, which for dense graphs gave several equivalent notions of quasi-randomness that, at least at face value, do not appear to be all that equivalent. But they are actually, you can deduce one from the other. There was one condition at the very end which had to do with eigenvalues. And basically, it said that if your second largest eigenvalue in absolute value is small, then the graph is pseudorandom. So that's something that I want to explore further today to better understand the relationship between eigenvalues of a graph and the pseudorandomness properties. For pretty much all of today, we're going to look at a special class of graphs known as MD lambda graphs. So this just means we have n vertices. And we're only going to consider, mostly out of convenience, d regular graphs. So this will make our life somewhat simpler. And the lambda stands for that if you look at the adjacency matrix. And if you write down the eigenvalues of the adjacency matrix, then, well, what are these eigenvalues? The top one, because it's d regular, is equal to d. And the lambda corresponds to the statement that all the other eigenvalues are, at most, lambda in absolute value. So the top one is equal to d. All the other ones in absolute value, so it could be basically the maximum of these two, is bounded above by lambda. And at the end of last time, we showed this expander mixing lemma, which, in this language, says that if G is MD lambda, then one has the following discrepancy type pseudo-randomness property, namely that if you look at two vertex sets and look at how many actual edges are between them compared to what you expect if this were a random graph of a similar density, then these two numbers are very similar. And the amount of error is controlled by your lambda. Particular, a smaller lambda gives you a more pseudo-random graph. So the second part of today's class, I want to explore the question of how small this lambda can be. So what's the optimal amount of pseudo-randomness? But first, I want to show you some examples. So so far, we've been talking about pseudo-random graphs. And the only example, really, I've talked about is that a random graph is pseudo-random, which is true. A random graph is pseudo-random with high probability. But somehow, the spirit of pseudo-randomness is to come up with non-random examples, come up with deterministic constructions that give you pseudo-random properties. So I want to begin today with an example. A lot of examples, especially for pseudo-randomness, come from this class of graphs called Cayley graphs, which are built from a group. So we're going to reserve the letter G for graphs. I'm going to use gamma for a group. And I have a subset S, S of gamma. And S is symmetric in that if you invert the elements of S, they remain in S. Then we define the Cayley graph, given by this group and the set S, to be the following graph, where V, the set of vertices, is just the set of group elements. And the edges are obtained by taking a group element and multiplying it by S to go to its neighbor. So this is a Cayley graph. And Cayley graphs are, when any group start with any subset of the group, you get a Cayley graph. And this is a very important construction of graphs that have lots of nice properties. In particular, an example of a Cayley graph is a Paley graph. They're not related. So a Paley graph is a special case of a Cayley graph obtained by considering the group, the cyclic group mod P, where P is prime 1 mod 4. And I'm looking at S being the set of quadratic residues mod P. So it's actually non-zero quadratic residues. So elements of mod P, that could be a square. So we will show in a second that this Paley graph has nice pseudorandom properties by showing that it is an MD lambda graph with lambda fairly small compared to the degree. Just a historical note, so Raymond Paley, so the Paley graph named after, he actually was from the earlier part of the 20th century, so from 1907 to 1933. So he died very young, at the age of 26, and actually died in an avalanche when he was skiing by himself in Banff. So Banff is a national park in Alberta in Canada. And when I was in Banff earlier this year for a math conference, so there's also a math conference center there, so I had a chance to go visit Raymond Paley's tomb. So there's a graveyard there where you can find his tomb. It's very sad that in his short mathematical time span, actually he managed to do a lot of amazing mathematical, find a lot of amazing mathematical discoveries. And there are many important concepts named after him, so things like Paley-Wiener theorem, Paley-Zigmund, Littlewood-Paley, all these important ideas in analysis named after Paley. And Paley graph is also one of his contributions. So what we'll claim is that this Paley graph has the desired pseudorandom properties in that if you look at its eigenvalues, then the top eigenvalues, except for the top eigenvalue, all the other eigenvalues are quite small. So keep in mind that the size of S is basically half of the group, so P minus 1 over 2. So for especially larger values of P, these eigenvalues are quite small compared to the degree. So the main way to show a Paley graph like that have small eigenvalues is to just compute what the eigenvalues are. And this is actually not so hard to do for Paley graph, so let me do this explicitly. So I will tell you very explicitly a set of eigenvectors. And they are the first eigenvector is just the O1's vector. The second eigenvector is the vector coming from 1, omega, omega squared, so omega to the P minus 1, where omega is a primitive P through the unity. The next one is 1, omega squared, omega 4, all the way to omega P, omega to the 2 times P minus 1, and so on. So I want to have, yeah, so OK, so I make this list, and I have P of them. So these are my eigenvectors. And let me check that they are actually eigenvectors, and then we can also compute their eigenvalues. So the top eigenvector corresponds to D. So the O1's vector in a D regular graph has always an eigenvector. That's eigenvalue D. And the other ones, we'll just do this computation. So instead of getting confused with indices, let me just compute as an example the j-th coordinate of the adjacency matrix times V2. So the j-th coordinate, so what it comes to, so is the following sum if I run over S, then omega raised to j plus S. So S is symmetric, so I don't have to worry so much about plus or minus. So let's say j plus S. So if you think about what this Cayley graph, how it is defined, if you hit this vector with that matrix, the j-th coordinate is that sum there. But I can rewrite the sum by taking out this common factor omega to j. And you see that this is the j-th coordinate of V2. And this is true for all j. So this number here is lambda 2. And more generally, lambda k is the following sum for k from 0, so from k being 1 through p. So when you plug in k equals to 1, you just get d. And the others are sums of these exponential sums. Now, this is a pretty straightforward computation. And in fact, we're not using anything about quadratic residues. This is a generic fact about Cayley graphs of Z mod p. So this is true for all Cayley graphs S, not necessarily for quadratic residues. And the basic reason is that here you have this set of eigenvectors, and they do not depend on S. So you might know this concept from other places, such as circular matrices and whatnot. But this is true and a simple computation. So now we have the values of lambda explicitly. I can now compute their sizes. I want to know how big this lambda is. Or the first one, when k equals to 1, is exactly d, the degree, which is p minus 1 over 2. But what about the other ones? So for the other ones, we can do a computation. As follows. So note that I can rewrite lambda k by noting that if I take twice it and plus 1, then I obtain the following sum. So here I'm using the S as a set of quadratic residues. So if I consider the sum here, every quadratic residue gets counted twice, except for 0, which gets counted once. And now I would like to evaluate the size of this sum, this exponential sum. And this is something that's known as a Gauss sum. So basically, a Gauss sum is what happens when you have something that's like a quadratic exponential sum with a quadratic dependence in the exponent. And the trick here is to consider the square of the sum, so the magnitude squared. Now, if I expand the square, so the squaring is a common feature of many of the things we do in this course. It really simplifies your life. You do the square. You expand the sum. You can reparameterize one of the summands like that. So do two steps at once. I'm reparameterizing, and I'm expanding. But now, you see, if I expand the exponent, we find. So that's just algebra. And now you notice that this sum here, this sum over A is equal to what when B is non-zero, claim that this sum is 0. When B is non-zero, then I'm summing over some permutations of the roots of unity. Here, I'm assuming that k is bigger than. Let's say here k is not 0. So I'm reparameterizing k a little bit. k is not 0. Then when B is not 0, the sum over A is 0. And otherwise, it equals to p. So the sum over here equals to p. And therefore, lambda k, lambda sub k, how about if I, so what should I change that to? So if k is 0, then I want this to be lambda sub k plus 1. So then lambda sub k plus 1 is equal to plus minus p plus 1 over 2 for all lambda not equal to 0. So really, except for the top eigenvalue, which is just the degree, all the other ones are one of these two values, and they're all quite small. So this is an explicit computation showing you that this Paley graph is indeed a pseudorandom graph. It's an example of a quasi-random graph. Yes? AUDIENCE 2 Don't we know what the sign is? YUFEI ZHAO Do we know what the sign is? So here, I am not telling you what the sign is, but you can look up. Actually, people have computed exactly what the sign should be. And so this is something that you can find in a number theory textbook like Ireland Rosen. Any more questions? There is a concept here I just want to bring out that you might recognize sums like these. So this kind of sum, that's a Fourier coefficient. So if you have some Fourier transform, this is exactly what Fourier transforms look like. And it is indeed the case that, in general, if you have an abelian group, then the eigenvalues and the spectral information of the corresponding Paley graph corresponds to Fourier coefficients. And this is a connection that we'll see also later on in the course when we consider additive combinatorics and giving a Fourier analytic proof of Roth's theorem. And there, Fourier analysis will play a central role. But actually, this analogy, as I've written it, is only for abelian groups. If you try to do the same for non-abelian groups, you will get something somewhat different. So for non-abelian groups, you do not have this nice notion of Fourier analysis, at least in the versions that generalizes what's above in a straightforward way. But instead, you have something else, which many of you have seen before but under a different name. And that's representation theory, which in some sense is Fourier analysis, except instead of one-dimensional objects, complex numbers, we're looking at higher dimensional representations. So I just want to point out this connection. And we'll see more of it later on. Any questions? So let's talk more about Paley graphs. So last time, we mentioned these notions of quasi-randomness. And I said at the end of the class that many of these equivalences between quasi-random graphs, they fail for sparse graphs. If your density, if your edge density is sub-constant, then the equivalence is no longer hold. But what about for Paley graphs? In particular, I would like to consider two specific notions that we discussed last time and try to understand how they relate to each other for Paley graphs. So for dense Paley graphs, it's a special case of what we did yesterday. So I'm really interested in sparser Paley graphs, even bounded degree, even bounded degree. So that's much sparser than the regime we were looking at last time. And the main result I want to tell you is that this condition is, in a very strong sense, actually equivalent to the eigenvalue condition for all Paley graphs, including non-abelian Paley graphs. So before telling you what the statement is, I first want to give an example showing you that this equivalence is definitely not true if you remove the assumption of Paley graphs. For example, if you, OK, so example that this is false for non-Paley. Because if you take, let's say, a large, so let's say, d-regular graph. So let's say a large, random d-regular graph. d here can be a constant or growing with n, but this is a pretty robust example. And then I add to it an extra, destroying copy of k sub d plus 1. That's much smaller in terms of number of vertices. The big, large, random graph, by virtue of being a random graph, has the discrepancy property. And because we're only adding in a very small number of vertices, it does not destroy the discrepancy property. So discrepancy property, if you're just adding a small number of vertices, it doesn't change much. So this whole thing has discrepancy. However, what about the eigenvalues? Claim that the top two eigenvalues are, in fact, both equal to d. And that's because you have two eigenvectors, one which is the all ones vector on this graph, another which is the all ones vector on that graph. These two disc components each give you a top eigenvector of d, so you get d twice. In particular, the second eigenvalue is not small. So the implication from disc to eigenvalue really fails for non-Cayley graphs, for general graphs. So the implication, the other direction, is actually OK. The fact that eigenvalue implies disc is actually the content of the expander mixing lemma. So this follows by expander mixing lemma. And that's because if you look at the expander mixing lemma for a Cayley graph, if you have the eigenvalue condition, then automatically you would find that these two guys here are at most n. So if lambda is quite small compared to the degree, then you still have the desired type of quasi-randomness. So I'll make the statements more precise in a second. So the question is, how can we certify, how can we show that, in fact, disc, which is a seemingly weaker property, implies the stronger property of eigenvalue for Cayley graphs? And what is special about Cayley graphs that would allow you to do this, that the statement is generally false for non-Cayley graphs? So let me define. So let me first tell you the results. So this is a result due to David Conlon and myself a few years ago. So many of you may not have been to too many seminar talks where there's this convention in mathematics talks where you don't write out your full name, only write the initial out of some kind of false modesty. But of course, we all love talking about our own results. But somehow, we don't like to write our own name for some reason. So here's the theorem. So I start with a finite group gamma. And let me consider a subset S of gamma that is symmetric. And consider G, the Cayley graph. Let me write n as the number of vertices and d the size of S. So this is a d-regular graph. Let me define the following properties. So the first property I'll call disc with epsilon. So I give you an explicit parameter. The number of edges between x and y differs from the number of edges that you would expect, so as in the expanded mixing lemma. So the disc property is that this quantity is small relative to the total number of edges. The second property, which we'll call the eigenvalue property, like, is that G is an nd lambda graph with lambda, at most, epsilon d. Lambda is quite small as a function of d. The conclusion of the theorem is that up to a small change of parameters, these two properties are equivalent. In particular, eigenvalue implies epsilon, implies disc of epsilon. And disc of epsilon, and this is the second one, is the more interesting direction. It implies like, well, you lose a little bit, but at most, a constant factor, like of 8 epsilon. Any questions about the statement so far? So as I mentioned, this is completely false if you consider non-Cayley graphs. And we also, using expanded mixing lemma, using that implication up there, this direction follows. One of the main reasons I want to show you a proof of this theorem is that it uses this tool, which I think is worth knowing. And this is an important inequality known as Grothendieck's inequality. So many of you probably know Grothendieck as this famous French mathematician who reinvented modern algebraic geometry and spent the rest of his life writing tomes and tomes of text that have yet to be translated to English. But he also did some important foundational work in functional analysis before he became an algebraic geometry nerd. And this is one of the important results in that area that he got. So Grothendieck's inequality tells us that there exists some absolute constant k such that for every matrix A, so a real-valued matrix, we have that the, so we have that if you, OK, so here's the idea. Let's consider the supremum. So let's consider the following quantity. This is a bilinear form. So this is a bilinear form. This is basically a bilinear form if you hit it by a vector x and y from the two sides. And I'm interested in what is the maximum value of this bilinear form if you are allowed to take x and y to be plus minus 1 valued real numbers. So this is an important quantity. And given your matrix, and I'm basically asking, you get to assign a plus or a minus to each row and column. And I want to maximize this number here. This is an important quantity that we'll see actually much more in the next chapter on graph limits. But for now, just take my word. This is a very important quantity. And this is actually a quantity that is very difficult to evaluate. If I give you a very large matrix and ask you to compute this number here, there's no good algorithm for it. And it's believed that there's no good algorithm for it. On the other hand, there is a relaxation of this problem, which is the following. It's still a sum. But now, instead of considering the bilinear form there, let's consider the xi's and yi's. Take them not from real numbers, but take vectors. So let's consider the sum where I'm taking a similar-looking sum, except that the xi's and yi's come from a unit ball in some vector space with an inner product, where B is the unit ball in some Rm, where here the dimension is actually not so relevant. Dimension is arbitrary. If you like, you can make m n or 2n, because you only have that many vectors. So this quantity here, just by very definition, is a relaxation of the right hand side of this quantity here. So it's at least as large. So in particular, if you have whatever plus minus, you can always look at the same quantity with m equal to 1, and you obtain this quantity here. But this quantity may be substantially larger. So the x's and y's have more room to put themselves in to maximize the sum. And Grothendieck's inequality tells us that the left hand side actually cannot be too much larger than the right hand side. It exceeds it by at most a constant factor. So in other words, the left hand side, which is known as a semidefinite relaxation, you are not losing by more than a constant factor compared to the original problem. And this is important in computer science, because the left hand side turns out to be a semidefinite program, an SDP, which does have efficient algorithms to compute. So you can give a constant factor approximation to this difficult to compute but important quantity by using semidefinite relaxation. And Grothendieck's inequality promises us that it is a good relaxation. You might ask, what is the value of k? So I said there exists some constant k. So this is actually a mystery. So the current proofs have been improved over time. Grothendieck himself proved this theorem. But a constant has been proved over time. And currently, the best known result is something along the lines of k roughly 1.78 works. But the optimal value, which is known as Grothendieck constant, is unknown. So this is Grothendieck's constant. Actually, what I've written down is what's called the real Grothendieck's constant, because you can also write a version for complex numbers and complex vectors. And that's the complex Grothendieck's constant. Yes? AUDIENCE 2 Is there a lower bound that's known that's substantially greater than 1? YUFEI ZHAO Yes, it's known that it is strictly bigger than 1. So there are some specific numbers, but I forget what they are. You can look it up. Any more questions? All right. So we'll leave Grothendieck's inequality. We'll use it as a black box. So if you wish to learn the proof, I encourage you to do so. There are some quite nice proofs out there. And we'll use it to prove this theorem here about quasi-random Cayley graphs. So let's suppose this holds. So what would we like to show? We want to show that this eigenvalue condition holds. And we'll use some min-max characterization of eigenvalues. But first, some preliminaries. Suppose you have vectors x and y, which have plus minus 1 coordinate values. Then, by letting, so let's consider the following vectors where I split up x and y according to where they are positive and where they're negative. So here, these are such that x plus is equal to. So if I evaluate it on a coordinate g, then it's 1. If x of g is plus 1 and 0 otherwise, g sub minus is 1 if x of g is minus 1 and 0 otherwise. So x splits into x plus minus x minus. And y splits into y plus minus y minus. Let's consider a matrix A where the g comma h entry of A is the following quantity. I have the set S. And I look at whether g inverse h lies in S. And I consider an indicator of that. So it's 1 or 0. And then subtract d over n so that this value has mean 0. So this is a matrix. And now, if I consider the bilinear form, hit A from left and right with x and y, then the bilinear form splits according to the plus and minuses of the x's. And I claim that each one of these terms is controlled because of DISC. So for example, the first term is if you expand out what this guy is. So here's an indicator vector. That's an indicator vector. And if you look at the definition, then this is precisely the number of edges between x plus and y plus minus d over n times the size of x plus times the size of y plus, where x plus is the set of group elements such that x sub g is 1 and so on. So the punchline up there is that this quantity, so this quantity is at most by discrepancy epsilon dn. So this sum here by triangle inequality is at most 4 epsilon dn. All right. So so far, we've reinterpreted the discrepancy property. And what we really want to show is that this graph here satisfies eigenvalue condition. So what does that actually mean, to satisfy the eigenvalue condition? So by the min-max characterization of eigenvalues, it follows that the maximum of these two eigenvalues, which is the quantity that we would like to control, is equal to the following. It is equal to the supremum of this bilinear form when x and y are unit length vectors. And this is simply because A is the matrix. It's not the adjacency matrix. A is not the adjacency matrix. A is the matrix obtained by essentially taking the adjacency matrix and subtracting that constant there. And subtracting that constant gets rid of the top eigenvalue. And what you remained is whatever that's left. And you want to show that whatever you remain has small spectral radius. So we would like to show that this quantity here is quite small. Well, let's do it. Give me a pair of vectors, x and y. And let's set the following quantities, where I take a twist of this x vector by rotating the coordinates, setting x super s sub g, the coordinate g, to be x sub sg. So x is a vector indexed by the group elements, and then rotating this indexing of the group elements by s. So that's what I mean by superscript s. And likewise, y superscript s is defined similarly. So I claim that these twists, these rotations, do not change the norm of these vectors. And that should be pretty clear, because I'm simply relabeling the coordinates in a uniform way. And likewise, same for y. So I would like to show this quantity up here is small. So let's consider two unit vectors and consider this bilinear form. If I expand out this bilinear form, it looks like that. Just writing it out. But now let me just throw in an extra variable of summation. What we'll do is essentially look at the same sum. But now I add in an extra s and put this s over here. So convince yourself that this is the same sum. So it's simply reparameterizing the sum. So this is the same sum. But now if you look at the definition of A, there's this cancellation. So the two s's cancel out. So let's rewrite the sum 1 over n, then g, h, s, all group elements. Now, if I bring this summation of s, now I bring it inside. And then you see that what's inside is simply the inner product between the two vectors, x sub g, between the two vectors. So this is what's inside is simply the inner product between these two. So I may need to redefine. Yeah, so when you're looking at, when you're talking about non-obedient groups, there's always the question of which side should you multiply things by. And either OK or I need to change this s over here. But anyways, it should work. Yes, question? AUDIENCE 1. Why h? YUFEI ZHAO. Thank you. Yeah, I think. OK. Question? AUDIENCE 1. I don't want to raise s over n. YUFEI ZHAO. Great. So maybe I need to switch the definition here. But in any case, some version of this should be OK. So we'll figure it out later in the notes. But now, OK, so you have this here. And if you look at this quantity here, it is the kind of quantity that comes up in Grothendieck's inequality. So this is basically the left-hand side of Grothendieck's inequality. What about the right-hand side of Grothendieck's inequality? Well, we already controlled that. We already controlled that because we said whenever you have up there little x and little y. So the conclusion of this board was that this bilinear form is bounded by at most 4 epsilon d for all x and y being plus minus 1 coordinate valued. So combining them by Grothendieck, we have an upper bound, which is the Grothendieck constant, times 4 epsilon dn. There's a m missing here. And therefore, because the Grothendieck constant is less than 2, we have a bound of 8 epsilon d. And this shows that this variational problem, which characterizes the largest eigenvalue and absolute value, is at most 8 epsilon d, thereby implying the eigenvalue property. So the main takeaway from this proof, two things. One is Grothendieck's inequality is a nice thing to know. So it's a semidefinite relaxation that changes the problem, which is initially somewhat intractable, to a semidefinite problem, which is both, from a computer science point of view, algorithmically tractable, but also has nice mathematical properties that, for this application here, there is this nice trick in this proof where I'm symmetrizing the coordinates using the group symmetries. And that allows me to obtain this characterization showing that eigenvalue condition and this discrepancy condition are equivalent for Cayley graphs. Let's take a quick break. Any questions so far? So we've been talking about nd lambda graphs, so d-regular graphs. And the next question I would like to address is, in an nd lambda graph, how small can lambda be? So smaller lambda corresponds to a more pseudo-random graph. So how small can this be? And the right kind of setting that I want you to think about is think of d as a constant. So think of d as a constant and n getting large. So how small can lambda be? And it turns out there is a limit to how small it can be. And it is known as the Arlon-Bolpana bound, which tells you that if you have a fixed d, and so g is an n-vertex graph with adjacency matrix eigenvalues, lambda 1 through lambda n, sorted in non-increasing order, then the second largest eigenvalue has to be at least basically 2 root d minus 1 minus a small error term little o n, little o 1, where the little o 1 goes to 0 as n goes to infinity. So the Arlon-Bolpana bound tells you that the lambda cannot be below this quantity here. And I want to explain what is the significance of this quantity, and you will see it in the proof. And this quantity is the best possible. And I also say, what do we know about the existence of graphs which have lambda 2 close to this number? So this is the optimal number you can put here. Question? AUDIENCE 2 Does it say anything about how negative lambda n can be? YUFEI ZHAO. Question, does it say how negative lambda n can be? So I'll address that in a second. But essentially, if you have a bipartite graph, then lambda n equals to minus lambda 1. More questions? So I want to show you a proof, and time permitting, a couple of proofs of Arlon-Bolpana bound. And they're all quite simple to execute. But I think it's a good way to understand how these spectral techniques work. So first, as with all of the proofs that we did concerning, most of them concerning eigenvalues, we're looking at the Courant-Fischer characterization of eigenvalues. So it suffices to show, to exhibit some vector z, so a non-zero vector, such that z is orthogonal to the O1 vector, and this quotient is at least the claimed bound. So by the Courant-Fischer characterization of the second eigenvalue, if you vary over all such d that are orthogonal to the unit vector, then the maximum value this quantity attains is equal to lambda 2. So to show that lambda 2 is large, it suffices to exhibit such a z. So let me construct such a z for you. So let r be a positive integer. And let's pick an arbitrary vertex v. So v is a vertex in the graph. And let v sub i denote vertices at distance exactly i from v. So in particular, v0 is equal to v. And I can just draw you a picture. So you have v0 and then the neighbors of v0. And each of them have more neighbors. So I'm calling v0 the set big V0 and then big V1, V sub 2, and so on. So I'm going to define a vector, which I'll eventually make into z, by telling you what is the value of this vector on each of these vertices. And we'll do this by setting very explicitly. So set x to be a vector with value x sub u to be wi, where wi is d minus 1 raised to power minus i over 2 whenever u lies in set big V sub i. So if u is distance exactly i from v, I set it to this number. So notice that they decrease as you get further away from v. And I do this for all distances less than r. So this is my x vector. And I set all the other coordinates to be 0 if the distance between u and v is at least r. So I give you this vector. And I would like to compute that quotient over there for this vector. And I claim that this quotient here is at least the following quantity. This is a computation, so let's just do it. So why is this true? If you compute the norm of x, so I'm just taking the sum of the squares of these coordinates, well, that comes from adding up these values. So for each element in the i-th neighborhood, so I have wi squared. And if I look at that quantity up there, so what is this? A is the adjacency matrix. So here, A is the adjacency matrix. So this quantity, I can write it as a sum over all vertices u. And I look at x sub u. And now I sum again over all neighbors of u and consider x sub u prime. It's that sum there. But this sum I have some control over, because it is, OK, so what's happening here? I claim it is at least the following quantity. Consider where u is. So u could be only non-zero if u lies in the r minus 1 neighborhood. So in that neighborhood, I have v sub i possible choices for the vertex u. For that choice, this x sub u is w sub i. But what about its neighbors? So it could have neighbors in the same set going left. So there's one neighbor going left. And all the other neighbors are, maybe it's in the same set, maybe it's in the next set. But in any case, I have the following inequality. There's one neighbor in the same, on the left, if you look at that picture just now. And then all the remaining neighbors have x sub u primes at least w sub i plus 1, because these weights are decreasing. So I can, the worst case, so to speak, is if all the neighbors point to the next set. So I have that inequality there. So there's an issue, because if you go to the very last set, think about what happens when I, in that very last set, I'm over counting neighbors that no longer have weights. So I need to take them out. So I should subtract d minus 1 times. So this is the maximum possible weight sum I could have, maximum possible over count. So each vertex here has d minus 1 neighbors at most. All right, so this should be pretty straightforward if you do the counting correctly. But now let's plug in what these weights are. And you'll find that this sum here, this quantity, is equal to, I mean, so the key point here is that this thing simplifies very nicely if you consider what this is. So what ends up happening is that you get this extra factor 2 root d minus 1. And then the sum minus half of V sub. So it's pretty straightforward computation using the specific weights that we have. And one more thing is that notice that this, OK, so notice that the sizes of each neighborhood cannot expand by more than a factor of d minus 1. Because, well, you only have d minus 1 outward edges going forward at each step. And as a result, I can bound this guy. And so what you find is that this whole thing here is at least 2 times root d minus 1. The main term is the sum. And this here is less than each individual sum. So I can do a 1 minus 1 over 2r. OK, putting these two together, you find the claim. All right, so I've exhibited this vector x, which has that quotient property. But that's not quite enough, right, because we need a vector called z up here that is orthogonal to the all 1's vector. And that you can do, because if the number of vertices is quite a bit larger than compared to the degree, then I claim that there exists u and v vertices that are at distance at least 2r. So this is the size of this tree. So if everything is within distance 2r from a vertex, then they all lie on this tree. And if you count the number of vertices in that tree, it's the sum I've written here. So if I consider these two vectors, so x be the vector obtained above, which is, in some sense, and I'm being somewhat informal here, centered at v. And if I let y be the vector, but I center it now at the vector at u, then I claim that essentially x and y are supported on disjoint vertex sets that have no edges even between them. So in particular, this inner product, this bilinear form, not inner product, but this bilinear form, is equal to 0 since no edge between the supports of x and y. So now I have two vectors that do not interact, but both have this nice property above. And now I can take a linear combination. Let me choose a constant C that's a real constant such that this z equal to x minus Cy has, I can choose this constant. So x and y are both non-negative entries. They're both non-zero. So I can choose this constant C so that this z is orthogonal to the all ones vector. And now I have this extra property I want. But what about the inner products? These two vectors, x and y, they do not interact at all. So the inner products split just fine, and the bilinear form splits just fine. So you have this inequality here as desired. And r, notice that I can take r going to infinity as n going to infinity because d is fixed. So if n goes to infinity, then r can go to infinity, roughly at logarithmic n. And that proves the Alon-Boppana bound. So to recap, to prove this bound, we needed to exhibit by the Courant-Fisher some vector with a nice quotient such that this quotient is large. And we exhibit this quotient by constructing the vector explicitly around the vertex and finding two such vertices that are far away and constructing these two vectors, taking the appropriate linear combination so that the final vector is orthogonal to the unit vector, to the all ones vector, and then showing that the corresponding bilinear form has this large enough. Any questions? I want to show you a different proof, which gives you a slightly worse result, but the proof is conceptually nice. So let me give you a second proof. So it's a slightly weakening. And just that will show that the earlier proof showed that lambda 2 is quite large. But next, we'll show that the max of lambda 2 and the lambda n is large. So not that the second largest eigenvalue is large, but the second largest eigenvalue in absolute value is large. So it's slightly weaker, but for all intents and purposes, it's the same spirit. So we'll show this one here. And this is a nice illustration of what's called a trace method, sometimes also a moment method. Here's the idea. As we saw in the proof relating the quasi-randomness of C4 and eigenvalues, well, C4's are, well, eigenvalues are related to counting closed walks in a graph. So we'll use that fact, counting closed walks in a graph. And specifically, the 2k-th moment of the spectrum is equal to the trace of the 2k-th power, which counts the number of closed walks of length exactly 2k. OK. So to lower bound the left-hand side, we want to lower bound the right-hand side. So let's consider closed walks starting at a fixed vertex. So the number of closed walks of length exactly 2k starting at a fixed vertex v. Here we're in a d-regular graph. So here we are in a d-regular graph. I claim whatever this number is, maybe different from each v, it is at least the same quantity if I do this walk in an infinite d-regular tree. So infinite d-regular tree is, well, this is an infinite 3-regular tree. So start with the vertex, and then go out d-regular. Why is this true? So think about how you walk. Let me just explain. This is, I think, pretty easy once you see things the right way. So start with the vertex v. Think about how you walk. And whatever way you can walk, well, you can walk the same way on the infinite d-regular tree. Well, no, I mean, sorry. Whatever walk you can do on an infinite d-regular tree, if you label first vertex, first edge, second edge, if you do a corresponding labeling on your original graph, you can do that walk on your original graph. Although the original graph might have some additional walks, namely things that involve cycles that are not available on your tree. But certainly, every walk you can do, every closed walk you can do on the tree, you can do the same walk on your graph. So you can make this more formal. So you can write down a bijection or injection to make this more formal. But it should be fairly convincing that this inequality is true. But this is just some number. So this is a number of 2k walks in a d-regular tree starting at the vertex. And this number has been well studied. I mean, we don't need to know the precise number. We just need to know some good lower bound. And here is one lower bound, which is that it is at least a Catalan number, the k-th Catalan number times d minus 1 to the k, where c sub k is the k-th Catalan number, which is equal to 2k choose k divided by k plus 1. So let me remind you what this is. One, there has many combinatorial interpretations. And it's a fun exercise to do bijections between them. But in particular, c3 is equal to 5, which counts the number of ups and down walks of length 6 that never dip below the horizontal line where you start. So then this corresponds to going away from the root versus coming back to the root. So you have at least that many ways. And when you are moving away from the root, you have d minus 1 choices on which branch to go to. Good. Given that, the right-hand side is at least then n, the number of vertices, times the quantity above related to Catalan numbers. On the other hand, the left-hand side is at most here, we're using that 2k's even number, is at most d to the 2k plus. So all the other eigenvalues are at most lambda in absolute value. So let me call this quantity lambda. So rearranging this inequality, we find that lambda to the 2k is at least this number here. So here I'm changing n minus 1 to n. You have that. And now what can we do? We let n go to infinity and k go to infinity slowly enough. So if k goes to infinity and n goes to, so k goes to infinity with n, but not too quickly, but k is little o of log n, then we find that this quantity here is essentially 2 to the k, 2 to the 2k. And this guy here is little o1. So lambda is at least 2 root d minus 1 minus little o1. So that proves essentially the along Bopondi bound, although a small weakening because we are this big eigenvalue, you might find might actually be very negative instead of very positive. But that's OK for applications. This is not such a big deal. These are two different proofs. And I want you to think about, are they really the same proof? Are they different proofs? Are they related to each other? So it's worth thinking about. They look very different, but how are they related to each other? And one final remark is to, you already saw two different proofs that shows you this number, and you see where this number comes from. And let me just offer one final remark on where that number really comes from. And it really comes from this infinitely regular tree. So it turns out that 2 root d minus 1 exactly is the spectral radius of the infinite d-regular tree. And that is the reason, in some sense, that this is the correct number occurring along Bopana bound. This is, if you've seen things like algebraic topology or topology, this is universal cover for d-regular graphs. And so I won't talk more about it, but just some general remarks. And you already saw two different proofs. So at the beginning of next time, I want to wrap this up and to show you, to explain some what we know about, are there graphs for which this bound is tight? And the answer is yes, and there's lots of major open problems as well related to what happens there. And then after that, I would like to start talking about graph limits. So that's the next chapter of this course. OK, great.