 OK. So main character in today and or in this lecture and the next are compact operators, which I introduced at the end of last lecture. And I'll recall so throughout H is a Hilbert space. OK, so we say an operator, let's call it k, is a bounded linear operator is a compact operator if the image of the unit ball, taking the closure of that, so this is just a set of all elements k u with u less than or equal to 1 closure, is compact in H. OK? And so how did this set of operators come up? So at the end of last time, we dealt with finite rank operators, which were just basically matrices. You can actually write them so that they're completely described with respect to a certain basis as a matrix. And the question we posed at the end of the last lecture was, well, these finite rank operators, they form a subspace of the space of all bounded linear operators. Is that subspace closed? And the answer is no. What is the closure of the finite rank operators, which we know and love because they're just matrices? And what we'll prove today is that it's exactly the space of compact operators. But let me just, last time I gave you an example of one that is a compact operator, kA equal to, let's say, a1 divided by a1, a2 divided by 2, a3 divided by 3, and so on, defined for a and L2, little l2. Then my claim is that k is a compact operator. And in the assignment, you show basically a variety of other operators that come up quite often are compact operators. For example, you also show essentially that if k is a continuous function on 0, 1, across 0, 1, and I define a linear operator T, f of x, to be the integral from 0 to 1 of kxy f of y dy. Now f is in big L2 of 0, 1. Then part of the assignment is to use and remind yourself of one of the very few compactness results one has, is to use the Arzela-Skoli theorem to show that this is, in fact, a compact operator. So from the assignment, you'll prove T is compact operator on L2. Now maybe you think I'm just coming up with something that looks like this to come up with examples of a compact operator, but the solution operator for differential equations take this form. So for example, so I guess this would be an example. As part of this example, if I take kxy to be, let's see if I can get it right, x minus y times y, or x minus 1 times y, x times y minus 1, for 0 less than or equal to y, less than or equal to x, less than or equal to 1, and then now reverse. Let's see. Yeah, I think that's it. Then u equals 0, 1 k of x, y. You can actually compute, solves the differential equation u double prime equals f, u of 0 equals u of 1 equals 0 on 0, 1. So maybe this has a minus in front. I can't exactly remember. So u of x. So the solution operator, meaning if my input is f, then the solution to this differential equation is actually, I can write it as a compact operator acting on the data, which is f. So these don't just pop up out of nowhere. They pop up quite naturally. Now, these are a bunch of examples on compact operators. One non-example, so it's easy to actually come up with non-examples of compact operators, but let's say the identity on little l2. This is not a compact operator. Why? Because the condition of being a compact operator is that k of the unit ball should be a compact subset of little l2. But let's let e sub n be the sequence, our favorite sequence, which is 0, except in the n spot where it's 1. Then each of these has norm 1. And so if I look at the image, en minus iem, and I compute the square of that, you can see quite easily this is equal to 2, since these are orthonormal. For all n not equal to m. And therefore, ien does not have a convergent subsequence. If i, the identity, was a compact operator, then the image of the closed unit ball in little l2 should be a compact set, meaning what? Meaning elements of this form, i evaluated on something that has norm less than or equal to 1, should form a compact set. So if I have a sequence of guys in the unit ball, closed unit ball, then when i hits it, I should be able to find a convergent subsequence. But here, if you just compute, then we have the distance between any two of these guys for n not equal to m is 2. So this sequence can't ever be Cauchy, nor can any subsequence ever be Cauchy. And therefore, this has no convergent subsequence. So the identity on little l2 is not a compact operator. And by this argument, you can, in fact, prove that the identity is not a compact operator on any infinite dimensional Hilbert space. So we have one way of showing that an operator is a compact operator, which is by verifying the definition. And then we now have this theorem, which I alluded to at the start, showing that compact operators are, or the set of compact operators is the closure of the finite rank operators. So let H be a separable Hilbert space. Then an operator T, bounded linear operator T on H, is a compact operator if and only if there exists a sequence Tn of finite rank ops, which converge to Tn, the operator norm. So this proves that the set of compact operators is the closure of the set of finite rank operators, which so we denoted by, so this shows compact operators is the closure of the finite rank operators, which we denoted by R of H. So first direction is to show that if I have a compact operator, then it's equal to a norm limit of finite rank operators. So since H is separable, it has a orthonormal basis. Now, since T is a compact operator, remember, this means that the image Tu of all those elements of norm less than or equal to 1 is contained in a compact set, namely the closure of this set. Now, since this set is contained in a compact set, I can use, so in fact, let me just put, it's a compact set. And so we have this characterization of compact subsets of a separable Hilbert space by being closed, bounded, and having equi-small tails. So then for every epsilon positive, there exists a natural number n such that what? If I look at sum over k bigger than n, ku ek squared, this is less than epsilon squared for all elements with norm less than or equal to 1. Because the kus, these are in this compact, or why am I using k? I should use T, sorry. The Tus are all in this compact set. So we have this equi-small tail condition that for every epsilon, we can find an n, which does not depend on the elements of the compact set so that the tails of the Fourier coefficients are small. So what this says is that we can basically, if we're taking any u with norm less than or equal to 1 and hitting it with T, then the end of the Fourier coefficients don't really matter. And so we should think we can approximate T by just a finite rank operator that consists of coefficients like this times ek for k between 1 to n. And that's basically what we're going to do. So for n, define Tn of u to be sum from k equals 1 to n of Tu ek ek. OK, and this is for u and h. All right, so one can check that this is a bounded linear operator, and it's also a finite rank. Because the range is, so it's clearly a bounded linear operator. Just take the norm of this. It's less than or equal to the norm squared of this. This is equal to the sum of squares of the Fourier coefficients, which is less than or equal to the norm of Tu squared, which is less than or equal to the norm of T squared. So it's clearly a bounded linear operator. And if I look at the range of Tn, this is a subset of the span of e1 up to en. And therefore, Tn is a finite rank operator for all n. So I claim this is our set of finite rank operators that converges to T as n goes to infinity. So to prove this, we'll just do standard epsilon n argument. Let epsilon be positive. Then let n be the n that appears in this condition star up here, meaning capital N is the natural number guaranteeing this. So now I claim that for all little n bigger than or equal to this capital N, the operator norm of Tn minus T is less than or equal to epsilon, which is still good enough. Then for all, then if norm of u equals 1, we compute that Tn of u minus Tu. So the fact, so we haven't used anything about this ek's yet, but this is where we use that they're an orthonormal basis, is that I can write Tn of u. Well, first off, this just is from the definition that this is k equals 1 to n. And let me put a square here of Tu ek ek minus. And now I expand this element as in this basis, Tu ek ek squared. You see what cancels out are the first n of these. And this equals the norm squared of k bigger than n of Tu ek ek norm squared. And this is equal to, since these are orthogonal, k bigger than e, or orthonormal I mean, k bigger than n Tu ek squared. And now n is bigger than or equal to capital N. So this is less than or equal to k bigger than n Tu ek squared. And that's less than epsilon squared. So what we've shown is that for all u of norm 1, Tn u minus Tu squared is less than an epsilon squared. And therefore, if I take the soup over all u that have norm 1, I get that the norm of Tn minus T, which is that soup, is less than or equal to epsilon. So thus, the Tn's converge to Tn operator norm. So OK, so now going the opposite direction, we will use this second characterization that we have of compact sets that we didn't prove, but I said you should look up if you want to. It's, again, a diagonalization argument, but that says that compact subsets can be approximated by finite dimensional or are sets that are compact, or a set is compact if and only if it can be approximated by finite dimensional subspaces. So now we're proving the opposite direction. And suppose that T can be approximated in operator norm by a sequence of finite rank operators. And we want to show that T is a compact operator. OK, so first note that if I look at Tu, norm less than or equal to 1, closure. So this is the set that I'm trying to show is a compact set. It's obviously closed because it's the closure of a set. The closure of a set is always the smallest closed set containing that set. And it's contained because T is a bounded operator. It's always contained in the set of, let's call it V and H, where the norm of V is less than or equal to the norm of T. So this shows that this set here is closed and bounded. So to conclude that this set is compact, we just need to show that it can be approximated by finite dimensional subspaces, meaning for every epsilon, there exists a finite dimensional subspace so that the distance from every element in this set to the subspace W is less than epsilon. So we want to show now, or we make this a claim, for all epsilon positive, there exists a finite dimensional subspace W such that for all u norm less than or equal to 1, we have that the distance from this element Tu to W is less than epsilon. That's what I mean by we can approximate this set by finite dimensional subspaces. Once we can do that, then we've proven the claim, or we can use that characterization of compact sets to conclude that this set is compact. Now, what's the idea? We have that T, which again, this set here is just the image of the closed unit ball by T taking the closure. And therefore, T can be approximated by a finite rank operator. Its range is finite dimensional. So this set should be approximated by the range of Tn, which is finite dimensional, and therefore should give us our claim. And that's exactly what we do. So since the norm of Tn minus T goes to 0, there exists a natural number n such that I have that, well, for all little n bigger than or equal to capital N, blah, blah, blah. But I don't need all little n. I just have this less than epsilon. Let W be the range of T sub n, which is a finite dimensional subspace. OK. And now I want to show that I have this condition here, that for all u with norm less than or equal to 1, this distance from Tu to capital W is less than. We can make that less than or equal to epsilon. That still proves what we want. Just go through and change epsilon to epsilon over 2. Then for all elements in H with norm less than or equal to 1, if I take Tu and subtract off TnU, which is an element in capital W, this is less than or equal to T minus, so the operator norm of T minus Tn times norm of u, which is less than or equal to T minus Tn, which is less than epsilon. Put a less than there. It doesn't matter. OK, so this holds for one particular element of W. And therefore, the inf, which is the distance from Tu to capital W, which is less than or equal to this, must be less than epsilon. OK? And therefore, this proves the claim. And the fact that this set is compact follows from that characterization of compact sets, which we proved, or didn't prove, but at least stated a few lectures ago. So I mean, you can use the definition to verify something's a compact operator. You could use this alternative characterization as a limit in operator norm of finite rank operators to verify something's a compact operator. It's just whatever is most convenient at the time. OK, so how about the end of when we talked about finite rank operators, we also went a little bit into their algebraic structure. We can do the same for compact operators. So again, let H be a separable Hilbert space. Then the first property of compact operators is that, and let's call it a name, K of H be the set of compact operators on H. And the first is K of H is a closed subspace of the space of bounded linear operators on the Hilbert space. So the fact that it's a linear subspace is not too difficult to prove. The fact that it's closed follows immediately from what we've done so far when we showed that the compact operators are the closure of the set of finite rank operators. Really, I mean, the fact that we've shown that proves both of these, because the closure of a subspace is, again, a subspace. If T is a compact operator, then its adjoint is also a compact operator. And for every pair of bounded linear operators and compact operator, if I multiply on the left and on the right by these bounded linear operators, that remains a compact operator. So a fancy way of combining 2 and 3 is that the set of compact operators is a star closed two-sided ideal in the algebra of bounded linear operators. But I'm just going to list the conditions and not give fancy words for them. OK, so 1 is clear. I'm not going to write the proof for that. We'll just do 2 and 3. And we'll use, although we could do it directly from the definition, let's use the theorem that we just worked hard to prove. So if T is a compact operator, then that implies that there exists a sequence of finite rank operators such that Tn converges to T in operator norm. And therefore, by what we proved last time, we showed that if we have a finite rank operator, then its adjoint is also a finite rank operator, which implies that if I take the norm of the adjoint of this finite rank operator minus the adjoint of the compact operator, remember we have this nice theorem that tells us the norm of the adjoint is equal to the norm of the original operator. This is equal to Tn minus T, which we're assuming goes to 0. And therefore, the adjoint is a norm limit of finite rank operators, and therefore, must be a compact operator. And to prove 3, we'll use this characterization again. So again, assume T is a compact operator so that there exists a sequence Tn's in a finite rank operators converging to T in the operator norm. So Tn finite rank operators, Tn minus T converges to 0. Implies then, now last time we showed that the set of finite rank operators also satisfies these two conditions. This one I just used here. But it also satisfies this one, that if I take a finite rank operator and multiply it on either side by a bounded linear operator, I still have a finite rank operator. So for all n, I also have A times TnB is also a finite rank operator by what we did last time. And if I take the norm of A TnB minus A Tb, this is equal to the norm of A Tn minus Tb. And now, the norm has this, preserves this kind of algebraic structure of bounded linear operators. I shouldn't say preserves, but respects it, meaning the norm of a product is less than or equal to the product of the norm. So this is less than or equal to, this just follows from the definition, Tn minus Tb. And so this norm is fixed. This norm is fixed. This thing is going to 0. So I have bounded above this non-negative quantity by something converging to 0. And therefore, this thing converges to 0. So A times T times B is the norm limit of a sequence of finite rank operators. So it's also compact. Compact. OK. All right. Now, coming back to, oh, one second. Hydration is key. Now, one of the most important set of numbers that one can associate to a matrix is the set of its eigenvalues. And not just, so these eigenvalues all typically represent possibly the modes of vibration of a string, or if you've taken quantum mechanics, in which case you're not necessarily dealing with matrices, depending on how it's being taught to you. These eigenvalues are the energy levels for bound states. So perhaps since compact operators are the inverses of differential operators, which most definitely appear in quantum mechanics, but also in other applications, it would be good to develop some sort of theory of eigenvalues and eigenvectors for more general bounded linear operators, and then specify to compact operators. But what am I going on about? So now we're going to discuss the spectrum of a bounded linear operator, which is a generalization of eigenvalues and eigenvectors that you encountered in linear algebra. In linear algebra, the spectrum, well, if I take h to be just a finite dimensional space, so Rn, Cn, then the spectrum as we're going to introduce it now consists entirely of the eigenvalues of that matrix. Not so if we move on to now bounded linear operators, which is what makes it pretty interesting, is that there's more stuff that's associated to the fact that you're working in infinite dimensions. And OK, rather than harp on about it, let's just dive in and start discussing the spectrum. First, I want to bring up a few facts that you proved in I think it was the first assignment, first or second assignment of the course. So throughout, again, h is a Hilbert space. I will say it maybe at one point that it's separable, but throughout, h is always a Hilbert space. So we have the following theorem, which tells you how to invert certain operators via what's called Neumann series. Let T be a bounded linear operator on the Hilbert space. If the norm of T is less than 1, then the operator I minus T is invertible. And if I want to compute the inverse, I can compute it just like I would if I minus T was 1 minus x, and we're talking about dividing by the function 1 minus x. You have a power series, not power series, but you can write it as a geometric series, sum from n equals 0 to infinity, T to the n. And this series over here is absolutely convergent. Why? Because the norm of every term, norm of T to the n is less than or equal to the norm of T raised to the n. And the norm of T is less than 1, so this thing is absolutely summable. And you use that when you actually prove this theorem in the assignment. So I'm not going to give the proof of it again. I didn't give it once before. You did, but I'm not going to prove it. We'll just move on. And so another fact that you proved using this is that the set GLH, which is the set of bounded linear operators that are invertible, meaning it's bijective. By the open mapping theorem, we know that if an operator is bijective, then it automatically has a bounded inverse. So I'll say bijective, that this is an open set, is an open subset of what? Of the space of bounded linear operators. OK. So the proof is quite quick using this previous theorem. So let T0 be an element of the invertible operators on H, GLH. So that means we need to find an epsilon so that the ball centered at T0 of radius epsilon is contained in GLH, meaning everything in that ball is invertible. So suppose that I have an operator T, and its distance in operator norm to T0 is less than the 1 over the operator norm of the inverse of T0. I claim that T then has to be invertible. Then if I look at T0 times T minus T0 inverse, so T0 inverse times T minus T0, an operator norm that's less than or equal to T norm of T inverse times the norm of T minus T0 and norm, and that's less than 1. So by the previous theorem, I get that I minus T inverse T0 or T0 inverse applied to T minus T0 is invertible, because this thing here has norm less than 1, which implies that T, which is equal to the product of two invertible operators, namely T0 and I minus T0 minus 1, T minus T0, is a bijective bounded linear operator or a bounded linear operator with a bounded inverse. OK? And so let me just summarize. i.e. the ball of radius 1 over the norm of the inverse is contained in the set of invertible bounded linear operators. And that proves that this is an open set. So another way to think about, if you go back to what eigenvalues, eigenvectors were, it's especially clean if you have what were, depending on if you did stuff with complex valued matrices or not, but let's say they're symmetric and you're dealing with real value matrices, so matrices with real entries, then a theorem of linear algebra is that you can diagonalize that matrix, meaning you can find an orthonormal basis of Rn so that in that basis, the matrix is simply diagonal and the numbers appearing on the diagonal are the eigenvalues of that operator. Now, another way to think about that is that for which lambda, say, can I invert the matrix A minus lambda? In fact, that's exactly how the eigenvalues are defined. You take the, if you want to find an eigenvector, then that should be in the null space of A minus lambda. So you compute the determinant of that and you get a polynomial that you solve for the lambdas and then you get to solve for the eigenvectors, which is a system of linear equations once you know lambda. But thinking back, those eigenvalues are impediments to being able to solve the equation A minus lambda, u equals f. And so with that in mind, that's how we'll define the spectrum of a bounded linear operator. So first, I'm going to define the complement of the spectrum. So let A be a bounded linear operator. The resolvent set of A, so this is the new bit, is the set res A. This consists of all complex numbers lambda with the property that A minus lambda I is an invertible operator. Now, I'm a bit lazy. And so throughout, instead of writing A minus lambda I, I will most often write A minus lambda with the understanding that there should be an identity here. So the resolvent set is a set of all lambdas, complex numbers, so that A minus lambda is invertible. In other words, that you can always solve the equation A minus lambda, u equals f for arbitrary f. And you can solve it uniquely, uniquely being the key. The spectrum of A, so this is a new bit of terminology, is simply the complement. So usually, we write it spec A equals C, take away the resolvent set of A. So this is the example I was saying a minute ago. Let's say I have just a matrix, linear transformation of C2 to C2. And A is just given by lambda 1, 0, 0, lambda 2, or some two numbers, lambda 1 and lambda 2. OK? So I'm giving you the simplest example possible. Then A minus lambda is equal to lambda 1 minus lambda 0, 0, lambda 2 minus lambda. And when is this matrix invertible? Precisely when lambda does not equal lambda 1 or lambda 2. So A minus lambda is invertible if and only if lambda does not equal lambda 1 or lambda 2. Right? And when lambda 1 equals lambda 2 or lambda 1, this matrix here has a non-trivial kernel. And that's really the only obstruction to being invertible. But in infinite dimensions, when we have operators on infinite dimensions, there's an additional wrinkle that could happen. Not just the, so let me finish with this example. Therefore, the residual set of A, or residual resolvent set of A equals C, take away lambda 1, lambda 2. And the spectrum of A equals C, set lambda 1, lambda 2. OK? All right? So we kind of have a special name for if something pops up in the spectrum in kind of the same way we have for regular matrices. So definition, if A is a bounded linear operator, and A minus lambda, so lambda is a complex number, is not injective. So we need two conditions for A minus lambda or for lambda to be in the resolvent set. We need A minus lambda to be both injective and surjective, one to one and onto. Then this implies that it has non-trivial kernel. And there exists a u and h take away 0, so that A u equals lambda u. We then call this lambda, which is in the spectrum of A. It's in the spectrum because A minus lambda is not invertible, an eigenvalue of A. And u, an eigenvector. OK? So the best way to learn about spectrum is to kind of do a lot of examples. And so I only have a finite amount of time in class to teach you new stuff. So I can't do too many examples. But let's take, for example, the operator from earlier, TA equals A1 over 1, A2 over 2, A3 over 3, and so on, OK? For A, an element of little l2, so a sequence in little l2. And so this was, in fact, a compact operator, right? What is the, what would be some eigenvalues of this? Well, note that if I apply T to one of the elements of the orthonormal basis, so let's call it EK, or I called it EN earlier, so let's stick to N, where EN is the sequence consisting of 1 in the N spot and 0 otherwise, what do I get? I get 0, 0, except in the N spot, 1 over N, 0, and so on, which is 1 over N times EN, right? So that proves that T minus 1 over N has non-trivial kernel. And therefore, 1 over N is an eigenvalue, and therefore, in the spectrum. In fact, so this was for all N. These are eigenvalues of T. So the spectrum contains at least this sequence of numbers. So unlike in the finite dimensional case, you can have infinitely many eigenvalues. OK? So it's also possible that you have no eigenvalues. Let me just bring up a small point here. Let's look at 0. Is 0 in the spectrum or in the resolvent? In other words, so T minus 0, that's just T. So I'm asking, is T itself injective and surjective? Is it bijective? So first off, it should be clear that this operator here is injective. If TA equals 0, then every one of these numbers is 0, and therefore, A has to be 0. So T is clearly injective. But try to convince yourself that T cannot be surjective, because if it was surjective, the inverse would have to be a bounded linear operator. What would the inverse have to be? It would have to be, I take the elements of my sequence in L2, and A1 gets multiplied by 1. A2 gets multiplied by 2. A3 gets multiplied by 3, and so on. And that would not be a bounded linear operator on little l2. So that argument tells you that it can't be surjective. But what one can show is that the range is dense, though, in little l2. So you can have this. Something can be in the spectrum. This is something that's completely different than what happens in finite dimensions. You can have the range of T, or T minus lambda, say. You could have the range be dense in H, but not closed. And therefore, the operator is not surjective. This one subtle difference between what happens in infinite dimensions and finite dimensions, that doesn't happen in finite dimensions, because the range is going to be a subspace of H. And in finite dimensions, the range is a finite dimensional subspace, and so it's always closed. In infinite dimensions, the range does not have to be closed. So this little difference, though, can result in something being in the spectrum. It's not always just something's in the spectrum, if and only if it's an eigenvalue. That is not the case. And so, for example, and I'll probably put this on the assignment, but what you can verify is that, let's say I define now an operator T from L2 of 0, 1 to L2 of 0, 1 by T f of x equals x times f of x. Then T has no, none eigenvalues. I'll probably put this example either on the homework or on the exam, but it has no eigenvalues. And you can compute that the spectrum, instead of all lambda so that T minus lambda is not invertible, equals 0, 1. Why 0, 1? It's not because the domain's 0, 1, but because that's essentially the range of x, the function x, as x ranges from 0 to 1. All right. So when you move on to infinite dimensions, the spectrum doesn't necessarily need to just be the eigenvalues of your operator. In fact, there may be no eigenvalues. It can be something that's much more subtle, which occurs when the range is not a closed subset, is not a, where the range can be dense in h, but not closed, and therefore not equal to all of h. So the operator is not surjective. OK. All right, so let's prove some general properties of spectrum and resolvent sets. So let A be a bounded linear operator, then spectrum of A is a closed subset of C, and in fact, it's contained in the set of lambdas in C with modulus less than or equal to the norm of A. OK? So in particular, this implies that the spectrum is a compact subset of the complex numbers. So in particular, since 1 over n is in the spectrum of this operator we have over here, and 0 is the limit as n goes to infinity of 1 over n, this automatically, from this result, implies 0 is in the spectrum of this operator. Now, of course, you can phrase this since the spectrum of A is the complement of the resolvent set of A. You could have phrased this theorem in terms of the resolvent, which is that the resolvent set of A is open, and the resolvent set of A contains the complement of this set. That's, in fact, how we'll prove this theorem. We'll show it's open, and that this thing, the complement of this set, is contained in the resolvent of A. And the set of lambdas with modulus bigger than the norm of A is contained in the resolvent set of A. So why is the resolvent open? Well, it follows from the fact that GLH, the space of bounded linear operators that are invertible, is an open set. So take something in the resolvent. We have to show that there's a small ball around this complex number, which is contained in so that that small ball is contained in the resolvent, meaning A minus lambda is invertible. So since GLH is open, this implies there exists some small epsilon such that if T is an operator which is close to A minus lambda 0, an operator norm, this implies that T is also invertible. OK? Then if lambda minus lambda 0 is less than epsilon, we get that the norm of A minus lambda minus A0 minus lambda 0, the As cancel. And I just get the operator norm of lambda minus lambda 0. And here I should have kept around the identities, which just equals lambda minus lambda 0, which we're assuming is less than epsilon. So this operator here is within epsilon of this invertible operator here, which guarantees by how we chose epsilon that A minus lambda is invertible. And therefore, lambda is in the resolvent set of A. So we started out with something within epsilon distance to lambda 0, where epsilon was coming from this condition here, and showed that lambda is in the resolvent, i.e. the ball lambda minus lambda 0 less than epsilon is contained in the resolvent set of A. So that proves that the resolvent set is open. Right. OK. So now let's show this condition, that if the absolute value of lambda is bigger than the norm of A, then A minus lambda is invertible. So suppose lambda is bigger than, strictly bigger than the norm of A. So in particular, it's non-zero. Then the norm of 1 over lambda times A is less than 1. And this implies that I minus 1 over lambda A is invertible, because then this has norm less than 1. So we can always invert something that is I minus that thing that has norm less than 1, which tells me that A minus lambda is the product of two invertible operators, namely multiplication by minus lambda and I minus 1 over lambda A. Let's see. Let's make sure everything cancels and OK. So multiplication by lambda, which is a positive or non-zero complex number, that's an invertible operator. This is an invertible operator. Product of two invertible operators is invertible. So that thing is in GLH. So it's invertible, which implies lambda is in, simply by definition, is in the resolvent set of A. So we suppose that we had something here and showed it's in here, which is what we wanted to do. OK, so the spectrum is a closed subset, in fact, a compact subset of a set of complex numbers. At least for this class, that's about as much as we can say. For general bounded linear operators about their spectrum, let me just make a small comment also that with a little, if you know a little complex analysis, the previous proof actually also allows you to, OK, so I'm just going to state this as a fact. One could ask, could the spectrum ever be empty? So that's certainly a compact subset of complex numbers. And the answer is no, but this class does not assume complex analysis. So I can't really give a state or write a proof of that and expect you to completely follow it. But here's the thinking. So if you don't know complex analysis, just skip ahead a few seconds. But the thinking is this, let's suppose that the spectrum is empty. Then A minus lambda applied, so let's assume the spectrum is empty. And take u and v that are in the Hilbert space, apply A minus lambda to u, or A minus lambda inverse applied to u, an inner product it would be. So this is well-defined because A minus lambda inverse always exists because the spectrum is empty. And this gives you a continuous function in lambda on the complex plane. And now what happens? So the proof, or at least one of the estimates I gave before, says that as the magnitude of lambda gets large, the operator norm of A minus lambda is bounded by a constant divided by the magnitude of lambda, which is going to 0. So the operator norm of the inverse of A minus lambda goes to 0 as the norm of lambda, as the magnitude of lambda goes to infinity. But you can show a bit more about this function I defined a minute ago involving two vectors that basically by this Neumann series calculation, that in fact, that's a complex differentiable function in lambda. And thus, you have this complex differentiable function defined on the entire plane that goes to 0 as lambda goes to infinity. And by a Liouville's theorem, not Louisville, Louisville's type of bat, the bat that you hit stuff with, not the bat that gives you COVID, then A minus lambda inverse applied to u in a product v has to be identically 0 because that's a complex differentiable function in lambda, which goes to 0 as lambda goes to infinity. And since this applies for all u and v, this implies that the operator A minus lambda inverse is identically 0. That's a contradiction. So the spectrum cannot be empty. So it will be very useful to have a different characterization. So now we're going to specialize to not just looking at the spectrum of arbitrary bounded operators, but now we're going to zero in on self-adjoint operators. And first, we need a little theorem that if I have a self-adjoint operator, meaning A is equal to A star, then two things are true for all u and h. h, the inner product Au applied to u is a real number. And two, I can give an alternative characterization of the norm of A. But the norm of A is, in fact, equal to the sup over norm of u equals 1 of the absolute value of Au applied to u. So this actually has nothing to do with the spectrum. This is just fact I'll need when we start studying the spectrum of self-adjoint operators. And that will be in the next lecture. So with the remainder of this lecture, we'll prove this theorem. So proof. So one is quite easy. If u is an h, and I look at Au applied to u, and I take its complex conjugate, something's real, if and only if its complex conjugate is equal to the number again. So I want to show that the complex conjugate of Au applied to u is equal to Au applied to u. So this is equal to, by the properties of the inner product, u, the complex conjugate of Au in a product, u applied to Au. Now, A is equal to its own adjoint. So this is equal to u applied to, or u inner product A star u. And by the definition of the adjoint, this moves over here. And that's the end. Now, I keep bringing up quantum mechanics, but I am a researcher in quantum mechanics, but I'm not. But in quantum mechanics, the observables, meaning the things that one would actually measure, like position, momentum, center of mass, these types of things, are modeled by self-adjoint operators. These are not bounded linear operators. They're unbounded, which we will not cover in this class. But they're self-adjoint. And therefore, by this first property, this quantity here, which is the expectation of that measurement. So what this number 1 tells you is that the expectation of this measurement is always a real number. It's always something that you can measure in nature. I mean, as far as I know, the things one measures in nature, not necessarily uses to model nature, are not complex numbers. OK, so number 2, let's verify that I can, for a self-adjoint operator, write the norm in this way. OK, so let little a be this thing. First thing to note is, actually, this is a finite number. And it's bounded above by the norm of a. Note that for all unit-length vectors, the absolute value of the inner product of au with u, by Cauchy-Schwarz, is less than or equal to the norm, the product of the norms of au and u. u has norm 1. So this is less than or equal to the norm of au. And by the definition of the operator norm of a, that's less than or equal to the operator norm of a. So this also proves two things. This thing on the right-hand side is a finite number. And it's bounded above by the norm of a. So little a, this soup, is less than or equal to norm of a. And what we'll do is prove the opposite inequality. OK? All right, and to do that, we'll use this first property that for all u, au applied to u is real. So I want to somehow estimate the operator norm of a, which means I need to estimate the norm of au where norm of u equals 1. So let's take something in H with norm 1. And since the operator norm of a, so let me just check. Now what we want to show is that the norm of a is less than or equal to this number little a. OK, so let's take something with norm 1. And so that when I hit it with a, I don't get 0. Because the operator norm of a is computed by taking the soup overall of the soup of norms of au where u has unit length. And I just need to estimate those au's that are non-zero. OK? OK, let v be the vector au over the norm of au. Then this thing has unit length. All right? And so now if I compute norm of au, which is the thing I want to estimate above by a, so this is equal to au applied to v, or au inner product v, because then I would get au inner product au, which gives me norm of au squared divided by the norm of au. So I just get au. Now, this is a real number because it's equal to this norm. So I can throw in a real part and not get into any trouble. And now I have a, let's see, what would I call that? Polarization identity, that's the name of that. Not the parallelogram law, but I have a type of polarization identity for this expression, which you can just verify in the quiet of your own dwelling, that this thing right here I can write as 1 4th times au plus v, u plus v minus au minus v, u minus v plus i times au plus iv, u plus iv minus au minus iv, u minus iv. OK, so now, so auv, this inner product here is equal to this thing, this expression I have here. The real part is still there. Now, I'm taking the real part of this number. Now, this number here is real. This number here is real by number 1. And therefore, i times this number is purely imaginary. So when I take the real part, it goes away. So this is equal to 1 4th times au plus v minus au minus v, u minus v. I'm missing something there. OK, now, this is a applied to a vector, inner product with that same vector. So that's less than or equal to 1 4th. Since little a is the supremum of the absolute values of these expressions, where as long as these things have unit length, this is less than or equal to a times norm of u plus v squared, this first expression, because this is less than or equal to the absolute value of this thing. And dividing by u plus v squared, this I get that that's less than or equal to a. But I still have this u plus v squared hanging around. And then the same thing with this u minus v. So plus a u minus v squared. And now, I use the parallelogram law. This is equal to a 4 u plus v norm squared plus u minus v norm squared. This is equal to 2 norm u squared plus 2 norm v squared times a over 4. u and v have unit length. So this is 1. This is 1. So I get 2 plus 2 is 4. And I get a. So thus, for all norm 1 vectors, I've proven that the norm of a u is less than or equal to a. I've done it just for those so that a u is non-zero. But I still have this inequality of a u equals 0. So I get that the norm of a is less than or equal to a, which is what I wanted to do. So next time, we will get into some properties of the spectrum of self-adjoint operators a little more. In particular, the type of subset of complex numbers it has to be, in fact, we'll find out that the spectrum of a self-adjoint operator has to be contained in the real line. So you can't have anything with a non-zero imaginary part that's contained within certain bounds related to this type of expression, a u applied to u. And this will also give us an interesting way of checking whether the spectrum is contained within two real numbers. In particular, it'll tell us something about when a self-adjoint operator is non-negative, meaning a u, inner product u, is always non-negative. And from there, we'll start talking about the spectral theory for compact self-adjoint operators. So this is the most complete thing we can say about the spectrum of a certain class of operators. And what we can say about self-adjoint compact operators is that it's pretty close to the finite dimensional case, namely that the spectrum is that these operators can essentially be diagonalized. You can find an orthonormal basis consisting entirely of eigenvectors for the operator. All right, and we'll stop there.