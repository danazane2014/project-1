 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. Today we are going to see how to use what we saw last time about partial derivatives to handle minimization or maximization problems involving functions of several variables. Remember last time we said that when we have a function, say, of two variables, x and y, then we have actually two different derivatives. Partial f, partial x, also called f sub x, is the derivative with respect to x keeping y constant. And we have partial f, partial y, also called f sub y, where we vary y and we treat x as a constant. And now, one thing I didn't have time to tell you about, but hopefully you heard about in recitation yesterday, is the approximation formula that tells you what happens if you vary both x and y. F sub x tells us what happens if we change x a little bit by some small amount delta x. F sub y tells us how f changes if we change y by a small amount delta y. If we do both at the same time, then the two effects will add up with each other. Because you can imagine that first you will change x and then you will change y or the other way around. It doesn't really matter. If we change x by a certain amount delta x, and if we change y by the amount delta y, and let's say that we have z equals f of x, y, then that changes by an amount which is approximately f sub x times delta x plus f sub y times delta y. And that is one of the most important formulas about partial derivatives. The intuition for this, again, is just the two effects add up. If I change x by a small amount and then I change y, well, first changing x will modify f. How much does it modify f? The answer is the rate of change. The rate of change is f sub x. And, if I change y, then the rate of change of f when I change y is f sub y. All together, I get this change in the value of f. And, of course, that is only an approximation formula. Actually, there would be higher order terms involving second and third derivatives and so on. One way to justify this, sorry, I was distracted by a microphone problem. OK, so how do we justify this formula? Well, one way to think about it is in terms of tangent plane approximation. Let's think about the tangent plane to the graph of a function f. I have some pictures to show you. It will be easier if I show the pictures. Remember, partial f over partial x was obtained by looking at the situation where y is held constant. That means I am slicing the graph of f by a plane that is parallel to the xz plane. And then, when I change x, z changes. And the slope of that is going to be the derivative with respect to x. Now, if I do the same in the other direction, then I will have similarly the slope in a slice now parallel to the yz plane that will be partial f over partial y. In fact, in each case I have a line. And that line is tangent to the surface. Now, if I have two lines tangent to the surface, well, then together they determine for me the tangent plane to the surface. Let's try to see how that works. We know that f sub x and f sub y are the slopes of two tangent lines to this plane, sorry, two tangent lines to the graph. And let's write down the equations of these lines. I am not going to write parametric equations. I am going to write them in terms of x, y, z coordinates. Let's say that partial f over partial x at a given point is equal to a. Then that means that we have a line given by the following conditions. I am going to keep y constant equal to y0. And I am going to change x. And, as I change x, z will change at a rate that is equal to a. That would be z equals z0 plus a times the change in x, x minus x0. That is how you would describe the line that is, I guess, the one that is plotted in green here. That is the intersection with the slice parallel to the xz plane. I hold y constant equal to y0. And z is a function of x that varies with a rate of a. And now, if I look similarly at the other slice, let's say that the partial with respect to y is equal to b, then I get another line which is obtained by the fact that z depends on y and the rate of change with respect to y will be b while x is held constant equal to x0. These two lines are both going to be in the tangent plane to the surface. They are both tangent to the surface, f of f. And, well, together they determine a plane. And that plane is just given by the formula z equals z0 plus a times x minus x0 plus b times y plus b. If you look at what happens, this is the equation of a plane, z equals constant times x plus constant times y plus constant. And if you look at what happens if I hold y constant and vary x, I recover the first line. If I hold x constant and vary y, I get the second line. Another way to do it, of course, would be to write actually parametric equations of these lines, get vectors along them and then take the cross-product to get the normal vector to the plane, and then get this equation for the plane using the normal vector. That also works and it gives you the same formula. If you are curious, exercise, do it again using parametrics and using cross-product to get the plane equation. OK, so that is how we get the tangent plane. And now, what this approximation formula here says is that, in fact, the graph of a function is close to the tangent plane. If we were moving on the tangent plane, this would be an actual equality. Delta z would be a linear function of delta x and delta y. And the graph of a function is near the tangent plane, but it is not quite the same. It is only an approximation for small delta x and small delta y. The approximation formula says the graph of f is close to its tangent plane. And we can use that formula over here now to estimate how the value of f changes if I change x and y at the same time. Questions about that? Now that we have caught up with what we were supposed to see on Tuesday, I can tell you now about max and min problems. Min problems. That is going to be an application of partial derivatives to look at optimization problems. Maybe ten years from now when you have a real job, your job might be to actually minimize the cost of something or maximize the profit of something or whatever. But typically, the function that you will have to strive to minimize or maximize will depend on several variables. If you have a function of one variable, you know that to find its minimum or its maximum you look at the derivative and you set that equal to zero. And you try to then look at what happens to the function. Here it is going to be kind of similar, except, of course, we have several variables. For today, we will think about a function of two variables. But it works exactly the same if you have three variables, ten variables, a million variables. The first observation is that if we have a local minimum or a local maximum, then both partial derivative, partial f, partial x and partial f, partial y are both zero at the same time. Why is that? Well, let's say that f sub x is zero. That means when I vary x, well, to first order the function doesn't change. Maybe that is because it is going through. If I look only at the slice parallel to the x-axis, then maybe I am going through the minimum. But then, if partial f, partial y is not zero, then actually by changing y I could still make the value larger or smaller. That wouldn't be an actual maximum or minimum. It would only be maximum or minimum if I stay in the slice. But if I allow myself to change y, that doesn't work. I need actually to know that if I change y the value will not change either to first order. That is why I also need partial f, partial y to be zero. Now, let's say that they are both zero. Well, why is that enough? It is essentially enough because of this formula telling me that if both of these guys are zero then to first order the function doesn't change. And then, of course, there will be maybe quadratic terms that will actually turn that. This won't really say that your function is actually constant. It will just tell you that maybe it will be actually quadratic or higher order in delta x and delta y. That is what you expect to have at a maximum or a minimum. That condition is the same thing as saying that the tangent plane to the graph is actually going to be horizontal. And that is what you want to have. Say you have a minimum. Well, see that the tangent plane at this point, at the bottom of the graph, is going to be horizontal. And you can see that on this equation of a tangent plane, when both these coefficients are zero, that is when the equation becomes z equals constant, the horizontal plane. Does that make sense? We will have a name for this kind of point because actually what we will see very soon is that these conditions are necessary but they are not sufficient. There are actually other kinds of points where the partial derivatives are zero. Let's give a name to this. We say that x0, y0 is a critical point of F if the partial derivative with respect to x and the partial derivative with respect to y are both zero. More generally, you would want all the partial derivatives, no matter how many variables you have, you want all the partials to be zero. Let's see an example. Let's say I give you the function f of x, y equals x squared minus 2xy plus py squared plus 2x minus 2y. And let's try to figure out whether we can minimize or maximize this. What we start doing immediately is taking the partial derivatives. What is f sub x? It starts with 2x minus 2y plus zero plus two. Remember that y is a constant so this differentiates to zero. Now, if we do f sub y, that is going to be zero minus 2x plus 6y minus two. And what we want to do is set these things equal to zero. And we want to solve these two equations at the same time. An important thing to remember, maybe I should have told you a couple of weeks ago already, if you have two equations to solve, well, it is very good to try to simplify them by adding them together or whatever. But you must keep two equations. If you have two equations, you should not end up with just one equation out of nowhere. For example, here we can certainly simplify things by summing them together. See, if we add them together, well, the x's cancel and the constants cancel. In fact, we are just left with 4y equals zero. That is pretty good. That tells us y should be zero. But then we should, of course, go back to these and see what else we know. Well, now it tells us, if you put y equals zero, that tells you 2x plus 2 equals zero. That tells you x equals minus one. We have one critical point. That is x, y equals minus one over two. Any questions so far? Well, you should have a question. The question should be how do we know if it is a maximum or a minimum? Well, if we had a function of one variable, we would decide things based on the second derivative. And, in fact, we will see tomorrow how to do things based on the second derivative. But that is kind of tricky because there are a lot of second derivatives. I mean, we already have two first derivatives. You can imagine that if you keep taking partials, you may end up with more and more. We will have to figure out carefully what the condition should be. We will do that tomorrow. For now, let's just try to look at how do we understand these things by hand. In fact, let me point out to you immediately that there is more than maxima and minima. Remember, we saw the example of x squared plus y squared that has a critical point. The critical point is obviously a minimum. And, of course, it could be a local minimum because it could be that if you have a more complicated function there is indeed a minimum here. But then elsewhere the function drops to a lower value. We call that just a local minimum to say that it is a minimum if you stick to values that are close enough to that point. Of course, you also have a local maximum. Well, I did not plot it, but it is easy to plot. That is a local maximum. But there is a third example of critical point, and that is a saddle point. The saddle point is a new phenomenon that you do not really see in a single variable calculus. It is a critical point that is neither a minimum nor a maximum because, depending on which direction you look in, it is either one or the other. The point in the middle at the origin is a saddle point. If you look at the tangent plane to this graph you will see that it is actually horizontal at the origin. You have this mountain pass. At the mountain pass the ground is horizontal. But, depending on which direction you go, you go up or down. We say that the point is a saddle point if it is neither a minimum nor a maximum. OK, so the possibilities could be a local min, a local max, or a saddle. Tomorrow we will see how to decide which one it is in general using second derivatives. For this time let's just try to do it by hand. I just want to observe. In fact, I can try to, you know, these examples that I have here, they are x2 y2 y2-x2. They are sums or differences of squares. And, if we know that we can put things as a sum of squares, for example, we will be done. Let's try to express this maybe in terms of a square. Well, the main problem is this 2xy. But, observe, we know something that starts with x2 minus 2xy that is actually the square of something else. It would be x2 minus 2xy, well, plus y2, not plus 3y2. Let's try to do that. We are going to complete the square. I am going to say it is x minus y squared. That gives me the first two terms and also y squared. Well, I still need to add two more y squares. And I also need to add, of course, the 2x minus 2y. It is simpler. It is still not simple enough for my taste. I can actually do better. See, I mean now what is, so these guys look like a sum of squares. But here I have this extra stuff. But, 2x minus 2y, well, that is twice x minus y. It looks like maybe we can simplify this and make this into another square. In fact, I can simplify this further to x minus y plus one squared. See, that would start like, maybe I should put parentheses here, that would be x minus y squared plus twice x minus y. And then there is a plus one. Well, we don't have that plus one, so let's remove it by subtracting one. Oh, let me find that minus one here. And I still have my 2y squared. Do you see why this is the same function as that one? Again, if I expand x minus y plus one squared, I get x minus y squared plus twice x minus y, that is those guys, plus one. But I will have a minus one and that will cancel out. And then I have a plus 2y squared. Now what I know, see, this is a sum of two squares minus one. And this critical point, x, y equals minus one, zero, that is exactly when this is zero and that is zero. That is the smallest value. This is always greater or equal to zero, same with that one. That is always at least minus one. And minus one happens to be the value at the critical point. It is a minimum. Now, of course, here I was very lucky. In general, I could not expect things to simplify that much. In fact, I cheated. I started from that. I expanded. And then that is how I got my example. The general method will be a bit different, but you will see it will actually also involve completing squares. Just there is more to it than what we have seen. We will come back to this tomorrow. How do I know that this equals? y is what negative one? How do I know that that whole function is greater or equal to negative one? Well, I wrote f of x, y as something squared plus 2y2 minus one. This squared is always a positive number, non-negative. It is a square. The square of something is always non-negative. Similarly, y2 is also always non-negative. If you add something that is at least zero plus something that is at least zero and you subtract one, you get always at least minus one. And, in fact, the only way that you can get minus one is if both of these guys are zero at the same time. That is how I get my minimum. More about this tomorrow. In fact, what I would like to tell you about now instead is a nice application of min-max problems that maybe you don't think of as a min-max problem. But you will see, I mean, you don't think of it that way because probably your calculator can do it for you. Or, if not, your computer can do it for you. But it is actually something where the theory is based on minimization in two variables. Very often in experimental sequences you have to do something called least squares interpolation. And what is that about? Well, it is the idea that maybe you do some experiments and you record some data. You have some data x and some data y. And I don't know, maybe, for example, maybe you are measuring frogs and you are trying to measure how big the frog leg is compared to the eyes of a frog or whatever, or trying to measure something. And if you are doing chemistry then it could be how much you put of some reactant and how much of the output product that you wanted to synthesize is going to be the same. All sorts of things. Make up your own example. You measure, basically, for various values of x what the value of y ends up being. And then you would like to claim, oh, well, these points are kind of aligned. I mean, of course, to a mathematician they are not aligned, but to an experimental scientist that is evidence that there is a relation between the two. And so you want to claim, and in your paper you will actually draw a nice little line that says, C, these two functions depend linearly on each other. The question is how do we come up with that nice line that passes smack in the middle of the points? The question is, given experimental data, xiyi, so maybe I should actually be more precise. You are given some experimental data. You have data points x1, y1, x2, y2, and so on, xn, yn. The question would be find the best fit line of the form y equals ax plus b that somehow approximates very well this data. You can also use that, by the way, to predict various things. For example, if you look at your new homework, actually the first problem asks you to predict how many iPods will be on this planet in ten years, looking at past sales and how they behave. One thing, by the way, before you lose all the money you don't have yet, you cannot use that to predict the stock market. Don't try to use that to make money. It doesn't work. One tricky thing here that I want to draw your attention to is what are the unknowns here? The natural answer would be to say the unknowns are x and y. That is not actually the case. We are not trying to solve for some x and y. I mean we have some values given to us. And when we are looking for that line we don't really care about a particular value of x. What we care about is actually these coefficients a and b that will tell us what the relation between x and y is. In fact, we are trying to solve for a and b that will give us the nicest possible line for these points. The unknowns in our equations will have to be a and b, not x and y. The question really is find the best a and b. And, of course, we have to decide what we mean by best. Best will mean that we minimize some function of a and b that measures the total error that we are making when we are choosing this line compared to the experimental data. Maybe, roughly speaking, it should measure how far these points are from the line. But now there are various ways to do it. And some of them are, I mean a lot of them are valid to give you different answers. You have to decide what it is that you prefer. For example, you could measure the distance to the line by projecting perpendicularly. Or, you could measure instead the difference between, for a given value of x, the difference between the experimental value of y and the predicted one. And that is often more relevant because these guys are actually maybe expressed in different units. They are not the same type of quantity. You cannot actually combine them arbitrarily. Anyway, the convention is usually we measure distance in this way. Next, you could try to minimize the largest distance. Say we look at where is the largest error and we make that the smallest possible. The drawback of doing that is experimentally very often you have one data point that is not good because maybe you fell asleep in front of the experiment. And so you did not measure the right thing. You tend to want to not give too much importance to some data point that is far away from the others. Maybe instead you want to measure the average distance. Or maybe you want to actually give more weight to things that are further away. And then you do not want to do the distance but the square of the distance. There are various possible answers. But one of them gives us a particularly nice formula for a and b. And so that is why it is the universally used one. Here it says least squares. That is because we will measure, actually, the sum of the squares of the errors. And why do we do that? Well, part of it is because actually it looks good. I mean, when you see these plots in scientific papers, they really look like the line is indeed the ideal line. And the second reason is because actually the minimization problem that we will get is particularly simple, well-proposed and it is easy to solve. We will have a nice formula for the best a and the best b. If you have a method that is simple and gives you a good answer then that is probably the good one. We have to define best. And here it is in the sense of defining a total square error. Maybe I should say total square deviation. What do I mean by that? The deviation for each data point is the difference between what you have measured and what you are predicting by your model. That is the difference between the two. Now what we will do is try to minimize the function capital D, which is just the sum for all the data points of the square of the deviation. Let me emphasize again this is a function of a and b. I mean, of course, there are a lot of letters in here. But xi and yi in real life will be numbers given to you. They will be the numbers that you have measured. You have measured all of this data. They are just going to be numbers. You put them in there and you get the answer. Any questions? How do we minimize this function of a and b? Let's use our new knowledge. Let's actually look for a critical point. We want to solve for partial d over partial a equals zero and partial d over partial b equals zero. That is how we look for critical points. Let's take the derivative of a. Well, the derivative of a sum is the sum of the derivatives. And now we have to take the derivative of this quantity squared. Remember how we take the derivative of a square. We take twice this quantity times the derivative of what we are squaring. We will get two times minus y i, sorry, y i minus a xi plus b times the derivative of this with respect to a. What is the derivative of this with respect to a? Negative xi, exactly. And so we will want this to be zero. And partial d over partial b, we do the same thing but differentiating with respect to b instead of with respect to a. Again, sum of squares twice y i minus a xi plus b times the derivative of this with respect to b is, I think, negative one. That is the equations we have to solve. Well, let's reorganize this a little bit. The first equation, see, there are a's and there are b's in these equations. I am going to just look at the coefficients of a and b. If you have good eyes, you can see probably that these are actually linear equations in a and b. There is a lot of clutter with all these x's and y's all over the place. Let's actually try to expand things and make that more apparent. The first thing I will do is I will actually get rid of these factors of two that are just not very important. I can simplify things by two. And next I am going to look at the coefficient of a. I will get basically a times xi squared. Well, let me just do it and it should be clear. I claim when we simplify this we get a times xi squared plus xi times b minus xi yi. And we set this equal to zero. Do you agree that this is what we get when we expand that product? No? OK, let's do the other one. We just multiply by minus one. We take the opposite of that. It will be axi plus b. I will write that as xia plus b minus yi. And let me just reorganize that by actually putting all the a's together. That means actually I will have sum of all the xi squared times xi times b minus sum of xi yi equals zero. If I rewrite this it becomes sum of xi squared times a plus sum of the xi's times b minus, let me move the other guys to the other side, equals sum of xi yi. And that one becomes sum of xi times a plus how many b's do I get from this one? Well, I get one for each data point. When I sum them together I will get n. Very good. So, n times b equals sum of yi. Now, these quantities may look scary, but they are actually just numbers. For example, for this one you look at all your data points. For each of them you take the value of x and you just sum all these numbers together. What you get, actually, is a linear system in a two-by-two linear system. And so now we can solve this for a and b. In practice, of course, first you plug in the numbers for xi and yi and then you solve the system that you get. And we know how to solve two-by-two linear systems, I hope. That is how we find the best fit line. Now, why is that going to be the best one instead of the worst one? We just solved for a critical point. That could actually be a maximum of this error function d. We will have the answer to that next time. But trust me, if you really want to go for the second derivative test that we will do tomorrow and apply it in this case, it is quite hard to check. But you can see it is actually a minimum. I will just say we can show that it is a minimum. Now, even for the linear case, it is the one that we are the most familiar with. Least squares interpolation actually works in much more general settings. Instead of fitting for the best line, if you think that there is a different kind of relation, then maybe you can fit in using a different kind of formula. Let me actually illustrate that for you. I don't know if you are familiar with Moore's law. It is something that is supposed to tell you how quickly, basically, computer chips become smarter, faster and faster over time. It is a law that says things about the number of transistors that you can fit onto a computer chip. Here I have some data about the number of transistors on a standard PC processor as a function of time. And, well, if you try to do the best line fit, you will see quickly that it doesn't seem to follow a linear trend. On the other hand, if you plot the diagram in a log scale, the log of the number of transistors as a function of time, then you get a much better line. And so, in fact, that means that you had an exponential relation between the number of transistors and time. And so, actually, that is what Moore's law says. It says that the number of transistors on a chip doubles, depending on the version, every 18 months or every two years. They keep changing the statement. How do we find that best exponential fit? Well, an exponential fit would be something of the form y equals a constant times exponential of a times x. That is what we want to look at. Well, we could try to minimize a square error like we did earlier, but that doesn't work well at all. The equations that you get are very complicated. You cannot solve them. But remember what I showed you on this log plot. If you plot the log of y as a function of x, then suddenly it becomes a linear relation. Observe this is the same as ln of x equals ln of c plus ax. And that is a linear best fit. What you do is you just look for the best straight line fit for the log of y. That is something we already know. But you can also do other examples. For example, let's say that we have something more complicated. Let's say that we have actually a quadratic law. For example, y is of the form ax2 bx c. And, of course, you are trying to find somehow the best. That would mean here fitting the best parabola for your data points. Well, to do that you would need to find a, b and c. And now you would have actually a function of a, b and c, which would be the sum of all data points of the quadratic equation. And if you try to solve for critical points, now you will have three equations involving a, b and c. And, in fact, you will find a three by three linear system. And it works just the same way. Just you have a little bit more data. Basically, you see that these best fit problems are an example of minimization problem. But maybe you didn't expect to see minimization problems come in. But that is really the way to handle these questions. OK, so tomorrow we will go back to the question of how do we decide whether it is a minimum or a maximum? And we will continue exploring functions of several variables.