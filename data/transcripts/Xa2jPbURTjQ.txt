 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So we're really moving along this review of the highlights of linear algebra. And today it's matrices Q. They get that name. They have orthonormal columns. So that's what one looks like. And then the key fact, orthonormal columns translates directly into that simple fact that you just keep remembering. Every time you see Q transpose Q, you've got the identity matrix. Let's just see why. So Q transpose would be, I'll take those columns and make them into rows. And then I multiply by Q with the columns. And what do I get? Well, hopefully I get the identity matrix. OK, why? Because, oh, yeah, the normal part tells me that the length of each vector, that's the length squared Q transpose Q. The length squared is 1. So that gives me the 1 in the identity matrix all along the diagonal. And then Q transpose times a different Q is 0. That's the ortho part. So that gives me the 0s. So that's a simple identity. But it translates from a lot of words into a simple expression. Now, does that mean that in the other order, Q Q transpose, is that the identity? So that's a question to think about. Is Q Q transpose equal the identity? Question. Sometimes yes, sometimes no. Easy to tell which. The answer is yes. Yes, when Q is square. The answer is yes. If Q is a square matrix, a square matrix, this is saying that the square matrix Q has that inverse on its left. But for square matrices, a left inverse, Q transpose, is also a right inverse. And for a square matrix, if you have an inverse that works on one side, it will work on the other side. So the answer is yes in that case. And then in that case, that's the case when we call Q is, well, we really, I don't know what the right name would be. But here's the name everybody uses, an orthogonal matrix. And that's only in the square case, square. Q is an orthogonal matrix. Right, right. Do you want to just see an example of how that works? And it will not. So if Q is rectangular, let me do a rectangular Q and a square Q. So I think there must be a board up there somewhere. Here. OK, square. All right, good to see some orthogonal matrices, because my message is that they're really important in all kinds of applications. Let's start two by two. I can think of two different ways to get an orthogonal matrix that's a two by two matrix. And one of them you will know immediately, cos theta, sine theta. So that's a unit vector. It's normalized. Cos squared plus sine squared is 1. And this guy has to be orthogonal to it. So I'll make that minus sine theta and cos theta. OK. Those are both length 1. They're orthogonal. Then this is my Q. And the inverse of Q will be the transpose. The transpose would put the minus sign down here and would produce the inverse matrix. And what does that particular matrix represent? Geometrically, where do we see that matrix? It's a rotation. Thank you. It's a rotation of the whole plane by theta. Yeah. So if I apply that to 1, 0, for example, I get the first column, which is cos theta, sine theta. And that's just a row. Let me draw a picture. That vector 1, 0 has got rotated up to. So there is the 1, 0. And there it got rotated through an angle theta. And similarly, 0, 1 will get rotated through an angle theta to there. So the whole plane rotates. Oh, that makes me remember a highly important, very important property of Q. It doesn't change length. The length of any vector is the same after you rotate it. The length of any vector is the same after you multiply by Q. Can I just do that? I claim any x, any vector x, I want to look at the length of Qx. And I claim that it has the same length as x. Actually, that's the reason in computations that orthogonal matrix are so much loved, because no overflow can happen with orthogonal matrices. Lengths don't change. I can multiply by any number of orthogonal matrices, and the lengths don't change. Can we just see why that's true? How would you show? So what do we have to go on? What we have to go on is Q transpose Q equal I. Whatever we're going to prove, it's got to come out of that, because that's all we know. So how do I use that to get that one? Well, we haven't said a whole lot about lengths, but you'll see it all now. It'll be easier to prove that the squares are the same. So what's the matrix expression for the length squared? What's the right-hand side of that equation? X transpose x, right? X transpose x gives me the sum of the squares. Pythagoras says that's the length squared. So that right-hand side is x transpose x. What's the left side? It's the length squared of this, of Qx. So it must be the same as Qx, transpose Qx. And the claim is that that equation holds. And do you see it? Where is it? So any property from Q has to just come out directly from that. Where is it here? Do I just push away a little bit at that left-hand side and see it? Qx transpose is the same as x transpose Q transpose. And Qx is Qx. And now I'm seeing, well, you might say, wait a minute. The parentheses were there and there. But I say the most important law for matrix multiplication is you can move parentheses or throw them away. Let's throw them away. So in here, I'm seeing Q transpose Q, which is the identity. So it's true. So that means that you never underflow or overflow when you're multiplying by Q. Every numerical algorithm is written to use orthogonal matrices wherever it can. And here's the first example. So I think it may be good for me to think of other examples or for us to think of other examples of orthogonal matrices. So I'm using that word orthogonal matrix. We should really be saying orthonormal. And I'm really thinking mostly of square ones. So in the square case, when Q transpose is Q inverse. Of course, that fact makes it easy to solve all equations that have Q as a coefficient matrix, because you want the inverse and you just use the transpose. OK, let's just take some minutes to think of examples of Qs, if they're so important there have to be interesting examples. And that was a first one. Now, there's one more 2 by 2 example that you should know. Do you know what that would be? So this will be example 2. And it's also going to be only 2 by 2 and real. And what possibility have I got left here? I'll use the same first column, cos theta, sine theta, because that's more or less any unit vector in two dimensions. This has got that form. So what do you propose for the second column? Yes? Yeah, put the minus sign down here. So you think, does that make any difference? So sine theta and minus cos theta. I don't know if you've ever looked at that matrix. We're trying to collect together a few matrices that are worth knowing, are worth looking at. OK, now what's happened here? You may say that was a trivial change, which it kind of was. But it's a different matrix now. It's not a rotation anymore. That's not a rotation. And yeah, somehow, now it's symmetric. And yeah, its eigenvectors must be something or other. We'll get to those. But what does that matrix do? I don't know if you've seen it. If you haven't, it doesn't jump out. But it's an important case. This is a reflection matrix. Notice that it's determined it is minus 1. You have minus cos squared theta minus sine squared theta. It's determined it is minus 1. There's some eigenvalue coming up that's got a minus. Yeah. OK, so what do I mean by a reflection matrix? Let me draw the plane. And I think, so 1, 0. Let's follow it again. 1, 0, where does that go? That gives me the first column. So as before, it goes to cos theta sine theta. And when I say reflection, let me put the mirror into the picture. So you see what reflection it is. The mirror is along here at angle theta over 2, theta over 2 line. So sure enough, 1, 0 at angle 0 got reflected into a unit vector at angle theta. And halfway between was the theta over 2 line. That's OK. Now, what about the other guy, 0, 1? Here's 0, 1. OK, I multiply that. Can I put the 0, 1 up here so your eye does the multiplication? Where is the result of 0, 1? What's the output from Q applied to 0, 1? Sine theta minus cos theta, right? It's the second column. And so where is that? Well, it's perpendicular to that guy. That's what I know, right? That was the point, that the two columns are perpendicular. So it must go down this way, right? Sine theta, cos theta. And it doesn't change length. All these facts that we just learned are key. So there's 0, 1, and it goes to this guy, which is whatever that second column is, sine theta minus cos theta. And if you check that, actually, gosh, this is like plain geometry. I believe, it's never occurred to me before, but I believe it, that this angle going down to there, that that goes straight through, and that the halfway one is that line. Yeah, I think that picture has got it, and I think it's in the notes. So that's a reflection matrix. Well, that's a 2 by 2 reflection matrix. Would you like to see some other matrices like this one, but larger? They're named after a guy named Householder. So these are Householder reflections. What am I doing here? I'm collecting together some orthogonal matrices that are useful and important. And Householder found a whole bunch of them, and his algorithm is a much used part of numerical linear algebra. So he started with a unit vector. Start with a unit vector, u transpose u equal 1. So the length of the vector is 1. And then he created, let's name it after him, H. He created this matrix, the identity minus 2 uu transpose. And I believe that that's a really useful matrix. So it's just like, I think this review is like going beyond 18.06 into what ones are really worth knowing individually. Could we just check what are the properties of Householder's reflection of that I minus 2? You recognize here a column times a row. So that's a matrix. And what could you tell me about that matrix uu transpose? It's, yeah. Or what can we say about H? So I guess I'm believing that H is a orthogonal matrix. Otherwise, it wouldn't be here today. So I believe that. And not only is it orthogonal, it is also, have a look at it, symmetric. It's also symmetric. The identity is symmetric. Uu transpose is symmetric. So this is a family of symmetric orthogonal matrices. And that was one of them. That's a symmetric orthogonal matrix. These matrices are really great to have. You just, like in using linear algebra, you just get a collection of useful matrices that you can call on. And these are definitely one family. Well, it's obviously symmetric. Shall we check that it's orthogonal? So to check that it's orthogonal, I just, so I'm going to check that H transpose H is the identity. Can I just check that? Well, H transpose is the same as H, because it was symmetric. So I'm going to square this guy. This is really H times H. I'm squaring it. And what do I get if I square? So I get, I hope I get the identity, but let's see it. What do I get when I square this? Get little, multiply it out. So I times I is I. And then I get some number of Uu transposes. How many do I get from that? So I'm squaring this thing, because H transpose H is the same as H times H. So I'm squaring it. So how many, what do I put here? Four, thanks. And now I've got this guy squared with a plus. So that's four Uu transpose Uu transpose. Yeah, I'm totally realizing I've practiced for a lifetime doing these dinky little calculations. But they are dinky. And you'll get the hang of it quickly. Now, what am I hoping out of that bottom line? That it is? I. We're hoping to get I. Do we get I? Yes. Who sees how to get I out of that thing? Yeah. U transpose U in here is a number. That was U. That was column times row times column times row. And I look at the in the middle here is row times column. And that's the number 1, right? Because it's there. So that's 1. And then I have minus 4 of that, plus 4 of that. They cancel each other. And I get I. So those are good matrices. We'll use them. We'll use them. Actually, they're better than Graham-Schmidt. So we'll use them in making things orthogonal. So what other orthogonal matrices? Let's create some. Creating good orthogonal matrices, it pays off. Let's think. So there's a family named after this French guy who lived to 100. He was a real old timer. Well, MIT had a faculty member in math when I came, Professor Struik, who lived to 106. And I heard him give a lecture at Brown University at age 100. And it was like perfect. You could not have done it better. So he's my inspiration. I keep going. I only have like n more years to get there. And n is, well, it's too many anyway. OK, Adamard. All right, Adamard. So Adamard, he created, well, that's the simplest, the smallest. Now, the next guy is going to be 4 by 4. I'm going to put that. So where I see a 1, I'm going to put Adamard in. 1, 1, 1 minus 1, 1, 1, 1 minus 1. And then when I see a minus, I'm going to put it in with a minus. You saw that picture? It was a picture of H2, H2, H2, and minus H2. That's what I've got there. And I believe those columns are orthogonal. Now, what could I do? Well, it's not quite an orthogonal matrix. What do I have to do to make it? This isn't quite an orthogonal matrix either. What do I do to make that an orthogonal matrix? Divide by square root of 2. I need unit vectors there. And here, those lengths are 1 squared, 1 squared, 4. Square root of 4 is 2. So I better divide by the 2 there. And now here, I'm up to, yeah, so that was that one. Tell me the next one up. What's that going to be? 8 by 8? So this is H4 here. Oops, 4. So tell me what I should do for 8 by 8. You'll think this is simple. But that's a good thing to say about it. In coding theory, all sorts of places, you want matrices of 1's and minus 1's. OK, what's H8? I'm going to build it out of H4. So what's it going to be? I'm going to put an H4 there. What am I going to put here? Another H4. And up here, another H4. And finally here, minus H4. Minus H4, and I think I've got orthogonal columns again. Because the columns within these, dot products with themselves, give 0 and 0. The dot products from these columns and these columns obviously have the minus. And the dot products in here are 0 from that and 0 from that. Yeah, it works. And we could keep going to 16 and 32 and 64. But then up comes a question. What about H12? Is there a 1's and minus 1's matrix of size 12, 12 by 12? It doesn't come directly from our little pattern, which is doubling size every time. But you could still hope. And it works. I don't know what it is, but there's a matrix of 1's and minus 1's, 12 orthogonal columns. So the answer is yes. OK, so we make an 18.065 conjecture. Every matrix size, well, not every matrix size, because 3 by 3 is not going to work. 1 by 1 is not going to work either. But H12 works, H8, H16. What's our conjecture? We won't be the first to conjecture it. There is a 1's and minus 1's orthogonal matrix with orthogonal columns of every size n. So always, so I'll start the conjecture. Always possible if n, which is the size of the matrix, let's say, what would you guess? Just take a shot. You won't be asked for a proof, because nobody has the proof. Even? Well, you could hope for even. You could look for try 6, I guess, would be the first guy there. And I don't think it's possible. I think 6 is not possible. So every even size is, I think, not going to work. But it's a natural idea to try. What's the next thought? Every multiple of 4. If n over 4 is a whole number. But nobody has a systematic way to create these things. So like some of them, at this point, we're down to doing it one at a time. And we're up to 668. But we haven't got that one yet. Isn't that crazy? So all this is coming from Wikipedia, my source of all that's good in mathematics is there on Wikipedia. Anyway, this is the conjecture. Conjecture means you don't have any damn idea of whether it's true or not. Is that divisible by 4? Yeah, I guess it would be. 600 certainly is, and 68 certainly is. So I think that's the first one. So anybody, if you find one of size 668, just skip the homework and tell us about that one. But I don't think I'll assign that. I guess, well, I must have searched for it online. But yeah, anyway, so those are the Adamard matrices. Now, where else do I remember orthogonal matrices coming from? Well, yeah, really, the biggest source of. So when I'm looking for orthogonal matrices, I'm looking for a basis of orthogonal vectors. And where in math am I going to find vectors that come out to be orthogonal? We haven't seen that. It's the next section of the notes, but maybe you're remembering. Where will we sort of like automatically show up with orthogonal vectors? They could be the eigenvectors of a symmetric matrix. And that's where the most important ones come from. Oh, I could tell you about wavelets, though. Wavelets are more like this picture. They're 1's and minus 1's. Or the simplest wavelets are 1's and minus 1's. Before I go on to the eigenvector business, can I mention the wavelets matrices? Yeah, these are really important, simple and important constructions. So wavelets, let me draw a picture. So I'm going to come up with four. I'll do the 4 by 4 case. And these are the orthogonal guys. And then the next one is up and down and 0. And the last one is 0 and up and down. So that's four things, but let me show you the matrix. So I'll call it W for wavelets. So that guy I'm thinking of as 1, 1, 1, 1. This guy I'm thinking of as 1, 1, minus 1, minus 1. It's looking sort of Adhemard way, but there's a difference here. This guy is 1, minus 1, 0, 0. So the wavelets rescale. That's the difference between Adhemard and wavelets. Wavelets are self-scaling. And what's the last guy here? What's the fourth column from that fourth wavelet? 0, 0, 1, minus 1. So Haar came up with that. This is the Haar wavelet, which was many years before the word wavelets was invented. He came up with this construction, the Haar matrix, the Haar functions. So they're very simple functions, but that makes them usable, the fact that they're so simple. Now, I don't know if you want to see the pattern in 8 by 8, but let me start the 8 by 8 so you'll know what wavelets are about. You'll know what these Haar wavelets are about anyway. They're the ones that were kind of easy to visualize. So if that's W4, let's just take a minute. It won't take long for W8. OK. So the first column is going to be 8 1's. And what's the next column going to be? 4 1's and 4 minus 1's, like so. So 4 1's and 4 minus 1's. OK, and now the next column. 2 1's, 2 minus 1's, and 0's. Right? 1, 1, minus 1, minus 1, and 0. And the fourth will be 0's and 2 1's and 2 minus 1's. We've got half a matrix now. Now, if we just tell me the fifth, what do you think? What do I put in the fifth one? So again, it's going to squeeze down and rescale. And what's the fifth column up here that's going to be 1's and minus 1's and 0's now? So it's not Adamard. It's Haar. And what shall I put? 1, 1. Shall I start with 1, 1? Oh, 1 minus? Yeah. And then all 0's? Oh, yeah, thanks. Perfect. 1 minus 1, and then all 0's. And then the next three columns will have the 1 minus 1 here, and the 1 minus 1 here, and the 1 minus 1 here. And otherwise, all 0's. So you see the pattern. It's scaling at every step. So that matrix has the advantage of being quite sparse. This, in my mind, is a 4, 1's. That's involved with taking the average. Then this guy is like taking the differences, differences between those and those. And then this is like taking the difference at a smaller scale, and that also at a smaller scale. So that's what we keep doing. Yeah. Yeah. So that's wavelets. It looks so simple, right? But just a one-minute history of wavelets, so Haar invented this in like 1910. I mean, along, forever. But then you wanted wavelets that were a little better, and not just 1's and minus 1's and 0's. And that took a lot of thinking. A lot of people were searching for it. And Ingrid Dovashy, so I'll just put her name, became famous for finding them. So in about 1988, she found a whole lot of families of wavelets. And when I say wavelets, she found a whole lot of orthogonal matrices that had good properties. So that's the wavelet picture. Now, to close today and to connect with the next lecture on eigenvalues, eigenvectors, positive definite matrices, we're really getting to the heart of things here. Let me follow through on that idea. Idea, so the eigenvectors of a symmetric matrix, but also of an orthogonal matrix are orthogonal. And that is really where people, where you can invent, because you don't have to work hard. You just find a symmetric matrix. Its eigenvectors are automatically orthogonal. That doesn't mean they're great for use, but some of them are really important. And maybe the most important of all is the Fourier. So you probably have seen Fourier series, sines and cosines. Those guys are orthogonal functions. But the discrete Fourier series is what everybody computes, and those are orthogonal vectors. So the orthogonal vectors that go into the discrete Fourier transform, and then they're done at high speed by the fast Fourier transform, those are eigenvectors of Q, eigenvectors of the right Q. So let me just tell you the right Q, whose eigenvectors. So here we go. Eigenvectors of Q. You will just be amazed by how simple this matrix is. It's just that matrix. It's called a permutation matrix. Those are the four Fourier discrete. Let me put the word discrete up here, transform. So I really meant to put discrete Fourier transform. The eigenvectors of that matrix, first of all, they are orthogonal. And then second and more important, they're tremendously useful. They're the heart of signal processing. In signal processing, they just take the discrete Fourier transform of a vector before they even look at it. I mean, that's the way to see it, is split it into its frequencies. And that's what the eigenvectors of this will do. So we're going to see the discrete Fourier transform. But my point here is to know that they're orthogonal just comes out of this fact that the eigenvectors are orthogonal for any Q. And that is certainly a Q. Everybody can see that those columns are orthogonal. The columns of a permutation matrix, right? You've taken the columns of the identity, which are totally orthogonal, and you just put them in a different order. So a permutation matrix is a reordering of the identity matrix. It's got to be a Q. And therefore, its eigenvectors are orthogonal. And they're just the winners. I mean, the matrix of the Fourier matrix with those four eigenvectors in it, I'll show you now. So this is really we're finishing today by leading into Wednesday, eigenvectors and eigenvalues. And we happen to be doing eigenvectors and eigenvalues of a Q, not today of an S. Most of the time, it's a symmetric matrix whose eigenvectors we take. But here, that happens to be a Q. Can I show you the eigenvectors, the four eigenvectors of that? Now, oh, the complex number I is going to come in. You have to let it in here. Sorry. If S is a real symmetric matrix, its eigenvectors are real. But if Q is a orthogonal matrix, its eigenvectors, even though you couldn't ask for a more real matrix than that, but its eigenvectors are at least, yeah, the good way are complex numbers. So can I show you the eigenvectors of? OK. So again, overall, the point today is to see orthogonal matrices. So I'll just repeat now while I can. Rotations here, reflections, wavelets, the householder idea of reflections of large matrices that have this form I minus 2 UU transpose are orthogonal. And now we're going to see the big guys, the eigenvectors of Q. Right. Yes? AUDIENCE 2. The wavelets, do you want to rescale the column? GILBERT STRANG, M.D. Oh, yeah. We don't have orthonormal. That's right. We don't have orthonormal. I better divide by square root of 8. Right. AUDIENCE 3. There's different forms for different. GILBERT STRANG, M.D. Oh, yes. Oh, you're right. Sorry, I thought I'd get away with that, but I didn't. Yeah. So these guys are square roots of 8. So are these. But these guys are square roots of 4. And these guys are square roots of 2. Thank you. Absolutely right. Absolutely. Yeah. OK. So what are the eigenvectors of a permutation? This is going to be nice to see. And I'll use the matrix F for the eigenvector matrix of that Q up there. And F is for Fourier. And it'd be the 4 by 4 Fourier matrix. OK. What are the eigenvectors of Q? OK. So Q is a permutation. So I'm going to ask you for one eigenvector of every permutation matrix. What vector can you tell me that actually the eigenvalue will be 1? What vector can you tell me where if I permute it, I don't change it? 1, 1, 1, 1. It's like it's everywhere here. 1, 1, 1, 1. So that's the zero frequency Fourier vector, the constant vector, all 1's. Everybody sees that if I multiply by Q, it doesn't change. OK. Now the next one, I'll show you the 4 now. The next one will be 1, i, i squared, and i cubed. Of course, i squared, I don't know how many course 6j people are in this audience. But this is a math building, we paid for it. It's 2, 190, and it's i in this room. Yeah. So yeah. Anyway, i is the first letter in what? Imaginary, thank you. The first letter in imaginary. You can't say jimaginary. So that's it. OK. And then the next one is 1, i squared, i fourth, i sixth. And the next one is 1, i cubed, i sixth, and i ninth. Isn't that just beautiful? And you could show that every one of those four columns, if you multiply them by Q, you would get the eigenvalues. And you would see that it's an eigenvector. And this is just sort of like a discrete Fourier stuff. And instead of e to the ix, e to the 2ix, e to the 3ix, and so on, we just have vectors. So those are the four eigenvectors of that permutation. And those are orthogonal. Could I just check that? How do you know that this first column and the second? Well, I should really say zeroth column and first column if I'm talking frequencies. Do you see that that's orthogonal to that? Well, y is 1 plus i plus i squared plus i cubed equals 0. That's the dot product. This is column 1 dot column 2. Yeah. You're right. This happens to come out 0. Is that right? Yes, that will come out 0. That'll come out 0. But somebody mentioned that this isn't right. It's true that that came out 0. But when I have imaginary numbers anywhere around, this isn't the correct dot product to test orthogonal. If I have complex vectors, if I have complex numbers, complex vectors, I should test column 1 conjugate dotted with column 2. Column i conjugate. Well, let me take column. Let's see. Which one shall I take? Maybe that guy and that guy. Many of them you luck out here. But really, I should be taking the conjugate. So these 1's. But the thing is the complex conjugate of 1 is 1. So that was OK. But in general, if I wanted to take column 2 dotted with column, I don't know, maybe 4 would be a little dodgy. Yeah, look what happens. Take that column and that column. Take their dot product. Do it the wrong way. So what's the wrong way? Forget about the complex conjugate and just do it the usual way. So 1 times 1 is 1. i times i cube is? 1. i squared times i6 is? 1. I'm getting all 1's. I'm not getting orthogonality there. And that's because I forgot that I should take the complex conjugate. Well, of these guys, 1. I should take minus i. Minus i squared. Well, that's real, so it's OK. Minus there. So minus i squared is still minus 1. So now if I do it, it comes out 0. So let me repeat again. Let me just make this statement. If Q transpose Q is i, and Qx is lambda x, and Qy is a different eigenvalue y. OK. So I'm setting up the main fact that in the last minute, I'm just going to write down. So I have an orthogonal matrix. I have an eigenvector with eigenvalue lambda. I have another eigenvector with a different eigenvalue mu. Then the claim is that, what is the claim about the eigenvectors? So here has one eigen. This has an eigenvalue. This has a different eigenvalue. I need them to be different to really know that the x's and the y's can't be the same. So what is it that I want to show? AUDIENCE 1. Conjugated x of y minus 1. Yes. That x, and I have to remember to do that, x transpose y is 0. That's orthogonality. That's orthogonality for complex vectors. I have to remember to change every i to a minus i in one of the vectors. And I can prove that fact by playing with these, by playing with starting from here, I can get to that. OK, that's it. We've done a lot today, a lot of stuff about orthogonal matrices, important ones and the sources of important ones, eigenvectors. And so it'll be eigenvectors on Wednesday.