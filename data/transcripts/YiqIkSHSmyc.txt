 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. Shall we start? Let me just say, this is a great adventure for me to be here all on my own teaching a course that involves learning from data. So it's an exciting subject, and a lot of linear algebra goes into it. So it's sort of a second course in linear algebra. So there is a stellar site established, and that will be the basic thing that we use. This is a public site, math.mit.edu, learning from data. And so a book is coming pretty quickly as we speak, or after we speak. And that site has the table of contents of the book, which would give you an idea of what could be in the course. And I printed out a copy for everybody just of that one page. It's probably the final, first and last handout, maybe, with a table of contents, which you'll see there. And also, you'll see there the first two sections of the book, which is what I'll talk about today and on a little bit into Friday. So that's linear algebra, of course, because the course sort of begins with linear algebra. Actually, things that you would know from 1806, but this is a way to say, this is really important stuff. So that's what I'll do today. I'd like to do some start on the linear algebra today. And here's the great facts about the course, at least we knew there would. So we taught it last year, several of us together. And we knew there wouldn't be a final exam, but we imagined there might be quizzes along the way. But then we couldn't think of anything to put on the quizzes. So we canceled those. But you do learn a lot, nevertheless. And so I guess we based the grades on the homeworks. So the homeworks will be partly linear algebra questions and partly online, like recognizing handwriting, stitching images together, many other things. And I'll talk about those as we go. Right, good. Yeah, so that's the general picture. And I'll say more about it today. And I could answer any question about it. So we're getting videotaped. So if anybody's bashful, sit at the far back. But it'll be fun. Yeah, you may know the videos for 18.06. So this is like the next step, 18.06.5. It's pretty exciting for me. So any question, or shall I do a little math? Why not? And then I'll say a little more about the course, just so you have an idea of what's ahead. Yeah, and it looks like this room is exactly the right size to me. So I'm pleased. OK, so what's the deal in linear algebra? Let me, I'll start, forgive me if I start with what I do the very first day in 18.06, which is multiply a matrix by a vector. And then I'll graduate to multiplying a matrix by a matrix. And you will say, I know that stuff. But do you know it the right way? Do you think of the multiplication the right way? So let me tell you what I believe to be the right way. So let me take a matrix, say 2, 3, 5, 1, 1, 7, and 3, 4, 12. And I'll always call matrices A. So first step is just A times x, A times a vector. So I multiply A by, let's say, x1, x2, x3. And how do I look at that answer? So the choice is think of the rows of the matrix or think of the columns. And if you think of the rows, which is the standard way to multiply, you would take the dot product. So the first way is dot product of row dotted with x. Right, 2x1 plus x2 plus 3x3. It gives you the answer a component at a time. That's the low level way. The good way to see it is vector wise. See this as x1 multiplies that first column, x2 times the second column, 1, 1, 7, and x3 times the third column, 3, 4, 12. Good. So it's a combination of vectors. And of course, it produces a vector. And here we have a 3 by 3 matrix. And our vectors are in R3. And most vectors will be in R3 or Rn for this course. So that's the right answer. And of course, the first component is 2x1 and 1x2 and 3x3. The same 2, 1, 3, the same dot product comes out right. But you see it like sort of all at once instead of piecemeal. So that's part of the course. I guess part of what I hope to get across is thinking of a matrix as a whole thing, not just a bunch of 9 or m times n numbers, but thinking of it as a thing. A matrix multiplies a vector to give another vector. Yeah, so when I say Ax, you immediately think that. You immediately think, OK, Ax has a clear meaning. It's a combination of the columns of A. So now let me take that next step. And the next step is think about all combinations of the columns of A. We take the matrix A. And we take all x's. And we imagine all the outputs. And what I want to ask you is, what does that look like? If I just take one x, one vector x, I get a vector output. Takes a vector to a vector. But now I take all x's, all of vectors x in 3D. And I get all these answers. And I think of them all together. And so I've got a bunch of vectors, infinitely many vectors, actually. And the question is, if I plot those infinitely many vectors, what do I have? And the beauty of linear algebra is that questions like this, you can answer them. And you intuitively see it. You certainly see it in 3D. And you kind of have the right idea in 10 dimensions, even though most of us don't see too well in 10D, in R10. But here we got three. OK, so do you see what I'm saying? I'm taking all x's. So now, so all Ax gives us a big bunch of vectors. And that collection of vectors is called the column space of A. It's a space, in other words. That's the key word there, the column space of A. And I'll just write it as C of A when I need letters for it. OK, so I'm going to ask you, what does this column space look like? And that depends on the matrix. Sometimes the column space would be the whole of R3. Sometimes it's a smaller set in R3, a space. Do you know what it is in this case? Do I get all of 3D out of these guys by choosing all x1 and x2 and x3? It seemed like if it was a random matrix, the answer would be surely yes. If a random 3 by 3 matrix, its column space is going to be all of R3, its columns are going to be independent, its rows are going to be independent, it's going to be invertible, it's going to be great. But is this matrix, what's up with that matrix? No, it's not. So what do I get instead of all of R3? I get a plane. Yeah, if you get that insight. So by taking all x's, everybody's with me, all x's here. So that means that it fills in whatever. And well, because it's linear algebra, it's going to be likely all of R3 or a plane or even a line. Let me just bat over here for a moment. Give me a matrix whose column space would only be a line. All ones. OK, wow. Let me liven it up a little. 3, 3, 3, 8, 8, 8. I think, OK. So I think that the combinations of those columns are all on the same line. That says that the column space is just a line. And I would say then that the matrix, so column space of this A is a line. Another way I could say this is that the rank of the matrix is 1. The rank is sort of the dimension of the column space. Well, not sort of. That's what it is. The rank is the dimension of the column space. Everybody sees that you get a line? Because any combination x1 of that plus x2 of that plus x3 of that is going to go in along that line. Here's the first column. Here's the second column. They're all on a line. So I'll never get off that line. If I allow all x's, I'll get the whole line. Now here, you said not a line, right? What was the column space for this guy? Plain. Now why isn't it all of R3? What do you see that's special there? Because it is special. That's making the column space sort of special, a plane instead of the whole thing. Yeah, what's up with these three columns? The third column is the sum of these two. The third column is the sum of these two. So the first column is fine, 2, 3, 5. Second column is in a different direction. And when I take combinations of the first two columns, what will I get? Sorry to keep asking you questions, but that's what I always do. So the combinations of those first two columns are a plane, because they're indifferent. Everybody kind of sees that picture. We've got column 1 going that way and column 2 going that way. And then if I take any multiple of column 1, I've got a whole line, any multiple of column 2. And then when I put the two together, it fills in the plane. Yeah, yeah, our ear intuition says that's right. So this is a matrix of rank. What's the rank of this matrix? 2, because it's got two independent columns, but the third column is dependent. The third column is a combination of the others. So matrices like this are really the building blocks of linear algebra. They're the building blocks of data science. They're rank 1 matrices. And let me show you a special way to write those rank 1 matrices. I think of this matrix as the column vector 1, 1, 1 times the row vector 1, 3, 8. So it's a column times a row. That's a rank 1 matrix. Do you see that? That's a true multiplication there. It looks a little weird, but it's a 3 by 1 matrix times a 1 by 3 matrix. These numbers have to be the same, and then the output is 3 by 3. And it's that. And you see that it factors. So I'm going to move on to that idea. The next idea coming up will be that we can see, well, that's coming, that we're going to see matrices with two factors. OK, let me move to that, but back for this original matrix. Matrix, OK. So what's with this? The column space is a plane. So if I think about now the key idea of independent columns, how many independent columns have I got here? Two, correct, two. The third column, if I want to especially pick on that one, so I'll often go left to right. So I'll say the first guy is good, second guy is good, the third guy is not independent of the others. So I just have two independent columns. And those two columns would be a basis for the column space. So that's the critical idea of linear algebra. That's what you compute with you. Find a basis. And everything in the column space is a combination of these, including that. That's already a combination of those. But everything else in the column space is a combination of those two. So they're a basis for it. OK, so you have the idea of A times x. You have the idea of column space of A, which allows all x's. Then now we're moving into the idea of independent columns. And the number of independent columns is the rank. So the rank, shall I write that somewhere? Maybe here, the rank is the number of independent columns. And right now, what do I mean by independent columns? Well, let's just see what that means by using it. Are we good? I know I'm reviewing here. But allow me for this first class and part of next time also to review. But you'll see something new. In fact, why don't we see something new right away? Let me follow up on the idea of independence in a systematic way. So here's my matrix A. Can I write that again? 2, 3, 5, 1, 1, 7. And this guy was the sum of those. So that's my matrix A. So let's start from scratch and find a basis for the column space in the most natural way. So I'm going to take a basis. What's a basis? A basis is independent columns. So all three together would not be a basis. But they have to be not just independent, but they have to fill the space. Their combinations have to fill the space. So 2, 3, 5, let's say I'm going to create a basis. I'll call the matrix C, a basis for the column space. So here's a natural way to do it. I look at the first column. It's not zeros. If it was all zeros, I wouldn't want that in a basis. But it's not, so I put it in. So that's the first vector in my basis. Then I go on to the second column. If that column was 4, 6, 10, what would I do? If that second column was 4, 6, 10, would I put it in the basis? No. But 1, 1, 7 is OK, right? 1, 1, 7 is in a different direction. It's not a combination of what we've already got. So I say, OK, that adds something new. Put it in. Then I move on to the third column. Do I put that into a basis? You know the answer by now? No, because I'm looking to see, is it a combination of these guys? And it is. It's 1 of that plus 1 of that. So it's not independent. So I've finished now. I've got a matrix C, which was taken directly from A. And I kept only independent columns. And I worked from left to right. And I can see that right away now that the rank is 2. The column rank. I should say the column rank. The number of independent columns is 2. Good? Now comes the key step. I'm going to produce a third matrix R, which is going to tell me how to get these columns from these columns. And its shape is going to be, well, its shape, I don't have any choice. This is 3 by 2. So R has to be 2 by something. So I'm like so. I guess it has to be 2 by 3, because I want to come out this way. 2 by 3. What am I going to do here? I'm just going to put in the numbers R that make this correct. So this is a first matrix factorization. It's not, well, it is a famous one, actually. When we see it, we'll recognize what it is. It's famous in teaching linear algebra. But now, actually, C times R, columns times rows, has become very, very important in large scale numerical linear algebra. So let's figure out what goes into R. So what's going on? What am I thinking here? I'm putting in R. So every one of these columns is a combination of these. That's the whole point. And I'm just going to put in the numbers that you need. So what goes in the first column of R? What goes in the first column of R? So I want to look, what combination of that column and that column gives me this one? Yeah. 1, 0. 1, 0. Because you remember how we multiply a matrix by a vector? When I multiply that matrix by that vector, I take 1 of this plus 0 of that. I see it vector-wise. And of course, I get it right. And what about the second column of R? So the second column should be the combination that produces the second column of A correctly. What will that be? 0, 1. Thanks. And finally, the third column. 1, 1. Yes, right. Because 1 of this plus 1 of this produces the third column. So all I did was put in the right numbers there, really. Right. And this is correct now. A is C times R. And so this is like, what I've done here is like the first two pages of section 1.1 in these notes. So 1.1. And actually, so I'll tell you literally what happened earlier this year. I'd finished this. I wrote this down with a different example. And then I realized something. That sitting here in front of me was the first great theorem in linear algebra, the fact that the column rank equals the row rank. The fact that if I have a matrix where that column plus that column gives that one. Oh, ooh. So what am I going to say here? I'm getting nervous about it. I believe that a combination of the rows gives 0. Do you believe that? I mean, you've got to believe it. This is like linear algebra. The matrix is not invertible. It's square. But the columns are dependent. So the rows have to be dependent. And I don't exactly see. There's some combination of that row and that row that gives that one. And of course, when I looked at the first column, I thought, OK, it's going to be too easy. One of that and one of that gives that. But then my eye went over to the second column. And I realized it's not easy at all. So you're entitled to pull out your phone and figure this out. But there is some damn combination of those two rows that gives the third row. Otherwise, the course is over. We stop. But maybe we're going to find it somehow. Yeah. Yeah. So this is the theorem. So I have to tell you, I was really pleased. So the first two pages got two more pages to follow up on that idea, that here we were seeing something that I sort of proved in 1806, but not in the first lecture, that's for sure, and not maybe so clearly. But now can I try to prove it? So there won't be a lot of proofs in this class. But this is such an important fact. A equals CR is important factorization. And out of it, we can connect them. So what am I saying? I am saying that all that the row, so what's the row rank? Yeah. Back up here. What's the row rank? What's the row space? So the row rank is going to be the dimension of that space. So I look at my matrix A. What's the row space of A? I'm going to look at its rows. Now, maybe just so we don't get whole new letters for the row space, for me, the row space of A of a matrix, so first of all, tell me in words what it is. What's the row space of the matrix? All combinations of the rows. All combinations of the rows. That's the space. So I would take all combinations of those rows. To get combinations of the rows, I, well, two ways I can get combinations of the rows. And the way I'll do it is I'll just transpose the matrix so those rows become columns. And then I'm back to what I did. So the row space of A is the column space of A transpose. And this has the advantage that we don't introduce a new letter. So it'll be the column space of A transpose. So we don't introduce a new letter. We keep the convention that vectors are column vectors, which would be the MATLAB and Julia and Python convention. So is that OK? The row space is the combination of these rows. But I'll flip. I'll make them stand up, 2, 1, 3, to be column vectors. So it's a totally different space. And actually, I happen to take a 3 by 3 example so that the column space is part of R3. And the row space is also part of R3 because my matrix is 3 by 3. A better example, and the whole point of data, data doesn't come in square matrices. Fortunately for us, data very, very often comes in matrices. But the two, the columns, might be sampled, might be patients. And the rows might be diseases or something. I mean, they're different spaces. So matrices are not likely to be square. But anyway, we're good here. So the row space. Now can I come back to the proof? What I want to say is that the proof of this fundamental fact is staring at us. But we don't quite see it yet. And I want to see it. So I claim that these rows are a basis for the row space. And we already saw that these columns are a basis for the column space. And 2 equals 2. Two vectors here were a basis for the column space. Now if I can see why this shows me that these two vectors are a basis for the row space, then my example is right. Both of these will give 2. The column rank is 2, two columns. The row rank is 2, two rows. So I have to explain why I believe that these two rows are a basis for the row space. Right? Are you with me? I have to prove. I have to see why. First, so when I say basis, what do I have to check? Basis is just the critical idea. I have to check that they're independent. So I haven't got too many vectors. I haven't got any extra vectors in there. And I also have to check that their combinations produce all the rows. Should I say that again? Because that's what I'm going to check. I'm going to check that those guys are independent. Well, you can see that they're independent. And I'm going to check that their combinations produce all three of these rows. We didn't create those numbers for this purpose. But what I'm saying is they work. So I claim that this is a basis because what combination of those two rows would produce this first row? Yeah, let me just ask you that. What combination, what numbers should I multiply these two rows by to get the first row of A? 2 and 1. And where do you find 2 and 1? It's sitting there in C. Will it work again? Does 3 of this plus 1 of that give 3, 1, 4? Yes. So far, so good. Does 5 of this and 7 of that? See, I'm multiplying. I guess I'm doing matrix multiplication a sort of backward way or a different way. I'm taking combinations of the rows of the second guy. The wonderful thing about matrix multiplication is you can do it a lot of ways. It comes out the same every way. And each way tells you something. So 5 of that row plus 7 of that row, sure enough, is here. Do you see that that is not like accident? It isn't. The proof is really to look at this multiplication C times R two ways. Look at it first as combinations of columns of C to give the columns. Look at it second as combinations of the rows of R, and that produces the rows. So that factorization A equals CR was the key idea. And actually, this R that we've come up with has a name. Anybody remember enough 1806? Have you all taken 1806? No. How many have? Yes. OK. Good. For a while, 1806 was taught in a very abstract way. I said, what's going on? But anyway, so if you took it that semester, you maybe never heard of the column space. I'm not sure. Or by a different name. It has another name. What's its other name? The column space of a matrix? Range. I think it's the range. Yeah, right. Yeah. And of course, all this is fundamental in mathematics. So of course, everything here has different languages and different emphasis. But you see what the emphasis is here. OK. So you see the proof that A equals CR just reveals everything. So it's a first idea of a factorization of a matrix. And we've multiplied C times R. I could just say, because so that's really, you've seen now the main point of section 1.1 of the notes to come up with that factorization and that conclusion. And you see why C has the same number of columns. The number of columns of C equals the number of rows of R. And those are the column rank and the row rank. Yeah, it's just pretty neat. And here was the special case where those column space, the column space is all multiples of U. It's a line through U. The row space is all multiples of V. It's a line through V. And that's the basic building block. Yeah. Can I just say another little word before I push on beyond CR? This has become, if you have a giant matrix, like size 10 to the fifth, you can't put that into fast memory. It's a mess. How do you deal with a matrix of size 10 to the fifth when you cannot deal with all the entries? That's just not possible. Well, you sample it. So later in the course, we'll be doing random sampling of a matrix. So how could you sample a matrix? So you have a matrix. Of course, you're not looking at it. And you want to get some typical columns. OK, here's the natural idea. You just look at A times x. When you take the x be a random vector, random vector, rand of, so it's got m rows and one column. It's a vector. And what can I say about Ax? It's in the, what space is it in? Column space, thanks. That was the first idea in this lecture. Ax is in the column space. So if you want a random vector in the column space, I wouldn't suggest to just randomly pick one of the columns. Better to take a mixture of columns by taking a random vector x and looking at Ax. And if you wanted 100 random vectors, you'd take 100 random x's. And that would give you a pretty good idea, in many cases, of what the column space looks like. That would be enough to work with often. Yeah, OK. Can I just throw in another question? So Ax is in the column space of A. Let me just ask you this question. Is A, B, C, X, is that in the column space of A? Suppose I have matrices A, B, and C, and a vector x, and I take their product. Does that give me something in the column space of A? Yes, good for you. Why, how do you know that? Yeah, it's A times something. Right. Putting parentheses in the right place is the key to linear algebra. And there it is. OK. That was just a question just occurred to me. And I thought, well, I wonder if you'd do it. OK. So we have still time to multiply matrices. Oh, I was going to say about C and R. So these are real columns from A. But R is the rows are not taken directly from the rows of A. Actually, there is a name for this. It's called the row reduced echelon form of the matrix. And it's a big goal in 1806. It has the identity there, and then the other columns tell you the right combinations. Yeah. Yeah. Another big factorization would be to take columns from A. So this is another. So I'll put maybe or A equal. And we won't be doing this for a month. We could start take columns of A and put them into C if they were independent. And suppose I took rows of A. Now I'm going to take literally rows of A and put them into R. Well, shall I call it R twiddle or something? Because it'll be a different R. I'm not going to use those rows, but I'm going to take two actual rows of A. Then what? So that's an important factorization, but it's not correct. It wouldn't, if I took two other rows of R, it wouldn't work. So you have to put in the middle some 2 by 2 matrix U that makes it correct. You'll see in that section 1.1 that I got excited and wrote a page about CUR. Yeah, so I'll just mention that. OK. So now I'm ready. Oh, I wanted to say something about the course. I get excited thinking about math, but there is this course. So what's up? So there'll be linear algebra problems. But what makes this course special is the other homeworks, which are online. And you would use, let's see, in principle, you could use any of the languages, MATLAB, Python, which has become the biggest, most used for deep learning, or Julia, which is the hot new language. And so last time, last year, the problems, oh, so what happened last year? Well, everything in this course is owed to a professor who visited from University of Michigan, Professor Rao, Raj Rao, who taught, gave most of the lectures a year ago, brought these homework problems, online homework problems, so that people brought laptops to class and we did things in class. And so he had and has a very successful course in EE in Michigan. But he was a PhD from here, and he came back on a sabbatical, and he created this, got us started. And we really owe a lot to him. Also, Professor Edelman was involved with this course. And you maybe know that he created Julia. How many know what Julia is? Oh, wonderful. That will make his day. He tells me every time I see him, Julia's good. And I tell him, I believe it. OK. Anyway, and it's become Professor Johnson in 1806 has used Julia. And every semester, Stephen Johnson gives in the first week a tutorial on Julia. And so that's arranged, and I promise to tell you where and when that is. So I think if you don't know anything about Julia, try to go. It's in Stata. It's on Friday at 5, 5 to 7. So Julia from Professor Johnson. So he has done this multiple times. He's good at it. So I don't think we know yet what. I guess I'm hoping that you'll have an option to use any of the three languages. But the online thing that we give you was created in Julia. So Professor Rao had to learn Julia last spring, and the class did too. And there was a certain amount of bitching about it. But I think with maybe one exception, who still got an A, everybody was kind of OK. And was glad to learn Julia. And Professor Rao now uses Julia entirely. So he's creating a new on-ramp with Julia. MATLAB, by the way, has just issued an on-ramp to deep learning that I'll tell you about, and probably get a MATLAB, somebody from MathWorks to say something about it. Yeah. So that's what's coming, that we don't quite know exactly how we'll organize those homeworks. We'll just take the first one and see what happens. So I'll certainly say more about homeworks when the, maybe even Friday. But are there other questions that I should answer? Because some people will be thinking, OK, am I going to do this? Or am I going to sit in 6036 or some other course in deep learning? Anything on your mind? And you can email me. So we will have a stellar site. And you'll see all the TAs are still to be named. But the wonderful thing is that undergraduates who took this course last year are volunteering to be graders for you guys. So they will know what those online homeworks were about. OK, so that's a sort of first word about what's coming and about the language. OK, I'm going to finish by a very important topic, multiplying A times B. Oh, look, a clean board. So now I want to multiply a matrix by a vector. OK, everybody knows how to do it. You take a row of A. So you take a row of A. And you take a column of B. And you take the dot product. So you get dot product. Row dot column. Row dot column. That's, again, low level, like OK for beginners. But we want to see that matrix multiplication A, B in a deeper way. And the deeper way is columns times rows. Columns of A, rows of B. Columns times rows. Oh, we had a column times a row. Right? That was this rank one example. We had a column times a row. And it produced a matrix. And that's what it looked like. And its rank was one. So those are what we have. So it's a combination of. It's very like AX. I'm really just extending the AX idea to AB. So this is the old way. The new way is columns. So there's column K. It will multiply. Sure enough, it multiplies row K. Everybody sees that it will happen that way. If you do it the old way, when you do a dot product of something here, something here, you're doing these multiplications. And when you hit column K here, you hit row K there. So these are connected. So I get things like column K of A times row K of B. And I don't know what notation to use, so I just wrote the words. OK. But now that's one piece of the final answer, AB. That's a rank one piece. That's like a building block. So I add from K equal 1, column 1 times row 1, column 1 of A times row 1 of B plus da, da, da, plus column K plus, and of course, stop at column N of A times row N of B. So it's a sum of outer products. So everybody sees a sum because I have column 1 times row 1, column K times row K, column N times row N. And then I add those pieces. It's just the generalization of AX to a matrix B there. So it's a sum of column K, row K of A, row K of B. And maybe we should check that that gives us the right answer. Or maybe I won't do that here. But all we're doing is the same multiplications in a different order. Actually, let's just quit. With one minute, we can figure out how many multiplications are there. How many multiplications to do a M by N matrix A times a N by P matrix B? So that's A times B. How many individual numbers, because this would determine the cost of it, how many numbers would we need? Well, suppose we do it the old way by inner product, row times column. So how many multiplications to do a row times a column and get one entry in the answer? N, right? The row has length N. The column has length N. N multiplications. So that's N. And now how many of those do I have to do? MP, because what's the size of this answer? The size of this answer is M by P. So if I do it in that old order, like N multiplications to do a dot product, and I've got this many dot products in the answer. So I've got M and P multiplies. Now, suppose I do it this way. How many multiplications to do one of those guys, to multiply a column by a row? To multiply a column by a row, this is an M by 1, and this is a 1 by P. One column, one row. How many multiplications for that guy? MP. And how many of those rank 1's do I have to do? N. You got it? MP times N. Now, the other way. Was N times MP. So it gives the same answer, M and P multiplications. In fact, they're exactly the same multiplications, just a different order. OK, we're at 1.55. Thanks for coming today. I'll talk more about the class and about linear algebra on Friday. Thank you.