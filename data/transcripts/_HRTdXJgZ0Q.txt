 All right, so last time we proved the fundamental theorem of calculus, and as a consequence, integration by parts formula and the change of variables formula were u-substitution. So let me just recall for integration by parts, it was that if I had two continuously differentiable functions, then I can shift the burden of taking a derivative. f prime times g integrate is equal to f of b, g of b minus f of a, g of a minus the integral from a to b of g prime f. So I can shift that derivative over from f to g. And this is, apart from the triangle inequality, probably one of the most useful theorems that comes out of calculus other than the fundamental theorem of calculus. In fact, for those of you who've taken quantum mechanics, which, I mean, you don't have to, or you heard of something called the Heisenberg uncertainty principle, which says something like something to the effect that you cannot measure the position of a particle and its momentum to arbitrarily good degree. You're bound by if you can measure the position of a particle very well, then your measurement of the momentum is going to be not so great and vice versa. And that's based on an inequality. And how do you prove that inequality? Integration by parts. So integration by parts is, in fact, responsible for one of the great head scratchers from quantum mechanics. So just to back up my claim a little bit. Now I'm not going to give that as an application. I'm going to give a different application related to Fourier series. So the Fourier series, what are these? So suppose we have a function, and I'm not going to say what type. Suppose f from minus pi to pi to r is 2 pi periodic. And so the question that arose due to Fourier in his study of heat transfer, so this is Fourier, I don't know, something like 200 years ago. He made the following claim, that the function f of x can be expanded in terms of simpler moving blocks, in terms of simpler functions. So we haven't talked about Taylor series. We will in just a minute. Or power series, which you've come into contact with, which is a way of, if you'd like, expanding a function in terms of polynomials, or monomials. Now Fourier suggested that f of x can be expanded as a superposition of functions which are 2 pi periodic, and kind of the most basic 2 pi periodic functions. Now what is so special about sine x and cosine x? Well this is a little bit deeper, the fact that they satisfy a certain second order differential equation, and they are all of the solutions to the second order differential equation that are 2 pi periodic. So you should think of these as kind of being building blocks. Another way to think of this is analogous to if you have a vector, so now this is not a partition, this is a vector, x1, x2, xn. Then you can expand this vector as a sum of coefficients a sub n, a sub j, j equals 1 to n. You know what, let me make this m, let's make this n, let's make this m so it looks a little a sub n, e sub n, where now e sub n, this is the basis vector, given by 0, 1, 0, where this is in the nth spot. So you can think of this expansion in terms of sines and cosines as being analogous to expanding a vector in terms of basis elements, or you can think of it as a different way to expand a function other than Taylor series or power series. But these components arose in sort of a natural way if one were to study the problem of the heat transfer, which is governed by the equation dTu equals dx squared u. And then along with the initial condition that u of 0x, so I times 0 equals f of x. So just like for how we expand a vector into basis elements, there's a formula for computing these coefficients. So a sub n should be x sub n over here, but what's a different way of obtaining these coefficients? So this is a vector in Rm. And so how do you obtain the coefficients a sub n? Well, if I take the inner product of both sides, the dot product, say, of e x dot, let's say, e sub n prime. Let's say, OK, so I've used mn, let's say, l. This is equal to the sum from n equals 1 to m of a sub n, e sub n dot e sub l. Now I wrote these basis vectors this way because that's kind of a standard choice for Rm. And what makes them standard is they have unit length, and they're orthogonal to each other. So they form an orthonormal basis. So when I take the product of e sub n with e sub l, I pick up what is usually referred to as delta nl, where here delta nl, this is 1 if n equals l and 0 if n does not equal l. And therefore, this just reduces to a l. So we see that a sub l is equal to x dot e sub l. So all of this discussion was in the setting of a finite dimensional vector in Rm and expanding in terms of the standard basis here. But it didn't have to be. It could have been, as long as it's an orthonormal basis, then I get this relation that the coefficient that appears in front of that vector is equal to the thing I'm interested in dotted with that vector, which is written here. So let's say we try and do the same thing now with f of x, except now, so these are functions. So instead of taking dot products, which is a sum of components, let's take an integral. So if I take f of x and if you like, dot it with sine of x and sum, which you can think of as, I said that the integral you should think of as maybe a continuous sum. What do we get, assuming that this expansion holds? This is equal to the sum from n equals 0. So let me make this l. This is the sum from n equals 0 to infinity of a sub n sine n x times sine. So I'm forgetting to write the integrals here. I'm skipping a point I want to make as well. And just remember the sum is starting from 0 to infinity. I don't want to keep writing it. a n sine n x sine l x plus b n cosine n x sine n l x sine l x. And then n equals 0 to infinity. OK, I'll just write it. Stop being lazy. Now assuming I can do what I'm about to do, and that's actually going to be a lot of the motivation for what we're going to discuss in our final chapter. Assuming I can take this infinite sum and interchange it with this integral, this is the interchanging of two limits. A sum is an infinite limit. An integration is a limit. So assuming I can switch these two limiting processes, then I pick up a sub n minus pi to pi sine n x sine n l x plus b sub n integral minus pi to pi cosine n x sine l x dx. Now you can actually sit down and compute this based on trigonometric identities. And what you get is that this is always equal to 0, and that this here equals pi times delta n l. OK? So this equals the sum from n equals 0 to infinity an pi delta n l, which equals pi times a l. OK? So we get this quantity here is, assuming everything we've done is kosher, equal to pi times a sub l. And then to pick up the b sub l is the same, except now you integrate against cosine of l x. So similarly, pi times b sub l is equal to the integral from minus pi to pi of f of x cosine l x dx. OK? So the b sub l's and a sub l's are referred to as a Fourier coefficients of the function f. So if f from minus pi to pi to r is continuous and 2 pi periodic, the numbers a sub n equals 1 over pi integral from minus pi to pi of f of x sine n x dx. b sub n equals 1 over pi integral from minus pi to pi. f of x cosine n x dx are referred to as the Fourier coefficients of f. OK? And so just using integration by parts, so what's the first question one should ask if it's even possible, or in what sense does f of x equal this infinite sum? Well, we haven't even gotten into that, but one question you can ask is, do these coefficients that come in front of these basic building blocks, sine n x and cosine n x, do they converge to 0? And if we expect f of x to be equal to the sum of these basic parts, then the contributions from each should be getting smaller and smaller. OK? So does a n and b n tend to 0 as n goes to infinity? And this is the content of what's usually referred to as the Riemann-Lebesgue Lemma, but it's usually stated in a different way. I'm just going to state it this way right now, which is the following. If f from a, b to R is continuously differentiable, then limit as n goes to infinity of a sub n equals the limit as n goes to infinity of b sub n equals 0. Now, the actual way the Riemann-Lebesgue Lemma is typically stated is, in fact, I don't need it to be continuously differentiable. I just need it to be continuous. This is still true. But we haven't done, or won't do in this class, approximation theorems for continuous functions, which says that if you can do this for continuously differentiable functions, then basically you can do it for continuous functions. But this will suffice. So what this says is that the contributions coming from these building blocks is getting smaller, at least in the sense that the coefficients are getting smaller. But it says nothing about if that sum up there with the a n's and b n's defined this way actually converges to f. I do want to emphasize that. In fact, trying to straighten out this question, in what sense this series converges to f, is really the motivation for a lot of analysis developed past, sort of in the first part of the last century and the last part of the century before that, and forms the basis of what's called harmonic analysis, which is a really beautiful subject and still an active area of ongoing research. So how do we prove this? Well, I stated the integration by parts formula earlier. So in fact, it'll follow pretty easily from that. Let's prove that the limit as n goes to infinity of b sub n equals 0. The one for a sub n is similar. There's just the extra piece, but I'm going to be a little bit lazy and do the easier one. So so let's look at b sub n. This is equal to the integral from minus pi to pi of f of x. And in fact, let me write cosine nx times f of x dx dx. Why am I writing dx dx? And now what I do is cosine nx I can write as the derivative of something, 1 over n times sine nx. If I take the derivative of that with respect to x, I get cosine nx. I didn't actually prove that, but you can look back in your calculus textbooks. We've proven enough to be able to make that precise. So by integration by parts, I can now shift the blame or shift the burden of this derivative onto f. But look what I've gained. I've gained a 1 over n here. So now this is equal to 1 over n sine n pi f of pi minus sine of n minus pi times f of pi minus 1 over n sine nx minus pi to pi f prime of x dx. And really what this computation is illustrating is the oscillatory nature of what's going on. Cosine of nx is oscillating as n gets very large between minus 1 and 1 on sort of equal footing. So on average, you're getting the same amount of positive f as negative f, or you're weighting f in such a way that it's both positive and negative in equal amounts. Now sine of n pi, no matter what n is, I get 0, sine of n of minus pi, I get 0. So this first part drops off. So this is equal to minus 1 over n minus pi pi sine nx times f prime of x dx. And therefore, if I take the absolute value of b sub n, this is less than or equal to 1 over n minus pi pi sine nx f prime of x dx. If I bring the absolute value inside, so I can bring the absolute value inside and still get this. So in fact, before when I had the absolute value outside, it's an equality, but now it's a less than or equality, I mean an inequality. So now sine of nx is always bounded by 1. So this is less than or equal to the integral of f prime of x dx. So this is equal to 1 over n integral ab f. Now this is just f prime. This is just a fixed number times 1 over n. So this converges to 0 as n goes to infinity. And b sub n, an absolute value, of course it's always bigger than or equal to 0, and it's bounded by something converging to 0 as n goes to infinity. So by the squeeze theorem, we conclude b sub n converges to 0. And that's the proof. The proof for the a sub n's is similar, except now you can't throw away necessarily the endpoints, but it's still not a very big deal. In fact, if we have time, I'll show you how much. One can prove, and this is proven in classes on harmonic analysis, that in fact for a function which is continuously differentiable, in 2 pi periodic, this series actually does converge uniformly to f on this interval, and I haven't even said what uniform convergence means, but actually does converge to the function f of x. So this is the case for continuously differentiable functions. I'll give a proof later that in fact this series converges if f is twice continuously differentiable. We can actually do that using the integration by parts again, essentially. But there are a few things here that are behind the scene that I kind of swept away. First off, when we computed these formulas, formulae, I guess, we interchanged summation, infinite summation of functions with integration. When can we do that? In what sense does, if this Fourier series converges, in what sense does it converge to f? For convergence of real numbers, there was just one sense of the convergence of real numbers. So when we have a sequence of functions, which is now what we're going to turn to, we'll have different notions of convergence to another function. And depending on in what sense that convergence takes place, some of these limiting operations may not interchange. So now we're going to move on to the final chapter of this class. And I know it seems like we're kind of hitting a lot of different things now towards the end of the class. And we took it slow during the first part of class, but that's, like I said, that's, I think I even said this at the start of class, we didn't have very much to go off of. We built things from the ground up. And the more technology you have, the more things you can prove, the more interesting questions you can ask. So now we're going to go on to sequences of functions. And you could also put sequences in series, because a series is just a special type of sequence of functions. So I motivated a little bit of why we would be interested in functions converging to other functions or sequences of functions converging to a function. But we could look at something much more basic. So let's take a step back and look at power series. And this should be thought of as motivation for what's to come, just like our discussion about Fourier series. And again, I'm not going to ask any deep questions about Fourier series on the homework or on the exam. So a lot of this is just, this discussion was to motivate this theorem here. But now I'm going to make a more precise motivation, I guess, for what's to come. So although we've had series forever, I never brought up power series. And it's for a reason. It's because I didn't think they belonged anywhere until we got to sequences of functions. So a power series about a point x0 is a series of the form sum from j equals 0 to infinity of a sub j x minus x0 to the j. So the x0 is given. And the things that could change are the coefficients or this number x here. So a theorem which immediately follows from, essentially, the root test. Suppose this number r, which is the limit as j goes to infinity of a sub j, 1 over j, exists. So it's a finite number, positive non-negative number. And define rho to be 1 over r if r is bigger than 0, infinity if r equals 0. Then we have the following conclusion that this power series, a sub j, converges absolutely if x minus x0 is less than rho and diverges if x minus x0, an absolute value, is bigger than rho. And this number rho we refer to as radius of convergence. So again, the proof follows immediately from the root test because if we take limit as j goes to infinity of a sub j absolute value x minus x0 j, 1 over j, this is equal to x minus x0. This kills that j. This is just a fixed number, so this pops out of the limit. And this limit exists, so this equals x minus x0 times r. And we have two things happening. This is less than 1 if x minus x0 is less than rho, bigger than 1 if x minus x0 is bigger than rho. This number here. And therefore by the root test, the theorem holds. So we see that this series where the, if you like, what's given are the coefficients a sub j and x0. And what could change is x. This series converges as long as x minus x0 is less than rho. So as long as we stay in that interval, a symmetric interval about x0, then this series converges absolutely. So we can define a function where if I take x in this interval, stick it into this series, I get out a number. So define function f now going from this interval, so x0 minus rho, x0 plus rho to r by f of x equals the number that gets spat out by this power series. So for example, what is f of x? Let's say I take all of the coefficients to be, let's say x0 is 0 and all the coefficients are 1. So let's say I look at sum of x sub j, then f of x. So we've already computed for a geometric series. This is equal to 1 over 1 minus x, or x and minus 1, 1. So in this simple setting, 1 over 1 minus x is equal to x to the j. Another example is that if I take a sub j equals 1 over j factorial, x0 equals 0, then we've done this in an exercise that this series here converges absolutely for all x, meaning rho equals infinity. And this is how we define the exponential function. Exponential function exponential of x to be what comes out of this series. And then simply from this definition, you can show things that an exponential function should satisfy, x e to the n is equal to n e to the e of 1 to the nth power and so on and so on, so that this really does obey what you believe an exponential function should look like. And also it grows faster than any power of x. As x goes to infinity, it goes to 0, those types of things. But this is how the exponential function is defined. So OK, we have this function that's defined by whatever this power series spits out for x inside this interval of convergence. So then I could write f of x as the limit as n goes to infinity of a sequence of functions, because this is just how it's defined, where fn of x is the partial sum. It's just a polynomial. So for a power series, the limiting function, you can write it as the limit of the partial sums, which are just polynomials. And so I should say for all x in this interval, we have this. So now some questions arise. So what? The limiting function is equal to the limit of these maybe simpler functions. These simpler functions are just polynomials for the case of power series. So like I said, 1 over 1 minus x is equal to the limit of these polynomials. Some questions should arise. I mean, analysis is about limits. You can think of that as half the story. First off, what is the limit? What are the important limiting processes that we consider? The second question is, how do different limits interact? So let's pose that as a question now, a three-parted question for a power series. And this will motivate. And this is, again, motivating all of what we're going to be doing now. So is the function that I get as this limit of polynomials as the output from a power series, is it continuous? The individual pieces that I take a limit to get f of x, these polynomials, the partial sums, are certainly continuous. They're just polynomials. So is the limiting thing continuous? Now if so, is f differentiable? And in particular, since f is equal to the limit as n goes to infinity of the fn's, is f prime equal to the limit as n goes to infinity of the fn prime? So the derivative is a limiting process. So I'm taking the derivative of the limit. So I'm asking, can I take that derivative inside the limit? Can I swap the two processes? And the same with integration. If one does the integral of f equal limit as n goes to infinity of the integrals of the fn's. So again, this is a limiting process that we're asking us to flip, because f is equal to the limit as n goes to infinity of fn. And what I'm asking is, can I take this integral inside that limit? Now you can ask these questions not just for power series, but in a more general setting, which is what we're going to turn to now. But this should be in the back of your mind as the motivation for what we're doing. And apart from being just an academic question, it's also somehow giving you information over whether the formal manipulations that you're doing with Fourier series that are actually somehow modeling some physical phenomenon, are these formal manipulations even meaningful? So these are the three questions that motivate what we're going to do going forward. But we don't have to just stick to the setting of being in power series. This should be a very important example of a sequence of functions converging to a function that's a limiting function. And then we can ask these questions, but we don't have to just stick to power series. So let me move on to a more general setting in which we'll answer these three questions. And two modes of convergence for limits of functions, or sequences of functions. So first definition, this is in fact what we showed or what we were talking about before. Or in natural number, let Fn be function from S to R. S is some non-empty subset of the real numbers. And let F be from S to R. We say Fn, so the sequence of functions Fn converges point-wise to the function F if for all x in S, if I stick in x into S, so for each fixed x in S, if I stick this into Fn of x, I get a limit, and this limit is F of x. So for example, going back to power series, if we defined F of x, so let me just rewrite that example that I had up there. If I define F of x equals 1 over 1 minus x, Fn of x to be sum from j equals 0 to n of xj, then for all x in minus 1 to 1, limit as n goes to infinity of Fn of x equals F of x, i.e. this, the sequence of partial sums corresponding to this power series converges to 1 over 1 minus x point-wise on minus 1, 1. So I said whenever you come across a definition, you should negate it, but the negation of this definition is not too difficult. A sequence of functions does not converge to another function point-wise if there exists some point so that when I stick them into Fn of x, Fn of x does not converge to F of x. So let's look at another example which is not a power series. Let's say we take Fn of x to be x to the n, where x is in the closed interval 0, 1. So what's happening here is n gets very large. There's 1, 1. There's F5 of x. And then as n gets very large, these guys are dropping down even more and more. And are we picking up something in the limit? Well, let's look. Well if x equals 1, it's pretty clear that the limit as n goes to infinity of Fn of 1, this equals 1. If I stick in 1 here, I get 1 for all n, and therefore the limit as n goes to infinity of Fn of 1 is 1. Now if x is in 0, 1, then we've done this limit before. Limit as n goes to infinity of Fn of x, this is equal to limit as n goes to infinity of x to the n. Now x is less than 1, so x is being raised to a higher and higher power. This equals 0. Thus, what do we conclude? For all x in 0, 1, this sequence of functions, x to the n, converges pointwise to the function F of x, which is equal to 0 if x is in 0, 1, and 1 if x equals 1. So I draw another picture here of what the limit looks like. And you can kind of start to see this. As n gets large, again, this is becoming more vertical there, but then going to 0. So for any fixed n, it's converging to this picture on the right. And so we can already pick up something, or at least answer one of these questions, if we take them as a question about convergence of functions in the pointwise sense. So we could have asked this question now, right after having this definition. Suppose Fn's are continuous, converging pointwise to a function F. Is the function F continuous? And what this example shows is that no, that's not the case. x to the n is always continuous, yet the limit as n goes to infinity, the pointwise limit, is given by this function, which is 0 from 0 to 1, and 1 at x equals 1, which is not a continuous function. So already, we're kind of seeing that pointwise convergence, which is this first weakest mode of convergence that we, as of now, can say about power series, is not good enough to ensure that the limit is even continuous, because this example shows that we have a sequence of continuous functions whose pointwise limit is not continuous. So as another example, you should always, I like this kind of last chapter, because you can draw a lot of pictures. So I'm going to draw pictures of Fn. So it's piecewise linear. So I could write down what the function is, but I don't want to. I'm just going to draw pictures. So Fn of x from 0, 1 to r, this is how it looks. There's 1, 1. And what I do is I go to the point 1 over n, and the function Fn of x is 0 up until then. And then it's a linear function connecting 1 over n to 2n here. And then it connects this point 2n, 0, or I should say 1 over 2n, 2n, connects that to the origin. OK, so that's F sub n of x. All right, it's just piecewise linear. So for example, if I wanted to draw F1, F1 would look like there's 1, 0, 2, 1, 1. And let's say I wanted to draw F100. What does that look like? So maybe I should make this one a little bit bigger. There's 1, 1 over 100. And then that should be 1 over 200. And then if I go up to 200, there's this piecewise linear function, which is getting, it's 0 from 1 up to 1 over 100. So it's 0 most of the time, and it's 0 at the origin. But in this in-between, it's very tall and very slim. And so my claim is that for all x in 0, 1, limit as n goes to infinity of Fn of x equals 0. OK, so this sequence of functions converges pointwise to 0. All right, so why is this? Well, let's just give a full proof of this rather than me talking it out. I mean, I'll talk it out and give a full proof. OK, so let's look at the easiest spot first. And I don't even need the formula for these guys. I just need to know that they have this basic characteristic that they're pointwise linear, I mean, that they're piecewise linear, connecting 0 to 1 over 2n, 2n here, and then down to 1 over n, 0. And then there's 0 between that and 1. So first off, if x equals 0, then all these functions are 0 at the origin. So they equal 0, so that equals 0. So that's fine. So now suppose x is in 0, 1. And so what do we want to show? We want to show limit as n goes to infinity of Fn of x equals 0. And here, so what's the point here? Now there's 1, there's x. So I have to give a, well, I'm not even going to do an epsilon delta, epsilon m argument. I'm just going to show you what happens. So there's x between 0 and 1. Now let's choose a very large integer so that 1 over m is less than x. So here's 1 over m strictly to the left of x. Now what does the graph of Fn of x look like for n bigger than or equal to m? It's 0 from x equals 1 to x equal 1 over m, and then shoots up and then comes back down over here to 0. But here's the point. It's 0 all the way from 1 to 1 over m. So in particular, at x, Fn of x is 0. So if I look at this sequence, Fn of x, which I'm trying to show converges to 0, it is 0. No, it is F1 of x, F2 of x, up to Fn minus 1 of x. And then at Fm of x, so this is Fm of x spot, it's 0. And this point is only going to the left. So for all n bigger than or equal to m, now this will be 1 over n will be to the left of x, and therefore Fn of x will be 0. So this is 0, 0, 0, 0, and so on. So I have the sequence is eventually 0 for all n bigger than or equal to capital M. And therefore, the limit, I mean, it's pretty easy to take a limit of a constant sequence, which proves that this sequence of functions converges pointwise to 0. Now, I didn't come up with this fancy example for just any old reason. It'll come back in a minute when we start answering some of these questions or asking them again within the context of these two ways of converging. So far, I've only given you one definition of convergence, pointwise convergence of a sequence of functions. And now I'm going to give a slightly, I'm going to give a stronger, it's not slightly, it's much stronger definition of convergence of a sequence of functions. So we have a sequence of functions and a given function from S to R. S is a non-empty subset of R. Then you say the sequence Fn converges pointwise, or uniformly to 0, uniformly to 0, uniformly to sine of a day. If I start mixing up some of my words, I'm always going to correct it, but the first word out of my mouth may not be the correct one. Converges uniformly to f of x, to f, if, now we have an epsilon n statement, for all epsilon positive, there exists an m, natural number, such that for all n bigger than or equal to m, for all x and s, Fn of x minus f of x is less than epsilon. Now I want to make a brief comment. This looks suspiciously like pointwise convergence. If you just wrote down what it means for the limit as n goes to infinity of Fn of x goes to f of x, except there's a very subtle and important point, and that is where does the for all x and s appear. For pointwise convergence, you can state pointwise convergence as this being at the start of the line, for all x and s, for all epsilon positive, blah, blah, blah. Here it appears at the end of the line of the quantifiers. This makes a very important difference between pointwise convergence and uniform convergence. Pointwise convergence means I take a point x, I stick it into Fn of x, that gives me a sequence of numbers, and pointwise convergence says that sequence of numbers converges to f of x. For each x, I get a sequence of numbers which converges to f of x. Now uniform convergence is actually saying something stronger. I'll say that. In fact, let me draw a picture that goes with this definition. Let's make s to be an interval. Let's say my limiting function is f, is given by this graph. So what I'm going to do is basically shift the graph up and down by epsilon, meaning this length is epsilon and so is this length, all the way across. Let me shade this in and re-outline. So I get this little, if you like, shaded area around my function f. So this is f of x, that's the graph. And the shaded part, this is the set of all x and y, such that f of x minus y is less than epsilon. So I get a little tube snaking with f. Now what uniform convergence says is that for all n bigger than or equal to some m, so given epsilon, for all n bigger than or equal to m, if I were to draw the graph of fn of x, it better fall inside this tube across all of ab. See this tube is defined for all x between ab. So it's making a statement about how close fn of x is to f of x across the entire set. Pointwise convergence just says if I put an x into fn of x, then eventually those numbers are getting close to f of x. Uniform convergence is a global property. It's saying across the entire set, as n is getting large, the graph of this fn is getting very close to the graph of f of x. Not just if I fix an x, the fn of x at that point are converging to f of x. So I've said a couple of times that uniform convergence is stronger than pointwise convergence. Let me actually prove this now. So let me prove that following theorem. So if I have a sequence of functions from s to r, and fn, rather than right, converges to f pointwise or uniformly, I'm going to put an arrow. And then with the description afterwards, uniformly on s. And then this implies fn converges to f pointwise on s. So it's very simple. Again, what is the picture that's going on for uniform convergence is that fn is getting close to f across the entire set that we're looking at. So certainly at one point, which is all you need for pointwise convergence. Each fixed point we should be getting close. So let epsilon be positive. So first off, let's fix a number in the set S. So now we want to prove that the limit as n goes to infinity of fn of c equals f of c. Let epsilon be positive. Once fn converges to f uniformly, there exists a natural number m0 such that for all n bigger than or equal to m0, for all x in S, fn of x minus f of x is less than epsilon. So choose m to be this m0, the m that is for this epsilon. Then for all n bigger than or equal to m, let's call this equation star. Star with at the single point x equals c implies fn of c minus f of c is less than epsilon. And thus, the limit as n goes to infinity of f of c, fn of c equals f of c. I don't think I have enough time to do the example that I want to do, so I'm just going to leave you on the edge of your seat by stating the following theorem, that in fact, so this is a one-way street, meaning pointwise convergence does not imply uniform convergence. So we just proved uniform implies pointwise, but the converse does not hold. And what we'll prove next time is for in the setting of this simple example of x to the n. And so what we'll prove next time is the following. If I take any b between 0 and 1, then fn converges to f uniformly on the set 0, b. So these functions are defined on 0, 1, so they're certainly defined on 0, b for b less than 1. But however, this sequence of functions does not converge uniformly to f on 0, 1. So here, this second part, since the fn's converge to f pointwise on 0, 1, the second part says that this is a one-way street. This is not a two-way street. These two modes of convergence, uniform convergence and pointwise, are not equivalent. All right, I think we'll stop there.