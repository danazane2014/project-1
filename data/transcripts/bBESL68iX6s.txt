 Last lecture, we introduced the notion of the limit of a function as x goes to c, which we write limit x arrow c, f of x equals L. What does this mean? This means for all epsilon positive, there exists a delta positive such that for all x and s satisfying 0 is less than x minus c is less than delta, we have that f of x minus L is less than epsilon. And we proved the following theorem last time that if we have a set, a cluster point of s, so this is where we look at limits and f is a function from s to r, then the limit as x goes to c of f of x equals L if and only if for every sequence x sub n converging to c, we have f of xn, the new sequence, converges to L. So this theorem here connects limits of functions to what we did previously, limits of sequences. Now using this theorem, we'll get analogs of theorems we proved for sequences, but now for limits. First, let me show a few simple applications of this theorem. So for example, we could prove the following that for all c and r, limit as x goes to c of x squared equals c squared. So we could have done this using just the definition, but now with this theorem, we can prove it a little bit quicker and easier because we essentially did all the hard work when we proved that the product of two convergent sequences is convergent, which is what we'll use here. So we're going to prove this theorem using the previous theorem. So let xn, so for this theorem, since it's not stated, you should take the function to be f of x equals x squared, and then the set that is defined on s is equal to r. So let xn be a sequence such that xn converges to c as n goes to infinity. We now want to show that f of xn converges to f of c, where f of x equals x squared. But this follows from what we proved for limits, right? So we proved the product of two convergent sequences is convergent by earlier theorem about the product of convergent sequences. We get that xn squared converges to c squared, which implies, so we've now verified for every sequence converging to c, f of xn, meaning xn squared, converges to c squared. And therefore, by the previous theorem, we've proven the claim. So now let's use this theorem to study a couple of more limits. And we'll use this theorem, in fact, to show that a certain limit does not exist. So limit as x goes to 0 of sine 1 over x, this limit does not exist. So remember, for a limit, I look at all those points x that are close to c but not equal to c. So this function, which I'm taking the limit as x goes to 0 of, doesn't need to be defined at x equals 0 in order to consider the limit. And if you'd like, the function sine of 1 over x is defined on s equals r take away 0. So this limit does not exist. But the limit as x goes to 0 of x times sine of 1 over x does exist and equals 0. Now I want you to note something. You can't just stick in x equals 0 here to evaluate the limit. I mean, because then you'll be taking sine of 1 over 0 times 0. You can't divide by 0. So you can't just say this limit equals 0 because I stick in x equals 0. In fact, we saw in the previous lecture that the limit need not equal the function evaluated at that point. So just to recall the example from last time, when I had f of x equals 1 if x equals 0 and 2 if x does not equal 0. And we showed that limit as x goes to 0 of f of x equals 2, which note does not equal f of 0. When we discuss continuity, that's what connects the limit to the function evaluated at that point. I just want to make that little comment. To prove this theorem, we're going to use, again, the previous theorem. Maybe I should, no, I'm not going to label it. You'll know when I point up to it that I'm referring to that theorem that I stated up there. So in fact, let's prove the second limit exists and equals 0 first. So suppose xn is a sequence converging to 0. We now want to show that x sub n times sine of 1 over x sub n converges to 0. And then that, by the previous theorem up there, will imply that the limit as x goes to 0 of x times sine of 1 over x equals 0. Now, if I look at the absolute value of x of n times sine of 1 over x of n, this is equal to the absolute value of x of n times the absolute value of sine of 1 over x of n. And no matter what you stick into sine, sine is always bounded between 1 and minus 1. So the absolute value is always bounded by 1. So just to summarize, we've shown that. OK. And now we apply the squeeze theorem. So if you like, that goes to 0. It's just the constant sequence equal to 0. Since x sub n is converging to 0, remember we're assuming that, this converges to 0. And therefore, what gets trapped in between goes to 0. So by squeeze theorem, x sub n times sine of 1 over x sub n converges to 0. And that proves the second claim. So now we'll prove one. And in the previous lecture, I did negate this definition. So let's actually negate this theorem, if you like, or use the negation of each side of this if and only if to state an equivalent theorem. So two statements are equivalent, which is in that theorem, if and only if their negations are also equivalent. So by the theorem, we also have the following fact. Limit x goes to c of f of x does not equal l, if and only if there exists. So negating the right-hand side of that if and only if there exists a sequence xn converging to c. So this sequence will consisting of elements of s take away c such that xn converges to c, but we don't have f of xn converging to l. And when I write this, you should read this as either this limit exists and does not equal l or this limit does not exist. So I'll even write that out. So and either limit does not exist or does not equal l. So again, that's an equivalent way of stating the theorem up there is in terms of the negation. Two statements are equivalent, which is right there, if and only if their negations are equivalent. All right. So now we're going to prove that the limit as x goes to 0 of sine of 1 over x does not exist by showing that there exists a sequence converging to 0 such that when I plug that into sine, the limit of that sequence does not exist. So to show there exists a sequence converging to 0 such that limit as n goes to infinity of sine of 1 over x sub n does not exist. OK? Now, sine oscillates between 1 and minus 1 depending on if I'm a certain multiple of pi over 2. Right? So here's the intuition behind it, behind the sequence I'm about to give you. Note that sine of, let's say, sine of x equals 1, if x equals pi over 2, 5 pi over 2, 9 pi over 2, and so on, minus 1 if x equals 3 pi over 2, 7 pi over 2, 11 pi over 2, and so on. So I can stick in things that are getting bigger to sine and get 1 or minus 1. And in fact, let me change this to y since we're using x to be essentially 1 over y. OK? So if I stick in pi over 2, 5 pi over 2, 9 pi over 2, and so on, I get sine equals 1. If it's 3 pi over 2, 7 pi over 2, 11 pi over 2, and I stick that into sine, I get minus 1. OK? But that means if I stick 1 over these numbers into sine of 1 over x, I get 1 or minus 1. And so then that sequence that I get will be 1 or minus 1 alternating. And we know that sequence does not converge. So that's the idea. So let me write that down. Let x sub n be 1 over these numbers, essentially. So 2n minus 1 pi over 2 minus 1, because we're going to stick this into sine of 1 over x. So let me just, which is 2 over 2n minus 1 over pi. OK? Now note, for all n, x sub n is less than or equal to, I can write this as 2 over n plus n minus 1 pi. And since n is bigger than or equal to 1, this is always bigger than or equal to 0. So this is less than or equal to 2 over n pi. And this goes to 0. So that shows by the squeeze theorem that this sequence I've defined here converges to 0. But what happens when I plug this sequence into sine of 1 over x? This is now equal to sine of 2n minus 1 pi over 2. OK? And this is therefore equal to 1 minus 1, 1 minus 1, 1 minus 1. OK? And this sequence, which is just equal to minus 1 to the n plus 1, I don't know why I capitalized that, does not converge. OK? So we found a sequence converging to 0, such that when I stick it into the function, that new sequence does not converge. So we've proven that this limit does not exist. All right, so I alluded to this fact that this theorem will give us theorems that are similar to what we proved for sequences, except now for limits of functions. So let me just state the simplest theorem you can get. So let S be a subset of R. See a cluster point of S. And suppose I have two functions, f going from S to R, and g going from S to R. OK? If these two limits exist, and one function is smaller than the other, then so we had an analogous statement for sequences, which was if I have two sequences converging, and one is less than or equal to the other, then the limits are one limit is less than or equal to the other. And it's an analogous conclusion for limits of functions. OK? So again, analogous statement for sequences was we have two sequences, one less than or equal to the other, which then the limit of the smaller sequence is less than or equal to the limit of the bigger sequence. OK? So let's get the proof. And we'll use this theorem connecting limits of functions to limits of sequences. And then we'll use the corresponding statement, which we do have for sequences, which I just stated a couple of times. So let L1 be the limit as x goes to c of f of x. And L2 be the limit as x goes to c of g of x. And what we want to show is that L1 is less than or equal to L2. Right? OK. Let xn be the sequence in S takeaway c such that xn converges to c. Such a sequence exists because c is a cluster point of S. All right? And you proved in the assignment that if I have a cluster point of S, then there exists a sequence in S takeaway c that converges to c. Now, since, I'll say it this way, by the previous theorem, this limit equals L1, this limit equals L2, if and only if for every sequence converging to c, f of xn converges to L1, g of xn converges to L2. By the previous theorem, we then conclude that L1 is equal to the limit as n goes to infinity. Actually, I'm getting ahead of myself a little bit. So let's pause right there and reset. So now we have the sequence converging to c. Then by the assumption here for all n, f of xn is less than or equal to g of xn. And since f of xn converges to L1 and f of xn converges, g of xn converges to L2, we get by, again, this theorem about sequences, which says if I have two sequences, f of x of n, this is one sequence, is less than or equal to another sequence g of xn, and they both converge, then the limits satisfy the same inequality, which is what we wanted to prove. So like I said, this theorem here follows from the analogous. In fact, I should have written this out. Or in fact, I will write it out now. So the analogous statement for sequences was if for all n, an is less than or equal to bn, then limit as n goes to infinity, assuming both limits exist, satisfy the same inequality. So this is an analogous theorem to this theorem, which we had for sequences. And we used this theorem from sequences to prove it. Now following that same philosophy, you can prove analogous statements for functions, limits of functions, as you did from sequences. You get these for free. And instead of stating all of the theorems we did for sequences, except now for limits of functions, I'm just going to quickly say you get the same thing. So by using the previous theorem, which connects convergence of functions to convergence of sequences, we have analogous theorems for, and let me state it this way, for limits of functions now. And for example, you have a squeeze theorem. Namely, if I have, so just talking this out, if I have three functions, say f is less than or equal to g is less than or equal to h, and f of x and this converges to l, h of x converges to l, then g of x converges to l. That's what I mean by analogous statement. We also have theorems about algebraic operations and limits, meaning that if I have two functions that have limits as x goes to c, then f plus g will have a limit as x goes to c, and that the limit of the sum is the sum of the limits. Same thing with the product, and same thing with the quotient, assuming the limit on the bottom is non-zero. And then similarly we also have a theorem about the absolute value and limits. Namely, if f of x converges to l as x goes to c, then the absolute value of f of x converges to the absolute value of l as x goes to c. So you have all of these analogous statements or theorems that are analogous to the statements from what we did for sequences, but now for limits of functions as x goes to c. And I'm not going to state them all. You can see this in the textbook. Maybe the proof of some of them I'll give as exercises. Now let's separate the notion of a limit of a function from that of a sequence just a little bit. So unlike when we talk about limits of sequences, here we're letting a point x converge or get close to a point c, but there's two ways it can get close to c. On the real number line, it can converge to c from the left, or it can converge to c from the right. And this leads to the notion of left and right limits of a function. Left and right limits of functions. So start the definition here, and we'll go to the next board. So let S be a subset of R. And suppose c is a cluster point of minus infinity, 0, intersect c, intersect S. So what I'm defining now is the notion of a function converging to something as x goes to c from the left. That's why I'm looking at only S intersect minus infinity to c. So I'm only looking to the left of c. We say f of x converges to L as x converges to c from the left by putting a minus sign up here. If it's a similar definition as that of the limit, but now we only look at x getting close to c from the left. If for all epsilon positive, there exists a delta positive such that for all x and S satisfying c minus delta is less than x is less than c, so it's close to c but to the left of c, we have that f of x minus L is less than epsilon. And in this case, we write the limit as x goes to c with a minus sign up top, f of x equals L. And then we have an analogous definition for converging to or taking a limit of a function as x goes to c from the right. If c is a cluster point of c comma infinity intersect S. So I should say if. Suppose c is a cluster point of c comma infinity. Now just taking S that's to the right of c. And we say that f of x converges to L as x converges to c plus, meaning as x converges to c from the right. If for all epsilon positive, there exists a delta positive such that for all x and S satisfying c is less than x is less than c plus delta. So now it's close to c but to the right of c. We have that f of x minus L is less than epsilon. And similarly to the notation up there, we write limit as x goes to c plus of f of x equals L. Now just like we proved this theorem for limits, you can state and prove an analogous statement for one-sided limits. So such a statement would be, for example, limit as x goes to c minus of f of x equals L if and only if for every sequence x sub n satisfying x sub n is less than c converging to c, we have f of xn converges to L. So these two just kind of limit how f behaves near a point c if we're just looking to the left of c or to the right of c, but not at c. So let's, for example, look at, I think this is usually referred to as the Heaviside function. So f of x equals 0 for x less than or equal to 0 and 1 if x is less, bigger than or equal to 0. So graph is like that. And why do people care about this function? Well, in a certain sense, if you take the derivative of this function, you get what's called the Dirac delta function, although that's not a function. That's a distribution. But that's why this function has a name attached to it, because if you take its derivative, you get something somewhat special. Anyways, we're not even at derivatives. We're not even going to talk about distributions in this class. So let's get back to one-sided limits. So if I look at this function for x close to 0 from the left, f is just 0. So in fact, I can, since I'm only looking at x to the left of 0 in this limit, this is just plugging in x less than 0. So I get 0, and that's just 0. And if I look at this function from the right, to the right of 0, then f of x is just 1. It's just 1 identically. And so I get, and it's again, so although I haven't shown that one-sided limits of constants equal the constant, I think that should be something you can easily believe or write out yourself. So for this function, we see that it does have two-sided limits, except those limits don't equal each other. And they certainly don't equal, so this one does equal f of 0, but I could have made f of 0 to be 1 half, and then this still would have been 0 and not equal to the function evaluated at the point. So again, I'm making this point that for limits, just limits, it does not matter what the function is doing at the point. Limit only cares about how a function behaves near a point. One-sided limits augment that by saying we're only going to care about the function near the point and to the left for the left limit and to the right for the right limit. So I didn't say that, but this we call the left limit, this we call the right limit, simply because we're getting close to see from the right and from the left. OK? All right, so what is the connection between left and right limits? We have the following. So let S be a subset of R, f be a function from S to R, and suppose that C is a cluster point of both sets minus infinity to C intersect S, and C infinity intersect S. OK? That way I can talk about the left and right limits of C or at C. Then, so first off, if C is a cluster point of any one of these sets, it's going to be a cluster point of the set S. OK? So we can actually look at the limit. So then the limit as X goes to C of f of X exists and equals L if and only if the limit as X goes to C from the left of f of X equals the limit as X goes to C from the right of f of X equals L. OK? So, you know, this kind of looks like the theorem we proved about lim sup and lim inf, but, you know, they don't have any connection. OK? You know, so, OK, kind of what if you want to make some sort of connection between the way this theorem looks and the statement of the theorem for lim inf and lim sup. So this is kind of saying that limit of a function equals L if we approach from the left or right, the f, the function f approaches L. OK? And for the lim sup, lim inf guy that we did for sequences, you could take that as saying that the limit of a sequence equals L if and only if kind of following the sequence from below, that approaches L and following the sequence from above, that also approaches L. OK? So, you know, there's kind of two directions there just as there's two directions here, but not really. OK? So let's give a quick proof of this theorem. It's not difficult. It follows almost immediately from the definitions. In fact, I'm going to do only one direction. So kind of this direction should, assuming this and proving this, should be pretty clear. OK? If I have this, this means that, you know, if I want to be close to L, I just need to be close to C. And therefore, it doesn't matter if I'm close to C from the left or right, I'll be close to L. And going this direction, this says I just need to be close to C from the left, sufficiently close, and I need to be close to C from the right, sufficiently close, to be close to L. OK? You know, so really there's not a lot of trickiness in the proof. It's just writing these things out. And so I'm just going to write out one direction and leave the other direction to you. So let's assume that left limit equals the right limit equals L. And now we want to show that the limit as X goes to C of f of X equals L. OK? Now we want to show limit as X goes to C of f of X equals L. So let's go back to the definition. Let epsilon be positive. We want to be able to find a delta so that f of X is within epsilon to L if X is within delta to C. And what's the point? Here's C and here's L. So this is the picture that goes along with this. Assuming these two limits equal L, I know that there exists a delta 1 so that if I'm in this interval, then here's L plus epsilon, L minus epsilon, then if I'm within delta 1 to C and to the left, then f will be close to L in that interval. OK? And then since the limit as X goes to C from the right equals L, there exists some delta 2 so that if I'm in this interval, then I'll be again close to L. OK? But this means that if I choose the smaller of these two and I look at the whole interval, then f will be close to L on the whole interval, and that's it. OK? So let epsilon be positive since limit as X goes to C minus f of X equals L. This implies there exists delta 1 positive such that if X minus C is less than delta 1, then I get that f of X, if C minus delta 1 is less than X is less than C, this implies that f of X minus L is less than epsilon. And similarly for the right limit. Since limit as X goes to C plus of f of X equals L, this implies by the definition there exists a delta 2 positive such that if C is less than X is less than C plus delta 2, then I get f of X minus L is less than epsilon. Now choose delta to be the minimum of delta 1 and delta 2. And we'll now show that this delta works. OK. So now we're going to show this delta works. So if less than delta, then this implies that either X is in C minus delta. So if we take something close within delta to C and delta is the minimum of these two distances, then for the sake of this picture, let's say delta 1 equals delta 2, so now I'm looking at this interval, then two cases, either X is in C minus delta C, which is a subset of C minus delta 1 C, since delta is the minimum of those two deltas, which implies by the first inequality here for delta 1, or X is in C, C plus delta, which is a subset of C, C plus delta 2, which implies our choice of delta 2 gives us that. Thus, we've shown that if X minus C is less than delta, then f of X minus L is less than epsilon. That's the end of the proof. OK. So I've said this over and over again. Limits of functions don't care about what the function is doing at the point. It cares about what the function is doing near the point. Now we're going to discuss the notion of continuity, which connects the limit of a function at a point to the function, so it connects how a function behaves near a point to the function evaluated at the point. OK? And so you can even write this down, how a function behaves near a point compared to, so near at the point. OK? And you'll see from writing, when I write down the definition, basically, it's what's kind of staring you in the face is that the definition of continuity is that the limit as X goes to C of f of X equals f of C. All right? So we had these examples where the limit, I think I already erased it, but where the limit exists but does not equal f of C. And here for continuity, the notion of continuity is that the limit as X goes to C of f of X actually equals the function evaluated at that point. So we have the following definition. Let S be a subset of R and C an element of S. OK? We say f is continuous at C if for all epsilon positive, there exists a delta positive such that for all X and S satisfying X minus C is less than delta. So in particular, look, I can now, for example, X equals C will satisfy this inequality. I don't have the 0 is less than that. So for all X and S which are close to C within delta, I have that f of X will be within epsilon of f of C. OK? So in this case, so just a little, if f is continuous at every point on its domain that we're considering, we just say f is continuous. OK? So for a function to be continuous at a point nearby X, X being near C, should mean that f of X should be near f of C. OK? And we'll go through, let's go through some examples. Remember, whenever you get a definition, you should look for examples and then potentially negate it. We'll negate this definition in just a second to show that a function I wrote down a minute ago is not continuous. So the function, so the affine function f of X equals A times X plus B, so S is R, so X is a real number, is a continuous function, meaning it's continuous at every real number C. All right? So let's prove this. Let C be an element of R. We want to show f is continuous at C, so we have to go through the definition. Let epsilon be positive. Choose delta to be epsilon over 1 plus the absolute value of A. OK? And last time in the previous lecture, I kind of gave the intuition on why you would choose this delta based on the function and epsilon. I did a computation here. I'm just going to choose delta this way, and you'll see that it works. So now we have to show this delta works. If X minus C is less than delta, we should be able to now show that f of X minus f of C is less than epsilon. This is equal to AX plus B minus AC plus B, so this is equal to A times X minus C, which equals the absolute value of A times the absolute value of X minus C. This is less than delta. Absolute value of X minus C is less than delta times A, which equals the absolute value of A over 1 plus the absolute value of A times epsilon, which is less than epsilon, OK? Because a number over 1 plus that number is always less than 1, OK? Maybe you were wondering why didn't I just choose delta to be epsilon over the absolute value of A. This is just a smidgen of sophistication that what happens if the absolute value of A is equal to 0? Then we would have divided by 0. So adding a 1 there takes care of that, OK? So this guy is continuous at every C, so this function is continuous. How about a function that's not continuous at a point? Make sure this is the next topic. Yeah. Like here is a non-example. The function f of x, which equals 1 if x equals 0, 2 if x not equal to 0. So that's the function. f is not continuous at 0, OK? So to prove this, let's negate the definition of continuity. So the negation of continuity is, so f is not continuous at C if, so the for all's become there exists, and the there exists become for all's. So if there exists some bad epsilon so that for all delta positive, there exists an x such that x minus C is less than delta, and we do not have the second inequality, f of x minus f of C is bigger than or equal to this bad epsilon, OK? Now for this guy, it's pretty clear which x to choose. So let's think this out for a minute. There should be some bad epsilon 0 so that if I take any small interval around 0, I can find a point in this interval so that f of x minus f of C is going to be bigger than or equal to epsilon 0, all right? Now here f of C is f of 0, which is 1, OK? Now what would be the bad epsilon so that f of 1 is greater than distance 1 or greater than distance epsilon 0 to f of x for x, for some x in this interval? Well, if I take any x in this interval other than 0 and stick it into f, I'm going to get 2, and that's with greater than or equal to distance 1 to f of 0, all right? So epsilon 0, I will choose to be 1. OK, so now we want to prove that f, this function here, is not continuous at 0. So I ought to tell you what the bad epsilon is. Choose epsilon 0 equals 1, all right? So now we have to show this bad epsilon 0 is indeed bad. With delta B positive, we have to now find a number in this interval so that f of x minus f of C, I mean f of 0, is bigger than or equal to 1. And like I said a minute ago, if you take any x in this interval other than 0 and stick it into f, I get 2, which is distance 1 to f of 0. x to be delta over 2, say. Then x minus 0 is less than delta. It's actually equal to delta over 2. And f of x minus f of 0, this is equal to 2 minus 1, which is bigger than or equal to epsilon 0. OK, so this function is not continuous. And so next time, and I'll just leave this question here, which we'll address in the next lecture, but it's a kind of simple question. So first off, if you look at this function, it shouldn't be too hard to convince yourself. So you're also told when you were a child that a function is continuous if you can draw the graph and not lift up the pencil, which I better not see on the exam if I ask you about continuity. But anyways, for the sake of this conversation, let's take that as the intuition. So you can convince yourself that this function is continuous over here, though, because if I'm getting close to, let's say this is minus 1, then the function is getting close to 2, and the value of the function at minus 1 is 2. So the function is getting close to the value of the function at minus 1. And the same for if I'm looking at c equals 1. So you should be able to convince yourself that this function is continuous at every point other than at the origin 0. So a natural question to ask is, let f be a function, let's say, defined on the whole real number line. Does there exist a point in R such that f is continuous at this point c? For this example over here, we were able to, any point other than 0, the function's continuous there. So the natural question is, let's say I take an arbitrary function. Does it have to have a point where it's continuous? And next time, we'll see that that answer is no. We'll give an example that's, I think, due to Dirichlet because it's named after him, but naming doesn't necessarily mean anything in math. Green's Theorem is named after Green, but he didn't prove it, so maybe it was due to somebody else. And we'll use a similar characterization of continuity that's kind of analogous to this first statement we had for limits. And we'll do that next time.