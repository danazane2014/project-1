 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So today, we'll actually just do a brief chapter on Bayesian statistics. And there's entire courses on Bayesian statistics. There's entire books on Bayesian statistics. There's entire careers on Bayesian statistics. So admittedly, I'm not going to be able to do it justice and tell you all the interesting things that are happening in Bayesian statistics. But I think it's important, as a statistician, to know what it is, how it works, because it's actually a weapon of choice for many practitioners. And because it allows them to incorporate their knowledge about a problem in a fairly systematic manner. So if you look at, say, the Bayesian statistics literature, it's huge. And so here, I give you a range of what you can expect to see in Bayesian statistics from your second edition of a traditional book, something that involves computation, some things that involve rethinking. And there's a lot of Bayesian thinking. There's a lot of things that talk about philosophy of thinking Bayesian. This book, for example, seems to be one of them. This book is definitely one of them. This one represents a broad literature on Bayesian statistics for applications, for example, in social sciences. But even in large-scale machine learning, there's a lot of Bayesian statistics happening, particularly using something called Bayesian non-parametrics or hierarchical Bayesian modeling. So we do have some experts at MIT in the CSAIL. Tamara Broderick, for example, is a person who does quite a bit of interesting work on Bayesian non-parametrics. And if that's something you want to know more about, I urge you to go and talk to her. So before we go in those more advanced things, we need to start with, what is the Bayesian approach? What do Bayesians do? And how is it different from what we've been doing so far? So to understand the difference between Bayesians and what we've been doing so far is we need to first put a name on what we've been doing so far. It's called frequentist statistics. So it's usually Bayesian versus frequentist statistics. I mean, by versus, I don't mean that there's naturally an opposition to them. Actually, often you will see the same method that comes out of both approaches. So let's see how we did it. The first thing, we had data. We observed some data. And we assumed that this data was generated randomly. The reason we did that is that because this would allow us to leverage tools from probability. So let's say by nature, measurements, you do a survey, you get some data. Then we made some assumptions on the data generating process. For example, we assumed they were IID. That was one of the recurring things. Sometimes we assume it was Gaussian if we wanted to use, say, t-test. Maybe we did some non-parametric statistics. We assume it was a smooth function or maybe linear regression function. So those are our modeling. And this was basically a way to say, well, we're not going to allow for any distributions for the data that we have, but maybe a small set of distribution that index by some small parameters, for example. Or at least remove some of the possibilities. Otherwise, there's nothing we can learn. And so for example, this was associated to some parameter of interest, say, data or beta in the regression model. All right, then we had this unknown problem and this unknown thing, a known parameter. And we wanted to find it. We wanted to either estimate it or test it or maybe find a confidence interval for the subject. So so far, I should not have said anything that's new. But this last sentence is actually what's going to be different from the Bayesian part. In particular, this unknown best fix thing is what's going to be changing. So in the Bayesian approach, we still assume that we observe some random data. But the generating process is slightly different. It's sort of a two-layer process. And there's one process that generates the parameter and then one process that, given this parameter, generates the data. So what the first layer does, I mean, nobody really believes that there's some random process that's happening about generating what is going to be the true expected number of people who turn their head to the right. When they kiss. But this is actually going to be something that brings us some easiness for us to incorporate what we call prior belief. So we'll see an example in a second. But often, you actually have prior belief of what this parameter should be. When we did, say, least squares, we looked over all of the vectors in all of r to the p, including the ones that have coefficients equal to 50 million. And so those are things that maybe we might be able to rule out. And maybe we might be able to rule out at a much smaller scale. For example, well, I mean, I don't know. I'm not an expert on turning your head to the right or to the left. But maybe you can rule out the fact that almost everybody's turning their head in the same direction or almost everybody's turning their head to another direction. So we have this prior belief. And this prior belief is going to play, say, hopefully less and less important role as we collect more and more data. But if we have a smaller amount of data, we might want to be able to use this information rather than just shooting in the dark. And so the idea is to have this prior belief. And then we want to update this prior belief into what's called a posterior belief after we've seen some data. Maybe I believe that there's something that should be in some range. But maybe after I see data, maybe it's comforting me in my belief. So I'm actually having maybe a belief that's more. So a belief encompasses basically what you think and how strongly you think about it. That's what I call belief. So for example, if I have a belief about some parameter theta, maybe my belief is telling me where theta should be and how strongly I believe in it in the sense that I have a very narrow region where theta could be. And so the posterior belief says, well, you see some data. And maybe you're more confident or less confident about what you've seen. Maybe you've shifted your belief a little bit. And so that's what we're going to try to see and how to do this in a principled manner. So of course, to understand this better, there's nothing better than an example. So let's talk about another stupid statistical question, which is let's try to understand p. Of course, I'm not going to talk about politics from now on. So let's talk about p, the proportion of women in the population. OK, and so what I could do is to collect some data, x1, xn, and assume that they're Bernoulli with some parameter p unknown. So p is in 0, 1. OK, let's assume that those guys are i.d. So this is just an indicator for each of my collected data, whether the person I randomly sample is a woman, I get a 1. And if it's a man, I get a 0. OK, and so now the question is I sample these people randomly. I denote their gender. And the frequentist approach was just saying, OK, let's just estimate p hat being xn bar. And then we could do some tests. So here there's a test. I want to test maybe if p is equal to 0.5 or not. That sounds like a pretty reasonable thing to test. But we want to also maybe estimate p. But here, this is a case where we definitely have prior belief of what p should be. We are pretty confident that p is not going to be 0.7. We actually believe that p should be extremely close to 1 half. But maybe not exactly. Maybe, I don't know, maybe this population is not the population in the world. But maybe this is the population of, say, some college. And we want to understand if this college has half women or not. So maybe we know it's going to be close to 1 half. But maybe we're not quite sure. And so we're going to want to integrate that knowledge. So I could integrate it in a blunt manner by saying discard the data and say that p is equal to 1 half. But maybe that's just a little too much. So how do I do this trade-off between adding the data and combining it with this prior knowledge? In many ways, in many instances, essentially what's going to happen is this 1 half is going to act like one new observation, essentially. So if you have five observations, this is just the sixth observation, which will play a role. If you have a million observations, you're going to have a million and one. And it's not going to play so much of a role. That's basically how it goes. That's basically how it goes, but definitely not always. Because we'll see that if I take my prior to be a point minus that 1 half here, it's basically as if I was discarding my data. So essentially, there's also your ability to encompass how strongly you believe in this prior. And if you believe infinitely more in the prior than you believe in the data you collected, then of course, it's not going to act like one more observation. So the Bayesian approach is a tool to, one, include mathematically our prior belief into statistical procedures. So maybe I have this prior knowledge, but if I'm a medical doctor, it's not clear to me how I'm going to turn this into some principal way of building estimators. And of course, the second goal is going to be to update this prior belief into a posterior belief by using the data. So how do we do this? And at some point, I sort of suggested that there's two layers. One is where you draw the parameter at random. And two, once you have the parameter, condition this parameter, you draw your data. Nobody believes this actually is happening, that nature is just rolling dice for us and choosing parameters at random. But what's happening is that this idea that the parameter comes from some random distribution actually captures very well this idea that how you would encompass your prior. How would you say my belief is as follows? Well, here's an example about p. I'm 90% sure that p is between 0.4 and 0.6. And I'm 95% sure that p is between 0.3 and 0.8. So essentially, I have this possible value of p. And what I know is that there's 90% here between, what did I say, 0.4 and 0.6. And then I have 0.3 and 0.8. And I know that I'm 95% sure that I'm in here. And this, if you remember, this sort of looks like the kind of pictures that I made when I had some Gaussian, for example. And I said, oh, here we have 90% of the observations. And here we have 95% of the observations. So in a way, if I were able to tell you all those ranges for all possible values, then I would essentially describe a probability distribution for p. And what I'm essentially saying is that p is going to have this kind of shape. So of course, if I tell you only twice this information, that there's 90% I'm here and I'm here, between here and here, and 95% I'm between here and here, then there's many ways I can accomplish that. I could have something that looks like this, maybe. I could be really, I mean, it could be like this. I mean, there's many ways I can have this. Some of them are definitely going to be mathematically more convenient than others. And hopefully, we're going to have things that I can parameterize very well. Because if I tell you this is this guy, then there's basically one, two, three, four, five, six, seven parameters. So I probably don't want something that has seven parameters. But maybe I can say, oh, it's a Gaussian. And all I have to do is to tell you where it's centered and what the standard deviation is. OK, so the idea of using this two-layer thing, where we think of the parameter p as being drawn from some distribution, is really just a way for us to capture this information, our prior belief being, well, there's this percentage of chances that it's there. But the percentage of chance, I'm deliberately not using probability here. So it's really a way to get close to this. All right, so that's what I said. The true parameter is not random. But the Bayesian approach does as if it was random. And then just spits out a procedure out of this thought process somehow, this thought experiment. So when you practice Bayesian statistics a lot, you start getting automatisms. So you start getting some things that you do without really thinking about it. Just like when you're a statistician, the first thing you do is, can I think of this data as being Gaussian, for example? When you're Bayesian, you're thinking about, OK, I have a set of parameters. So here, I can describe my parameter as being theta in general, in some big space parameter theta. But what spaces did we encounter? Well, we encountered the real line. We encountered the interval 0, 1 for Bernoulli's. And we encountered maybe some positive real line for exponential distributions, et cetera. And so what I'm going to need to do, if I want to put some prior on those spaces, I'm going to have to have a usual set of tools for this guy, usual set of tools for this guy, usual set of tools for this guy. And by usual set of tools, I mean I'm going to have to have a family of distributions that support it on this. So in particular, this is the space in which my parameter that I usually denote by p for Bernoulli lives. And so what I need is to find a distribution on the interval 0, 1, just like this guy. The problem with the Gaussian is that it's not on the interval 0, 1. It's going to spill out in the end, and it's not going to be something that works for me. And so the question is, I need to think about distributions that are probably continuous. Why would I restrict myself to discrete distributions that are actually convenient? And for Bernoulli, one that's actually basically the main tool that everybody's using is the so-called beta distribution. So the beta distribution has two parameters. So x follows a beta with parameters, say, a and b, if it has a density f of x is equal to x to the a minus 1, 1 minus x to the b minus 1, if x is in the interval 0, 1, and 0 for all other x's. So why is that a good thing? Well, it's a density that's on the interval 0, 1 for sure. But now I have these two parameters, and the set of shapes that I can get by tweaking those two parameters is incredible. I mean, it's going to be a unimodal distribution. It's still fairly nice. It's not going to be something that goes like this and this, because if you think about this, what would it mean if your prior distribution on the interval 0, 1 had this shape? It would mean that maybe you think that p is here, or maybe you think that p is here, or maybe you think that p is here, which essentially means that you think that p can come maybe from three different phenomena. And there's other models that are called mixtures for that that directly account for the fact that maybe there are several phenomena that are aggregated in your data set. But if you think that your data set is sort of pure and that everything comes from the same phenomenon, you want something that looks like maybe like this, or maybe it looks like this, or maybe it's sort of symmetric. You want to get all this stuff, right? Maybe you want something that says, well, if I'm talking about p being the probability of the proportion of women in the whole world, you want something that's probably really spiked around 1 half, almost the point mass, because you know, I mean, OK, let's agree that 0.5 is the actual number. So you want something maybe that says, OK, maybe I'm wrong, but I'm sure I'm not going to be really that way off. So you want something that's really pointy. But if it's something you've never checked, and again, I cannot make references at this point, but something where you might have some uncertainty, then that should be around 1 half. Maybe you want something that's like a little more allows you to say, well, I think there's more around 1 half, but there are still some fluctuations that are possible. And in particular here, I talk about p where the two parameters a and b are actually the same. I call them a. One is called scale, the other one's called shape. Oh, by the way, sorry, this is not a density, so it actually has to be normalized. When you integrate this guy, it's going to be some function that depends on a and b, actually depends on this function through the beta function, which is this combination of gamma function. So that's why it's called beta distribution. But that's the definition of the beta function when you integrate this thing anyway. So you just have to normalize it. It's just a number that depends on a and b. So here, if you take a equal to b, you have something that essentially is symmetric around 1 half. Because what does it look like? Well, it's something. So my density f of x is going to be what? It's going to be my constant times x times 1 minus x to the a minus 1. And this function, x times 1 minus x, looks like this. We've drawn it before. That was something that showed up as being the variance of my Bernoulli. So we know it's something that takes its maximum at 1 half. And now I'm just taking the power of this guy. So I'm really just distorting this thing into some fairly symmetric manner. So this distribution that we actually take for p, so here I assume that p, the parameter, notice that this is kind of weird. First of all, this is probably the first time in this entire course that something has a distribution when it's actually a lowercase letter. That's something you have to deal with, because we've been using lowercase letters for parameters. And now we want them to have a distribution. So that's what's going to happen. And this is called the prior distribution. So really, I should write something like f of p is equal to a constant times p1 minus p to the a minus 1. Well, no, actually I should not, because then it's confusing. So let me not do this. One thing in terms of notation that I'm going to write, I'm going to write, when I have a constant here and I don't want to make it explicit, and we'll see in a second why I don't need to make it explicit, I'm going to write this as f of x is proportional to x1 minus x to the a minus 1. So that's just to say equal to some constant that does not depend on x times this thing. OK? So if we continue with our experiment, now if p, so that's the experiment where I'm drawing this data, x1 to xn, which is Bernoulli p. If p has some distribution, it's not clear what it means to have a Bernoulli with some random parameter. So what I'm going to do is then I'm going to first draw my p. Let's say I get a number, 0.52. And then I'm going to draw my data conditionally on p. So here comes the first and last flowchart of this class. So I'm going to first, all right, so nature first draws p. So p follows, say, some beta aa. Then I condition on p. And then I draw x1, xn that are iid Bernoulli p. Everybody understand the process of generating this data? So you first draw a parameter. And then you just flip those independent bias coins with this particular p. So there's this layered thing. So now, conditionally on p, right? So here, I have this prior about p, which was the thing. So this is just the thought process again, right? It's not anything that actually happens in practice. This is my way of thinking about how the data was generated. And from this, I'm going to try to come up with some procedure. Just like if your estimator is the average of the data, you don't have to understand probability to say that my estimator is the average of the data, right? I mean, anyone outside this room understand that the average is a good estimator for some average behavior. And they don't need to think of the data as being a random variable, et cetera. So same thing, basically. Now, we will see. I mean, actually, we won't. But in this case, well, we will. In this case, you can see that essentially the posterior distribution is still a beta, all right? So what it means is that I had this thing. Then I observed my data. And then I continue. And here I'm going to update my prior into some posterior distribution, pi. And here, this guy is actually also a beta, all right? So pi now, p, my posterior distribution on p, is also a beta distribution with the parameters that are on this slide. And I don't have space to reproduce them. So I start the beginning of this flow chart as having p, which is a prior. I'm going to get some observations. And then I'm going to update what my posterior is, OK? So this posterior is basically something that's in Bayesian statistics was beautiful, is as soon as you have the distribution, it's essentially capturing all the information about the data that you want for p. And it's not just a point, right? It's not just an average. It's actually an entire distribution for the possible values of theta. And it's not the same thing as saying, well, if theta hat is equal to xn bar in the Gaussian case, I know that this is some mean, mu, and then maybe it has variance sigma square over n. That's not what I mean by this is my posterior distribution, right? This is not what I mean. This is going to come from these guys, right? The Gaussian thing and the central limit theorem. But what I mean is this guy. And this came exclusively from the prior distribution. If I had another prior, I would not necessarily have a beta distribution on the output. So when I have the same family of distributions at the beginning and at the end of this flowchart, I say that beta is a conjugate prior, meaning I put in beta as a prior, and I get betas at posteriors. And that's why betas are so popular. Conjugate priors are really nice, because you know that whatever you put in, what you're going to get in the end is a beta. So all you have to think about is the parameters. You don't have to check again what the posterior is going to look like, what the PDF of this guy is going to be. You don't have to think about it. You just have to check what the parameters are. And there's families of conjugate priors. Gaussian gives Gaussian, for example. There's a bunch of them. And this is what drives people into using specific priors, as opposed to others. It has nice mathematical properties. Nobody believes that the p distribution is p is really distributed according to beta, but it's flexible enough and super convenient mathematically. All right, so now let's see for one second before we actually go any further. What I did, so a and b, I didn't mention it. a and b are both, in here, a and b are positive numbers. They can be anything positive. So here what I did is that I updated a into a plus the sum of my data, and b into b plus n minus the sum of my data. So that's essentially a becomes a plus the number of ones, and b becomes b. Well, that's only when I have a and a. So the first parameters become itself plus the number of ones, and the second one becomes itself plus the number of zeros. And so just as a sanity check, what does this mean? If a goes to 0, what is the beta when a goes to 0? We can actually read this from here. So we had a, sorry, actually, let's take a goes to, no, actually, sorry, let's just do this. OK, let's not do this now. I'll do it when we talk about non-informative prior, because it's a little too messy. OK, so how do we do this? How did I get this posterior distribution given the prior? How do I update this? Well, this is called Bayesian statistics. And you've heard this word Bayes before. And the way you've heard it is in the Bayes formula, right? What was the Bayes formula? The Bayes formula was telling you that the probability of A given B was equal to something that depended on the probability of B given A, right? That's what it was. And I mean, you can actually either remember the formula, or you can remember the definition. And this is what P of A and B divided by P of B. So this is P of B given A times P of A divided by P of B. Right? That's what Bayes formula is telling you. Agree? So now, what I want is to have something that's telling me how this is going to work, OK? So what is going to play the role of those events A and B? Well, one is going to be what is the distribution? So this is going to be the distribution of my parameter theta, given that I see the data. And this is going to tell me what is the distribution of the data, given that I know what my parameter theta is. But that part, if this is data and this is the parameter theta, this is what we've been doing all along. The distribution of the data, given the parameter here, was n iid Bernoulli p. I know that. I know exactly what their joint probability mass function is. Then that was what? So we said that this is going to be my data, and this is going to be my parameter. OK? So that means that this is the probability of my data, given the parameter. This is the probability, given the parameter. This is the probability of the parameter. What is this? What did we call this? This is the prior. It's just the distribution of my parameter. Now, what is this? Well, this is just the distribution of the data itself, all right? So this is essentially the distribution of this if this was indeed not conditioned on p. So if I don't condition on p, this data is going to be a bunch of iid Bernoulli with some parameter, but the parameter is random. So for different realization of this data set, I'm going to get different parameters for the Bernoulli. And so that leads to some sort of convolution. I mean, it's not really a convolution in this case, but it's like some sort of composition of distributions. I have the distribution that comes, the randomness that comes from here, and then the randomness that's come from realizing the Bernoulli. So that's just the marginal distribution. And it actually might be painful to understand what this is. I mean, in a way, it's sort of a mixture, and it's not super nice. But we'll see that this actually won't matter for us. This is going to be some number. It's going to be there, but it won't matter for us what it is, because it actually does not depend on the parameter, and that's all that matters to us. OK, so let's put some names on those things. I mean, this was very informal. So let's put some actual names on what we want to call prior. So what is the formal definition of a prior? What is the formal definition of a posterior? And what are the rules to update it? So I'm going to have my data, which is going to be x1, xn. And so let's say they're iid, but they don't actually have to. And so I'm going to have given theta. And when I say given, it's either given like I did in the first part of this course in all previous chapters, or conditionally on. So if you're thinking like a Bayesian, what I really mean is conditionally on this random parameter. So it's like as if it was a fixed number. Then they're going to have distribution, x1, xn, is going to have some distribution. Let's assume for now it's a PDF, Pn of x1, xn. And I'm going to write theta like this. So for example, what is this? Well, so let's say this is a PDF. It could be a PMF. Everything I say, I'm going to think of them as being PDFs. I'm going to combine PDFs with PDFs, but I could combine PDFs with PMFs, PMF with PDFs, or PMF with PMFs. So everywhere you see a d, it could be an m. All right, so now I have those things. So what does that mean? So here's some example, x1, xn are iid and theta 1. So now I know exactly what the joint PDF of this thing is. So it means that Pn of x1, xn given theta is equal to what? Well, it's like 1 over sigma root, sorry, 1 root 2 pi to the power n e to the minus sum from i equal 1 to n of xi minus theta squared divided by 2. So that's just the joint distribution of n iid and theta 1 random variables. So that's my Pn given theta. Now, this is what we denoted by sort of like f sub theta before. We had the subscript before, but now we just put a bar in theta because we want to remember that this is actually conditioned on theta. But this is just notation. You should just think of this as being just the usual thing that you get from some statistical model. So now that's going to be Pn. And here I'm going to assume that theta is, why do I put pi here? So if theta has prior distribution pi, so for example, so think of it as either PDF or PMF again. So for example, pi of theta was what? Well, it was some constant times theta to the a minus 1, 1 minus theta to the a minus 1. So it has some prior distribution, and that's another PMF. So now I'm given the distribution of my x's given theta. I'm given the distribution of my theta, so I'm given this guy. That's this guy. I'm given that guy, which is my pi. So that's my Pn of x1, xn given theta. That's my pi of theta. And then I have here just, this is what? Well, this is just the integral of Pn x1, xn times pi of theta d theta, overall possible sets of theta. That's just when I integrate out my theta, or I compute, say, the marginal distribution, I get this by integrating. That's just basic probability, conditional probabilities. Then if I had the PMF, I would just sum over the values of thetas. So now what I want is to find what's called, so that's the prior distribution, and I want to find the posterior distribution. So it's pi of theta given x1, xn. And so if I use Bayes' rule, I know that this is Pn of x1, xn given theta times pi of theta. And then it's divided by the distribution of those guys, which I will write as integral over theta of Pn x1, xn given theta times pi of theta d theta. Everybody's with me still? So if you're not comfortable with this, it means that you probably need to go read your couple pages on conditional densities and conditional PMFs from your probability class. There's really not much there. It's just a matter of being able to define those quantities. F density of x given y, this is just what's called a conditional density. You need to understand what this object is and how it relates to the joint distribution of x and y, or maybe the distribution of x or the distribution of y. But it's the same rules. I mean, one way to actually remember this is this is exactly the same rules as this. When you see a bar, it's the same thing as the probability of this and this guy. So for densities, it's just a comma divided by the second guy, the probability of the second guy. That's it. So if you remember this, you can just do some pattern matching and see what I just wrote here. So now I can compute every single one of these guys. This is something I get from my modeling. So I did not write this. It's not written in the slides. But I give a name to this guy that was my prior distribution. And that was my posterior distribution. In the chapter 3, maybe, what did we call this guy? Yeah, well, the one that does not have a name and that's in a box, this guy. How did we call it? It's like the joint distribution of the xi's. It is the joint distribution of the xi's. And we gave it a name. It's the likelihood. It's the likelihood, right? This is exactly the likelihood. This was the likelihood of theta. And this is something that's very important to remember. And that really reminds you that these things are really not that different, maximum likelihood estimation, Bayesian estimation, because your posterior is really just your likelihood times something that's just putting some weights on the thetas, depending on where you think theta should be. So if I had, say, a maximum likelihood estimator and my likelihood in theta looked like this, but my prior in theta looked like this, I said, oh, I really want thetas that are like this. So what's going to happen is that I'm going to turn this into some posterior that looks like this. So I'm just really weighting this posterior. This is a constant that does not depend on theta, right? Agreed? I integrated over theta, so theta's gone. So forget about this guy. I have basically that the posterior distribution up to scaling, because it has to be a probability density and not just any function that's positive, is the product of this guy. It's a weighted version of my likelihood. That's all it is. I'm just weighting the likelihood using my prior belief on theta. And so given this guy, a natural estimator, if you follow the maximum likelihood principle, would be the maximum of this posterior. Agreed? That would basically be doing exactly what maximum likelihood estimation is telling you. So it turns out that you can. It's called maximum a posteriori, and I won't talk much about this or map. So that's maximum a posteriori. So it's just the theta hat is the argmax of pi theta given x1, xn. It sounds like it's OK. I give you a density, and you say, OK, I have a density for all values of my parameters. You're asking me to summarize it into one number. I'm just going to take the most likely number of those guys. But you could summarize it otherwise. You could take the average. You could take the median. You could take a bunch of numbers. And the beauty of Bayesian statistics is that you don't have to take any number in particular. You have an entire posterior distribution. This is not only telling you where theta is, but it's actually telling you the difference if you actually give as something. It gives you the posterior, right? So now let's say the theta is a p between 0 and 1. If my posterior distribution looks like this, or if my posterior distribution looked like this, then those two guys have, one, the same mode. This is the same value. And they're symmetric, so they also have the same mean. So these two posterior distributions give me the same summary into one number. However, clearly, one is much more confident than the other one. So I might as well just spit that as a solution. Some people can do even better. People actually do things such as drawing a random number from this distribution, so this is my number. Well, that's kind of dangerous, but you can imagine you could do this, right? All right, so this is what works. That's what we went through. So here, as you notice, I don't care so much about this part here, because it does not depend on theta. I know that given the product of those two things, this thing is only the constant that I need to divide so that when I integrate this thing over theta, it integrates to 1, because this has to be a probability density on theta. So I can write this and just forget about that part, and that's what's written on the top of this slide. Just this notation, this sort of weird alpha or, I don't know, infinity sine cropped to the right, whatever you want to call this, this thing is actually just really emphasizing the fact that I don't care. I write it because I can, and you know what it is. But you don't actually have to. Well, OK, in some instances, you have to compute the integral. In some instances, you don't have to compute the integral. And a lot of Bayesian computation is about saying, OK, it's actually really hard to compute this integral, so I'd rather not doing it. So let me try to find some methods that allow me to sample from the posterior distribution without having to compute this, and that's what's called Monte Carlo Markov chains or MCMC, and that's exactly what they're doing. They're just using only ratios of things like that for different thetas, which means that if you take ratios, the normalizing constant is gone, and you don't need to find this integral. So we won't go into those details at all. That would be the purpose of an entire course on Bayesian inference. Actually, even Bayesian computations would be an entire course on its own. There's some very interesting things that are going on there, the interface of stats and computation. All right, so let's go back to our example and see if we can actually compute any of those things, because it's very nice to give you some data, some formulas, but let's see if we can actually do it. All right, and in particular, can I actually recover this claim that the posterior associated to a beta prior with Bernoulli likelihood is actually giving me a beta again? All right, so what was my prior? Well, it was beta. So p was following a beta aa, which means that p, the density, so that was pi of theta. Well, I'm going to write it as pi of p was proportional to p to the a minus 1 times 1 minus p to the a minus 1. So that's the first ingredient I need to compute my posterior. I really need only two if I want it up to constant. The second one was pn. Well, we've computed that many times, and we had even a nice compact way of writing it, which was that pn of x1, xn given a parameter p, so the joint density of my data given p, that's my likelihood, the likelihood of p, was what? Well, it was p to the sum of the xi's 1 minus p to the n minus sum of the xi's. Anybody wants me to parse this more, or do you remember seeing that from maximum likelihood estimation? Yeah? So when you condition on the random variable, you really just treat that random variable as something that's going to be there. Yeah, that's what conditioning does. OK? Yeah? On the previous slide, for the bottom there, it says d pi of p. Does it seem to be like d to d pi of p, or is it? So d pi of t is a measure theoretic notation, which I use without thinking, and I should not, because I can see it upsets you. d pi of t is just a natural way to say that I integrate against whatever I'm given for the prior of theta. In particular, if theta is just a mix of a PDF and a point mass, maybe I say that my p takes value 0.5 with probability 0.5, and then is uniform on the interval with probability 0.5. So for this, I neither have a PDF nor a PMF, but I can still talk about integrating with respect to this. It's going to look like if I take a function f of t, d pi of t is going to be 1 half of f of 1 half. That's the point mass with probability 1 half at 1 half, plus 1 half of the integral between 0 and 1 of f of t, dt. So this is just a notation, which is actually, funnily enough, is interchangeable with pi of dt. But if you have a density, it's really just the density pi of t, dt, if pi is really a density. But that's when pi is a measure and not a density. So everybody else, forget about this. I mean, this is not something you should really worry about at this point. This is more graduate level probability classes. But yeah, it's called measure theory, and that's when you think of pi as being a measure in an abstract fashion. You don't have to worry whether it's a density or not, or whether it has a density even. So everybody's OK with this? Yeah. All right, so now I need to compute my posterior. And as I said, my posterior is really just the product of the likelihood weighted by the prior. So hopefully, by this stage of your education, you can multiply two functions. So what's happening is if I multiply this guy with this guy, well, p gets this guy to the power of this guy plus this guy. And then 1 minus p gets to the power n minus sum of xi's. So this is always from i equal 1 to n, and then plus a minus 1 as well. And this is up to constant, because I still need to solve this. And I could try to do it, but I really don't have to, because I know that if my density has this form, then it's a beta distribution. And then I can just go on Wikipedia and see what should be the normalization factor. But I know it's going to be a beta distribution. So this is actually the beta with parameter. So this is really my beta with parameter sum of xi i equal 1 to n plus a minus 1. And then the second parameter is n minus sum of the xi's plus a minus 1. I just wrote what was here. Oh, what happened to my 1? Oh, no, sorry, sorry, sorry. Beta has the power minus 1, right? So that's the parameter of the beta. And this is the parameter of the beta. So beta, well, I don't think it's anywhere. That beta is over there, right? So I just replaced a by what I see. a is just becoming this guy plus this guy and this guy plus this guy. Everybody's comfortable with this computation? All right, so we just agreed that beta priors for Bernoulli observations are certainly convenient, right? And because they're just conjugate, and we know that's what's going to come out in the end, that's going to be a beta as well. So I mean, I just claim it was convenient. It was certainly convenient to compute this, right? I mean, there was certainly some compatibility when I had to multiply this function by that function. And you can imagine that things could go much more wrong than just having p to some power and p to some power, 1 minus p to some power, 1 minus p to some other power. Things were nice. Now, this is nice, but I can also question the following things. Why beta, for one? I mean, the beta tells me something, but that's convenient. But then how do I pick a? I know that a should definitely capture the fact that where I want to have my p most likely located, but it also actually captures the variance of my beta. And so choosing different a's is going to have different functions. If I have a and b, if I started with the beta with parameter here, I started with a b here, I would just pick up the b here. Agreed? And that would just be asymmetric, but they're going to capture mean and variance of this thing. And so how do I pick those guys? I mean, if I'm a doctor and you're asking me, what do you think the chances of this drug working on this kind of patients is? And I have to say, to spit out the parameters of a beta for you, it might be a bit of a complicated thing to do. So how do you do this, especially for problems? So by now, people have actually mastered the art of coming up with how to formulate those numbers. But in new problems that come up, how do you do this? What happens if you want to use Bayesian methods, but you actually do not know what you expect to see? Maybe this is the first time you've, I mean, to be fair, before we started this class, I hope all of you had no idea whether people tend to bend their head to the right or to the left before kissing. Because if you did, well, you have too much time on your hand and I should double your homework. And so in this case, you have to sort of, maybe you still want to use the Bayesian machinery. Maybe you just want to do something nice. It's nice, right? I mean, it worked out pretty well. And so what if you want to do? Well, you actually want to use some priors that have to carry no information, that basically do not prefer any theta to another theta. Now, you could read this slide, or you could look at this formula. We just said that this pi here was just here to weigh some thetas more than others, depending on our prior belief. If our prior belief does not want to put any preference towards some thetas than to others, what do I do? AUDIENCE MEMBER 2 Yeah, remove it. And the way to remove something we multiply by is just replace it by 1. That's really what we're doing. So if this was a constant, not depending on theta, then that would mean that we're not preferring any theta. And we're looking sort of at the likelihood, but not as a function that we're trying to maximize, but as a function that we normalize in such a way that it's actually a distribution. So if I have pi, which is not here, this is really just taking the likelihood, which is a positive function, may not integrate to 1, so I normalize it so that it integrates to 1. And then I just say, well, this is my posterior distribution. Now, I could just maximize this thing and spit out my maximum likelihood estimator, but now I can also integrate and find what the expectation of this guy is. I can find what the median of this guy is. I can sample data from this guy. I can understand what the variance of this guy is, which is something we did not do when we just did maximum likelihood estimation, because given a function, all we cared about was the argmax of this function. These priors are called uninformative. So this is just replacing this number by 1. And if I have a, or by a constant, because it still has to be a density. So if I have something which is, if I have a bounded set, I'm just looking for the uniform distribution on this bounded set, the one that puts constant 1 over the size of this thing. But if I have an unbounded set, what is the density that takes a constant value on the entire real line, for example? What is this density? Doesn't exist. I mean, it just doesn't exist. The way you can think of it is a Gaussian with the variance going to infinity, maybe, or something like this. But you can think of it in many ways. You can think of the limit of the uniform between minus t and t with t going to infinity. But this thing is actually 0. I mean, there's nothing there. And so you can actually still talk about this. You could always talk about this thing where you think of this guy as being a constant, remove this thing from this equation, and just say, well, my posterior is just the likelihood divided by the integral of the likelihood over theta. And if theta is the entire real line, so be it. As long as this integral converges, you can still talk about this stuff. And so this is what's called an improper prior. An improper prior is just a non-negative function defined on theta. But it does not have to integrate neither to 1 nor to anything. It does not have to. If I integrate the function equal to 1 on the entire real line, what do I get? Infinity. I mean, it's not a proper integral. And so it's not a proper prior. And it's called an improper prior. And those improper priors are usually what you see when you start to want non-informative priors on infinite set thetas. I mean, that's just the nature of it. It's just you should think of it as being the uniform distribution on some infinite set if that thing were to exist. So let's see some examples about non-informative priors. So if I'm on the interval 0, 1, this is a finite set. So I can talk about the uniform prior on the interval 0, 1 for a parameter p of a Bernoulli. And so if I want to talk about this, then it means that my prior is p follows some uniform on the interval 0, 1. So that means that the density is f of x is 1 if x is in 0, 1 and 0 otherwise. There's actually not even a normalization. This thing integrates to 1. And so now, if I look at my likelihood, it's still the same thing. So my posterior becomes theta x1, xn. So that's my posterior. I don't write the likelihood again, because we still have it here. Well, we don't have it there anymore. Is it here? Or did I just erase it? Yeah, the likelihood is given here. So copy, paste over there. And so the posterior is just this thing times 1. So you will see it in a second. So it's p times the sum to the power sum of the xi's, 1 minus p to the power n minus sum of the xi's. And then it's multiplied by 1 and then divided by this integral between 0 and 1 of p sum of the xi's, 1 minus p n minus sum of the xi's dp, which does not depend on p. And I really don't care what this thing actually is. So now, sorry, that's for a prior posterior of p. And now I can see, well, what is this? Well, it's actually just the beta with parameters this guy plus 1 and this guy plus 1. So I didn't tell you what the expectation of a beta was. We don't know what the expectation of a beta is. Agreed? I mean, if I wanted to find, say, the expectation of this thing that would be some good estimator, we know that the maximum of this guy, what is the maximum of this thing? Oh, it's just this thing, right? I mean, it's the average of the xi's, right? That's just the maximum likelihood estimator for Bernoulli. We know it's the average. Do you think if I take the expectation of this thing, I'm going to get the average? So actually, I'm not going to get the average. I'm going to get this guy plus this guy divided by n plus 1. So I'm going to do as if I had, oh, sorry. OK, let me not say it like that. Let's look at what this thing is doing. It's looking at the number of 0's, the number of 1's, and it's adding 1. And this guy is looking at the number of 0's, and it's adding 1. Why is it adding this 1? What's going on here? Well, what would happen if I had? So this actually is going to matter mostly when the number of 1's is actually 0 or the number of 0's is 0. Because what it does is just pushes the 0 from non-zero. And why is that something that this Bayesian method actually does for you automatically is because when we put this non-informative prior on p, which was uniform on the interval 0, 1, in particular, we know that the probability that p is equal to 0 is 0, and the probability that p is equal to 1 is 0. And so the problem is that essentially, if I did not add this 1 with some positive probability, I would be allowed to spit out something that actually had p hat, which was equal to 0. In the case, if by chance, let's say I have n is equal to 3, and I get only 0, 0, 0, that could happen with probability 1 over p cubed, 1 over 1 minus p cubed. Then this thing is just going to not, that's not something that I want. And I'm actually using my prior. So my prior is not informative, but somehow it captures the fact that I don't want to believe that p is going to be either equal to 0 or 1. And so that's sort of taken care of here. So let's move away a little bit from the Bernoulli example, shall we? I mean, I think we've seen enough of it. And so let's talk about the Gaussian model. Let's say I want to do Gaussian inference in the, I want to do inference in a Gaussian model using Bayesian methods. So I'm going to actually look at, so say, so what I want is that xi, x1, xn, or say n 0, 1, iid, sorry, theta 1, iid conditionally on theta. So that means that pn of x1, xn given theta is equal to, well, exactly what I wrote before. So 1 square root 2 pi to the n exponential minus 1 half sum of xi minus theta squared. So that's just the joint distribution of my n Gaussians with mean theta. Another question is, what is the posterior distribution? Well, here I said, let's use the uninformative prior, which is an improper prior. It puts weight 1 on everyone. That's the so-called uniform on the entire real line. So that's certainly not a density. But I can still just use this. So all I need to do is to get this divided by normalizing this thing. So that's what I need to do. But if I look at this, so essentially I want to understand, so this is proportional to exponential minus 1 half sum from i equal 1 to n of xi minus theta squared. And now I want to see this thing as a density not on the xi's, but on theta. What I want is a density on theta. So it looks like I have chances of getting something that looks like a Gaussian. But if I really need to have a Gaussian, I would need to see minus 1 half. And then I would need to see theta minus something here, not just the sum of something minus theta. So I need to work a little bit more so I can see what this, to expand the square here. So this thing here is going to be equal to exponential minus 1 half sum from i equal 1 to n of xi squared minus 2 xi theta plus theta squared. OK? Now basically what I'm going to do is, everything remember is up to this little sign, right? So every time I see a term that does not depend on theta, I can just push it in there and just make it disappear. Agreed? OK, this term here, exponential minus 1 half sum of xi squared, does it depend on theta? No, so I'm just pushing it here. This guy, yes, and the other one, yes. So this is proportional to exponential sum of the xi, and then I'm going to pull out my theta. The minus 1 half cancel with the minus 2, and then I have minus 1 half sum from i equal 1 to n of theta squared, right? Agreed? So now what this thing looks like? Well, this looks very much like some theta minus something squared. This thing here is really just n over 2 times theta. So, sorry, times theta squared. So now what I need to do is to write this of the form theta minus something, let's call it mu, squared, maybe divided by 2 sigma squared, right? I want to turn this into that, maybe up to terms that do not depend on theta. That's what I'm going to try to do. So that's called completing the square, and that's some exercise you do. You've done it probably already in the homework, and that's something you do a lot when you do Bayesian statistics in particular. So let's do this. Well, what is going to be the leading term? Well, theta squared is going to be multiplied by this thing. So I'm going to pull out my n over 2, and then I'm going to write this as theta squared minus theta over 2, and then I'm going to write theta minus something squared. And this something is going to be 1 half of what I see in the cross product, right? Well, I need to actually pull this thing out. So let me write it like that first. So that's theta squared, and then I'm going to write it as minus 2 times 1 over n sum from i equal 1 to n of xi's times theta. That's exactly just a rewriting of what we had before, and that should look much more familiar. x squared minus a squared minus 2 blab a, and then I missed something. So this thing I'm going to be able to rewrite as theta minus xn bar squared, but then I need to remove the square of xn bar because it's not here. So I just complete the square, and then I actually really don't care what this thing actually was because it's going to go again in the little alpha sign over there. So this thing eventually is going to be proportional to exponential of minus n over 2 times theta minus xn bar squared. And so we know that if this is a density that's proportional to this guy, it has to be some n with mean xn bar and variance. Well, this is supposed to be 1 over sigma squared, this guy over here, this n. So that's really just 1 over n. So the posterior distribution is a Gaussian centered at the average of my observations and with a variance 1 over n. Everybody's with me? So just why I'm saying this, I mean, this was the output of some computation, but it sort of makes sense. It's really telling me that the more observations I have, the more concentrated this posterior is. Concentrated around what? Well, around this xn bar. So that looks like something we've sort of seen before, but it does not have the same meaning somehow. This is really just the posterior distribution, and it's not really, I mean, it sort of says, it's sort of a sanity check that I have this 1 over n when I have xn bar. But it's not the same thing as saying that the variance of xn bar was 1 over n like we had before. So as an exercise, well, you probably will have it, but I would recommend if you don't get it, just try pi of theta to be equal to some n mu 1. So here, the prior that we used was completely not informative. What happens if I take my prior to be some Gaussian, which is centered at mu, and it has the same variance as the other guys? So what's going to happen here is that we're going to put a weight, and everything that's away from mu is going to actually get less weight. And I want to know how I'm going to be updating this prior into a posterior. So everybody see what I'm saying here? So that means that pi of theta has the density proportional to exponential minus 1 half theta minus mu squared. So I need to multiply my posterior with this, and then see what it's actually going to be a Gaussian. This is also a conjugate prior. It's going to spit out another Gaussian. You're going to have to complete a square again, and just check what it's actually giving you. And so spoiler alert, it's going to look like you get an extra observation, which is actually equal to mu. So it's going to be the average of n plus 1 observations, the first n1's being x1 to xn, and the last one being mu. And it sort of makes sense. So that's actually a fairly simple exercise. But rather than going into more computation, this is something you can definitely do in the comfort of your room. I want to talk about other types of priors. So the first thing I said is, OK, there's this beta prior that I just pull out of my hat, and that was just convenient. Then there was this non-informative prior. It was convenient. It was non-informative. So if you don't know anything else, maybe that's what you want to do. The question is, are there any other priors that are sort of principled and generic, in the sense that the uninformative prior was generic? I mean, it was equal to 1. That's as generic as it gets. And so is there anything that's generic as well? Well, there's these priors that are called Jeffries priors. And Jeffries prior is a prior which is proportional to square root of the determinant of the Fisher information of theta. And so this is actually kind of a weird thing to do. It says, look at your model. Your model is going to have a Fisher information. Let's say it exists. And because we know it does not always exist. For example, in the multinomial model, we didn't have a Fisher information. And so the determinant of a matrix is somehow measuring the size of a matrix. And if you don't trust me, just think about the matrix being of size 1 by 1. Then the determinant is just the number that you have there. And so this is really something that looks like the Fisher information. I mean, it's just basically the amount of information is proportional to the amount of information that you have at a certain point. And so what my prior is saying, it's saying, well, I want to put more weights on those thetas that are going to just extract more information from the data. So you can actually compute those things. So in the first example, Jeffries prior is something that looks like this. I mean, in one dimension, Fisher information is essentially 1 over the variance. So that's just 1 over the square root of the variance, because I have the square root. And when I have the uniform, sorry, the Jeffries prior, when I have the Gaussian case, so this is the identity matrix that I would have in the Gaussian case. So the determinant of the identity is 1. So square root of 1 is 1. And so I would basically get 1. And that gives me my improper prior, my uninformative prior that I had. So the uninformative prior 1 is fine. I mean, clearly, all the thetas carry the same information in the Gaussian model. I mean, whether I translate it here or here, it's pretty clear none of them is actually better than the other. But clearly, for the Bernoulli case, the p's that are closer to the boundary carry more information. So I sort of like those guys, because they just carry more information. So what I do is that I take this function. So p1 minus p, remember, is something that looks like this on the interval 0, 1, 0, and 1. So this guy, 1 over square root of p1 minus p, is something that looks like this. Agreed? And so what it's doing is sort of wants to push towards the p's that actually carry more information. I mean, whether you want to bias your data that way or not is something you need to think about. I mean, when you put a prior on your data, on your parameter, you're sort of like biasing towards this idea, your data. And maybe that's maybe not such a good idea when you have some p that's actually close to 1 half, for example. You're actually saying, no, I don't want to see a p that's close to 1 half. Just make a decision one way or another, but just make a decision. So it's sort of forcing you to do that. And so Jeffrey's prior, so I'm running out of time, so I don't want to go into too much details. But we'll probably stop here, actually. So Jeffrey's prior, OK, sorry. What happened here? Yeah, so Jeffrey's priors have this very nice property is that they actually do not care about the parameterization of your space. So if you actually have p, and you suddenly decide that p is not the right parameter for Bivernally, but it's p squared, you could decide to parameterize this by p squared. Maybe your doctor is actually much more able to formulate some prior assumption on p squared rather than p. You never know. And so what happens is that Jeffrey's priors are invariant in this. And the reason is because, well, the information carried by p is the same as the information carried by p squared somehow. I mean, those are essentially the same. I mean, well, yeah, they're essentially the same thing. And so, I mean, you need to have a one-to-one map, right, where you basically, for each parameter, before you have another parameter. So let's call eta the new parameters. Then the PDF of the new prior indexed by eta this time is actually also Jeffrey's prior. But this time, the new Fisher information is not the Fisher information with respect to theta, but it's the Fisher information associated to the statistical model indexed by eta. So essentially, when you change Jeffrey's prior, when you change the parameterization of your model, you still get Jeffrey's prior for the new parameterization, which is, in a way, a desirable property. So Jeffrey's priors, just like non-informative priors, are priors you want to use when you want a systematic way without really thinking about what to pick for your model. OK, so well, OK, I'll finish this next time. And we'll talk about Bayesian confidence regions. We'll talk about Bayesian estimation. Once I have a posterior, what do I get? And basically, the only message is going to be that, well, you might want to integrate against the posterior. Find the posterior, the expectation of your posterior distribution. That's a good point estimator for theta. And then we'll just do a couple of computation. All right, so.