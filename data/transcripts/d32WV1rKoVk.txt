 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. OK, why don't I start? So I was hoping that we would have the next online assignment ready, but Julia 6, the new version, is slowing us down. And it'd probably be next time. But my lectures, of course, are, well, what I want to say perhaps is this isn't intended to be a course in numerical linear algebra. But I thought I couldn't let the whole semester go by without saying something about how to compute eigenvalues and singular values. Of course, you're going to call I or SVD or the equivalent in Python or Julia. But actually, the QR factorization that we spoke about, that we spoke entirely about last time is the key, unexpectedly, unexpectedly. You have a matrix A whose eigenvalues you want. So let's start with eigenvalues. And it might be a symmetric matrix. I'll stay with A rather than S because it doesn't have to be. But you get special always. You get something special if the matrix is symmetric. OK, so this method of computing eigenvalues, to me at least, and I think to many people, came like out of the blue a while ago, but not that long ago. And it worked very well. So here's the idea. It's called the QR method because you start by factoring your matrix into QR. So here's A. Can we call it A0? That's the matrix we start with whose eigenvalues we want. And I'll call these Q0 and R0. And you remember what that means, what's hiding behind those letters that I've written? You have the columns of A, possibly symmetric, as I said, but not orthogonal, usually. So you find an orthogonal basis. You orthogonalize them. You line them up perpendicular to each other. And then there is a matrix R, which happens to be upper triangular, that connects. The not orthogonal basis with the orthogonal basis. Right, we constructed R step by step. And then the idea is write these in the reverse order. And that will be A1, the next A. OK, and then do it again, and again, and again. And so we're hoping, of course, that the eigenvalues didn't change. We're hoping that we can forget A0, start again with A1, and produce A2, and continue. So we're hoping two things. One will not be a hope. We can see that the eigenvalues of A1 and A0 are the same. So we have not changed the eigenvalues. How do we see that? If you had to show that two matrices had the same eigenvalues, would you compute all the eigenvalues and compare them? Certainly not. What would you do? What's the best test, the usual test, the usual thing you would want to show? They're similar. That's right. So the claim would be that these two matrices are similar. Right. So maybe we should show that. So we want to write A1 in a different way. So the claim is A1 is similar to A. So we just have to figure out. We have to get A1 to A0. So here's A1. OK. So A1, I want to show that that's right. So that's R0, Q0. But what is Q0? From here, Q0 is what we is R. Q0 is, what do I want? I want to put R0 inverse over there. So R0. Now for Q0, I'm going to substitute. So what is Q0? I multiply both sides of that equation by R0 inverse. So Q0 is A0 is, sorry, I said A and wrote Q. A0 R0 inverse. Right. For Q0, I've put in what it equals. And that's done it. That's exactly telling me that A1, which is this, equals this. And that's a similarity transformation. I have not changed the eigenvalues. So that's OK. The other thing, which is the, like you could say, the miracle in this thing, is that when I continue to do that for almost every matrix, the matrices begin to lose stuff off the diagonal, especially below the diagonal by the ordering that QR is done. So you tend to start with a matrix A0. You get a matrix A1, which is like a little smaller here. This part being not especially smaller. You do it again. This is even smaller. So I put even smaller A2, even smaller. And you keep going. And for most matrices, the result is that I don't know how many steps we want to think of taking, but we get quite small numbers here. So once we get small numbers here, little epsilons, that's everybody's shorthand for small numbers, what would you expect to see on the diagonal? The eigenvalues. Because this has the same eigenvalues as this, as this, as this. And these little epsilons are not going to change the eigenvalues too much. So these will be, on the diagonal, will be close to the eigenvalues. And actually, what happens is this one comes first. That one is quite accurate first. I guess we should probably do a simple example to see this happen. Actually, I do an example in the notes. And let me say what happens. So I do a 2 by 2 example, which has something like cos theta, sine theta. I don't know what. I've forgotten what I took. And I don't have that page of notes here. Something here. Something here. Something here as a 0. And then a 1, after just one step, has sine cubed theta there. And numbers there that are getting much closer to the eigenvalues. Sorry that this isn't a full scale example. But the point of the example is here, that this off-diagonal entry gets cubed. And the next step will be its ninth power, and then the 27th power. So it's really quickly going to 0. And this happens. So cubic convergence is a prize. In numerical linear algebra. So that happens. So this arrived on the scene and quickly blew away all other methods that were being used to compute eigenvalues. But numerical people, being what they are, they wanted to improve it. Like, is there a way to make it faster? And turned out there is. It turned out that the better way, basically the same idea, but just you're always looking for a simple change. And the idea of introducing a shift was tried. And turned out to work extremely well. So that's the improvement. So we take a1. No, so how does the shift work? So instead of a0, have I got space here to do this? Instead of a0, I take a0 minus a shift. So a shift is some multiple of the identity. If I just move a matrix by a multiple of the identity, what happens to its eigenvectors? What happens to its eigenvalues? Something pretty simple, right? Because I'm just shifting by si. What happens to its eigenvectors? They're the same. And what happens to the eigenvalues? They change by s. If a0v equals lambda v, then when I multiply this by v, there will be an extra. It'll be lambda minus s. Because the identity times the v is just v. So it just shifts all the eigenvalues by s. And you try to shift. You look for a shift. This would be great. If you knew lambda n, a shift there, you would be looking for 0 then, if you shifted them all by that lambda. And it turns out that would speed things up. So you work instead with this matrix as now, again, I'm factoring in a q0, r0. So that's the work of the method, is in doing Gram-Schmidt at every step. And then there's a little work in reversing the order. And then I want to undo the shift. So I do that factorization. Now, let's see. You may have to help me to remember what I should do here. So I factor those. I reverse those. And then I think I add back the shift. And that's my A1. So I took A0. I shifted it. I worked with it, qr, reversed the order, rq, added back the shift to get a matrix. And what am I, of course, hoping about the matrix A1 and A0? I'm hoping they're still similar. I'm hoping. So I did a shift. And I undid a shift. But of course, after doing a qr, I have to check, are these really still similar? So let me just try to check that one again. Maybe I'll just, it's sitting right there. So let me do it again. I'm hoping that something, you know, where did we change? Oh, yeah, here. We showed that A1 was similar to A0. And I'm hoping that's probably still true, even, you know, the shift didn't mess that up. Let's just try. So A0, that's r0, q0 plus si. OK. And now what am I going to do? I'm going to, what did I do before? I figured out what q0 was from this. You remember? So this is r0. Now I have to put in q0. But q0 is this thing inverse times this. Is this going to work? I'm like hoping, but I don't think I wanted to get that. No, it's not q0 there. What do I put here? And if it doesn't work, we'll leave it as an exercise. OK. So yeah, because I didn't sort of start right somehow. OK. So but let me just push along to see what happens. So I'm plugging in for q0 here. I'm plugging in this matrix, so it shouldn't have inverted it, times r0 inverse. Who knows? It might work. Who knows? So that's the r0, q0. Right? Is everybody with me? Sorry about this. So r0 inverse. And then I have to add Si. So what have I done? I've just pushed on, believing that this would work. Because it's the method that is constantly used. Now do I have? What do I have there? Is it working? This is r0 A. That was, of course, A0. r0 A0 r0 inverse. r0 A0 r0 inverse. Good. Minus S. What have I got there? From the r0 minus Si r0, what is that? That's minus Si. Look, success. The r0 cancels the r0 inverse. So that term from that, that, that is minus Si. Cancels plus Si. I'm finished. And lo and behold, we have the same similarity. So we messed around by a multiple of the identity, and it actually makes the thing converge faster if we choose the shifts well. But basically, the same idea is still working. OK. So that's the QR method. Well, that's the method we haven't shown and won't show. Except for this half-completed example, I don't plan to prove that the lower triangular part begins to disappear, gets smaller and smaller and smaller, and then the eigenvalues pop up on the diagonal. It's amazing. Amazing. OK. Now, is there any other improvement we can make? So that's the method. And where is the work in using that method? Because that's what we always focus on. Where are we spending computer time? Well, we're spending computer time in doing the factorization. So it didn't cost anything to shift by the identity, but then we had to factor that into Q0, R0. Then it didn't cost much to multiply them in the opposite order. So the work was in QR. So could we think of anything to improve that aspect? Could we think of anything there? And then we've got a really first-class method. Well, if the matrix A, A0, the matrix we started with, had some zeros that allowed us to skip steps in doing the QR factorization. So what am I going to say? I'm going to say if A or A0, our original matrix, has a bunch of zeros, let's say it's got a whole lot of zeros there. Maybe it's, well, OK. I overdid it here. I know the eigenvalues right off. But so the truth is I can't, saying that if that, it's not going to happen. But we can get zeros with one extra diagonal. That turns out, so here's the main diagonal. Everybody's got his eye on the main diagonal. And one diagonal, I can get a lot of zeros. But I can't by simple computations. And I'll show you how to get one. But I can't get all those to be 0, because then I would have the eigenvalues right there. And that, well, how do I know that I can't? In elimination, ordinary solving AX equal B, you really do get to an upper triangular form. Some operator, some elimination steps, you plug away, and your matrix becomes upper triangular U. And you're golden. But that's too much to expect here. In fact, we know we can't do it by simple steps. Because if we could do it, if we could get to a U with a whole lower triangular part 0, we would have found the eigenvalues. And we know that the eigenvalues solve an equation of nth degree. And we know somebody proved centuries or more ago that you can't solve an nth degree equation by simple little steps. Right? Do you know who that was? And you know that fact? And what degree does it apply to? So that's an important fact that you sort of pick up in math. Yeah, what do you? Fifth degree, yeah. So 5 by 5 and up would be, this is impossible. Impossible. There's no formula to find a simple formula for the lambdas. And similarly for the sigmas, the singular values. So the eigenvalues is definitely a level of difficulty beyond Ax equal b, the inverse matrix or something, the pivots. All that you can do exactly if you do exact arithmetic. We cannot find the lambdas exactly. But we can get as close as we like by continuing with the QR method. So yeah, in other words, we have to settle for, if we want to, at the beginning, improve our matrix before we start doing that stuff. We can get it with one extra diagonal. And do you know what kind of a matrix, whose name? And I don't know why. Yeah, say it again. Upper Hessenberg. So upper is just like upper triangular. It's up there. But the key person's name is Hessenberg. As I say, that's a Hessenberg matrix. So Hessenberg matrix is a matrix with one triangular plus one more diagonal. But lots of zeros. Order of n squared, something like almost like half n squared, not quite, but close, zeros. And you could show that those zeros stay zeros in QR. So that really pays off. It cuts the work down significantly. So the full QR method is step one, reduce A to Hessenberg form with these zeros. And when I say reduce, I mean find a similarity transformation, of course, because I want the eigenvalues of this to end up the same as the Hessen. I want to keep the same eigenvalues as I go. And then step two is QR on this Hessenberg matrix with shifts. So that's the code that would be programmed in IGAVE. That's what MATLAB and, well, really, MATLAB is appealing, like other matrix systems, is appealing to LAPAC and LINPAC. The team of professional numerical analysts really spent a lot of effort and time. The book LAPAC has 10 authors. And you can download any of these codes, like the eigenvalue code. So that's where MATLAB naturally, that's sort of the Bible for codes in linear algebra. OK, now, I think it's sort of interesting to know. And there is one more good thing to tell you about this method. And it applies if the matrix is symmetric. If the matrix is symmetric, then if we check all this, we could find that the matrices stayed symmetric. If A0 is symmetric, I can check. You could easily check through this, and you would discover that A1 is also symmetric. It turns out you could rewrite this with a Q0 and a Q0 inverse on the other side. But that Q0 inverse is the same as Q0 transpose, because it's an orthogonal matrix, and symmetry would fall out. So if it's symmetric, and it's in Hessenberg form, and it stays symmetric at every step, what can you tell me about a symmetric Hessenberg matrix? It's only got, yeah, you just erase all these. If there are zeros there, and if the matrix is symmetric, then we can safely predict that it will only have one diagonal above, one non-zero diagonal above the main diagonal. In fact, it'll stay symmetric. So now I should write symmetric Hessenberg matrix, n equals tri-diagonal matrix. Yeah, three diagonals. So now you really have reduced the time to do QR, because you've got a tri-diagonal matrix. It'll stay tri-diagonal in all these steps. So you're working with just 3n numbers. Well, actually, 2n, because the diagonal above and the diagonal below are the same. You're working with just 2n numbers instead of order n squared, and it just goes like a bomb. So that's I for symmetric matrices. And you see that it was all based, that really the heart of the algorithm was QR. So that's my, it took half the class to report on the favorite way, the EIG way, to find eigenvalues. Oh, I should say something about singular values. So singular values, of course, the singular values of the matrix are the eigenvalues of A transpose A, square root of those eigenvalues. But you wouldn't do it that way. You would never form A transpose A. Oh, I didn't mention the other thing you would never, ever, ever do. So let me just put it here. This is like in disgrace to solve that equation. It's like, OK, back to first grade, because that's not, that's very bad. A determinant, first of all, it's extremely slow, extremely slow. And the determinant is packing all this n squared pieces of information into n coefficients, and it's hopelessly ill-conditioned. Yeah, you just, you lose information all the time. So really, if this is going on camera, it better go on camera with an x, because don't do it. OK. Yeah. So where was I? Singular values. So A transpose A. So again, let's think about what you could do at the beginning before starting on QR for A transpose A, or for the matrix A. What could you do with orthogonal matrices? So I guess, what did we say about symmetric matrices? So here's what I said about symmetric matrices. If you give me a symmetric matrix, I can, in just a simple number of simple steps, make it tridiagonal. I can't make it diagonal, because then I'd be finding the eigenvalues, and Abel, who was the first person to see that that was impossible, forbids it. But so let me write down what I'm saying here. If I have a symmetric matrix S, I can find a bunch of Q's and Q transposes, and I can put them all together into one big Q, and it's transposed. And what do I know about the eigenvalues of that matrix? Q is orthogonal always. So what can you tell me about the, this is the same as Q S Q inverse, and therefore, the eigenvalues are the same. It's similar to S. And it becomes tridiagonal after I find a good Q. Now, and therefore, same lambdas, tridiagonal with the same lambdas. Now, what am I thinking about here? I'm thinking about, tell me the corresponding possibility about singular values. I want to do something to my matrix. Now, I'm always taking a general matrix A. And I'm looking for its singular values. And I'm looking to simplify it. And what am I allowed to do? Yeah, I guess my question is, similarity transformations left the eigenvalues alone. What can I do that leaves the singular values alone? That's a fundamental question, because it was so fundamental for eigenvalues. By doing this, a matrix and its inverse, I got something similar. And I checked even in this class that the eigenvalues, same lambdas. Now I want a whole line that ends up with the same sigmas. And I want you to tell me what I'm allowed to do to the matrix without changing the sigmas. So maybe don't shout it out immediately. Let everybody think. What am I allowed to do to a matrix? Every matrix has got these singular values. And now I want to make it a better matrix with more 0's or something. If I do that to it, does that change the sigmas? Can I do more than that to it? What can I do? What group of matrices will have the same sigmas as my starting matrix A? So that's a basic, basic question about singular values and the SVD. So let's think of the answer together. So it's connected to the SVD. So let me remember the SVD. The SVD, I have some orthogonal matrix, then the singular value matrix, SV for singular values, and then another orthogonal matrix. What could I do to that equation that would not touch this guy? So I'm asking what invariance, because not touching it means leaving it not varying. So I'm looking for under what operations are the singular values invariant? When I was looking at eigenvalues, this was the operation. Well, it didn't have to be orthogonal. It's something, and it's inverse. But now what is it up there? What could I do to that matrix A? Could I multiply by Q? Could I throw in a Q? Maybe not even on the other end. If I throw in an orthogonal Q, do I change the singular values or do I not change them? Fundamental question. The answer is no, I don't change them. I'm allowed to do that, because here's an orthogonal matrix, a Q times U. If both of those are orthogonal, then the product is. Everybody knows that a product of two orthogonal matrix is still orthogonal. Better know that. Better know that. Yeah. So if I have an orthogonal matrix Q and an orthogonal matrix U, I claim that this is still orthogonal. And how do I check it? Well, I use some test for orthogonality. What would be the test you like to use? The inverse is the same as the transpose. You like that test? So I'll invert it. Q U inverse. Of course, for any matrix, that's U inverse Q inverse. But these were separately orthogonal, so that's U transpose Q transpose. And that is the same as Q U transpose. Yeah. So I use the orthogonality of U and the orthogonality of Q to conclude that the inverse is the transpose. So the answer is yes, I could do that. Now, with singular value, with eigenvalues, I had to multiply on the other side by Q inverse or Q transpose. Do I have to do that now? No. What can I do on the right-hand side? I can multiply by. I can leave it alone. Then it has the same singular values because it's the same sigma in there. If I have a orthogonal matrix times a diagonal times an orthogonal, that diagonal, positive diagonal, is going to be sigma. So what can I do on this side? I could multiply by any orthogonal matrix on that side, too. So let's call this guy Q1 and this guy Q2. I still have an orthogonal matrix there, orthogonal matrix there, and the same sigma popped in the middle. So that's what you're allowed to do. That gives us more freedom. Before, when we had to do similarity transformations with the same guy, we got it to be tridiagonal. But now we're allowed to do more stuff. We're allowed to use different orthogonal matrices on the left and right. And we can reduce it even further from tridiagonal to bidiagonal. So the first step is getting 0. The step of getting 0s reduces it all the way to that with all 0s there. So it's easier. Then I work on this. I work on using a QR type idea, some method like that. So everybody's seeing that our algorithm has got two stages. One is get a lot of 0s and get them in places that will stay 0 as part 2 of the algorithm gets going. And then run part 2 of the algorithm. You're staying with each step is very fast now because doing a QR factorization is fast. Was there a question? Yeah. So I would call this bidiagonal, of course. And everybody recognizes that if I have a bidiagonal matrix, call it A or A0 or whatever. Then what do you think about A transpose A? What would A transpose A? If that was A, what could you tell me about A transpose A? Could you multiply matrices, like knowing where the non-zeros are, like in your head, and get an idea of where? So if I have a bidiagonal matrix A, then sort of implicitly in the SVD, I'm looking at A transpose A. And what would be true about A transpose A? It would be tridiagonal. So what I've done here and what I've done there just match up. If you don't want to change singular values, you can get all the way to here. But then to find those singular values, that would involve A transpose A. It would be symmetric and tridiagonal. And then you'd be in that game. So those are the basic facts of I and SVD. For matrices of order up to 1,000, say. I'm not enough of an expert to know where. Maybe higher, because in perfect math, it's going to take infinitely many steps. Or Abel would be very surprised. He would see you solving for eigenvalues and then to re-equation by a whole lot of little steps and getting them exactly right. That won't happen. But you get them within epsilon in a number of steps that's like n cubed. So that's pretty impressive. The eigenvalue problem is being solved, in quotes, by a fast method that gets you a good answer within a tolerance in n cubed steps. Right. So that's great as long as n isn't too big. And then when n is too big, which of course happens, you have to think again. So this method is a giant success up to large matrices. But then you have to think again. And what is involved in thinking again? Well, I guess more thinking. So what do you do if the matrix is bigger? I guess that Krylov would say, use my method. So Krylov would say, especially if your matrix is sparse. Can we just remember what Krylov was? Krylov started with a vector b, multiplied it by a, multiplied that by a, and got up to, let's say, a to the 999b. So now Krylov has got 1,000 dimensional space. He's got a basis for it, 1,000 vectors that span 1,000 dimensional space. And he'll look at the matrix A only on that space. In other words, I won't go into detail about that. He restricts the matrix to this 1,000 dimensional space. And he hopes that it's captured. We hope that the eigenvector is virtually in that space. And actually, I wouldn't go up to, let me take a 9 out of that. 100 dimensional would probably catch the eigenvector. So if the eigenvector is virtually in this space, then we can look at a matrix of order 100. We can bring A down to just see its action on that space. So I look at vectors V, which are some combination, C1b plus C2ab plus C3a squared b and C100 a to the 99th b, plus an error. And I'm going to ignore that error. Because I've gone up to dimension 100. I probably say it's pretty safe to ignore that error. And then in this space, just looking at the matrix A. So wherever A to the 100th comes in, forget it. Just think about the matrix A as multiplying vectors of this kind in this space. Then I have a 100 by 100 eigenvalue problem. And so the big matrix A is reduced to a matrix of size 100 by, do you see sort of what I'm saying, even though I'm not giving the details? Think of a matrix A of size a million. And you apply it to Krylov vectors. So I call them little k for a Krylov vector in this 100-dimensional space. So they have a million minus 100, zero components, you could say, this k. This is in the Krylov space. This is A, a million, k, 100. It's a full, it's got a million components, but it's out of just 100-dimensional space. So when I multiply by A, it'll be mostly in, mostly in, partly in the Krylov space, k 100, and a piece out of k 100. And I just ignore that part of the matrix. So I have a 100 by 100 problem, and I've solved to find the eigenvalues. And they're a pretty good approximation to the eigenvalues, to the, hopefully, like the lowest 100 eigenvalues. I'd like to know that. But I might not be sure that this idea would give me the lowest 100, the first 100 eigenvalues of the million, of the matrix of size a million. I'm just taking a few minutes here to sort of wave hands about what Krylov, the Krylov idea would do. And I probably won't mention Krylov again this semester. So what it can do is look at this particular type of space, because we can get a basis for it quickly, just multiply again and again by A. Then we can orthogonalize that basis. That's Graham-Schmidt in some form. We're always going back to Graham-Schmidt. Then I have a 100 by 100. I have a subspace of size 100. I look at what the matrix does in that space. And I could look for, I could find eigenvalues restricted to that space. They wouldn't be the perfect eigenvalues, but they would be accurate. OK, so I didn't know it would take one class time to talk about finding eigenvalues and singular values, but we did some important things. We remembered that similarity is the thing to check, thing to preserve, because it doesn't change the eigenvalues. And then for singular values, what was the thing? You could multiply left and right by different orthogonal matrices. And somehow, maybe that doesn't have an established name, multiplying left and right by a Q1 and a Q2 transpose. Yeah, but the idea is clear. And that doesn't change the singular values. We're ready to move now into maybe our next step, which we don't spend a long time on. We'll be like random sampling. What if your matrix is just way too big? So that's a new, very new idea, very different idea in numerical linear algebra, is just to sample the matrix. Could you believe that the answer is going to come out right just for a random sample? Well, the odds are in your favor. OK, so that'll be Wednesday, and then we have lots of new, we'll move onward after that. OK, see you Wednesday. Thanks.