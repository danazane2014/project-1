 So last time, we finished by proving Bolzano's intermediate value theorem. Which states that a continuous function achieves all values in between the function evaluated at the endpoints. So if I have a continuous function, and either I take a value y between f of a and f of b, so either f of a is less than f of b and y is in between, is bigger than f of a and less than f of b, or f of a is bigger than f of b, but y is still in between them, then there exists c in the interval a, b, so that f of c equals y. So every value in between f evaluated at the endpoints is achieved. I drew the picture, went along with this. a, b, f of a, f of b, and that could be the function. And if we take anything in between, then there has to be some c so that f of c equals y. At least in the picture I've drawn, there's three different guys, but there's always at least one. And then the last thing we talked about at the end of last lecture was that the image of a closed and bounded interval by a continuous function is, again, a closed and bounded interval where e corresponds to the absolute max of f, and f corresponds to the absolute max of f, and f corresponds to the absolute min of f. So this is what we proved last time is that f of, if f is continuous, then there exists ed and r such that the range of this closed and bounded interval is, again, a closed and bounded interval. OK, so f is very well behaved on closed and bounded intervals. And we'll see another way in which continuous functions are well behaved on a closed and bounded interval in just a second. Now a simple application of the Bolzano Intermediate Value Theorem is the following. Really, it's an application of that theorem that I labeled the bisection method, but let's chalk that up to the Bolzano Intermediate Value Theorem, which is the following, that if f of x is any odd polynomial, then f has at least one real root. OK, so any odd polynomial has at least one real root. So by the fundamental theorem of algebra, every polynomial has exactly n roots, n corresponding to the degree of the polynomial, but these roots can be complex value. OK, but for odd polynomials, there must be at least one real root, and this is a consequence of the Bolzano Intermediate Value Theorem. But instead of proving this theorem in its full generality, let me just give you kind of a representative example. So let's take f of x equals x to the 20, 21 minus x to the 20, 20 plus, I don't know, what's today, I think the 25th or 24th, I don't know, 1025, x minus 300. OK, so what's the point? As long as I plug in a big enough x, this will swamp everything here. So for example, if I stick in f of 100,000, I leave it to you to check that this is positive, because this will be 100,000 to the 20, 20 minus 100,000 to the 20, 20 or 20, 21, and then plus this, which will be positive, minus 300, this doesn't matter. So what really matters is this big guy or this guy with the largest power will swamp all the other ones and eventually be positive if I stick in a large enough positive value. And likewise, in fact, for this guy, if I stick in a negative value, then I get something very big, but now with a minus sign, which will again swamp all these earlier guys. In fact, I don't have to choose minus 100,000. If I just stick in zero, I will get minus 300, which will be negative. But you can check that if I stick in minus 100,000, this will still be negative. As discussed by the intermediate value theorem, there exists a c in 100,000 so that f of c equals zero, because zero is something in between f evaluated at these two endpoints. But again, you can prove this and the idea is the same, is that as long as you go far enough out and positive, then the polynomial will be positive if the coefficient out in front is positive. And if you go far enough out negative, then you'll get something that's negative. And then you apply the intermediate value theorem to be able to find a root. Now of course, one other thing we did with the statement of the min-max theorem was, can we drop any of the assumptions on this theorem? And you can see that the example is no. So the question is, if f from a closed and bounded interval is not continuous, and let's say f of zero is less than zero, f of a, f of b is bigger than zero, then does there exist a c, a b such that f of c equals zero? OK? Let me put that question mark at the end. All right? And it's easy to come up with an example of a function that is not continuous, that satisfies these conclusions, and there does not exist a point in between so that f of c equals zero. Simplest being, I'll just draw a picture. Let's go from minus one, one, two. OK, so then the function is f of x equals x minus one, and this is for x not equal to one, and let's make it a half there. So this function looks like half there. OK, so there is no value, there is no c in between. So x is between zero and two. So you see f of zero is equal to minus one, that's negative. f of two is equal to one, it's positive. But there exists no c in between so that f of c equals zero. OK? So what's the point I'm making? We really do need this hypothesis that the function is continuous for this theorem to be true. OK? If you drop that hypothesis, then the theorem is no longer true. OK. So now we're going to move on to a new concept called uniform continuity. And before I do this, I'm going to bring up, or I'm going to rewrite the definition of a continuous function. So this is an old definition, but I want to highlight a specific aspect of it. So this is me writing the definition of a continuous function again. So function f from a set S to R is continuous. So we said that means it's continuous at every point, meaning if for all c in S, it's continuous at c, which means for all epsilon positive, there exists a delta. Now this delta, and we saw this in examples that we did proving continuity, this usually depends on epsilon and c, the point where I'm looking at, such that for all x in S, x minus c less than delta implies that f of x minus f of c is less than epsilon. OK? So this is, again, a bit of review, but I just want to show you that in general, when we verify that a function is continuous on a set, we have to show it's continuous at every point in the set. And that means for every epsilon, there exists a delta. But this delta can depend on, usually depends on epsilon, but it can also depend on the point c, which is where we're checking continuity at. So for a simple example, let's take f of x equals 1 over x on the interval 0, 1, open interval 0, 1. OK? So I claim f of x equals 1 over x is continuous. So proof, we have to show that for every c in 0, 1, for all epsilon, there exists that. So let c be in 0, 1. Let epsilon be positive. Now choose, again, I mean, I'm not, since this is kind of review, I'm not going to go through the process of really the thinking behind choosing this delta. I mean, there's calculations you do off to the side to try and get this delta, but let's choose delta to be the minimum of c over 2 and c squared over 2 times epsilon. So what you notice, what I want to, again, emphasize is this delta depends on c, the point we're looking at, and epsilon. OK? So I claim this delta works. Suppose x is in 0, 1 and x minus c is less than delta. OK? Then absolute value of x, this is equal to the, this is by the triangle inequality, so let me write it this way. c absolute value is equal to c minus x plus x, which is less than or equal to c minus x plus x, which is less than delta, which is less than or equal to the minimum of these two, so it's certainly less than or equal to c over 2 plus x. And now, you know, c and x, these are in 0, 1, so they're all positive. And therefore, I get that, let's go to the next board. So if I subtract the c over 2 over to the other side, I get that c over 2 is less than x. All right? So this is as long as x is in 0, 1 and x minus c is less than delta. OK? This should not be a shock. All right? Here's 0, 1, c, c over 2, 3c over 2. OK? And so if x is in this interval, it's certainly bounded away by c over 2. OK? Or if x is in here, then x is certainly bigger than c over 2. All right? And now we look at f of x minus f of c. This is equal to 1 over x minus 1 over c equals c minus x over x times c, which is less than delta over x times c. And because this is a, OK? And now x is less than, x is bigger than c over 2, so if I take 1 over that, I get 1 over x is less than 2 over c, so this is less than or equal to 2 delta over c squared. And delta is the minimum of now c over 2 and c squared over 2 times epsilon. So this is less than or equal to 2 over c squared times c squared over 2 epsilon equals epsilon. OK? So I just went through the proof of continuity for 1 over x, which, you know, using better machinery we could have proven. But I did this to highlight this point that's showing continuity of 1 over x on 0, 1, that in showing that, you know, this delta that you are choosing depends kind of on the point you're looking at. All right? So now I want to make a... So, you know, what the definition of continuity says is for all c, for all epsilon, you can find a delta depending on epsilon and c. Now uniform continuity removes that necessity of delta depending on c. OK? And I'm going to make a possibly bad analogy in just a minute, but first let me write down the definition of uniform continuity. Let S be a subset of R, F going from S to R. We say F is uniformly continuous on S if for all epsilon positive there exists a delta depending only on epsilon such that for all x, c in S with x minus c less than delta, we get that F of x minus F of c is less than epsilon. OK? So, you know, a function being continuous means that for all epsilon you can find a delta depending on one of these points, depending on c, so that if I had this, then I have this. Uniform continuity says you can find a delta that works across the board. OK? So I'm going to make a... Try and make an analogy that hopefully I don't have to cut out of this lecture. But it's a bit like this. Let's take a trip down memory lane and imagine being at a party. So you know everybody's in a party, and let's say that everybody can hear, their ears work just about the same, and the goal is, you know, to try and understand each other's conversations, try and hear each other's conversations. Now in order for, you know, me to hear this person's conversation, depending on how loud they talk, I have to be close enough to be able to hear them. You know, and this changes from person to person in the room. Maybe I have to, I can stand a foot away from this person over here, maybe this person over here is a low talker and I have to be almost up in their face to be able to hear them. Maybe that person just whispers and I have to be almost nose to nose to be able to hear them. You know, that's a bit like a function being continuous. Okay? You know, the threshold of hearing what they're saying is the epsilon, and I have to change my delta depending on what point I'm at so that I can hear them. Now uniform continuity is kind of like everybody's more or less speaking around the same level, speaking at the same loudness. All right? So I just can be within, say, one foot of everybody and be able to understand them. I go to this person, if I'm within one foot, I can hear them well. If I go to this person, I hear, and within one foot I can hear them well, and so on. All right? So that's kind of the difference between, you know, or at least a loose analogy of the difference between continuity and uniform continuity. All right? So this is a, you know, reasonably interesting definition, so we should see examples and negate it. First off, you know, you should be able to convince yourself that a function that's uniformly continuous is actually continuous. I'm not going to write the proof of that down. It follows essentially straight from the definition. But let's look at a function which is uniformly continuous. So the function f of x equals x squared on the interval 0, 1 is uniformly continuous. So what's the proof? I want you to notice I'm going to use kind of in a very essential way that we're on this closed and bounded interval. So we need to show that for every epsilon there exists a delta so that no matter what two points, so if you like, here's a picture that goes along with this, so that if I look at any two points that are close to each other, less than delta, their function's value is within epsilon and so let's say I'm on the interval a, b. But not just this point, I could go over here to two other points that are within delta of each other and if I look at the function evaluated at the two points, then the difference between these two guys is also less than epsilon. So I can move through the party and as long as I'm close enough within a uniform distance to somebody, I can hear them just fine. So what a nice trip down memory lane when we can be around other people. So let's show this function is uniformly continuous, let epsilon be positive, choose delta to be epsilon over 2. Now you see this delta just depends on epsilon, okay? Then if x and c are in 0, 1 and x minus c is less than delta, then if I compute the difference x squared minus c squared, this is, I can write as x plus c times x minus c and now I use the triangle inequality, this is less than or equal to x minus c. Now these guys are in 0, 1 so their absolute value is always bounded by 1. So this is less than or equal to 1 plus 1 and then x minus c is bounded by delta which equals 2 times delta and delta is chosen to be epsilon over 2 so I get epsilon. Now again, how would you choose delta? So it looks like I just chose this delta out of nowhere. In practice, how do you choose delta? This is the computation you do. You take x squared minus c squared, split it, you have this, you can estimate with what you know above by 2 here times delta. So how do you choose delta so that 2 times delta is, say, equal to epsilon? You choose delta to be epsilon over 2. Okay? Alright, so let's negate this definition and then look at 1 over x once again and check to see if it's uniformly continuous. We'll show it's not. So the negation f from s to r is not uniformly continuous. I saw a little bit of the new Borat movie that came out so I almost wanted to say f is uniformly continuous. Not, but I didn't. It's not uniformly continuous if, and now you negate the rest of the definition, if there exists some bad epsilon such that no matter what delta you choose, there exists some bad x and c and s such that they're close to each other but the values are not. Bigger than or equal to. So let's revisit 1 over x. This is not uniformly continuous on the open interval 0, 1. So you see that this function is continuous but it's not uniformly continuous. Alright? So we just proved that a minute ago that it's continuous. So let me now give you the proof that this function is not uniformly continuous and what's the idea behind this? Here's 1. This is not to scale but there's 1 as well and 1 over x shoots up to plus infinity as x approaches 0. So what uniform continuity says is that if I take any two points in the interval that are close to each other, then the outputs have to be close to each other. Alright? Continuity remember says if I fix a point, then as long as I'm close to that point, the outputs will be close to each other. Uniform says no matter what two points you choose, as long as they're close together, the outputs will be close together in a controlled way. Now if I take two points very close to each other here, very close to 0, they can be pretty close to each other but their differences, I mean their values, can be quite large even though the difference between the arguments are small, the value's going to be quite large just because 1 over x shoots up. So let's quantify this. I have to tell you what the bad epsilon-naught is. Choose epsilon-naught equals 2. Now any epsilon-naught will work just fine. Let delta be positive. So now we have to find x and c in S within distance delta to each other that whose values differ by at least 2. So let's choose c to be the minimum of delta and a half and I'm going to choose x to be c over 2. So just ignoring this half for a minute, c is equal to delta, x is equal to delta over 2, and then 1 over c minus 1 over x is going to be 2 over delta and if delta's very small, that's certainly bigger than or equal to 2. But I had to throw in this one half here because I chose epsilon-naught to be 2, right, and delta's arbitrary. So then we see that x minus c, this equals c over 2, which is certainly less than or equal to since c is the minimum of delta and 1 over half is less than delta over 2, which is less than delta. And if we compute 1 over x minus 1 over c, this is, remember x is c over 2, so this 1 over x is 2 over c, so 1 over c, right? And now from c being less than or equal to 1 over half, I get that 1 over c is going to be bigger than or equal to 2. Alright? And that's the end of the proof. So we see that even though these two arguments are very close to each other, the outputs are in fact separated by some fixed distance, at least 2. But we could actually make it bigger than 3, bigger than 4, whatever you like. And I'm not going to go over this example, but in fact I'll leave it to you or maybe I'll put it on the assignment, f of x equals x squared is not uniformly continuous on r. So we saw that it's uniformly continuous on the closed interval 0, 1, but it's not uniformly continuous on all of r. The reason being again, kind of because x squared is starting to get big, so I can take two things close to each other, stick it into x squared, and their outputs could be very different from each other. I didn't write out the proof of this one, so we're going to go a little bit by the seat of our pants. So we want to show this is not uniformly continuous, so I think epsilon 0 equals 1 is going to work just fine. Let delta be positive. And the idea is we want to choose x to be some number, let's call it a, plus, well c will be some number, and c equals some number x equals c plus delta over 2. So this is kind of scratch work off to the side. And let's see if we can somehow choose c so that, so first off, x minus c will be c over 2, will be delta over 2, which is less than delta. And now let's see if we can choose c so that x squared minus c squared is bigger than or equal to 1. So c is kind of the thing that we get to play with here. I mean this is how it actually goes if you're wanting to figure out the proof before you write it down, you know, the scratch work to the side. Alright so choose c so that this, now let's start off with the thing that we're trying to bound from below and start computing some stuff. This is equal to x plus c, and c will be positive, okay, so this will be, c and x will be positive, so x plus c times x minus c. Now x plus c will be 2c plus delta over 2. X minus c will be delta over 2. So so far all of these things are equality, and we get this is equal to 2c plus, so delta, 2c plus delta squared over 2, and here we're going to choose c positive, alright, so we're trying to show it's not uniformly continuous on r, so we just need to find a value of c and a value of x so that they're within delta distance to each other but their outputs are bigger than or equal to 1. So so far if c is just something positive, x is c plus delta over 2, then they're close. Their difference however is going to be just by this computation equal to 2c plus delta over 2 times delta over 2 squared. Now delta squared over 2 is always non-negative, so it's certainly bigger than or equal to delta times c, and now remember we wanted this thing to be bigger than or equal to 1. Right now we have it's bigger than or equal to delta c. So let's choose c to be 1 over delta, okay? And let's see that that works, choose c to be 1 over delta, x to be 1 over delta, so c plus delta over 2. Then these two values are close, x minus c equals delta over 2, it's less than delta, and x squared minus c squared equals x plus c times x minus c, x plus c as we did a minute ago is going to be 2 over delta plus delta over 2, that's x plus c, x minus c is equal to delta over 2, which equals 1 plus delta squared over 4, which is bigger than or equal to 1, our bad epsilon. So f of x equals x squared is not uniformly continuous on r, even though it is uniformly continuous on this closed and bounded interval. So uniform continuity is really a relationship between a function and the domain on which it's defined. But okay, so this is kind of to wrap up our theme of continuous functions and how they're behaved on closed and bounded intervals, and we see here that uniform continuity is a much stronger notion than continuity, as I've said. You can easily prove from the definition that if a function is uniformly continuous on s, then it's continuous on s. And we've seen that in general, the reverse inclusion does not hold. So let me just write this up as a remark. In general, f uniformly continuous implies that f is continuous, this just follows from the definition. This is not difficult to show. But the converse does not hold. Okay, as we've just seen, right, we have two examples here, f of x equals 1 over x on the open interval 0, 1, which is continuous, but it's not uniformly continuous. And then we have the function f of x equals x squared, which is not uniformly continuous on r, but in fact it is uniformly continuous on this closed and bounded interval 0, 1. This example here, though, is in fact the rule, that if I'm looking at a continuous function, or a function on a closed and bounded interval, then in fact these two notions are equivalent. So that's the following theorem. Let f be a function from a to b to r, so a closed and bounded interval now, not just an arbitrary set. Then f is continuous on a, b, meaning it's continuous at every point, if and only if f is uniformly continuous on a, b. And the necessity of working on a closed and bounded interval is seen again by these examples, by this example we had here, of f of x equals 1 over x, and f of x equals x squared on r and 0, 1. But as long as we're looking on a closed and bounded interval, continuous is equivalent to uniform continuity. Now this is also not the sharpest thing you can say. In fact, what you can say is that if I replace this with what's called a compact set, which I may put in the assignment, I may not, then the statement is still true. And that's in some sense the sharpest statement. But I think this is interesting enough to just work on a closed and bounded interval. So let's prove this. So again, this direction, I should say, I leave this to you as an exercise. Uniform continuity implying continuity, I leave it to you. And I will do a more difficult one, and we'll use kind of a philosophy like we did in the previous lecture, where we used the definition of continuity, or this theorem about continuity in terms of sequences, and also the Bolzano-Weierstrass theorem. OK, good. We still have the negation of the definition of uniform continuity. And that's what we'll need. All right, so we're going to prove f continuous implies uniformly continuous, and we're going to do this by contradiction. So suppose f is continuous on AB, but not uniformly continuous. All right, and then we're going to break the universe some way, which shows that our assumption that f is not uniformly continuous does not hold. OK, so we use this negation of the definition of uniform continuity, which says that there exists a delta 0. So let me write this out. I'm just going to rewrite this here. Then there exists a delta 0 such that for all delta positive, there exists xc in AB, which depend on delta, such that x minus c is less than delta, and f of x minus f of c is greater than or equal to epsilon 0. Now this holds for every delta. You can find the x and c so that x minus c is less than delta, so f of x minus and f of x minus f of c is greater than or equal to epsilon 0. So let's choose delta to be 1 over n for each natural number n. For all natural numbers n, there exists xn, cn, and AB such that, so this is a statement now with delta equals 1 over n, xn minus cn is less than 1 over n, and f of x sub n minus f of c sub n is bigger than or equal to epsilon 0. Now I have two sequences here, and they're bounded because they're in this closed and bounded interval. So I can pass to a subsequence of one of them to start by the Bolzano-Weierstrass theorem, not BZ, Bolzano-Weierstrass, I think that's probably somewhere else in the lecture or the previous, I wrote BZ, but I meant BW. By Bolzano-Weierstrass, there exists subsequence xn sub k of x sub n and x and AB such that as k goes to infinity of x sub n sub k equals x. So by Bolzano-Weierstrass, from any bounded sequence, we can find a convergent subsequence. And because this subsequence is always between A and B, the limit x will be between A and B. Now if I look at, since the subsequence cn sub k is bounded between A and B, so I have a subsequence of x sub n sub k, and I then obtain a subsequence of the c sub n's by just picking these integers to be the same. So now that subsequence of the c sub n is still, they're all coming from AB, so that's bounded by AB. So it's bounded. So by Bolzano-Weierstrass applied now to this subsequence, there exists a subsequence c sub n sub k sub j of c sub n sub k and an element c in AB such that the limit as j goes to infinity of c sub n sub k sub j equals c. So I found a subsequence of the x sub n's, and now I look at a corresponding subsequence of the c sub n's, where now this subsequence, the n sub k's are coming from the n sub k's chosen for this subsequence. That's still a bounded sequence, so by Bolzano-Weierstrass I can take a subsequence of that and an element c so that I have convergence. So now let me just summarize. In summary, let's call these, so in summary, the sequence is x sub n sub k sub j. So now this is even a subsequence of x sub n sub k and c sub n sub k sub j. These are subsequences of the original sequences x sub n and c sub n, and there exists x and c in AB such that the limit as j goes to infinity of x sub n sub j equals x and the limit as j goes to infinity of c sub n sub kj equals c. Now I claim x equals c. Now, zero is certainly bigger than or equal to x sub n sub k sub j minus c sub n sub k sub j, which is less than or equal to by how we've defined these sequences is less than or equal to 1 over n sub k sub j. And as j goes to infinity, so first off, by the definition of subsequences, j is always bigger than or equal to n sub k sub j is always bigger than or equal to j. So this is always less than or equal to 1 over j. This is just a consequence of how subsequences are defined. You always have to move to the right after you've made a choice of entry in the sequence that you're going to take as an element of your subsequence. So zero is bounded, bounds this from below, and is bounded above by 1 over j, which converges to zero. So the middle thing has to converge to zero by the squeeze theorem. But these two things, that converges to x, this converges to c. And so the absolute value of x minus c equals zero, and therefore, x equals c. Now we're almost home free. Thus, limit as j goes to infinity of x sub n sub k sub j equals c. And limit as j goes to infinity c sub n sub k sub j equals c. Now where's the kicker? This. So these are subsequences of x sub n and c sub n. But each of those sequences is converging to a number c. So by the theorem about continuity, since f is continuous at c, this implies that zero equals the limit as j goes to infinity of f of x sub n sub. So write it this way. f of c minus f of c equals limit j goes to infinity of f of n sub c sub n sub k sub j, which is bigger than or equal to epsilon zero. All of these are supposed to be bigger than or equal to some fixed epsilon zero, which is positive. But we've just shown it's equal to zero. That's the contradiction. Thus, f is uniformly continuous on a, b. So that concludes what we'll say about continuity. And as you'll note, going forward, we're going to cover topics faster. That's just because we have more machinery to work with. At the start, it was very slow going just because, one, it's the start of your first proof-based class, so I'm going to go slow. And two, we didn't have much to work with. We just had the real numbers are an ordered field with the least upper bound property. But now we've covered sequences, continuity, we know more, and it'll be easier to prove new things. And also, hopefully, familiarity with proof is getting better so that my arguments can be a little bit shorter and not so involved. So we're moving on to something now. This is supposed to be about calculus. We've covered limits, continuity. The next big topic has to be differentiability. So this is a new chapter about the derivative. So let me first give you the precise definition. So first off, I'll be saying, let I be an interval, f goes from I to R, and C an element of I. Now, when I say an interval, I don't mean it has to be closed and bounded like the ones we've been working on. I don't mean it has to be open. It doesn't have to be a bounded interval, like from 0 to 1. It could be 0 to infinity. It can include the endpoints, except, of course, infinity. But I think you know what I mean, and I don't have to write all this out for what exactly an interval is. So let I be an interval and f be a function from I to R and C a point of I. So first off, convince yourselves of this, but I think this should be pretty clear. For an interval, every point in the interval is a cluster point of the interval. This is not too difficult to prove. But I will not require you to prove it, but I'm just saying this because I'm about to define a limit at C, so it had better be a cluster point of a set containing of a set that this limit is defined using. So of I take away C. And if you don't like this statement, just forget I even wrote it, and we'll continue on. You say f is differentiable at C, so this is the new bit of terminology, if the limit x goes to C, f of x minus f of C over x minus C exists. If this limit exists, we say the function is differentiable at C. And we use a notation f prime f apostrophe C for this limit. If f is differentiable at every point of the interval I, we get a new function, this function f prime of C, which we denote by, we denote the function that's a derivative of f as f prime or df dx. So the simplest guys that are differentiable are, of course, polynomials. I think this is one of the first things you learn back in calculus. And I'm not going to go through what this is supposed to represent. You know this from calculus. The best way to say what this number represents is the instantaneous rate of change of the function f at this point. How is f changing at that point in the sense of increasing, decreasing, and so on? Or if you like, you interpret this as being the slope of the line that is perfectly tangent to the curve to the graph of the function f. But what does that mean to be tangent to the graph of a function? So that takes a bit of explaining. So I'm not going to get into what the actual interpretation of this number is. I think you have a pretty good idea of what that is. We're just going to start proving some properties of it. So the simplest example of a function which is differentiable is that of a monomial. So f of x equals x to the n. Let's put even a number in there. So let's write it this way. Let alpha be an r, n be a natural number, 0. Then function f of x equals alpha times x to the n is differentiable. So I should have said here as well, if a function is differentiable at every point where it's defined or if a function is differentiable at every point in its domain i, then we just say the function is differentiable. So then this function, this monomial is differentiable. And for all c, f prime of c equals n times alpha x to the n minus 1. That's c. So let's give a proof of that. And we'll use this kind of simple formula. So let's take a look at x minus c times this kind of weird looking sum to x n minus 1 minus j c to the j. So this is equal to multiply the x through. I get sum from j equals 0 to n minus 1 x to the n minus j c to the j minus. And I'm not going to use j again in the sum when I carry this c through. I'm going to use l. Sum from l equals 0 to n minus 1 x to the n minus, I'll put it this way, j plus 1 c to the j plus 1, or l. And now I'm going to make a change of variables in the second sum. And let j equals l plus 1. So in that case, when l equals 0, j starts at 1. When l hits n minus 1, j equals n. So this equals the sum from j equals 0 n minus 1 x n minus j c to the j minus sum j equals 1 to n x n minus j c to the j. So it's the same terms in the sum, except starting at different places. So what doesn't get killed here is the j equals 0 term. Because this one starts at j equals 1. This one ends at n. And so it kills the n minus 1 term. So the only thing that survives from this sum is the j equals 0 term. x to the n minus 0 c to the 0 minus. And now from here, everything gets killed except the n term. So x to the n minus n c to the n equals x to the n minus c to the n. So this is therefore equal to this. So we use this to compute this limit. So then f prime of c is equal to the limit as x goes to c of x to the n alpha times x to the n times alpha times c to the n over x minus c. Now alpha is just a number. It can pop out. x to the n minus c to the n divided by x minus c is this thing divided by x minus c. So this is equal to alpha times the limit as x goes to c of sum from j equals 0 to n minus 1 x to the n minus 1 minus j c to the j. Now this is just a polynomial in x. Polynomials are continuous functions. So the limit as x goes to c is just you plug in c to x. So this equals alpha times sum from j equals 0 to n minus 1 c to the n minus 1 minus j times j equals. So c to the n minus 1 minus j times c to the j gives me c to the n minus 1. I'm summing in j. There's no j here. So that just pops out. Alpha times the sum j equals 0 n minus 1. And that's just there's n of the times 1. So this is just n. And that's the proof. OK. So I'm sure you saw that at some point. Next time we'll continue with, we'll give an example of a function which is not differentiable at a point. Of course, I think you probably already know this. f of x equals the absolute value of x. If there's time at some point, I'll go over an example of a function which was very surprising to people at one point. So now that we have differentiability and continuity on the board, for a long time, some long since dead people believed that if you have a continuous function, any continuous function, then there has to be at least one point where it's differentiable. There must be at least one point where it's differentiable. And people tried to prove that for a long time. And they couldn't because it's false. Weierstrass, who is, again, kind of the godfather of everything we're doing, came up with a whole class of examples of functions that are continuous. But are differentiable nowhere. They're not differentiable at a single point. And perhaps, depending on time, we'll go over this example. And I think I'll stop there.