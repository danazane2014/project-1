 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So last time was orthogonal matrices, Q. And this time is symmetric matrices, S. So we're really talking about the best matrices of all. Well, I'll start with any square matrix and about eigenvectors. But you've heard of eigenvectors more than once, more than twice, more than 10 times probably. OK. So eigenvectors. And then let's be sure we know why they're useful. And maybe compute one or two. But then we'll move to symmetric matrices and what is special about those. And then even more special and more important will be positive definite symmetric matrices. So when I say positive definite, I mean symmetric. So start with A. Next comes S. Then come the special symmetric matrices that have this extra positive definite property. OK. So start with A. So an eigenvector. If I multiply A by x, I get some vector. And sometimes if x is especially chosen well, Ax comes out in the same direction as x. Ax comes out some number times x. So normally there would be for an n by n matrix. So let's say A is n by n today. Normally if we live right, there will be n different independent vectors, x eigenvectors, that have this special property. And we can compute them by hand if n is 2 or 3. 2 mostly. But the computation of the x's and the lambda. So this is for i equal 1 up to n, if I use this sort of math shorthand, that I have n of these almost always. And my first question is, what are they good for? Why does course after course introduce eigenvectors? And to me, the key property is seen by looking at A squared. So let me look at A squared. So it's another n by n matrix. And we would ask, suppose we know these guys. Suppose we found those somehow. What about A squared? Is x an eigenvector of A squared also? Well, the way to find out is to multiply A squared by x and see what happens. Do you see what's going to happen here? This is A times Ax, which is A times Ax is lambda x. And now what do I do now? Because I'm shooting for the answer, yes. x is an eigenvector of A squared also. So what do I do? That number, that lambda is just a number. I can put it anywhere I like. So I can put it out front. And then I have Ax, which is lambda x. Thanks. So I have another lambda x. So that's lambda squared x. So I learn the crucial thing here, that x is also an eigenvector of A squared. And the eigenvalue is lambda squared. And of course, I can keep going. So A to the nth x is lambda to the nth x. We have found the right vectors for that particular matrix A. What about A inverse x? That will be, if everything is good, 1 over lambda x. Well, yeah, so any time I write 1 over lambda, my mind says, you've got to make some comment on the special case where it doesn't work, which is? Yeah, if lambda is not 0, I'm golden. If lambda is 0, it doesn't look good. But what's happening if lambda is 0? A doesn't even have an inverse. If lambda was 0, which it could be, no rule against it. If lambda was 0, this would say A times the eigenvector is 0 times the eigenvector. So that would tell me that the eigenvector is in the null space. It would tell me that the matrix A isn't invertible. It's taking some vector x to 0. And so everything clicks. This works when it should work. And if we have any function of the matrix, we could define the exponential of a matrix. 18.03 would do that. Let's just write it down as if we know what it means. Does it have the same eigenvector? Well, sure, because e to the At, the exponential of a matrix, if I see e to the something, I think of that long infinite series that gives the exponential. All the terms in that series have powers of A. So everything's working. Every term in that series, x is an eigenvector. And when I put it all together, I learn that the eigenvalue is e to the lambda t. That's just a typical and successful work, use. So that's eigenvectors and eigenvalues. And we'll find some in a minute. Now, so I'm claiming that from this first thing, which was just like about certain vectors are special, now we're beginning to see why they're useful. So special is good. Useful is even better. So let me take any vector, say, v. And OK, what do I want to do? I want to use eigenvectors. This v is probably not an eigenvector. But I'm supposing that I've got n of them. You and I are agreed that there are some matrices for which there are not a full set of eigenvectors. That's really the main sort of annoying point in the whole subject of linear algebra, is some matrices don't have enough eigenvectors. But almost all do. And let's go forward, assuming our matrix has. OK, so if I've got n independent eigenvectors, that's a basis. I can write any vector v as a combination of those eigenvectors. Right. And then I can find out what A to the 2n e power. So that's the point. This is going to be the simple reason why we like to have, we like to know the eigenvectors. Because if I choose those as my basis vectors, v is a combination of them. Now, if I multiply by A or A squared or A to the k power, then it's linear. So I can multiply each one by A to the k. And what do I get if I multiply that guy by A to the kth power? OK, well, I'm just going to use, or here I said n, but let me say k. Because n, I'm sorry, I'm using n for the size of the matrix. So I better use k for the typical case here. So what do I get? Just help me through this, and we're happy. So what happens when I multiply that by A to the k? It's an eigenvector, remember. So when I multiply by A to the k, I get c1, that's just a number. And A to the k times that eigenvector gives lambda 1 to the k times the eigenvector. That's the whole point. And linearity says, keep going, cn lambda n to the kth power, xn. In other words, I can apply any power of a matrix. I can apply the exponential of a matrix. I can do anything quickly because I've got the eigenvector. So I'm saying the first use for eigenvectors, maybe the principal use for which they were invented, is to be able to solve difference equations. So if I call that vk, the kth power, then the equation I'm solving here is a one-step difference equation. This is my difference equation. And if I wanted to use exponentials, the equation I would be solving would be dv dt equal Av. dv, solution to discrete steps or continuous time. Evolution is trivial if I know the eigenvectors, because here is the solution to this one. And the solution to this one is the same thing, c1 e to the lambda 1 t x1. Is that what you were expecting for the solution here? Because if I take the derivative, it brings down a lambda. If I multiply by A, it brings down a lambda. So plus the other guys. OK. Not news, but important to remember what eigenvectors are for in the first place. Good. Yeah, let me move ahead. Oh, one matrix fact is about something called similar matrices. So I have on my matrix A, then I have the idea of what it means to be similar to A. So B is similar to A. Or yeah, what does that mean? So here's what it means, first of all. It means that B can be found from A by, this is the key operation here, multiplying by a matrix M and its inverse, M inverse AM. When I see two matrices, B and A, that are connected by that kind of a change, M could be any invertible matrix. Then I would say B was similar to A. And that change, that appearance of M is pretty natural. If I change variables here by M, then I get that similar matrix will show up. So what's the key fact? Do you remember the key fact about similar matrices? If B and A are connected like that, they have the same eigenvalues. So this is just a useful point to remember. So this is like one fact in the discussion of eigenvalues and eigenvectors. So similar matrices, same eigenvalues. So in some way, in the eigenvalue, eigenvector world, they belong together. They are connected by this relation that just turns out to be the right thing. Actually, that gives us a clue of how eigenvalues are actually computed. Well, they're actually computed by typing I of A with parentheses around A. That's how they're in real life. But what happens when you type I of A? Well, you could say the eigenvalue shows up on the screen. But something had to happen in there. And what happened was that Matlab or whoever took that matrix A started using good choices of M, better and better, took a bunch of steps with different M's. Because if I do another M, I still have a similar matrix. If I take B and do a different M2 to B, so I get something similar to B, then that's also similar to A. I've got a whole family of similar things there. And what does Matlab do with all these M's, M1 and M2 and M3 and so on? It brings the matrix to a triangular matrix. It gets the eigenvalues showing up on the diagonal. It's just tremendously. It was an inspiration when the good choice of M appeared. And let me just say, because I'm going on to symmetric matrices, that for symmetric matrices, everything is sort of clean. You not only go to a triangular matrix, you go toward a diagonal matrix. You choose M's that make the off-diagonal stuff smaller and smaller and smaller. And the eigenvalues are not changing. So they're shooting up on the diagonal are the eigenvalues. So I guess I should verify that fact, that similar matrices have the same eigenvalues. There can't be much to show. There can't be much in the proof, because that's all I know. And I want to know its eigenvalues and eigenvectors. So let me say, suppose M inverse A M has the eigenvector y and the eigenvalue lambda. And I want to show, do I want to show that y is an eigenvector also of A itself? No. Eigenvectors are changing. Do I want to show that lambda is an eigenvalue of A itself? Yes, that's my point. So can we see that? Can I see that lambda is an eigenvalue? There's not a lot to do here. I mean, if I can't do it soon, I'm never going to do it. So what am I going to do? AUDIENCE 1. Define the vector x equals my. GILBERT STRANGEVOICE. Yeah. My is going to be a key. And I can see my coming. Just when I see M inverse over there, what am I going to do with the darn thing? I'm going to put it on the other side. I'm going to multiply that equation by M. So I'll have, that will put the M over here. And I'll have A My equals lambda My, right? And is that telling me what I want to know? Yes. That's saying that My that you wisely suggested to give a name x to is lambda times My. Do you see that, that the eigenvalue lambda didn't change? The eigenvector did change. It changed from y to My. That's the x, the eigenvector of x. This is lambda x. Yeah. So that's the role of M. It just gives you a different basis for eigenvectors. But it does not change eigenvalues. Right. Yeah. OK. So those are similar matrices. Yeah, some other good things happen. A lot of people don't know. In fact, I wasn't very conscious of the fact that A times B has the same eigenvalues as B times A. Well, I should maybe write that down. AB has the same eigenvalues, the same non-zero ones. You'll see I have to, as BA. This is any A and B, same size. I'm not talking similar matrices here. I'm talking any two A and B. Yeah. So that's a good thing that happens. Now, could we see why? And then I'm going to be really pretty happy with basic facts about eigenvalues. So if I want to show that two things have the same eigenvalues, what do you propose? Show that they are similar. I already said if they're similar. So is there an M? Is there an M that will connect this matrix? So is there an M that will multiply this matrix that way? So that would be similar to AB. And can I produce BA then? So I'll just put the word want up here. I want. If I have that, then I'm done. Because that's saying that those two matrices, AB and BA, are similar. And I know that then they have the same eigenvalues. So what should M be? M should be? So what is M here? I want that to be true. Should M be B? Yeah, M equal B. Boy, not the most hidden fact here. Take M equal B. So then I have B times A times B, B inverse, which is the identity. So I have B times A. Yes. OK. So AB and BA are fine. Now, what do you think about this question? Are the eigenvalues, I now know that AB and BA have the same eigenvalues. And the reason I had to be careful about non-zero is that if I had zero eigenvalues, then yeah, I can't count on those inverses. Right. Right. So that's why I put in that little qualifier. But now I want to ask this question. If I know the eigenvalues of A separately by itself, A, and of B. Now, I'm talking about any two matrices, A and B. If I have two matrices, A, I have a matrix A and a matrix B, and I know those are eigenvalues and they're eigenvalues, what about AB? A times B. Can I multiply the eigenvalues of A times the eigenvalues of B? Don't do it. Right. Yes. Right. The eigenvalues of A times the eigenvalues of B could be damn near anything. Right. They're not connected to the eigenvalues of AB, especially. And maybe something could be discovered, but not much. And similarly for A plus B. So yeah. So let me just write down this point. Eigenvalues of A plus B are generally not eigenvalues of A plus eigenvalues of B. Generally not. Just there is no reason. And the reason that I get that no answer is that the eigenvectors can be all different. If the eigenvectors for A are totally different from the eigenvectors for B, then A plus B will have probably some other totally different eigenvectors. And there's nothing happening there. So that's sort of thoughts about eigenvalues in general. And there'd be a whole section on eigenvectors, but I'm really interested in eigenvectors of symmetric matrices. So I'm going to move on to that topic. So now, having talked about any matrix A, I'm going to specialize to symmetric matrices, see what's special about the eigenvalues there, what's special about the eigenvectors there. And I think we've already said it in class. So let me ask you to tell me again. So I'll call that matrix S now, as a reminder always that I'm talking here about symmetric matrices. So what are the key facts to know eigenvalues are real? Real numbers? If the matrix is. I'm thinking of real symmetric matrices. Of course, other real matrices could have imaginary eigenvalues. Other real matrices. So let's just think for a moment. Yeah, maybe I'll just put it here. Can I back up before I keep going with symmetric matrices? So take a matrix like that. Q, yeah, that would be a Q, but it's not especially a Q. Maybe the most remarkable thing about that matrix is that it's anti-symmetric. So I'll call it A for right. If I transpose that matrix, what do I get? The negative. So that's like anti-symmetric. And I claim that an anti-symmetric matrix has imaginary eigenvalues. So that's a 90 degree rotation. And you might say, well, it could be simpler than that. A 90 degree rotation, that's not a weird matrix. But from the point of view of eigenvectors, something a little odd has to happen, right? Because if I have a 90 degree rotation, if I take a vector x, any vector x, could it possibly be an eigenvector? Well, apply A to it. You'd be off in this direction, Ax. And there is no way that Ax can be a multiple of x. So there's no real eigenvector for that anti-symmetric matrix or any anti-symmetric matrix. Yeah. So you see that when we say that the eigenvalues of a symmetric matrix are real, we're saying that this couldn't happen if A were symmetric. And here, it's the very opposite. It's anti-symmetric. Well, while that's on the board, you might say, wait a minute. How could that have any eigenvector whatsoever? Yeah. So what is an eigenvector of that matrix A? How do you find the eigenvectors of A? When they're 2 by 2, that's a calculation we know how to do. You remember the steps there? I'm looking for Ax equal lambda x. So right now, I'm looking for both lambda and x. I've got 2. It's not linear, but I'm going to bring this over to this side and write it as A minus lambda I x equals 0. And then I'm going to look at that and say, wow, A minus lambda I must be not invertible, because it's got this x in its null space. So the determinant of this matrix must be 0. 0. I couldn't have a null space unless the determinant is 0. And then when I look at A minus lambda I, for this A, I've got minus lambdas minus A. Oh, A is just a 1. And that's minus 1. I'm going to take the determinant. And what am I going to get for the determinant? Lambda squared plus 1. And I set that to 0. So I'm just following all the rules. But it's showing me that the lambda, the two lambdas, there are two lambdas here. But they're not real, because that equation, the roots are I and minus I. So those are the eigenvalues. And they have the nice, they have all the, well, they are the eigenvalues, no doubt about it. With 2 by 2, there are two quick checks that tell you, yeah, you did the calculation right. If I add up the two eigenvalues in this, if I add up the two eigenvalues for any matrix, and I'm going to do it for this one, I get what answer? I get the same answer from the add the lambdas. Add the lambdas gives me the same answer as add the diagonal of the matrix, which I'm calling A. So if I add the diagonal, I get 0 and 0. So it's 0 plus 0. And this number adding the diagonal is called the trace. Trace. And we'll see it again, because it's so simple. Just adding the diagonal entries gives you a key bit of information. When you add down the diagonal, it tells you the sum of the eigenvalues, sum of the lambdas. Doesn't tell you each lambda separately, but it tells you the sum. So it tells you one fact by doing one thing. Yeah. That's pretty handy. Gives you a quick check if you've, you know, when you compute this determinant and solve for lambda, the thing you, this is a way to compute eigenvalues by hand. You could make a mistake, because it's a quadratic formula for 2 by 2. But you can check by adding the two roots, do you get the same as the trace, 0 plus 0. Well, there's one other check equally quick for 2 by 2. So 2 by 2s, you really get them right. What's the other check to, we add the eigenvalues, we get the trace? AUDIENCE 1. GILBERT STRANGE TRIANGLE We multiply the eigenvalues, so now I'll multiply the lambdas. So then I get i times minus i. And that should equal, let's don't look yet, what should it equal? If I multiply the eigenvalues, I should get the determinant. Right. Now then. So that's two handy checks. Add the eigenvalues for any size, 3 by 3, 4 by 4, but it's only two checks. So for 2 by 2, it's kind of, you got it. 3 by 3, 4 by 4, you could still have made an error, and the two checks could potentially still work. Let's just check it out here. What's i times minus i? 1. 1. Because it's minus i squared, and that's plus 1. And the determinant of that matrix is 0 minus, is 1. Yeah. OK. So we got 1. Good. Those are really the key facts about eigenvalues. But of course, it's not as simple as solving Ax equal b to find them. But if you follow through on this idea of similar matrices and sort of chop down the off-diagonal part, then sure enough, the eigenvalues got to show up. OK. Symmetric. Symmetric matrices. OK. So now we're going to have symmetric. And then we'll have the special even better than symmetric is symmetric positive definite. OK. Symmetric, you told me the main facts, are the eigenvalues real, the eigenvectors orthogonal. And I guess, actually, yeah. So I want to put those into math symbols instead of words. So yeah. Yeah. I guess, yeah. Shall I just jump in? And the other thing hidden there, but very important, is there is a full set of eigenvectors. Even if some eigenvalues happen to be repeated, like the identity matrix, it's still got plenty of eigenvectors. So that's an added point that I've not made there. And I could prove those two statements, but why don't I ask you to accept them and go onward? What are we going to do with them? OK. And yeah. OK. Can you just like, let's have an example. Just let me put an example here. Suppose S, now I'm calling it S, is 0's, 1, and 1. So that's symmetric. What are its eigenvalues? What are the eigenvalues of that symmetric matrix S? Plus and minus 1. Well, if you propose two eigenvalues, I'll write them down, 1 and minus 1. And then what will I do to check them? Trace and determine it. OK. So is it true that the eigenvalues are 1 and minus 1? OK. How do I check the trace? What is the trace of that matrix? 0. And what's the sum of the eigenvalues? 0. Good. What about determinant? What's the determinant of S? Minus 1. The product of the eigenvalues is minus 1, so we've got it. OK. What are the eigenvectors? What vector can you multiply by and it doesn't change direction? In fact, doesn't change at all. I'm looking for the eigenvector that's a steady state. 0, 1? I think it's 1, 1. Yeah, so here is the lambdas. And then the eigenvectors are, I think, 1, 1. Is that right? Yeah, sure. S is just a permutation here. It's just exchanging the two entries. So 1 and 1 won't change. And what's the other eigenvector? 1 and minus 1. And then I'm thinking, remembering about this similar stuff, I'm thinking that S is similar to a matrix that just shows the eigenvalues. So S is similar to, I'm going to put in an M. Well, I'm going to connect S, that matrix, with the eigenvalue matrix, which has the eigenvalues. So here is my, everybody calls that matrix capital lambda, because everybody calls the eigenvalues little lambda. So the matrix that has them is called capital lambda. And my claim is that these guys are similar, that this matrix S that you're seeing up there, I believe there is an M. I believe there is an M. So that S, what did I put in here? So I'm following this pattern. I believe that there would be an M and an M inverse, so that this would mean that. And that's nice. First of all, it would confirm that the eigenvalues stay the same, which was certain to happen. And then it would also mean that I had got a diagonal matrix. And of course, that's a natural goal, to get a diagonal matrix. So we might hope that the M that gets us there is like an important matrix. So do you see what I'm doing here? It comes under the heading of diagonalizing a matrix. I start with a matrix S. I find its eigenvalues. They go on into lambda. And I believe I can find an M, so that I see they're similar. They have the same eigenvalues, 1 and minus 1, both sides. So only remaining question is, what's M? What's the matrix that diagonalizes S? What have we got left to use? The eigenvectors. The matrix that, so can I put the M like over there? Yeah. That M inverse is going to go over to the other side. Oh, it goes here, doesn't it? I was worried there. It didn't look good. But yeah, so this is all going to be right if this is what I'd like to have, SM equal M lambda. SM equal M lambda. That's diagonalizing a matrix. That's finding the M using the eigenvectors. That produces a similar matrix lambda, which has the eigenvalues. That's the great fact about diagonalizing. That's how you use it. That's another way to say this is how the eigenvectors pay off. You put them into M. You take the similar matrix, and it's nice and diagonal. And do you see that this will happen? S times, so M has the first eigenvector and the second eigenvector. And I believe that the first eigenvector times the second, and the second eigenvector, that's M again on this side. Now let me just write in 1, 0, 0, minus 1. 1. I believe this has got to be like confirming that we've done their thing right, confirming that the eigenvectors work here. Could just like, please, make sense out of that last line. When you see that last line, what do I mean to make sense out of it? I want to see that that's true. How do I see that? How do I do this? So what's the left side, and what's the right side? So if I multiply S by a couple of columns, what's the answer? Sx1 and Sx2. That's the beauty of matrix multiplication. If I multiply a matrix by another matrix, I can do it a column at a time. There are four great ways to multiply matrices. So this is another one, a column at a time. So this left-hand side is Sx1, Sx2. I just do each column. And what about the right-hand side? I can do that multiplication. x1 minus x2, did somebody say? Death. No, I don't want. Oh, x1, sorry, you said it right. When you said x1 minus x2, I was subtracting. But you meant that the first column is x1, and the second column is minus x2. Correct. Sorry about that. And did we come out right? Yes, because now I compare. Sx1 is lambda 1 x1. Sx2 is lambda 2 x2. And I'm golden. So what was the point of this board? What did we learn? We learned, well, we kind of expected that the original S would be similar to the lambdas, because the eigenvalues match. S has eigenvalues lambda. And this diagonal matrix certainly has eigenvalues 1 and minus 1. A diagonal matrix, the eigenvalues are right in front of you. So they're similar. S is similar to the lambda. And there should be an M. And then somebody suggested maybe the M is the eigenvectors. And that's the right answer. So finally, let me write that conclusion here, which isn't just for symmetric matrices. So maybe I should put it for matrix A. So it has lambdas and eigenvectors. And the claim is that A times the eigenvector matrix is the eigenvector matrix times the eigenvalues. And I would shorten that to A x equals x lambda. And I could rewrite that, and then I'll slow down, as A equal x lambda x inverse. Really, this is bringing it all together in a simple, small formula. It's telling us that A is similar to lambda. It's telling us the matrix M that does the job. It's the matrix of eigenvectors. And so it's like a shorthand way to write the main facts about eigenvalues and eigenvectors. What about A squared? Can I go back to the very first? I see time is close to the end here. What about A squared? What are the eigenvectors of A squared? What are the eigenvalues of A squared? That's like the whole point of eigenvalues. Well, or I could just square that stupid thing. x lambda x inverse x lambda x inverse. And what have I got? x inverse x in the middle is identity. So I have x lambda squared x inverse. And to me, and to you, that says the eigenvalues have been squared. The eigenvectors didn't change. Yeah. OK, and now finally, last breath is, what if the matrix is symmetric? Then we have different letters. That's the only significant change. The eigenvector matrix is now an orthogonal matrix. I'm coming back to the key facts of what makes symmetric. How do I read? How do I see symmetric helping me in the eigenvector and eigenvalue world? Well, it tells me that the eigenvectors are orthogonal. So the x is Q. The eigenvalues are real. And the eigenvectors is x inverse. But now I'm going to make those eigenvectors unit vectors. I'm going to normalize it. So I'm really allowing. I have an orthogonal matrix Q. So I have a different way to write this. And this is the end of today's class. Q lambda, and what can you tell me about Q inverse? It's Q transpose. Thanks. So that was the last lecture. So now the orthogonal lecture is coming up at the last second of the symmetric matrices lecture. And this has the name spectral theorem, which I'll just put there. And the whole point is that it tells you what every symmetric matrix looks like. Orthogonal eigenvectors, real eigenvalues.