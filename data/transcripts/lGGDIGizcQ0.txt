 Two, one, and OK, here is a lecture on the applications of eigenvalues, and if I can, so that will be Markov matrices. I'll tell you what a Markov matrix is. So this matrix A will be a Markov matrix, and, I'll explain how they come in applications. And then if I have time, I would like to say a little bit about Fourier series, which is a fantastic application of the projection chapter. OK. What's a Markov matrix? Can I just write down a typical Markov matrix, say, point one, point two, point seven, point oh one, point nine nine, zero, let's say, point three, point three, point four. OK. There's a, a totally just invented Markov matrix. What, what makes it a Markov matrix? Two properties that this matrix has. So two properties are, one, every entry is greater equal zero. All entries greater than or equal to zero. And, and of course, when I square the matrix, the entries will still be greater equal zero. I'm going to be interested in the powers of this matrix. And this property, of course, is going to stay there. It, it, it, really, Markov matrices, you'll see, are connected to probability, ideas, and probabilities are never negative. The other property, do you, do you see the other property in there? If I add down the columns, what answer do I get? One. So all columns add to one. All columns add to one. And actually, when I square the matrix, that will be true again. So that, the powers of my matrix are all Markov matrices, and I'm interested in, always, the eigenvalues and the eigenvectors. And this question of steady state will come up. You remember we had steady state for differential equations last time? When, what was the steady state, what was the eigenvalue? What was the eigenvalue in the differential equation case that led to a steady state? It was lambda equals zero. When, you remember that we did an example and one of the eigenvalues was lambda equals zero, and that, so then we had an e to the zero t, a constant, one, as time went on, there that thing stayed steady. Now what in the powers case, it's not a zero eigenvalue. Actually, with powers of a matrix, a zero eigenvalue, that part is going to die right away. It's an eigenvalue of one that's all important. So this steady state will correspond, will be totally connected with an eigenvalue of one and its eigenvector. In fact, the steady state will be the eigenvector for that eigenvalue. OK. So that's what's coming. Now, for some reason, then, that we have to see, this matrix has an eigenvalue of one. This property that the columns all add to one, turns out, guarantees that one is an eigenvalue. So that you can actually find the eigenvalue, find that eigenvalue of a Markov matrix without computing any determinants of A minus lambda I, that matrix will have an eigenvalue of one, and we want to see why. And then the other thing is, so the key points, let me write these underneath. The key points are, the key points are, lambda equal one is an eigenvalue. I'll add in a little, an additional, well, a thing about eigenvalues, key point two, the other eigenvalues, all other eigenvalues are, in magnitude, smaller than one. In absolute value, smaller than one. Well, there could be some exceptional case when, when an eigen, another eigenvalue might have magnitude equal one. It never has an eigenvalue larger than one. So these two facts, somehow we ought to, linear algebra ought to tell us. And then, of course, linear algebra is going to tell us what the, what's, what happens if I take, if, you remember when I solve, when I, when I multiply by A time after time, the k-th thing is A to the k u0? And I'm asking what's special about this, these powers of A? And very likely the quiz will have a problem to compute, to, to compute some powers of A, or, or apply to an initial vector. So you remember the general form? The general form is that there's some amount of the first eigenvalue to the k-th power times the first eigenvector. And a, another amount of the second eigenvalue to the k-th power times the second eigenvector and so on. I just, my conscience always makes me say at least once per lecture that this requires a complete set of eigenvectors. Otherwise we might not be able to expand u0 in the eigenvectors and we couldn't get started. But once we're started with u0 when k is zero, then every A brings in these lambdas. And now you can see what the steady state is going to be. If lambda one is one, so lambda one equals one to the k-th power, and these other eigenvalues are smaller than one, so I've sort of scratched over the equation there to, we had this term, but what happens to that term if the lambda's smaller than one, then the, as we take powers, as we iterate, as we go forward in time, this goes to zero, right? Can I just, having scratched over it, I might as well scratch further. That term and all the other terms are going to zero because all the other eigenvalues are smaller than one, and the steady state that we're approaching is just whatever there was, this was the, this is the x1 part of u- of the initial condition u0, is the steady state. This much we know from general, from, you know, what we've already done. So I want to see why, let's at least see number one, why one is an eigenvalue. And then there's actually, in this chapter, we're interested not only in eigenvalues, but also eigenvectors, and there's something special about the eigenvector. Let me write down what that is. The eigenvector x1, x1 is the eigenvector, and all its components are positive. So the steady state is positive, if the start was. If the start was, so, well, actually, in general, this might have a, might have some component zero, always, but no negative components in that eigenvector. OK. Can I come to that point? How can I look at that matrix? So that was just a example. How could I be sure, how can I see that a matrix, if the columns add to zero, add to one, sorry, if the columns add to one, this property means that lambda equal one is an eigenvalue. OK. So let's, let's just think that through. What am I saying about, let me, let me look at A, and if I believe that one is an eigenvalue, then I should be able to subtract off one times the identity, and then I would get a matrix that's, what, minus point nine, minus point oh one, and minus point six, when I took the ones away, and the other parts, of course, are still what they were, and this is still point two and point seven, and, OK, what's, what's up with this matrix now? I've shifted the matrix, this Markov matrix, by one, by the identity, and what do I want to prove? I, what is it that I believe this matrix, about this matrix? I believe it's singular. Singular will, if, if A minus I is singular, that tells me that one is an eigenvalue, right? The eigenvalues are the numbers that I subtract off, the shifts, the numbers that I subtract from the diagonal, to make it singular. Now, why is that matrix singular? I, we, we could compute its determinant, but, but we want to see a reason that would work for every Markov matrix, not just this particular random example. So what is it about that matrix? Well, I guess you could look at its columns now. What do they add up to? Zero. The columns add to zero, so all columns, I'll let me put all columns now of, of, of A minus I add to zero. And then I want to realize that this means A minus I is singular. OK. Why? So I could, I, you know, that could be a quiz question, a sort of theoretical quiz question. If I give you a matrix and I tell you all its columns add to zero, give me a reason, because it is true, that the matrix is singular. OK. I guess, actually, now what, I think of, you know, I'm thinking of two or three ways to see that. How would you do it? We don't want to take its determinant, somehow. For the matrix to be singular, well, it means that these three columns are dependent, right? The determinant will be zero when those three columns are dependent. You see, we're, we're at a point in this course now where we have several ways to look at an idea. We can take the determinant, here we don't want to. But we met singular before that, those columns are dependent. So how do I see that those columns are dependent? They all add to zero. Let's see. Ooh. Well, oh, actually, what com- another thing I know is that the- I would like to be able to show is that the rows are dependent. Maybe that's easier. If I know that all the columns add to zero, that, that's my information. How do I see that those three rows are linearly dependent? What, what combination of those rows gives the zero row? How, how could I combine those three rows, those three row vectors to produce the zero row vector? And that would tell me those rows are dependent, therefore the columns are dependent, the matrix is singular, the determinant is zero. Well, you see it. I just add the rows. One times that row plus one times that row plus one times that row is the zero row. The rows are dependent. In a way, that one one one, because it's multiplying the rows, is like an eigenvector in the- it's, it's in the left null space, right? One one one is in the left null space. It's, it's singular because the rows are dependent. And can I just keep the reasoning going? Because this vector one one one is, it's not in the null space of the matrix, but it's in the null space of the transpose, is in the null space of the transpose. And that's good enough. If, if we have a square matrix, if we have a square matrix and the rows are dependent, that matrix is singular. So it turned out that the immediate, guy we could identify was one one one. Of course, the, there will be somebody in the null space, too. And actually, who, who will it be? So what's, so, so now I want to ask about the null space of, of the matrix itself. What combination of the columns gives zero? I, I don't want to compute it because I just made up this matrix and I, it would take me a while. It looks sort of doable because it's three by three, but my point is, what, what vector is it if we, once we found it, what have we got that's in the, in the null space of A? It's the eigenvector, right? That's where we find x1. Then x1, the eigenvector, is in the null space of A. That's the eigenvector corresponding to the eigenvalue one, right? That's how we find eigenvectors. So those three columns must be dependent. Some combination of columns, of those three columns is the zero column, and that, the three components in that combination are the eigenvector. And that guy is the steady state. Okay. So I'm happy about the, the, the thinking here, but I haven't given, I haven't completed it because I haven't found x1. But it's there. Can I, another thought came to me as I was doing this. A little, another little comment that you, about eigenvalues and eigenvectors, because of A and A transpose. What can you tell me about eigenvalues of A, of A, and eigenvalues of A transpose? Whoops. They're the same. This, so this is a little comment, we, it's, it's useful, since eigenvalues are generally not easy to find, it's always useful to know some cases where you've got them, where, and, and this is, if you know the eigenvalues of A, then you know the eigenvalues of A transpose. Eigenvalues of A transpose are the same. And can I just, like, review why that is? So to find the eigenvalues of A, this would be determinant of A minus lambda I equals zero. That gives me an eigenvalue of A. Now, how can I get A transpose into the picture here? I'll use the fact that the determinant of a matrix and the determinant of its transpose are the same. The determinant of a matrix equals the determinant of A, of the transpose. That was property ten, the very last guy in our determinant list. So I'll transpose that matrix. This leads to this, I just take the matrix and transpose it, but now what do I get when I transpose lambda I? I just get lambda I. So that's, that's all there was to the reasoning. The reasoning is that the eigenvalues of A solve that equation, the determinant of a matrix is the determinant of its transpose, so that gives me this equation and that tells me that the same lambdas are eigenvalues of A transpose. So that, backing up to the Markov case, one is an eigenvalue of A transpose, and we actually found its eigenvector, one one one, and that tells us that one is also an eigenvalue of A, but of course it has a different eigenvector, that the left null space isn't the same as the null space, and we would have to find it. So there's some vector here, which is x1, that produces zero zero zero. Actually, it wouldn't be that hard to find, you know, as I'm talking I'm thinking, OK, am I going to follow through and actually find it? Well, I can tell from this one, look, if I put a point six there and a point seven there, that's what then I'll be OK in the last row, right? Now I only remains to find one guy. And let me take the first row, then. Minus point fifty-four plus point twenty-one, there's some big number going in there, right? So I have just to make the first row come out zero, I'm getting minus point fifty-four plus point twenty-one, so that was minus point thirty-three, and what do I want, like thirty-three hundred? This is the first time in the history of linear algebra that an eigenvector has ever had a component thirty-three hundred. But I guess it's true, because then I multiply by minus one over a hundred, oh no, it was point thirty-three, so is this just, oh, shoot, only thirty-three. OK. Only thirty-three. OK. So there's the eigenvector. Oh, and notice that it's, that it turned, did turn out at least to be all positive. So that was, like, the theory predicts that part, too. I won't give the proof of that part. So thirty-three, point six, thirty-three, point seven. OK. Now, those are the, that's the linear algebra part. Can I get to the applications? Where do these Markov matrices come from? Because that's, that's part of this course and absolutely part of this lecture. OK. So where's, where's an application of Markov matrices? OK. Markov matrices, so my equation, then, that I'm solving and studying is this equation u k plus one equals A u k. And now A is a Markov matrix. A is Markov. And I want to give an example. Can I just create an example? It'll be two by two. And it's one I've used before because it seems to me to bring out the idea. It's, because we have two by two, we have two states, let's say, California and Massachusetts. And I'm looking at the populations in those two states, the people in those two states, California and Massachusetts. And my matrix A is going to tell me, in a, in a year, some movement has happened. Some people stayed in Massachusetts, some people moved to California, some smart people moved from California to Massachusetts, some people stayed in California and made a billion. OK. So that, there's a matrix there with four entries, and those tell me the fractions of my population. I'm making, I'm going to use fractions, so no, they won't be negative, of course, because, because only positive people are in, involved here. And they'll add up to one, because I'm accounting for all people. So that's why I have these two key properties. The entries are greater or equal zero, because I'm looking at probabilities. Do they move, do they stay? Those probabilities are all between zero and one. And the probabilities add to one, because everybody's accounted for. I'm not losing anybody, gaining anybody in this, in this Markov chain. It's, it, it, it, conserves the total population. OK. So what would be a typical matrix, then? So this would be U, California and U, Massachusetts at time t equal k plus one. And it's some matrix, which we'll think of, times U, California and U, Massachusetts at time k. And notice, this matrix is going to stay the same, you know, forever. So that's a severe limitation on the example. The example has a, the same Markov matrix, the same probabilities act at every time. OK. So what's a reasonable say, say, point nine of the people in California at time k stay there. And point one of the people in California move to Massachusetts. Notice why that column added to one. Because we've now accounted for all the people in California at time k, nine-tenths of them are still in California, one-tenths are, are here at time k plus one. OK. What about the people who are in Massachusetts? This is going to multiply column two, right? By our fundamental rule of multiplying matrix by vector, it's the, it's the total, the population in Massachusetts. Shall we say that, that after the Red Sox, fail again, eight, only eighty percent of the people in Massachusetts stay and twenty percent move to California. OK. So again, this adds to one, which accounts for all people in Massachusetts where they are. So there is a Markov matrix. Non-negative entries adding to one. What's the steady state? If everybody started in Massachusetts, say, at, you know, when the Pilgrims showed up or something, then where, where are they now? Where are they at time a hundred, let's say, or maybe, I don't know how many years since the Pilgrims, three hundred and something? Or, and, and actually, where will they be, like, way out, a million years from now? I could multiply, take the powers of this matrix. In fact, you, you ought to be able to figure out, what is the hundredth power of that matrix? Why don't we do that? But let me follow the steady state. So what's my starting, my starting u cal u mass at time zero is, shall we say, shall we put anybody in California? Let's make, let's make zero there. And say the population of Massachusetts is, let's say, a thousand, just a -. Okay. So the population is, the, so the populations are zero and a thousand at the start. What can you tell me about this population after, after k steps? What will u cal plus u mass add to? A thousand. Those thousand people are always accounted for. But, so u mass will start dropping from a thousand, u cal will start growing, actually, we could see, why don't we figure out what it is after one? After one time step, what, what are the populations? At time one. So what happens in one step? You multiply once by that matrix, and let's see. Zero times this column, so it's just a thousand times this column, so I think we're getting two hundred and eight hundred. So after the first step, two hundred people have, are, are in California. Now at the following step, I'll multiply again by this matrix, more people will move to some, to California, some people will move back, twenty people will come back, and, the net, the net result will be that the California population will be above two hundred, and the Massachusetts below eight hundred, and they'll still add up to a thousand. Okay, I do that a few times. I do that a hundred times. What's the population? Well, okay, to answer any question like that, I need the eigenvalues and eigenvectors, right? As soon as I've, I've created an example, but as soon as I want to solve anything, I have to find eigenvalues and eigenvectors of that matrix. Okay. So let's do it. So there's the matrix, point nine, point two, point one, point eight, and tell me its eigenvalues. Lambda equals, so tell me one eigenvalue? One, thanks. And tell me the other one. What's the other eigenvalue? From the trace or the determinant, from the tra- I, the trace is what, is, like, easier. So the trace of that matrix is one point seven, so the other eigenvalue is point seven. And it, notice that it's less than one. And notice that that, the determinant is point seventy two minus point oh two, which is point seven. Right. Okay. Now to find the eigenvectors. This is, so that's lambda one. And the eigenvector, I'll subtract one from the diagonal, right? So can I do that in light, let, in light here? Subtract one from the diagonal, I'll have minus point one and minus point two, and of course these are still there. And I'm looking for its, here's, here's, this is going to be x one. It's the null space of A minus I. Okay, everybody sees that it's two and one. Okay? And now how about, so the, and it, notice that, that, that eigenvector is positive. And actually, we can jump to infinity right now. What's the population at infinity? It's a multiple, this is, this eigenvector is giving the steady state. It's, it's some multiple of this, and how is that multiple decided? By adding up to a thousand people. So the steady state, the c1 x1, this is the x1, but that adds up to three, so I really want two, it's, it's going to be two thirds of a thousand and one third of a thousand, making a total of the thousand people. That'll be the steady state. That's really all I need to know at infinity. But if I want to know what's happened after just a finite number like a hundred steps, I better find this eigenvector. So can I, can I look at, I'll subtract point seven time, ti- from the diagonal, and I'll get that, and I'll look at the null space of that one, and I, this is going to give me x2 now, and what is it? So what's in the null space of that, certainly singular, so I know my calculation is right, and, one and minus one. One and minus one. So I'm prepared now to write down the solution after a hundred time steps, the, the populations after a hundred time steps, right? Can, can we remember the point one, the, the one with this two one eigenvector, and the point seven with the minus one one eigenvector? So I'll, let me, I'll just write it above here. U after k steps is some multiple of one to the k times the two one eigenvector and some multiple of point seven to the k times the minus one one eigenvector. Right? That's, I, I, this is how I take, how powers of a matrix work when I apply those powers to a u0. What I, so it's u0, which was zero a thousand. That has to be correct at k equals zero. So I'm plugging in k equals zero and I get c1 times two one and c2 times minus one one. Two equations, two constants, certainly independent eigenvectors. So there's a solution and you see what it is? Let's see, I guess we already figured that c1 was a thousand over three, I think. Did we think that had to be a thousand over three? And maybe c2 would be, excuse me, get an eraser. We can, I just, I think we'd get it here. c2, we want to get a zero here, so maybe we need plus two thousand over three. I think that has to work. Two times a thousand over three minus two thousand over three, that'll give us the zero, and the thousand over three and the two thousand over three will give us three thousand over three, the thousand. So this is what we approach. This part with a point seven to the k-th power is the part that's disappearing. Okay. That's, that's Markov matrices. That's an example of where they come from, from, from modeling, movement of people with no, gain or loss, with total, total count conserved. Okay. I, I, just, if I can add one more comment, because you'll see Markov matrices in electrical engineering courses, and often you'll see them, so here's my little comment, sometimes in a lot of applications they prefer to work with row vectors. So they, instead of, this was natural for us, right, for all the eigenvectors to be column vectors. So our columns added to one in the Markov matrix. Just so you don't think, well, what, what, what, what's going on, if we work with row vectors and we multiply vector times matrix, so we're, we're, we're multiplying from the left, then it'll be the, then we'll be using the transpose of, of this matrix. And it'll be the rows that add to one. So in other textbooks, you'll see, instead of calling, columns adding to one, you'll see rows add to one. Okay. Fine. Okay. That's what I wanted to say about Markov. Now I want to say something about projections and even leading in, a little into Fourier series. Because, but before any Fourier stuff, let me make a comment about projections. So this is a comment about projections onto with an orthonormal basis. So, of course, the, the basis vectors are q1 up to qn. Okay. I have a vector b. Let, let me imagine, let me imagine this is a basis. Let me, let's say I'm in n by n. I'm, I've got, n orthonormal vectors, I'm in n-dimensional space, so they're a complete, they're a basis. Any vector v could be expanded in this basis. So any vector v is some combination, some amount of q1 plus some amount of q2 plus some amount of qn. So, so any v. I just want you to tell me what those amounts are. What are x1, what's x1, for example? So I'm looking for the expansion. This is, this is really a projection, I could, I could really use the word expansion. I'm expanding the vector in the basis. And the special thing about the basis is that it's orthonormal. So that should give me a special formula for the answer, for the coefficients. So how do I get x1? What, what's a formula for x1? I could, I can go through the projection, the q transpose q, all that, normal equations, but, and I'll get, I'll come out with this nice answer that I think I can see right away. How can I pick, get a hold of x1 and get these other x's out of the equation? So how can I get a nice, simple formula for x1? And, and then we want to see, sure, we knew that all the time. Okay. So what's x1? The good way is take the inner product of everything with q1. Take the inner product of that whole equation, every term, with q1. What will happen to that last term? The inner product, if I take the dot product with q1, I get? Zero, right? Because this basis was orthonormal. If I take the dot product with q2, I get zero. If I take the dot product with q1, I get one. So that tells me what x1 is. q1 transpose v, that's taking the dot product, is x1 times q1 transpose q1 plus a bunch of zeros. And this is a one, so I can forget that. I get x1 immediately. So, so, do you see what I'm saying is that if I have an orthonormal basis, then the coefficient that I need for each basis vector is a cinch to find. Let me, let me just, I, I, I have to put this in the matrix language, too, so you see it there also. If I write that first equation in matrix language, what, what is it? I'm writing in matrix language, this equation says, I'm taking these columns, are you guys good at this now? I'm taking those columns times the x's and getting v, right? That's the matrix form. Okay, that's the matrix Q. Q x is v. What's the solution to that equation? It's, of course, it's x equal q inverse v. So x is q inverse v, but what's the point? Q inverse in this case is going to, is simple, I don't have to work to invert this matrix Q because of the fact that the, these columns are orthonormal, I know the inverse to that. And it is Q transpose. When you see a Q, a square matrix with that letter Q, the, that just triggers, Q inverse is the same as Q transpose. So the first component, then, the first component of x is the first row times v, and what's that? The first component of this answer is the first row of Q transpose. That's just, that's just Q one transpose times v. So that's what we concluded here, too. Okay. So, so nothing Fourier here. The, the key ingredient here was that the Q's are orthonormal. And now that's what Fourier is doing. The Q's are orthonormal. And now that's what Fourier series are built on. So now I'll, in, in the remaining, time, let me say something about Fourier series. Okay. So Fourier series is, well, we've got functions. We've got a function f of x. As a combination of, maybe it has a constant term. And then it has some cosine x in it. And it has some sine x in it. And it has some cosine 2x in it. And a, and some sine 2x, and forever. So what's, what's the difference between this type problem and the one above it? This one's infinite. But the key property of things being orthogonal is still true for sines and cosines. So it's the property that makes Fourier series work. So that's called a Fourier series. Better write his name up. Fourier series. So it was Joseph Fourier who realized that, hey, I could work in function space. Instead of a vector v, I could have a function f of x. Instead of orthogonal vectors, q1, q2, q3, I could have orthogonal functions. The constant, the cosine x, the sine x, the cos 2x, but infinitely many of them. I need infinitely many because my space is infinite dimensional. So this is like the moment in which we leave finite dimensional vector spaces and go to infinite dimensional vector spaces and our basis, so the vectors are now functions, and of course there are so many functions that it's, that we've got an infinite dimensional space, and the basis vectors are functions too, a0, the constant function one, so my basis is one, cos x, sine x, cos 2x, sine 2x, and so on. And the reason Fourier series is a success is that those are orthogonal. OK. Now what do I mean by orthogonal? I know what it means for two vectors to be orthogonal. Y transpose x equals zero, right? Dot product equals zero. But what's the dot product of functions? I'm claiming that whatever it is, the dot product, or we would more likely use the word inner product, of, say, cos x with sine x is zero, and cos x with cos 2x, also zero. So let me tell you what I mean by that, by that dot product. How do I compute a dot product? So let's just remember, for vectors, v trans- v transpose w, for vectors, so this was vectors, v transpose w was v1 w1 plus vn wn. OK. Now functions. Now I have two functions, let's call them f and g. What's with them now? The vectors had n components, but the functions have a whole, like, continuum. To, to graph the function, I just don't have n points, I've got this whole graph. So I have functions, I'm really trying to ask you, what's the inner product of this function f with another function g? And I want to make it parallel to this the best I can. So the best parallel is to multiply f of x times g of x at every x, and here I just had n multiplications, but here I'm going to have a whole range of x-s, and here I added the results, what do I do here? So what's the analog of addition when you have, when you're in a continuum? It's integration. So that the, the dot product of two functions will be the integral of those functions dx. Now I have to say, say, well, what are the limits of integration? And for this Fourier series, this function f of x, if, if I'm going to have, if, if that right-hand side is going to be f of x, that function that I'm seeing on the right, all those sines and cosines, they're all periodic, with, with, with period two pi. So, so that's what f of x had better be. So I'll integrate from zero to two pi. My all, everything is, is on the interval zero two pi now. Because if I'm going to use these sines and cosines, then f of x is equal to f of x plus two pi. This is periodic. Periodic functions. OK. So now I know what, I've, I've got all the right words now. I've got a vector space, but the vectors are functions. I've got inner products, and, and the inner product gives a number all right. It just happens to be an integral instead of a sum. I've got now, then I have the idea of orthogonality, because, actually, just, let's just check. Orthogonality, if I take the integral, let, let me do sine x times cos x, sine x times cos x dx, from zero to two pi, I think we get zero. That's the differential of that, so it would be one half sine x squared, was that right? Between zero and two pi, and, of course, we get zero. And the same would be true with a little more, some trig identities to help us out, of every other pair. So we have now an orthonormal infinite basis for function space, and all we want to do is express a function in that basis. And so I, the end of my lecture is, OK, what is a1? What's the coefficient, how much cosine x is there in a function, compared to the other harmonics? How much constant is in that function? That'll be, that would be an easy question. The answer a0 will come out to be the average value of f. That's the amount of the constant that's in there, its average value. But let's take a1 as more typical. How will I get, here's the end of the lecture, then, how do I get a1? The first Fourier coefficient. OK. I do just as I did in the vector case. I'm, I take the inner product of everything with cos x. Take the inner product of everything with cos x. Then on the left, on the left I have the inner product is the integral of f of x times cos x dx. And on the right, what do I have? When I, so what am I, when I say take the inner product with cos x, let me put it in ordinary calculus words. Multiply by cosine x and integrate. That's what inner products are. So if I multiply that whole thing by cos x and I integrate, I get a whole lot of zeros. The only thing that survives is that term. All the others disappear. So, and that term is a1 times the integral of cos x squared dx, zero to two pi, equals, so this was the left side and this is all that's left on the right-hand side. And this is not zero, of course, because it's the length of the function squared. It's the inner product with itself, and a simple calculation gives that answer to be pi. That's an easy integral and it turns out to be pi, so that a1 is one over pi times there, times this integral. So there is actually, that's Euler's famous formula for the, or maybe Fourier found it, for the coefficients in a Fourier series. And you see that it's exactly an expansion in an orthonormal basis. OK, thanks. So I'll do a quiz review on Monday and then the quiz itself in Walker on Wednesday. OK, see you Monday. Thanks.