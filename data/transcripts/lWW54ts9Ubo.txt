 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So I apologize. My voice is not 100%. So if you don't understand what I'm saying, please ask me. So we're going to be analyzing, actually not really analyzing. We described a second order method to optimize the log likelihood in a generalized linear model when the parameter of interest was beta. So here I'm going to rewrite the whole thing as a beta. So that's the equation you see. But we really have this beta. And at iteration k plus 1, beta is given by beta k. And then I have a plus sign. And the plus, if you think of the Fisher information that beta k as being some number, if you were to say whether it's a positive or negative number, it's actually going to be a positive number, because it's a positive semi-definite matrix. So since we're doing gradient ascent, we have a plus sign here. And then the direction is basically gradient ln at beta k. So this is the iterations that we're trying to implement. And we could just do this. At iteration, we compute the Fisher information. And then we do it again and again. That's called the Fisher scoring algorithm. And I told you that this was going to converge. And what we're going to try to do in this lecture is to show how we can reimplement this using iteratively reweighted least squares, so that each step of this algorithm consists simply of solving a weighted least square problem. So let's go back quickly and remind ourselves that we are in the Gaussian. Sorry, we're in the exponential family. So if I look at the log likelihood for one observation, so here it's ln. This is the sum from i equal 1 to n of Yi minus. OK, so it's Yi times theta i, sorry, minus b of theta i. Then there's going to be some parameter. And then I have plus c of Yi phi. So just the exponential went away when I took the log of the likelihood. And I have n observations. So I'm summing over all n observations. All right, then we had a bunch of formulas that we came up to be. So if I look at the expectation of Yi, so that's really the conditional of Yi given Xi. But here, it really doesn't matter. It's just going to be different for each i. This is denoted by mu i. And we showed that this was beta prime of theta i. Then the other equation that we found was that so what we want to model is this thing. We want it to be equal to Xi transpose beta. I'm sorry, g of this thing. All right, so that's our model. And then we had that the variance was also given by the second derivative. I'm not going to go into it. What's actually interesting is to see if we want to express theta i as a function of Xi, what we get going from Xi to mu i by g inverse and then to theta i by b inverse, we get that theta i is equal to h of Xi transpose beta, where h is the inverse. So which order is this? Is the inverse of g and composed with b prime. So we remembered that last time. Those are all computations that we've made. They're going to be useful in our derivation. And the first thing we did last time is to show that if I look now at the derivative of the log likelihood with respect to one coordinate of beta, which is going to give me the gradient if I do that for all the coordinates, what we ended up finding is that we can rewrite it in this form, sum of Yi tilde minus mu tilde. Sum of Yi tilde minus mu tilde, so let's remind ourselves that so Yi tilde is just Y divided. Well, OK, Yi tilde i is Yi. Is it times or divided? Times g prime of mu i. Mu tilde i is mu i times g prime of mu i. And then that was just an artificial thing so that we could actually divide the weights by g prime. But the real thing that builds the weights are this h prime, and there's this normalization factor. And so if we write it like that, so if I also write that Wi is h prime of Xi transpose beta divided by g prime of mu i times phi, then I could actually rewrite my gradient, which is a vector, in the following matrix form, the gradient ln at beta. So the gradient of my log-likelihood at beta took the following form. It was X transpose W and then Y tilde minus mu tilde. And here, W was just the matrix with W1, W2, all the way to Wn on the diagonal and 0 on the up diagonals. OK? So that was just taking the derivative and doing a slight manipulation that said, well, let's just divide whatever's here by g prime and multiply whatever's here by g prime. So today, we'll see why we make this division and multiplication by g prime, which seems to make no sense, but it actually comes from the Hessian computations. So the Hessian computations are going to be a little more annoying. Actually, let me start directly with the coordinate-wise derivative. So to build this gradient, what we used in the end was that the partial derivative of ln with respect to the j-th coordinate of beta was equal to the sum over i of Yi tilde minus mu i tilde times Wi times the j-th coordinate of Xi. So now, let's just take another derivative, and that's going to give us the entries of the Hessian. So we're going to the second derivative. So what I want to compute is the derivative with respect to beta j and beta k. So where does beta j? So here, I already took the derivative with respect to beta j. So this is just the derivative with respect to beta k of the derivative with respect to beta j. So what I need to do is to take the derivative of this guy with respect to beta k. Where does beta k show up here? It's hidden in two places. No, it's not in the y's. The y's are my data, right? But I mean, it's in the y tildes. Yeah, because it's in mu, right? Mu depends on beta. Mu is g inverse of Xi transpose beta, and it's also in the Wi's. Actually, everything that you see is directly, well, OK, W depends on mu and on beta explicitly, but the rest depends only on mu. And so we might want to be a little, well, we can actually use the, did I use the chain rule already? Yeah, it's here. But OK, well, let's go for it. Oh, yeah, OK. Sorry, I should not write it like that, because that was actually, right, so I make my life miserable by just multiplying and dividing by this g prime of mu. I should not do this, right? So what I should just write is say that this guy here, I'm actually going to remove the mu prime, the g prime of mu, because I just make something that depends on theta appear when it really should not. So let's just look at the last but one equality. OK, so that's the one over there, and then I have Xij. OK, so here it makes my life much more simple, because Yi does not depend on beta, but this guy depends on beta, and this guy depends on beta. So when I take the derivative, I'm going to have to be a little more careful now, but I just have a derivative of a product, nothing more complicated. So this is what? Well, the sum is going to be linear, so it's going to come out. Then I'm going to have to take the derivative of this term. So it's just going to be 1 over psi. Then the derivative of mu i with respect to beta k, which I will just write like this, times h prime of Xi transpose beta Xij. And then I'm going to have the other one, which is mu Yi minus mu i over phi times the second derivative of h of Xi transpose beta. And then I'm going to take the derivative of this guy with respect to beta j with beta k, which is just Xi k. So I have Xi j times Xi k. OK, so I still need to compute this guy. So what is the partial derivative with respect to beta k of g? So mu is g inverse of Xi transpose beta. So what do I get? Well, I'm going to get definitely the second derivative of g. Well, OK, that's actually not a bad idea. Well, no, that's OK. I can make the second. What makes my life easier, actually? Give me one second. Well, there's no one that actually makes my life so much easier. Let's just write it. Let's go with this guy. So it's going to be g prime prime of Xi transpose beta times Xi k. So now what do I have? If I collect my terms, I have that this whole thing here, the second derivative is, well, I have the sum from 1 equal 1 to n. Then I have terms that I can factor out, right? Both of these guys have Xi j, and this guy pulls out an Xi k, and it's also here, Xi j times Xi k. So everybody here is Xi j, Xi k. And now I just have to take the terms that I have here, the 1 over phi I can actually pull out in front. And I'm left with the second derivative of g times the first derivative of h, both taken at Xi transpose beta. And then I have this Yi minus mu i times the second derivative of h taken at Xi transpose beta. But here I'm looking at Fisher scoring. I'm not looking at Newton's method, which means that I can actually take the expectation of the second derivative. So when I start taking the expectation, what's going to happen? So if I take the expectation of this whole thing here, well, this guy is not. And when I say expectation, it's always conditionally on Xi. So let's write it. X1, Xn. So I take conditional. This is just deterministic. But what is the conditional expectation of Yi minus mu i times this guy conditionally on Xi? 0, right? Because this is just the conditional expectation of Yi. And everything else depends on Xi only. So I can push it out of the conditional expectation. So I'm left only with this term. OK. Now, so now I need to, sorry, and I have Xi, Xj. Xij, Xik. So now I want to go to something that's slightly more convenient for me. So maybe we can skip that part here, because this is not going to be convenient for me anyway. So I just want to go back to something that looks eventually like this. OK, that's what I'm going to want. So I need to have my Xi show up with some weight somehow. And the weight should involve h prime divided by g prime. And the reason why I want to see g prime coming back is because I had g prime coming in the original w. This is actually the same definition as the w's that I used when I was computing the gradient. Those are exactly these w's, those guys. So I need to have g prime that shows up. And that's where I'm going to have to make a little bit of computation here. And it's coming from this kind of considerations. So this thing here, oh, actually I'm missing the phi over there. Right? There should be a phi here. OK, so we have exactly this thing, because this tells me that if I look at the Hessian, so this was entry-wise. And this is exactly the form of something of the k. This is exactly the jk-th entry of Xi, Xi transpose. We've used that before. So if I want to write this in a vector form, this is just going to be the sum of something that depends on i times Xi, Xi transpose. So this is 1 over phi sum from i equal 1 to n of g prime prime Xi transpose beta h prime Xi transpose beta Xi, Xi transpose. OK? And that's for the entire matrix here that was just the jk-th entries of this matrix. And you can just check that if I take this matrix, the jk-th entry is just the product of the j-th coordinate and the k-th coordinate of Xi. All right. So now I need to do my rewriting. Can I write this? So I'm missing something here, right? Oh, I know where it's coming from. Hm? Mu is not g prime of x beta. Mu is g inverse of x beta, right? So the derivative of x prime is not g prime prime. It's like this guy. No, 1 over this, right? OK, yeah. OK? The derivative of g inverse is 1 over g prime of g inverse. I need you guys, OK? I'm not. All right. So now I'm going to have to rewrite this. This guy is still going to go away. It doesn't matter. But now this thing is becoming h prime over g prime of g inverse of Xi transpose beta, which is the same here. Which is the same here. OK? Everybody approves? All right. Now it's actually much nicer. What is g inverse of Xi transpose beta? Well, that was exactly the mistake that I just made, right? It's mu i itself. So this guy is really g prime of mu i. Sorry, just the bottom, right? So now I have something which looks like a sum from i equal 1 to n of h prime of Xi transpose beta divided by g prime of mu i phi times Xi Xi transpose, which I can certainly write in matrix form as X transpose WX, where W is exactly the same as before. So it's W1 Wn, and Wi is h prime of Xi transpose beta divided by g prime of mu i. There's a prime here times phi, which is the same that we had here. And it's supposed to be the same that we have here, except that the phi is in white. It's not there. OK. All right? So it's actually simpler than what's on the slides, I guess. All right, so now if you pay attention, I actually never forced this g prime of mu i to be here. Actually, I even tried to make a mistake to not have it. And so this g prime of mu i shows up completely naturally. If I had started with this, you would have never questioned why I actually multiplied by g prime and divided by g prime completely artificially. Here, it just shows up naturally in the weights. But it's just more natural for me to compute the first derivative first than the second derivative second. And so we just did it the other way around. But now let's assume we forgot about everything. We have this. This is a natural way of writing it, x transpose Wx. If I want something that involves some weights, I have to sort of force them in by dividing by g prime of mu i and therefore multiplying Yi and mu i by this Wi. So now if we recap what we've actually found, we got that. Let me write it here. We also have that the expectation of Hln of beta, x transpose xW. So if I go back to my iterations over there, I should actually update beta k plus 1 to be equal to beta k plus the inverse. So that's actually equal to negative i of beta k. Well, yeah, that's negative i of beta, I guess. So that's going to be. And beta here shows up in W. W depends on beta. So that's going to be beta k. So let me call it Wk. So that's the diagonal of H prime xi transpose beta k this time divided by g prime of mu i k phi. So this beta k induces a mu by looking at g inverse of xi transpose beta k. So mu i k is g inverse of xi transpose beta k. That's an iteration. And so now if I actually write these things together, I get minus x transpose Wx inverse. So that's Wk. And then I have my gradient here that I have to apply at k, which is x transpose Wk. And then I have y tilde k minus mu tilde k, where again, the superscript k are pretty natural. y tilde k just means that. So that's just yi times g prime of mu i k. And mu tilde k is, if I look at the i-th coordinate, it's just going to be mu i times g prime of mu i. So I just add superscript k to everything so I know that those things get updated real time. Every time I make one iteration, I get a new value for beta. I get a new value for mu. And therefore, I get a new value for W. Yes? Yeah, that's a good point. So that's definitely a plus because this is a positive semi-definite matrix. So this is a plus. And where did I? Well, that's probably where I erased it. OK, let's see where I made my mistake. So this, there should be a minus here. There should be a minus here. There should be a minus even at the beginning, I believe. So that means that what is my? Oh, yeah. So you see, when we go back to the first, so what I erased was basically this thing here, yi minus mu i. And when I took the first derivative, so there was the derivative with respect to h prime. So the derivative with respect to the second term, I mean, the derivative of the second term was actually killed because we took the expectation of this guy. But when we took the derivative of the first term, which is the only one that stayed, this guy went away. But there was a negative sign from this guy because that's the thing we took the negative off. So it's really when I take my second derivative, I should carry out some minus signs everywhere. So it's just I forgot this minus throughout. You see, the first term went away on the first line there. The first term went away because the conditional expectation of yi given xi is 0. And then I had this minus sign in front of everyone, and I forgot it. All right. All right. Any other mistake that I made? You good? All right. So now this is what we have. We have that beta k plus 1 is equal to beta k plus this thing. And if you look at this thing, it sort of reminds us of something. Remember the least squares estimator? So here I'm going to actually deviate slightly from the slides, and I will tell you how. The slides take beta k and put it in here, which is one way to go. And just think of this as a big least square solution. Or you can keep the beta k, solve another least squares, and then add it to the beta k that you have. It's the same thing. So I will take the different route so you have the two options. All right? OK. So when we did the least squares, so parentheses least squares, we had y equals x beta plus epsilon. And our estimator, beta hat, was x transpose x inverse x transpose y. And that was just solving the first order condition, and that's what we found. Now look at this. x transpose bleep x inverse x transpose bleep something. So this looks like if this is the same as the left board, if wk is equal to the identity matrix, meaning we don't see it, and y is equal to y tilde k minus mu tilde k. So those similarities, the fact that we just squeeze in, so the fact that the response variable is different is really not a problem. We just have to pretend that this is equal to y tilde minus mu tilde. I mean, that's just the least squares. When you call a software that does least squares for you, you just tell it what y is, you tell it what x is, and it makes the computation. So you would just lie to it and say, oh, the actual y I want is this thing. And then we need to somehow incorporate those weights. And so the question is, is that easy to do? And the answer is yes, because this is a setup where this would actually arise a lot. So one of the things that's very specific to what we did here and with least squares, we assumed that epsilon, when we did at least the inference, we assumed that epsilon was normal 0, and the covariance matrix was the identity. What if the covariance matrix is not the identity? If the covariance matrix is not the identity, then your maximum likelihood is not exactly this least squares. If the covariance matrix is any matrix, you have another solution, which involves the inverse of the covariance matrix that you have. But if your covariance matrix in particular is diagonal, which would mean that each observation that you get in this system of equations is still independent, but the variances can change from one line to another, from one observation to another, then it's called heteroscedastic. Hetero means not the same. Skedastic is scale. And heteroscedastic case, you would have something slightly different. And it sort of makes sense that if you know that some observations have much less variance than others, you might want to give them more weight. So if you think about your usual drawing, and maybe you have something like this, but the actual line is really, OK, let's say you have this guy as well. Just a few here. Like if you start drawing this thing, it's going to be, if you do least squares, you're going to see something that looks like this on those points. But now if I tell you that on this side, the variance is equal to 100, meaning that those points are actually really far from the true one, and here on this side, the variance is equal to 1, meaning that those points are actually close to the line you're looking for, then the line you should be fitting is probably this guy, meaning do not trust the guys that have a lot of variance. And so you need somehow to incorporate that. If you know that those things have much more variance than these guys, you want to weight this. And the way you do it is by using weighted least squares. So we're going to put in parentheses on weighted least squares. It's not a fundamental statistical question, but it's useful for us because this is exactly what's going to spit out something that looks like this with this matrix W in there. So let's go back in time for a second, assume we're still covering least squares regression. So now I'm going to assume that Y is X beta plus epsilon, but this time epsilon is a multivariate Gaussian in, say, p dimensions with mean 0 and covariance matrix. I will write it as W inverse because W is going to be the one that's going to show up. So this is the so-called heteroscedastic. That's how it's spelled. And yet another name that you can pick for your soccer team or a cappella group. All right, so the maximum likelihood in this case. So actually, let's compute the maximum likelihood for this problem. So the log likelihood is what? Well, we're going to have the term that tells us that it's going to be. OK, what is the density of a multivariate Gaussian? OK, what is the density of a multivariate Gaussian? So it's going to be a multivariate Gaussian in p dimension with mean X beta and covariance matrix W inverse, right? So that's the density that we want. Well, it's of the form 1 over determinant of W inverse times 2 pi to the p over 2 times exponential. And now what I have is X minus X beta transpose W. So that's the inverse of W inverse. X minus X beta divided by 2. So this is X minus mu transpose sigma inverse X minus mu divided by 2. And if you want a sanity check, just assume that sigma, yeah? Is it X minus X beta or 1? Well, if you want this to be Y, then this is Y, right? Sure, yeah, maybe it's less confusing. So if you do p is equal to 1, then what does it mean? It means that you have this mean here. So let's forget about what it is. But this guy is going to be just 1 sigma squared, right? So what you see here is the inverse of sigma squared. So that's going to be 2 over 2 sigma squared, like we usually see it. The determinant of W inverse is just the product of the entry of the 1 by 1 matrix, which is just sigma square. OK, so that should be actually, yeah, no, that's actually, yeah, that's sigma square. And then I have this 2 pi. So square root of this, because p is equal to 1, I get sigma square root 2 pi, which is the normalization that I get. This is not going to matter, because when I look at the log likelihood as a function of beta, so I'm assuming that W is known, what I get is something which is a constant. So it's minus n times p over 2 times log that W inverse times 2 pi. OK, so this is just going to be a constant. It won't matter when I do the maximum likelihood. And then I'm going to have what? I'm going to have plus 1 half of y minus x beta transpose W, y minus x beta. So if I want to take the maximum of this guy, sorry, there's a minus here. So if I want to take the maximum of this guy, I'm going to have to take the minimum of this thing. And the minimum of this thing, if you take the derivatives, you get to see, so that's what we have, right? We need to compute the minimum of y minus x beta transpose W minus y minus x beta. And the solution that you get is this. And the solution that you get, I mean, you can actually check this for yourself. The way you can see this is by doing the following. If you're lazy and you don't want to redo the entire thing, maybe I should keep that guy. W is diagonal, right? I'm going to assume that, so W inverse is diagonal. And I'm going to assume that no variance is equal to 0 and no variance is equal to infinity, so that both W inverse and W have only positive entries on the diagonal. So in particular, I can talk about the square root of W, which is just the matrix, the diagonal matrix, with the square roots on the diagonal. And so I want to minimize in beta y minus x beta transpose W y minus x beta. So I'm going to write W as square root of W times square root of W, which I can, because W, and it's just the simplest thing, right? If W is W1 Wn, so that's my W. Then square root of W is just square root of W1 square root of Wn, and then 0 is elsewhere. OK? So the product of those two matrices gives me definitely back what I want. And that's the usual matrix product. Now what I'm going to do is I'm going to push one on one side and push the other one on the other side. So that gives me that this is really the minimum over beta of, well, here I have this transpose, so I have to put it on the other side. W is clearly symmetric, and so is square root of W, so the transpose doesn't matter. And so what I'm left with is square root of W y minus square root of W x beta transpose, and then times itself. So that's square root W y minus square root W. I don't have enough space. x beta. OK, and that stops here. But this is the same thing that we've been doing before. This is a new y. Let's call it y prime. This is a new x. Let's call it x prime. And now this is just the least squares estimator associated to a response y prime and a design matrix x prime. So I know that the solution is x prime transpose x prime inverse x prime transpose y prime. And I'm just going to substitute again what my x prime is in terms of x and what my y prime is in terms of y. And that gives me exactly x W square root W square root W x inverse. And then I have x transpose square root W for this guy. And then I have square root W y for that guy. And that's exactly what I wanted. I'm left with x transpose W x inverse x transpose W y. OK? So that's a simple way to take into account the W that we had before. And you could actually do it with any matrix that's positive semi-definite, because you can actually talk about the square root of those matrices. It's just the square root of a matrix is just a matrix such that when you multiply it by itself, it gives you the original matrix. OK? So here that was just a shortcut that consisted in saying, OK, maybe I don't want to recompute the gradient of this quantity, set it equal to 0, and see what beta hat should be. Instead, I'm going to assume that I already know that if I did not have the W, I would know how to solve it. And that's exactly what I did. I said, well, I know that this is the minimum of something that looks like this when I have the primes. And then I just substitute back my W in there. All right? So that's just the lazy computation. But again, if you don't like it, you can always take the gradient of this guy. Yes? Why is the solution written in slides different? Because there's a mistake. Yeah, there's a mistake on the slides. How did I make that one? I'm actually trying to parse it back. I mean, it's clearly wrong, right? Oh, no, it's not. No, it is. So it's not clearly wrong. Actually, it is clearly wrong. Because if I put the identity here, those are still associative, right? So this product is actually not compatible. So it's wrong, but there's just this extra thing that I probably copy-pasted from some place. So this is one of my latest slides. I'll just color it in white. But yeah, sorry, there's a mistake. This parenthesis is not here. Thank you. Yeah? OK. Because I have two of them. I have one that comes from the x prime that's here, this guy. And then I have one that comes from this guy here. So the solution, let's write it in some place that's actually legible, which is the correction for this thing is x transpose wx inverse x transpose wy. So you're just squeezing this w in there. And that's exactly what we had before, x transpose wx inverse x transpose w some y. And what I claim is that this is routinely implemented. As you can imagine, heteroscedastic linear regression is something that's very common. So every time you have a least squares formula, you also have a way to put in some weights. You don't have to put diagonal weights, but here that's all we need. And so here on the slides, again, I took the beta k and I put it in there so that I have only one least squares solution to formulate. But let's do it slightly differently. What I'm going to do here now is I'm going to say, OK, let's feed to some least squares. So let's do weighted least squares on response y being y tilde k minus mu tilde k and design matrix being, well, just x itself. So that doesn't change. And the weights are what? The weights are the wk that I had here. So wk i is h prime of xi transpose beta k divided by g prime of mu i at time k times 5. And so this, if I solve it, will spit out something that I will call a solution. I will call it u hat k plus 1. And to get beta hat k plus 1, all I need to do is to do beta k plus u hat k plus 1. And that's because, so here that's not clear, but I started from there, remember? I started from this guy here. So I'm just solving a weighted least square that's going to give me this thing. That's what I called u hat k plus 1. Then I add it to beta k, and that gives me beta k minus 1. So I just have this intermediate step, which is removed in the slides. So then you can repeat until convergence. What does it mean to repeat until convergence? AUDIENCE 2. You can just set a threshold. You can just set a minimum. You can just set a minimum. You can just set a minimum. You can just set a minimum. Yeah, exactly. So you just set some threshold, and you say, I promise you that this will converge. So you know that at some point you're going to be there, but you're going to go there, but you're never going to be exactly there. And so you just say, OK, I want this accuracy on my beta. Actually, the machine is a little strong, especially if you have 10 observations to start with. You know you're going to have something that's going to have some statistical error. So that should actually guide you into what kind of error you want to be making. So for example, a good rule of thumb is that if you have n observations, you just take some within, if you want the L2 distance between the two consecutive beta to be less than 1 over n, you should be good enough. It doesn't have to be the machine precision. And so is it clear how we do this? So here I just have to maintain a bunch of things. So remember, at every step, I have to recompute a bunch of things. So I have to recompute the weights. But if I want to recompute the weights, not only do I need the previous iterate, but I need to know how the previous iterate impacts my means. So at each step, I have to recalculate mu i k by doing g prime. Remember, mu i k was just g inverse of xi transpose beta k. So I have to recompute that. And then I use this to compute my weights. I also use this to compute my y. So my y depends also on g prime of mu i k. I feed that to my weighted least squares engine. It spits out the u hat k that I add to my previous beta k. And that gives me my new beta k plus 1. So here's the pseudocode if you want to take some time to parse it. So here again, the trick is not much. It's just saying, if you don't feel like implementing Fisher scoring or inverting your Hessian at every step, then a weighted least squares is actually going to do it for you automatically. And that's just a numerical trick. There's nothing really statistical about this, except the fact that this closed form solution for each of the step reminded us of some least squares, except that there was some extra weights. So to conclude, we only need to know, of course, xy, the link function. Why do we need the variance function? I'm not sure we actually need the variance function. No, I don't know why I say this. You need phi, not the variance function. So where do you start, actually, from? So where do you start, actually? So clearly, if you start very close to your solution, you're actually going to do much better. And one good way to start, so for the beta itself, it's not clear what it's going to be. But you can actually get a good idea of what beta is by just having a good idea of what mu is, because mu is g inverse of xi transpose beta. And so what you could do is to try to set mu to be the actual observations that you have, because that's the best guess that you have for their expected value. And then you just say, OK, once I have my mu, I know that my mu is a function of this thing. So I can write g of mu and solve it using a least squares estimator. So g of mu is of the form x beta. So you just solve for, once you have your mu, you pass it through g, and then you solve for the beta that you want. And then that's the beta that you initialize with. And actually, this was your question from last time. As soon as I use the canonical link, Fisher scoring and Newton-Raphson are the same thing, because the Hessian is actually deterministic in that case. Which is because when you use the canonical link, h is the identity, which means that its second derivative is equal to 0. So this term goes away even without taking the expectation. So remember, the term that went away was of the form yi minus mu i divided by phi times h prime prime of xi transpose beta. That's the term that we said, oh, the conditional expectation of this guy is 0. But if h prime prime is already equal to 0, then there's nothing that changes. There's nothing that goes away. It was already equal to 0. And that always happens when you have the canonical link, because h is g prime is g b prime inverse. And g is equal to b. The canonical link is b prime inverse. So this thing is the identity. So the second derivative of f of x is equal to x is 0. My screen says end of show. So we can start with some questions. I just wanted to clarify. So iterative, what is the term for iterative? Reweighted least squares. Reweighted least squares is an implementation of the Fisherian scoring. That's an implementation that's just making calls to weighted least squares oracles. It's called an oracle sometimes. An oracle is what you assume the machine can do easily for you. So if you assume that your machine is very good at multiplying by the inverse of a matrix, you might as well just do Fisherian scoring yourself. It's just a way so that you don't have to actually do it. And those things are implemented, and I just said routinely, in statistical software. But they're implemented very efficiently in statistical software. So this is going to be one of the fastest ways you're going to have to solve to do this step, especially for large-scale problems. So if you were to do this in the simplest possible way, your iterations for the weighted, say, Fisher scoring is just multiplied by the inverse of the Fisherian information. Yeah, so it takes a bit of time. Whereas since you know you're going to multiply directly by something, if you just say, those things are not as optimized as solving least squares. Actually, the way it's typically done is by doing some least squares. So you might as well just do the least squares that you like. And there's also less, well, no, there's no. Well, there is less recalculation, right? Here, you would have to recompute the entire matrix of Fisherian information, whereas here you don't have to. You really just have to compute some vectors. And the vector of weights, right? So the Fisherian information matrix has, say, n choose 2 entries that you need to compute, right? It's symmetric, so it's order n squared entries. But here, the only things you update, if you think about it, are this weight matrix. So there's only the diagonal elements that you need to update. And this vector is in there also. So those are our size. There's 2n versus n squared. So there's much less thing to actually put in there. It does it for you somehow. Any other question? All right, so yeah? So if I have a data set of points, then I can only try to model it with least squares, right? Yeah, you can. And so this is setting my weight to be equal to the identity, specifically, right? Well, not exactly, right? Because the g also shows up in this correction that you have here, right? Yeah. I mean, I don't know what you mean by it. I'm just trying to say, are there ever situations where I'm trying to model a data set, and I would want to take my weight in a particular way? Yeah. OK. I mean, well, OK, there's the heteroscedastic case, for sure. So if you can actually compute those things. And more generally, I don't think you should think of those as being weights. You should really think of those as being matrices that you invert. And don't think of it as being diagonal, but really think of them as being like full matrices. So if you have, right, when we wrote weighted least squares here, this was really the w I said is diagonal. But all the computations really never really use the fact that it's diagonal. So what shows up here is just the inverse of your covariance matrix. And so if you have data that's correlated, this is where it's going to show up.