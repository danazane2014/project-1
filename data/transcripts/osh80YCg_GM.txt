 One and, OK, here's lecture sixteen, and if you remember, I ended up the last lecture with this formula for what I called a projection matrix. And, maybe I could just recap for a minute. What is that, what is that magic formula doing? For example, it's supposed to be, it's supposed to produce a projection. If I multiply by a b, so I take p times b, I'm supposed to project that vector b to the nearest point in the column space. OK, can I just, one way to recap is to take the two, extreme cases. Suppose the vector b is in the column space. Then what do I get when I apply the projection p? So I'm projecting into the column space, but I'm starting with a vector in this case that's already in the column space. So of course, when I project it, I get b again, right. And we want to, I want to show you how that comes out of this formula. Let me do the other, the other extreme. Suppose that vector is perpendicular to the column space. So imagine this column space as a plane, and imagine b as sticking straight up, perpendicular to it. What's the nearest point in the column space to b in that case? So what's the projection onto the plane, the nearest point in the plane, if the vector b that I'm looking at is got no component in the column space, it's sticking completely per- ninety degrees with it, then pb should be zero, right. So those are the two extreme cases. The average vector has a component p in the column space and a component perpendicular to it, and what the projection does is it kills this part and it preserves this part. OK. Can we just see why that's true? Just, just that formula ought to work. Let me start with this one. What vectors are in the, are perpendicular to the column space? Now how do I see that I really get zero? I have to think. What does it mean for a vector b to be perpendicular to the column space? So if it's perpendicular to all the columns, then it's in some other space. We've got our four spaces, so the reason I do this is it's perfectly using what we know about our four spaces. What's, what vectors are perpendicular to the column space? Those are the guys in the null space of A transpose, right? That's the first section of this chapter that's the key geometry of these spaces. If I'm perpendicular to the column space, I'm in the null space of A transpose. OK. So if I'm in the null space of A transpose and I multiply this big formula times b, so now I'm getting Pb, this is now the projection, Pb. Do you see that I get zero? Of course I get zero. Right at the end there. A transpose b will give me zero right away. So that's why that zero's here, because if I'm perpendicular to the column space, then I'm in the null space of A transpose and A transpose b is zilch. OK, what about the other possibility? How do I see that this formula gives me the right answer if b is in the column space? So what's a typical vector in the column space? It's a combination of the columns. How do I write a combination of the columns? So, so tell me, how would I write, you know, your everyday vector that's in the column space? It would have the form A times some x, right? That's what's in the column space, A times something. That makes it a combination of the columns. So these b's were in the, in the null space of A transpose, these guys in the column space, those b's are Ax's, right? If b is in the column space, then it has the form Ax. I'm going to stick that on the quiz or the final for sure. That you have to real- realize, because we've said it, like, a thousand times, that the things in the column space are vectors A times x. OK, and do you see what happens now if we use our formula? There's an A transpose A, gets canceled by its inverse, we're left with an A times x, so the result was Ax, which was b. Do you see that it works? This is that whole business, cancel, cancel, leaving Ax, and Ax was b. So that turned out to be b in this, in this case. OK, so geometrically what we're seeing is we're taking a vector- we've got the column space and perpendicular to that is the null space of A transpose, and our typical vector b is out here. There's zero, so there's our typical vector b, and what we're doing is we're projecting it to p. And the- and of course, at the same time, we're finding the other part of it, which is e. So the diff- the two pieces, the projection piece and the error piece add up to the original b. OK. That's, that's, like, what our matrix does. So this is p, p is, this p is A b, is, sorry, is p b, it's the projection, applied to b, and this one is, OK, that's a projection too. That's a projection down onto that space. What's a good formula for it? Suppose I ask you for the projection of the, of the projection matrix onto the, this space, this perpendicular space. So if this projection was p, what's the projection that gives me e? It's the, what I want is to get the rest of the vector, so it'll be just I minus p times b. That's a projection too. That's the projection onto the perpendicular space. OK. So if p is a projection, I minus p is a projection. If p is symmetric, I minus p is symmetric. If p squared equals p, then I minus p squared equals I minus p. It's just, the algebra is, is only doing what your, your, picture is, is completely telling you. It's the, but the algebra leads to this, expression. That expression for p, given, given, a basis for the subspace, given the matrix A whose columns are a basis for our column space. OK. That's recap, because you, you need to see that formula more than once. And now can I pick up on using it? So now, and I, and the, it's, it's like, let me do that again, I'll do, go, go right through a problem that I started at the end, which is, find a best straight line. You remember that problem? I, I picked a particular, set of points. They weren't especially brilliant. T equal one, two, three, the heights were one, two, and then two again. So they were, heights were that point, that point, which makes it look like I've got a nice forty-five degree line, but then the third point didn't lie on the line. And I wanted to find the best straight line. So I'm looking for the, the, this line, y equals c plus dt, and it's not going to go through all three points, because no line goes through all three points. So I'm going to pick the best line, the, the best being the one that makes the, the overall error as small as I can make it. Now I have to tell you, what is that overall error? And, because that determines what's the winning line. If we don't know, I mean, we have to decide what we mean by the error, and then we minimize, and we find the right, the best c and d. OK. So if, again, if, if I went through this, if I went through that point, I would solve the equation c plus d equals one. Because at t equal to one, I'd have c plus d and it would come out right. If, if, if it went through this point, I'd have c plus two d equal to two. Because at t equal to two, I would like to get the answer two. At the third point, I have c plus three d, because t is three, but the, the answer I'm shooting for is two again. So those are my three equations. And they don't have a solution. But they've got a best solution. And what do I mean by best solution? So let me take time out to remember what I'm talking about for best solution. So this is my equation A x equals b. A is this matrix, one one one, one two three. x is my only have two unknowns, c and d, and b is my right-hand side, one two three. OK. No solution. Three, I have a three by two matrix. I do have two independent columns. So I do have a basis for the column space. Those two columns are independent, they're a basis for the column space, but the column space doesn't include that vector. So best possible in this, what would best possible mean? The, the way that comes out to linear equations is I, I want to minimize the sum of these. I'm going to make an error here. I'm going to make an error here. I'm going to make an error there. And I'm going to sum and squ- square and add up those errors. So it's a sum of squares, it's a least squares solution I'm looking for. So if I, those errors are the difference between A x and b. That's what I want to make small. And the way I'm measuring this, this is a vector, right? This is e1, e2, e3, A x minus b, this is the, this is the e, the error vector, and small means its length. The length of that vector. That's what I'm going to try to minimize. And it's convenient to square. If I make something small, I make, this is a, this is a non- this is a never negative quantity, right? The length of that vector. The length will be zero ex- exactly when the, when I have the zero vector here. That's exactly the case when I can solve exactly, b is in the column space, all great. But I'm not in that case now. I'm going to have an error vector, e. What's this error vector in my picture? I guess what I'm trying to say is there's, there's two pictures of what's going on. There are two pictures of what's going on. One picture is in this, in this, is the three points and the line. And in that picture, what are the three errors? The three errors are what I missed by in this equation, so it's this, this little bit here, that vertical distance up to the line. There's one, sorry, there's one and there's c plus d, and it's that difference. Here's two and here's c plus 2d. So vertically, it's that distance. That's, that's, that, that little error there is e1. This little error here is e2. This little error coming up is e3. e3. And what's my overall error is e1 squared plus e2 squared plus e3 squared. That's what I'm trying to make small. Some statisticians, this is a big part of statistics. Fitting straight lines is a big part of science, and specifically statistics, where the right word to use would be regression. I'm doing regression here. Linear regression. And I'm, and I'm using this sum of squares as the measure of error. Again, some statisticians would be, they would say, OK, I'll solve that problem because it's the clean problem. It leads to a beautiful linear system. But they would be a little careful about least squares for the, in this case. If one of these points was way off, suppose I had a measurement at t equals zero that was way off. Well, would the straight line, would the best line be the same if I had this fourth point? Suppose I have this fourth data point. No, certainly the line would, it wouldn't be, that wouldn't be the best line. Because that line would have a giant error, and when I squared it, it would be, like, way out of sight compared to the others. So the, this, this would be called by statisticians an outlier. And they would not be happy to see the whole problem turned topsy-turvy by this one outlier, which could be a mistake after all. So they wouldn't, so they wouldn't like maybe squaring if there were outliers, they would want to identify them. OK. I'm not going to, I'm, I'm taking the, the, the, but the, by, I don't want to suggest that least squares isn't used. It's the most used. But it's not exclusively used because it's a little overcompensates for outliers. But because of that squaring. OK. So suppose we don't have this guy. We just have these three equations, and I want to make, minimize this error. OK. Now, what I said is there's two pictures to look at. One picture is this, is this one. The three points, the best line. And the errors. Now, on this picture, what are these points on the line, the points that are really on the line? So they're, they're points, let me call them p1, p2, and p3. Those are three numbers. So this, this height is p1, this height is p2, this height is p3. And what are those guys? Suppose those were the three values instead of, there's b1, everybody's seeing all these, sorry, my art is, as usual, not the greatest, but there's the given b1, the given b2, and the given b3. I promise not to put a single letter more on that picture. OK. There's, there's b1, p1 is the one on the line, and e1 is the distance between. And same at points two and same at points three. OK. So what's up? What's up with those p's? p1, p2, p3, what are they? They're the components, they lie on the line, right? They're the, they're the points which, if I, if, if instead of one, two, two, which were the b's, suppose I put p1, p2, p3 in here. I'll figure out in a minute what those numbers are, but I just want to get the picture of what I'm doing. If I put p1, p2, p3 in those three equations, what would be good about the, the three equations? I could solve them. A line goes through the p's. So the p1, p2, p3 vector, that's in the column space. That is a combination of these columns. It's the closest combination. It's this picture. See, I've got the two pictures, like here's the picture, that shows the points. This is a picture in a, in a blackboard plane. Here's a picture that's showing the vectors. The vector b, which is in this case, in this example, is the vector one, two, two. The column space is in this case spanned by the, well, you see a there. The column space of the matrix one, one, one, one, two, three. And this picture shows the nearest point. There's the, that point p1, p2, p3, which I'm going to compute before the end of this hour, is the closest point in the column space. OK. Let me ins- let me not, I don't dare leave it any longer. Like, can I just compute it now? So I want to compute find p. All right. Find p. Find x, which is cd, find p and p. OK, and I really should put these little hats on to, to remind myself that they're the estimated, the best, the best line, not the perfect line. OK. OK, how do I proceed? Let's just run through the mechanics. What's the equation for x? The equa- x hat. The equation for that is A transpose A x hat equals A transpose x. A transpose b. The most, I, I, I will venture to call that the most important equation in statistics. And in estimation. And whatever you're, wherever you've got error and noise, this is the estimate that you use first. OK. Whenever you're fitting things by a few parameters, that's the equation to use. OK, let's solve it. What is A transpose A? So I have to figure out what these matrices are. One one one, one two three, and one one one, one two three. That gives me some matrix. That gives me a matrix. What do I get out of that? Three, six, six, and one and four and nine, fourteen. OK. And what do I expect to see in that matrix? And I do see it, just before I keep going with the calculation. I expect that matrix to be symmetric. I expect it to be invertible. And near the end of the course, I'm going to say I expect it to be positive definite. But that's a, that's a future fact about this crucial matrix A transpose A. OK. And now let me figure A transpose B. So let me, can I tack on B as an extra column here? One two two. And tack on the extra A transpose B is, looks like five and one and four and six, eleven. So I think my equations are three C plus six D equals five, and six D plus four- oh, six C plus fourteen D is eleven. Can I just, for safety, see if I did that right? One one one times one two two is five. One two three, that's one four and six eleven. Looks good. These are my equations. That's my, my, they're called the normal equations. I'll just write that word down, because it, so I solve them. I solve that for C and D. I would like to, before I solve them, could I do one thing that's on the, that's just above here? I would like to, I'd like to find these equations from calculus. I'd like to find them from this minimizing thing. So what's the first error? The first error is what I missed by in the first equation. C plus D minus one squared. And the second error is what I missed in the second equation. C plus two D minus two squared. And the third error squared is C plus three D minus two squared. That's my overall squared error that I'm trying to minimize. OK. So how do, how would you minimize that? OK, linear algebra has, has given us the equations for the minimum. But we could use calculus, too. That's, that's a function of two variables, C and D, and we're looking for the minimum. So how do we find it? Directly from calculus? We take partial derivatives, right? We've got two variables, C and D. So take the partial derivative with respect to C and set it to zero, and you'll get that equation. Take the partial derivative with respect, I'm, I'm not going to write it all out, just, you will. The partial derivative with respect to D, it, you know, it's going to be linear. That's the beauty of least squares, that if I have the square of something, and I take its derivative, I get something linear. And this is what I get. So this is the derivative of the error with respect to C being zero, and this is the derivative of the error with respect to D being zero. Wherever you look, these equations keep coming. So now I guess I'm going to solve it. What will I do? I'll subtract, I'll do elimination, of course, because that's the only thing I know how to do. Two of these away from this would give me, let's see, six, so would that be two Ds equals one? Huh. So it wasn't, I, I, I was afraid these numbers were going to come out awful. But if I take two equa- two of those away from that, the equation I get left is two D equals one. So I think D is a half and C is whatever back substitution gives. Six D is three, so three C plus three is five. I'm doing back substitution now, right? Three, can I do it in light letters? Three C plus that six D is three equals five, so three C is two, so I think C is two thirds. One half and two thirds. So the best line, the best line is the best line is the constant two thirds plus one half T. And am I, is my picture more or less right? Let me write, let me copy that best line down again. Two thirds and a half. Let me, I'll put in the two thirds and the half. OK. So what's this P1? That's the value at T equal to one. At T equal to one, I have two thirds plus a half, which is, what's that, four six and three six, so P1, oh, I promised not to write another thing on this. I'll, look, I'll, I'll erase P1 and I'll put seven six. OK. And, yeah, it's above one and E1 is one sixth, right. You see it all, right? What's P2? OK. At the point T equal to two, where's my line here? At T equal to two, it's two thirds plus one, right? That's five thirds, two thirds, and T is two, so that's two thirds and one make five thirds, and that's, sure enough, that's smaller than the exact two. And then the final P3, when T is three, oh, what's two thirds plus three halves? It's the same as three halves plus two thirds. It's, so maybe four sixths and nine sixths, maybe thirteen sixths, OK, and again, look, oh, look at this, OK. You have to admire the beauty of this answer. This, what's this first error? So here are the errors, E1, E2, and E3. OK, what was that first error, E1? Well, if we decide the error's counting up, then it's one sixth. And the last error, thirteen sixths minus the correct two is one sixth again. And what's this error in the middle? Let's see, the correct answer was two, two, and we got five thirds, and it's the other direction minus one third, minus two sixths. That's our error vector. In our picture, in our other picture, here it is. We just found P and E. E is this vector, one sixth minus two sixths, one sixth, and P is this guy. Well, maybe I have the signs of E wrong. I think I have. Let me, let me fix them, because I would like this one sixth, I would like this plus the P to give the original B. I want P plus E to match B. So I want minus a sixth plus seven sixths to give the correct B equal one. OK. Now, OK, my, my, I'm going to take a deep breath here and ask, what do we know about this error vector E? You've seen now this whole problem worked completely through, and I even think the numbers are right. So this P, so let me, I'll write, I'll write out what, if I can put it down here, B is P plus E. B, I believe, was one two two. The nearest point had seven sixths, what were the others? Five thirds and thirteen sixths. And the E vector was minus a sixth, two sixths, one third, in other words, and minus a sixth. OK. Tell, just, like, tell me some stuff about these two vectors. Tell me something about those two vectors. Well, they had to be, right? Great. OK. What else? What else about those two vectors, the P, the projection vector P and the error vector E? What else do you know about them? They're perpendicular, right. Do we dare verify that? Can you take the dot product of those vectors? I'm, like, getting, like, minus seven over thirty-six. Can I change that to ten sixths? Oh, god, come out right here. Minus seven over thirty-six plus twenty over thirty-six minus thirteen over thirty-six. Thank you, god. OK. And what else should we know about that vector? Actually, we know, I got to say we know even a little more. This vector, E, is perpendicular to P, but it's perpendicular to other stuff, too. It's perpendicular not just to this guy in the column space. This is in the column space, for sure. This is perpendicular to the column space. So, like, give me another vector it's perpendicular to. Another, because it's perpendicular to the whole column space, not just to this, this particular projection that, that is in the column space, but it's perpendicular to other stuff, whatever's in the column space. So tell me another vector in the col- oh, well, I've written down the matrix, so tell me another vector in the column space. Pick a nice one. One one one. That's what everybody's thinking. OK. One one one is in the column space, and this guy is supposed to be perpendicular to one one one. Is it? Sure. If I take the dot product with one one one, I get minus a six, plus two six, minus a six, zero. And it's perpendicular to one two three, because if I take the dot product with one two three, I get minus one, plus four, minus three, zero again. OK. You see the -- I hope you see the two, pictures. The picture here for vectors and the picture here for the best line. And it's the same picture, just like it's in- this one's in the plane and it's showing the line. This one never did show the line. This, in this picture, c and d never showed up. In this picture, c and d were, you know, they determined that line. But the two are exactly the same. c and d is the combination of the two columns that gives p. OK. So that's, least squares. And the special but most important example of fitting by straight lines. So the homework that's coming then Wednesday asks you to fit by straight lines. So you're just going to end up solving the key equation. You're going to end up solving that key equation and then p will be a x hat. That's it. OK. Now, can I put in a little piece of linear algebra that I mentioned earlier, mentioned again, but I never did right? And I, I should do it right. It's the, it's about this matrix A transpose A. There. I was sure that that matrix would be invertible. And of course, I wanted to be sure it was invertible, because I planned to solve the system with that, with that matrix. So, and I announced, like, before the cha- as the chapter was just starting, I announced that it would be invertible. But now I, can I come back to that? OK. So, so what I said was that if A has independent columns, then A transpose A is invertible. And I would like to first, repeat that important fact, that that's the requirement that makes everything go here. It's the, it's this independent columns of A that, that guarantees everything goes through. And think why. Why does, why does this matrix A transpose A, why is it invertible if the columns of A are independent? OK. There's, I guess I'm, I'm, I'm wondering, so, so if it wasn't invertible, so I want to prove that. If it isn't invertible, then what? I want to reach, I want to follow that, follow that line of, of thinking and see what I come to. Suppose, so proof, suppose A transpose A x is zero. I'm trying to prove this. This is now two proofs. I, I, I don't, like, hammer away at too many proofs in this course. But this is, like, the central fact and it brings in all the stuff we know. OK. So I'll start the proof. Suppose A transpose A x is zero. What, what, and I, and I'm aiming to prove A transpose A is invertible. So what do I want to prove now? So I'm, I'm aiming to prove this fact. I'll use this, and I'm aiming to prove that this matrix is invertible. OK. So if I suppose A transpose A x is zero, then what conclusion do I want to reach? I'd, I'd like to know that x must be zero. I want to show x must be zero. To show now, to prove x must be the zero vector. Is that right? That's what we worked in the previous chapter to understand, that a matrix was invertible when its null space is only the zero vector. So that's what I want to show. How come if A transpose A x is zero, how come x must be zero? What's going to be the reasoning? Actually, I have two ways to do it. Let me show you one way. This is, here, trick. Take the dot product of both sides with x. So I'll multiply both sides by x transpose. x transpose A transpose A x equals zero. I shouldn't have written trick. That makes it sound like just a dumb idea. Brilliant idea, I should have put. OK. I'll just put idea. OK. Now, I got to that equation, x transpose A transpose A x equals zero, and I'm hoping you can see, the right way to look at that equation. What can I conclude from that equation? That if I have x transpose A --- well, what is x transpose A transpose A x? Does that do you see the- do you see what it- what it- what it's giving you? I'm- it's again going to be putting in parentheses. I'm looking at A x, and what am I seeing here? It's transpose. So I'm seeing here, this is A x transpose A x. Equaling zero. Now, if A x transpose A x, so like, let's call it y or something, if y transpose y is zero, what does that tell me? That the vector has to be zero, right? This is the length squared. That's the length of the vector A x squared. That's A x times A x. So I conclude that A x has to be zero. Well, I'm getting somewhere. Now that I know A x is zero, now I'm going to use my little hypothesis. Somewhere every mathematician has to use the hypothesis. Right? Now, if A has independent columns, and we've- we're at the point where A x is zero, what does that tell us? I could- I mean, that could be, like, a- a fill-in question on the final exam. If A has independent columns and if A x equals zero, then what? Please say it. x is zero. Right. Which was just what we wanted to prove. That- do you- do you see why that is? If A x equals zero, now we're using- here we use this was the square of something. So I- I'll put in little parentheses the- the- the observation we made. That was a square which is zero, so the thing has to be zero. Now we're using the hypothesis of independent columns at the- A has independent columns. If A has independent columns, this is telling me x is in its null space and the only thing in the null space of such a matrix is the zero vector. OK. So that's the- the argument and you see how it really used our understanding of the- of the, of the null space. OK. That's- that's great. All right. So where are we, then? That- that board is like the backup theory that tells me that this matrix had to be invertible because these columns were independent. OK. There's one case of independent- there's one case where the geometry gets even better. When the- when I- there's one case when columns are sure to be independent. And let me put that- let me write that down and that'll be the subject for next time. Columns are sure- are certainly independent, definitely independent if they're perpendicular. Oh, I've got to rule out the zero column. Let me give them all length one, so they can't be zero. If they are, if they are perpendicular unit vectors. Like the vectors one zero zero, zero one zero, and zero zero one. Those vectors are unit vectors, they're perpendicular, and they certainly are independent. And what's more, suppose they're- oh, that's so nice. I mean, what is A transpose A for that matrix? For the matrix with these three columns? It's the identity. So here's the key to the- to the lecture that's coming. If we're dealing with perpendicular unit vectors, and the word for that will be ortho- see, I could have said orthogonal, but I said perpendicular. Ortho- and this unit vectors gets put in as the word normal. Orthonormal vectors. Those are the best columns you could ask for. Matrices with- whose columns are orthonormal. They're perpendicular to each other, and they're unit vectors, well, they don't have to be those three. Let me do a final example over here. How about one at an angle like that, and one at ninety degrees? That vector would be cos theta sine theta, a unit vector, and this vector would be minus sine theta cos theta. That is our absolute favorite pair of orthonormal vectors. They're both unit vectors and they're perpendicular, that angle is ninety degrees. So, like, our job next time is first to see why orthonormal vectors are great, and then to make vectors orthonormal by picking the right basis. OK, see you. Thanks.