 We've been spending quite a few lectures so far discussing Szemeredi's regularity lemma, its applications, and various variants of the regularity lemma. So I want to spend one more lecture before moving on to a different topic to tell you about other extensions and other perspectives on Szemeredi's regularity lemma. So hopefully, you all will become experts of the regularity lemma, especially with practice, especially the next homework problem set, where there will be plenty of problems for you to practice using the regularity lemma. So one of the things I would like to discuss today is a hypergraph extension of the triangle removal lemma. So as we saw, the triangle removal lemma was one of the important applications of Szemeredi's graph regularity lemma. So that works inside graphs, but now let's go to hypergraphs. In particular, even in the case of three uniform hypergraphs, where so let's set some terminology. So an R-uniform hypergraph, or simply abbreviated as an R-graph, consists of a vertex set and an edge set, where the edge set consists of R tuples. So the edges are R element subsets of the vertex set. So R equals to 2 corresponds to the graph case. And you can talk about subgraphs, density, various counts, all analogously to how we did it for graphs. And you can imagine what the hypergraph removal lemma might look like. So let me write down the statement. For all R-graph H and epsilon bigger than 0, there exists a delta such that, so last time, there was some complaints about sentences going on too long. So let me try to cut sentences into smaller parts. So if G is an n-vertex R-graph with small number of copies of H, so H has subgraphs, then G can be made H-free. So so far, everything's still the same as in the graph case. But now, in the graph case, the number of edges is quadratic, at most quadratic. Here, it's at most n to the R. So I want to make this R-graph H-free by removing less than epsilon n to the R edges from G. So that's the statement of the hypergraph removal lemma. So it's an extension of the graph removal lemma. Any questions about the statement? So before discussing the proof, let me show you why you might care about this statement. And we used the triangle removal lemma to deduce Roth's theorem. So remember, there was this graph-derivative setup where we start with a 3-AP-free subset. We set up a graph. And then that graph has some nice properties that allows us to use a corollary of the triangle removal lemma, namely the corollary that says that if you have a graph where every edge sits on exactly one triangle, then it has a subquadratic number of edges. So we can do a similar type of deduction, showing that the hypergraph removal lemma implies Szemeredi's theorem. So that's what we'll do. So let's deduce Szemeredi's theorem from the hypergraph removal lemma. So recall Szemeredi's theorem says that for every fixed positive integer k, if A is a subset of 1 through N, that is k-AP-free, has no k-term arithmetic progressions, then the size of A is sublinear. Instead of illustrating how to do this proof for general k, I'm just going to do it for the case of k equals to 4. And you can look at the proof, and it will be clear how to generalize. So we'll just illustrate the proof for 4-AP's. Now, before even showing you what this proof looks like, you might wonder, do we really need the hypergraph removal lemma? Could it be that with the graph removal lemma and a more clever choice of a graph, you could prove Szemeredi's theorem using just that? So we set up some graph previously where triangles correspond to 3-AP's. Maybe you can set up some other graph where some other kinds of structure correspond to 4-AP's. And it turns out the answer is emphatically no. So it is actually, in some ways, there's a very good reason for this, that for, I mean, these are things which we might go into more when we discuss additive combinatorics. But 4-AP's is a pattern that's sometimes called complexity 2, whereas 3-AP's is a pattern which is called complexity 1. I won't go into the precise definitions of what this means. But the message is that you cannot prove the 4-AP theorem with just graph machinery, that you really have to use something stronger. There's a very real sense in which just the graph removal lemma is, or at least Szemeredi's theorem, Szemeredi's regularity lemma is not enough. And so we really do have to go to hypergraphs. And this extra layer of complexity, like in this word sense of a complexity, introduces also additional difficulties in there. So that theorem is significantly harder than the graph removal lemma. So I won't even show you anything that's close to a complete proof. But I will illustrate some of the ideas and highlight some of the difficulties today. But deducing Szemeredi's theorem from the hypergraph removal lemma is actually not so bad. So I will show you how to do that right now. So from the hypergraph removal lemma, even just for three uniform hypergraphs, even for tetrahedron, so instead of triangles, you can have tetrahedron, we can have the following corollary, analogous to what we had for triangles. So if G is a free graph such that every edge is contained in a unique tetrahedron, then G has subcubic number of edges. So completely analogous to the one we have for triangles. And the proof is identical. You read the proof, and everything works exactly the same way once you have the removal lemma. So by tetrahedron, I mean the complete graph on four vertices, a complete three graph on four vertices. So you have four vertices, and you look at all possible triples of vertices. So now let's prove Szemeredi's theorem, at least the 4-AP case. The general case is completely analogous. Of course, you have to go to higher order instead of three graphs. You have to look at r graphs. So just as in the proof of Roth's theorem, we're going to set up some particular three graph where let's look at a certain modulus, so m being 6n plus 1. I mean, the exact number is not so important here. I really just want that this number is bigger than 3n, and it's co-prime to 6. So we'll see some divisibilities by 2 and 3 that will be useful. So let's build a 4-partite three graph G, where the four vertex parts x, y, z, w, they all have m vertices each. And I'll show you what the edges are. So four vertices, little x and big X and so on. So here are the rules for putting the edges. I'll just tell you exactly what they are. So I put in an edge x, y, z if and only if the following expression, 3x plus 2y plus z lies in A. I put in the edge x, y, w if and only if 2x plus y minus w is in A. So x, z, w if and only if x minus z minus 2w is in A. And finally, y, z, w if and only if minus y minus 2z minus 3w is in A. So these are the rules for putting in edges of this hypergraph. So you have this hypergraph. And you might wonder, why did I choose these expressions? Well, if it's not clear yet, it will soon be very clear why we do this. Just as in the proof of Roth's theorem using triangle removal lemma, let's examine the tetrahedra in this three graph. So what are the tetrahedra? So notice that x, y, z, w is a tetrahedron. So if all three triples are present if and only if all four of these expressions lie in A. Well, just like in the proof of Roth's theorem, all four expressions form a 4AP. And the common difference is minus x, minus y, minus z, minus w. So they were chosen to satisfy this property. But furthermore, notice that I can't just put any expressions in. I put these expressions in with the very nice property that the i-th linear form does not use the i-th variable. So each expression really corresponds to an edge in this three graph. All right. But we started with a set A that is 4AP-free. It follows that you don't have this kind of configurations unless the common difference is 0. So the only tetrahedra correspond to these trivial 4APs. And just as in the proof of Roth's theorem that we saw from triangle removal lemma, the conclusion is that every edge lies in exactly one tetrahedron. And therefore, by the corollary, the number of edges is equal to little o of m cubed. But on the other hand, how many edges are there? So for each of the four conditions here, so for each of these four parts, the number of edges is, well, you get to choose x and y, whatever you want. And the last variable has A choices. A choices. All right, so this implies that the size of A is little o of m, and m is on the same order as n. And that proves the theorem. Any questions so far? Yeah. AUDIENCE 1. Why do you use the m as a component of the state? YUFEI ZHAO. Where do we use the condition that m is co-primed to 6? Anyone know the answer? AUDIENCE 2. To solve for the last variable, you have to divide by 2 or 3. YUFEI ZHAO. So to solve for the last variable, we have to maybe divide by 2 or 3. And to do that, you need to have co-primality with 6. So I'm hiding a bit of details here. Yeah. OK, but it's a great question. So if you work out the details of this statement here, every edge lies in exactly one tetrahedron. And also counting the number of edges, but actually more especially the first sentence, you need to actually do just a tiny bit of work. Any more questions? Yeah. So this deduction is the same deduction as the one that we did for triangles, but you have to come up with a slightly different set of linear forms. And usually, if you're given a specific pattern, you can play by hand and try to come up with this set of linear forms. You can think about also how to do this generally. And more generally, this hypergraph removal lemma, using this type of ideas, it allows you to deduce multidimensional Szemeredi theorem. So if you give me some pattern, then a subset of the integer lattice in the fixed dimension, avoiding that pattern, must have density going to 0. So we stated this in the very first lecture. And I won't spell all the details, but you can follow this kind of framework and get you that theorem. And I will post the problem where you're asked to do this for a specific pattern, namely that of a geometric square, an axis-aligned square. So if you have that pattern in Z2. OK, so we're thinking about how would you run this argument for that pattern in Z2. Yes? AUDIENCE 2 How close can the small o n be? YUFEI ZHAO So you're asking what is the rate this little o n gives. Let me address the case. Hold on to this question. I'll address it once I discuss what is known about hypergraph removal lemma. And that's a great question. And there's a lot of mystery surrounding what happens there. OK, questions? Any others? Great, so let's discuss this hypergraph removal lemma. And as I had warned you already at the beginning of lecture, this one is very difficult. So I mentioned in the very first lecture that the development of Szemeredi's regularity lemma was a stroke of ingenuity. But this one here, we saw that the proof, we did a proof of Szemeredi's graph regularity lemma in one lecture. And once you understand it, it's not so bad. You do the energy increment conceptually, it's not so bad. But that one there is actually incredibly difficult. It's incredibly difficult, both conceptually and technically. But I want to at least illustrate some of the ideas and give you some sense of the difficulty, like why is this difficult. So as we imagine, we have graph regularity. And to prove hypergraph removal, one would develop some kind of a hypergraph regularity method. And the basic idea in hypergraph regularity, or just regularity in general, is I give you some arbitrary graph or hypergraph. And you want to find some kind of partitioning, some kind of regularization into some bounded number of pieces, some bounded amount of data, so that that's a good approximation for the actual graph, just like in the graph regularity case. So let's try to do this. So what does this partition even look like? So here's an attempt. And of course, I call it attempt because eventually it will not work, but it's a very natural first thing to try. And maybe it's I shouldn't call it naive, because it's actually not. I mean, it's not a bad idea to begin with. So let's see. So suppose you are given a free graph G. I'm going to, just to help you remember, what's the uniformity of each graph, I will denote in parentheses in the superscript. So just to help you remember that this is a 3-graph. So in geometry, they also do this with manifolds. So if you put an n on top, it's an n manifold. But this 3 is for 3-graph. Suppose we partition the vertex set of this 3-graph similar to proof of Szemeredi's regularity lemma. So think about how the proof of Szemeredi's regularity lemma goes. So you have this partition. But in the proof, there is this iterative refinement. And each step, you say, well, I have this notion of regularity. If it doesn't satisfy the regularity, I can keep cutting things up further. So what's the notion of regularity that might get you some kind of vertex partition? Anyone think of a notion of a regularity for three uniform hypergraphs? Yeah. AUDIENCE 2. It's the same sort of thing if you have vertex partitions, like u, v, w. I think that's also something you can see in the density. YUFEI ZHAO. OK, so let me try to rephrase what you're saying. So if we have some notion of regularity, let's say I have three vertex sets, v1, v2, v3. And I want that the density between these three, they do not differ from if I restrict these vertex sets to subsets that are not too small. Does this make sense? So here, this d is the fraction of triples that are edges of the hypergraph. So this is the natural extension, natural generalization of the notion of regularity that we saw earlier for graphs. And indeed, it's a very natural notion. And it is a nice notion. And actually, if you use this notion, if you use more or less precisely what I've written, you can run through the entire proof of Szemeredi's graph regularity lemma and produce a regularity theorem that tells you, given an arbitrary three uniform hypergraph, you can decompose the vertex set in such a way that most triples of vertex sets have this property. So the same proof as Szemeredi's regularity lemma implies, so I won't write down the entire statement, but you get the idea. So for every epsilon, there exists M such that we can partition the vertex set into at most M parts, even equitable if you like, so that at most an epsilon fraction of triples of parts are not epsilon regular in the sense that I just said. So not only is the same proof. So you really look at the proof, and you get that. So so far, everything seems pretty good, pretty easy. So why did I say initially that actually hypergraph regularity is incredibly difficult? So what's not good about this one? Well, so remember, in our application of the regularity method, there were three steps. Partition, clean, and count. So partition, do partition, clean, or do some kind of cleaning. But counting, that's a big thing. And that's something that wasn't so hard. We had to do a little bit of work, but it wasn't so hard. And you can ask, is there a counting lemma associated to this regularity lemma? And the answer is emphatically no. And I want to convince you that for this notion of regularity, there is no counting lemma. Yes? AUDIENCE 1. Is this version true? YUFEI ZHAO. It is true. So you're asking, is this version true? So this statement is true with this definition. And you can prove it by literally rerunning the entire proof of Szemeredi's graph regularity lemma. So the regularity statement I've written down is true. But it is not useful. For example, it cannot be used to prove the tetrahedron removal lemma, because if you try to run the same regularity proof of the removal lemma, you run into the issue that you do not have a counting lemma. So why is it that you do not have a counting lemma? So let me show you an example. And keep in mind that the notions of regularity, they are supposed to model the idea of pseudorandomness, which is the topic we'll explore in a further length in the next chapter. But the idea of pseudorandomness is that I want some graph which is not random, but in some aspects look random. So this is an important concept in mathematics and computer science, and it's a very important idea. But of course, you can generate a pseudorandom object by just taking a random object. And it should hopefully satisfy some properties of pseudorandomness. So let's see what this notion of regularity, how it works even for random hypergraphs. What's a random hypergraph? So there are different ways to generate a random hypergraph. One way is to have a bunch of triples all appearing uniformly at random, independently at random. So I have a bunch of possible triples I can make as edges. Each one, I flip a coin. But there's a different way. Let me show you a different way to generate a random 3-graph. Let me give you two parameters, p and q. They are constants between 0 and 1. So let's build first a graph, so a random 2-graph, which I'll call G2. So this is just the Erdos-Renyi random graph, G and p, the usual one where you flip a coin for each edge so that each edge appears with probability p independently. And now I make the actual 3-graph that I want, G3, by including every triple, so every triangle of G2 as an edge. So here, an edge means a triple. An edge is three vertices in a hypergraph with probability q. So it's a two-step process. I first generate a random graph. And then I look at the triangles on top of that graph. And each triangle I include as a triple with probability q. If you like, q can be even 1. So I do a random graph. And my hypergraph is a set of triangles. And let's compare this construction to the more naive version of a random hypergraph, where we look at this hypergraph where I put in each triple appearing independently with probability p cubed q. So these are two different constructions of random hypergraphs. And you can check that they have basically the same edge density. So how many edges appear in the first one? Well, the density of triangles in G2 is p cubed. And each of those triangles appears an edge further with probability q. So they have similar edge densities. And furthermore, you can check that this condition here is true for both graphs. So both graphs satisfy this notion of epsilon regularity as just defined with high probability. So if you had a counting lemma, it should give you some prediction as to the number of tetrahedra that come directly from the densities. And in particular, they should be the same for these two constructions. But are they the same? So the density of tetrahedra, in the first case, actually, let's do the second case first in B. So what's the density of tetrahedra? So if I have four vertices, so each of those three edges appear uniformly at random independently. So the density of tetrahedra is just the edge density raised to the power of 4. What about the first one? So in the first one, to get a tetrahedra, the underlying graph needs to have a K4. So P raised to 6. And then on top of that, I want four Q's. So P raised to 6, Q raised to 4. And when P is different, these numbers are different. So this is an example showing why there is no counting lemma. Because you have two different graphs that have the same type of regularity and densities, but have vastly different densities of tetrahedra. Any questions about this example? It shows you at least why this naive attempt does not work, at least if you follow our regularity recipe. But in any case, it's good for something. So you don't have a counting lemma for tetrahedra. But actually, you can still salvage something. So it turns out there is a counting lemma if your graph H, if this R graph H is linear. So linear means every pair of edges intersecting at most one vertex. So for example, if you look at that hypergraph, where each line is an edge of triples, so that's a linear hypergraph, each pair intersecting at most one vertex. Tetrahedron is not linear. Two faces of a tetrahedron can intersect in two vertices. So we can try to prove that this is true. And actually, the proof is basically the same as the counting lemma that we saw for graphs. Yes? AUDIENCE 1 How many edges can a linear hypergraph have? YUFEI ZHAO How many edges can a linear hypergraph have? You mean given a bounded number of vertices. I'll leave you to think about it. Any more questions? But for the graph that we really care about, namely tetrahedra, which relates to Szemeredi's theorem, this method does not work. So what should we do instead? Let's come up with a different notion of regularity. And that's somewhat inspired by that example up there, where we need to look at not just triple densities between three vertex sets, but also what happens to triples that sit on top of a graph. So we should come up with some notion of an edge density on top of two graphs. So given A, B, and C being edge sets of a complete graph, so these are graphs, so A, B, and C, you should think of them as graphs, and a three-graph G, we can define this quantity, D of ABC, so there's always a hidden G, which I'll usually omit, to be diffraction of triples x, y, z, where x, y, z are such that they sit on top of ABC. So y, z lie in A, x, z lie in B, x, y lie in C. So diffraction of such triples that are actually triples of G. So in the case when ABC are the same, it's asking what fraction of triangles are edges of G. But you are allowed to use three different sets. So think of ABC as the red, green, blue, asking for fractions of red, green, blue triangles that are actually triples of the hypergraph. So now we can try to come up with some notion of regularity. And as you might expect at this point, it's not sufficient to partition the vertex set. Instead, we'll go further. We'll partition the edge set of the complete graph. So we'll partition the set of pairs of vertices. So let's partition the edge set of the complete graph as a union of graphs such that we would like a similar type of regularity condition, but for those types of densities, such that for most i, j, and k, such that we have lots of triangles on top of these three graphs in the partition. So for most i, j, k, such that this is the case, this partition, so this triple is regular in the sense that for all subgraphs with not too few copies, so not too few triangles on top of these a's, one has that the density, the triple density among the g's is similar to the triple density on top of the a's. So I'm doing some kind of partition where g1 is like that, g2 that, and g3 that. And what I'm saying is that if you take subsets of g1, g2, g3 so that there are still lots of triangles, and that's analogous to this condition of a i's not being too small, then counting the number of triples, the fraction of those triangles that are edges of g, that fraction is roughly the same when you pass down the subgraphs. Don't worry about the specific details, and I'm not going to try to give you the specific details. But think about an analogy to instead of partitioning the vertex set, we are partitioning the edge set of a complete graph. But actually, hypergraph regularity involves one more step, namely that we need to further regularize these g's using, so via partitioning the vertex set, similar to what happens in Szemeredi's graph regularity lemma, but actually more similar to the strong regularity lemma that we discussed last time. So the data of hypergraph regularity is not simply a partition of the vertex set, but it's twofold. One is a partition of the edge set of the complete graph, so a partition of the vertex pairs into pseudorandom graphs, so that the hypergraph G sits pseudorandomly on top. And furthermore, there's also a partition of the vertex set of G, so that the graphs in part 1 are extremely pseudorandom with respect to this partition. And this idea of extremely random we saw in last lecture, you have some sequence of epsilons that depend on how many parts you have in the first step of the regularity. Any questions? So yes? AUDIENCE 1 What happens with triples of G's that don't have a lot of triangles? YUFEI ZHAO What happens to triples of G's that do not have lots of triangles? So they are similar to, in graphs, you have these small sets of vertices. So you have to deal with them somehow. But I'm, again, leaving out all these technical details. And in fact, I'm writing down a very sketchy version of hypergraph regularity. You could write down a more precise version. You can find it in literature. In fact, you can find more than one version of the statement of hypergraph regularity in literature. And they're not all obviously equivalent. It actually takes a lot of work even to show that different versions of the statement are equivalent to each other. And it's still somewhat mysterious to what is the right, the most natural formulation of hypergraph regularity. That's something that I think we still do not yet have a satisfactory answer. There was a question earlier about bounds. So what kind of bounds do you get for hypergraph regularity? So let me address that issue now. So what kind of bounds do you get? Well, for Szemeredi's graph regularity lemma, the bound is a tower function, because we have to iterate the exponential, which comes out of the partitioning. And in hypergraph regularity, because of this extremely pseudorandom, so you are doing some kind of partition in the first stage, and then you are iterating that on top for the second stage, similar to how we did strong regularity in the last lecture. So the bounds for hypergraph regularity is also iterated tower, which we saw last time. And this is known as a wowser. So it's even worse than graph regularity. And just like in the case of graph regularity, this wowser type bound is necessary, at least for most statements, any of these useful statements of hypergraph regularity. What about the applications? So applications to multidimensional Szemeredi theorem. So first of all, to Szemeredi theorem, well, you can prove Szemeredi theorem this way, and you would get inverse wowser type bounds, which is not so great. But there are better proofs. So there are more efficient proofs quantitatively. So for Szemeredi's theorem, the best result for general k is due to Gowers, which tells you that A must be at most n over something that's log log n raised to power some constant c, depending on k. For k equals 3 and 4, you can do somewhat better. But for general k, this is the best bound so far. But for multidimensional patterns, it turns out that, well, historically, the first proof of the multidimensional Szemeredi theorem was done using ergodic theory, which has even worse bounds compared to this approach in that the ergodic theoretic proof gives no bounds, because it has to use compactness arguments. So they actually give no quantitative bounds. And one of the motivations for this hypergraph regularity method, the removal lemma, is to produce quantitative proof of multidimensional Szemeredi theorem. So in general, still, the best bounds come from this removal lemma, so hypergraph removal lemma. Although in special cases, and really not that many special cases, but really just the case of a corner, as we saw earlier, you have somewhat better bounds. So for a corner, you have bounds which have density like poly log log. But even for a geometric square, we do not know any Fourier analytic methods. We do not know other methods. And this is basically the best bound coming out of hypergraph regularity. And there are serious obstructions to trying to use Fourier methods to do other patterns, such as geometric square. Any questions? Yes. AUDIENCE 1 What are the bounds like for higher degree uniformity? Or are they still just Wowsers? So this is Wowsers for 3 uniform. And for 4 uniform, you iterate Wowsers. So you go up in Ackermann hierarchy. You iterate Wowsers. You get a 4 uniform hypergraph regularity lemma. And so on. OK? Great, let's take a short break. All right, so the second topic I want to discuss today is a different approach to proving Szemeredi's graph regularity lemma. And this is a good segue into our next topic, which is the next lecture, which is about pseudorandom graphs, and in particular, the idea of the spectrum eigenvalues, in particular, play a central role. So I want to consider a spectral approach, getting an alternative way to prove the Szemeredi regularity lemma. And if you're already sick of the regularity lemma at this point, so this will be the last topic on regularity lemma for now, although it will come up again later in this course when we discuss graph limits. But for now, this is the last thing I want to say. And just like the discussion about hypergraph regularity, it will be somewhat sketchy. So this idea has appeared in literature in the past, but it was popularized by many good things in life by Terry Tao's blog. So it's a good place to look up a discussion of what I'm about to say. OK, so we saw the proof of regularity lemma via this iterated partitioning and keeping track of our progress through the use of an energy. But here's a different perspective. Namely, if we start with a graph G, I can look at the adjacency matrix A sub G. So this is the n by n matrix, where n is the number of vertices whose ij-th entry is 0 if i is not adjacent to j and 1 if i is adjacent to j. So this is a pretty standard thing to look at to associate a graph to this matrix. So this graph here would be like that and so on. It's a real symmetric matrix, and that's always really nice. Symmetric matrices have lots of great properties that will be convenient to use. In fact, if you're like myself, if you're too used to working with symmetric matrices, you forget that some of these properties do not apply in general to non-symmetric matrices. But it is symmetric, so we're happy. So for symmetric matrices, we have a set of real eigenvalues. We have real eigenvalues and eigenvectors. And for now, let me enumerate the eigenvalues by lambda 1 through lambda n. So multiplicity included, and I sort them according to the size of their absolute value. So the spectral theorem tells us a decomposition. So here, again, we're using that A is real symmetric. So it tells us that this matrix A can be written as the sum coming from the eigenvalues and eigenvectors, where the ui's are the eigenvectors, but I can choose them so that they form an orthogonal basis, orthonormal basis. So they're all unit vectors. So when I say the spectrum, I mean this data, also specifically the set of eigenvalues. All right, so let's go through some basic properties of the spectrum. So first, how big can the lambdas be? So I claim that, well, so first of all, the sum of the squares of this lambdas is, let me not even call this a lemma, so it's observation. So the sum of the squares is this one. So this is the trace of A squared, which is also the sum of the squares of the entries. So here, I'm always using that A is real symmetric, sum of squares of entries of A. And I mean, in the case when you have A being an adjacency matrix, this is simply twice the number of edges, which is, at most, n squared. So that's always a good thing to remember. OK, so as a result, the i-th eigenvalue cannot be bigger than what? So you have i eigenvalues. So they're sorted in decreasing order. So the i-th eigenvalue cannot be too large. In particular, it cannot be larger than n over root i, because otherwise, the sum of the first i eigenvalues squared would exceed n squared. So these things, they do decay. So second observation is that if you have some epsilon and an arbitrary function, so this is known as a growth function. That's just a name. Don't worry about it. So we'll call f. So it's function from the positive integers to positive integers. And for convenience, I'm going to assume that f of j is always at least j for every j. There exists some C, which depends only on your epsilon and this growth function. So this growth function plays the same role as the sequence of decaying epsilons in the strong regularity lemma. So there exists some constant bound such that for every graph G and A G as above. So associated with it, the lambdas and the u's. There exists a j less than C such that if I sum up the eigenvalues squared for eigenvalues i indexed between j and C of j, this sum is fairly small. It's at most epsilon n squared. I'll let you ponder that for a second. So choose your favorite growth function. It can be as quickly growing as you can. It can be exponential, tower, or whatever. And it's saying that I can look up to a bounded point so that this stretch of spectrum squared is at most epsilon n squared. Question? AUDIENCE 1. C of j. AUDIENCE 2. AUDIENCE 3. AUDIENCE 4. AUDIENCE 5. AUDIENCE 6. C, OK, f of j. Thank you. Well, the statement hopefully will become clearer once I show you the proof. All right. OK, so here's how you would prove it. So you first let j1 equal to 1. And I obtain the subsequent j's by applying f to j. All right, so I claim that one cannot have this inequality violated for too many of these ji's. So one cannot have the sum going between jk and jk plus 1 for all k from 1 to 1 over epsilon. So maybe let's change this to 0. You cannot have this, because if you have this, then you sum up all of these inequalities. You would get that their total sum would exceed n squared, which would violate the inequality about the sum of the squares of the spectrum. And so therefore, so thus, to the claimed inequality, so this is true, star holds for sum j equal to ji, where jk, where k is less than 1 over epsilon. And this j, in particular, is less than, well, it's whatever it is. It's bounded. So it's less than f applied to itself at most 1 over epsilon times. So this should look somewhat familiar. And you'll see, so I'll ask you to think about later on how this proof of spectral proof of Szemeredi's graph regularity lemma compared to the proof that we saw earlier. And you should see where the analogous step is here. This is the density. It's the energy increment step. So what's the regularity decomposition? So I give you this graph. I give you this adjacency matrix. And I want to be able to find a partition. But there's a different way to view a partition. This is, I think, a important idea, which, again, is popularized by Terry Tell, that instead of looking at things as a regularity partition, we can view these ideas as regularity decompositions, namely, I will pick j as in the lemma. And I now write my adjacency matrix, A sub g, as a sum of three matrices, which we'll call A structured plus A small plus A pseudorandom, where A structured equals to the sum for basically that sum, this sum here, so this spectral decomposition, but only for the first j minus 1 eigenvalues. So those of you coming from, who have taken classes in something like statistics, might recognize this as a principal component analysis. So this has many names. It's a very powerful idea. You look at the top spectral data, and that should describe you most of the information that you care about about the graph or a matrix in general. The small piece is the sum, but only for i between j and f of j. And the pseudorandom piece is for i at least f of j. So we decompose this adjacency matrix into these three pieces. And the question now is, what does this have to do with Szemeredi's graph regularity lemma? So what do the individual components correspond to in the version of the regularity lemma that you've seen and are now familiar with? So here is what's going on. So I want to show you that this structured piece roughly corresponds to the partition. So this is the bounded partition. And the small piece roughly corresponds to the small fraction of irregular pairs. And the pseudorandom piece roughly corresponds to the idea of pseudorandomness between pairs. All right. First, to understand what the spectral data have anything to do with partitions, let me remind you of a basic fact about how the spectrum, how the eigenvalues of a real symmetric matrix relate to other properties of this matrix, and namely, this notion of a spectral radius, or sometimes called spectral norm. So far, I'm only going to discuss real symmetric matrices. So many of the things I will say are not true for if you're not in a real symmetric case. So the spectral radius, spectral norm of A is the largest eigenvalue of A in absolute value. And this quantity turns out to be equal to the operator norm, which is the norm of this A as a linear operator. Namely, it is the max, or the super, it turns out to be a max, of A v over length of Av divided by length of v. So if you hit it with a unit vector, how far can you go? So it's also equal to this bilinear form. If you hit it from left and right by unit vectors, how big can you get? So for real symmetric matrices, these quantities are equal to each other. And that will be an essential fact for relating the spectral data with combinatorial quantities. All right. So if you give me this decomposition, how can I produce for you a partition? Basically, you can look at a structure which has in its data a bounded number of eigenvectors. And by rounding, we can basically round these guys. So when you round the individual values by rounding the coordinate values. So let's pretend that they take only a bounded, so let's say a small number of values. So just to simplify things, in your mind, pretend for a second well, of course, this is far from the truth. Pretend for a second that these guys are 0, 1 valued. Of course, that's not going to be the case, but 0, 1 or plus minus 1 if you like. So this is definitely not true. But for the purpose of exposition, let's pretend this is the case. And you can more or less achieve it by rounding the individual values to their nearby closest multiple of something. Then the level sets of these top eigenvectors, they partition the vertex set into a bounded number of parts. So if you, for example, in the simplified version where you only have plus minus 1 values for these eigenvectors, then you have at most 2 to the j parts. But you might get some more because some epsilons, but OK, so for the purpose of illustration, let's not worry about that. And this is basically the regularity partition. I want to show that this set here has very nice properties, that they basically behave like the regularity partition we've gotten previously. So what I would like to show is that the other two parts, they do not contribute very much in the sense of our regularity partition. So for example, if you look at the pseudo-random piece, if I hit it left and right with indicator vectors of vertex sets, how big can this number get? So this number here is at most, well, the norm of u plus times the norm of this w, which is just, so let me write down. So the norm of indicator of u, norm indicator of w, multiplied by the operator norm of the pseudo-random part of A. But these two guys here, so they are most root n each. So this number here is at most n. But we know from our hypothesis on the pseudo-random part of A that the spectral norm is no more than this quantity over here. And by choosing F appropriately large, I can make sure that this number is extremely small. So F to be large compared to the number of parts in the partition. So this quantity is small. And this is basically the notion of epsilon regularity that you saw in the usual version, with the version of Szemeredi's regularity lemma I presented in the first version, in the very first lecture that we discussed regularity. This quantity here, it's something which measures the difference between. So for now, if you, for a second, ignore the middle piece. If you ignore the small piece, then this is precisely the difference between the actual densities between u and w and the predicted density between u and w. AUDIENCE 2 Why is there not a square root here? YUFEI ZHAO OK, question is, why is there a square root here? There should not be a square root here. Then it becomes square root of the, yeah, so there's no square root, but the length of this vector is the square root of the size of u, which is at most n. Yeah? AUDIENCE 2 Do you need this thing to be small, like in general, or just small in comparison to S squared? Because I guess, isn't like, abtex technically this constant function that you choose before you pick n? YUFEI ZHAO OK, the question is, do we need this? How small do we want this f of j to be? So I want this quantity to be quite a bit smaller than, let's say, OK, so basically, I want to be less than f of n squared, but f of n over the number of parts squared. Because this quantity is, let's say, the sizes of each part. So let me be not precise and say much less than. So this quantity here is the size of each part. And I want to think about the case when u and w, they lie inside each part, in which case, I want the difference to be much less than epsilon times the size of the part squared. Yeah? AUDIENCE 3 Is it true that this j is different based on the graph? YUFEI ZHAO Questions, is the j different based on the graph? Yes, and that's also the case for Szemeredi's regularity lemma. Szemeredi's regularity lemma, you don't know when you stop, but you know that you stop before a certain point. And finally, what's happening with a small part of A? So in A small, the sum of the squares of entries. So this also has a convenient name. It's called the Hilbert-Schmidt norm. So the sum of the squares of the entries, we basically saw this calculation earlier, is the sum of the squares of the eigenvalues, in which case, we've truncated all the other eigenvalues. So the only eigenvalues left are between j and index between j and f of j. And we chose j so that this number is small. So A small adds in a bunch of noise, adversarial noise, if you will, into your graph, but only a very small amount, a most epsilon amount. So it might destroy the epsilon regularity for, let's say, around epsilon fraction of pairs, but that's all it could do. So all but an epsilon fraction of your pairs will still be epsilon regular, and that is the consequence of Szemeredi's graph regularity lemma that we saw earlier. Yeah? AUDIENCE 2. Does the large f have to be special? YUFEI ZHAO. OK, so question, does the large f have to be special? The f should be chosen. If you want to achieve Szemeredi's graph regularity lemma, you should find this f so that basically, this inequality is true. So f should be quite a bit larger than the number of parts. But if you choose even bigger values of f, you can achieve more regularity. And this is akin to what happened with strong regularity. So there's this idea, if you iterate one version regularity, you can get a strong version of regularity. And there's some iteration happening over there. So if you choose your f to be a much bigger function of j, you can achieve a much stronger notion of regularity, which is similar and perhaps even equivalent to strong regularity that we discussed last time. So you get to choose what f you want to put in here. Yeah? AUDIENCE 3. Last question, how do you make it equitable? YUFEI ZHAO. The question is, how do you make it equitable? You can, OK, so let me not discuss that. So in this case, you can also, I mean, do very similar things to what we've done before, but to massage the partitions. So it's not entirely clear from this formulation. But the message here is that there's this equivalence between operator norm on one hand and combinatorial discrepancy on the other hand. And we'll explore this notion further in the next several lectures.