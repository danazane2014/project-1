 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So this is a big day mathematically speaking, because we come to this key idea, which is a little bit like eigenvalues, well a lot like eigenvalues, but different, because the matrix A now is usually rectangular. So for a rectangular matrix, the whole idea of eigenvalues is shot, because if I multiply A times a vector x in n dimensions, out will come something in m dimensions, and it's not going to equal lambda x. So Ax equal lambda x is not even possible if A is rectangular. And even if A is square, what are the problems? Just thinking for a minute about eigenvalues. The case I wrote up here is the great case, where I have a symmetric matrix, and then it's got a full set of eigenvalues and eigenvectors, and they're orthogonal, all good. But for a general square matrix, either the eigenvectors are complex, eigenvalues are complex, or the eigenvectors are not orthogonal. So we can't stay with eigenvalues forever. That's what I'm saying. And this is the right thing to do. A is now. So what are these pieces? So these are the left, and these are the right singular vectors. So this is the new word is singular. And in between go the, not the eigenvalues, but the singular values. So we've got the whole point now. You got to pick up on this. There are two sets of singular vectors, not one. For eigenvectors, we just had one set, the Q's. Now we have, for a rectangular matrix, we've got one set of left eigenvectors in m dimensions, and we've got another set of right eigenvectors in n dimensions. And numbers in between are not eigenvalues, but singular values. So these guys are, let me write what that looks like. This is, again, a diagonal matrix, sigma 2 to sigma r, let's say. So it's, again, a diagonal matrix in the middle, but the numbers on that diagonal are all positive or 0, and they're called singular values. So it's just a different world. So really, the first step I have to do, the math step, is to show that any matrix can be factored into u times sigma times v transpose. So that's the parallel to the spectral theorem that any symmetric matrix could be factored that way. So you're good for that part. We just have to do it to see what are u and sigma and v? What are those vectors and those singular values? OK, let's go. So the key is that A transpose A is a great matrix. So that's the key to the math is A transpose A. So what are the properties of A transpose A? A is rectangular again, so maybe m by n. A transpose is, so this was m by n, and this was n by m. So we get a result that's n by n. And what else can you tell me about A transpose A? It's symmetric. That's a big deal. And it's square. And well, yeah, you can tell me more now, because we talked about something, a topic, that's a little more than symmetric last time. The matrix A transpose A will be positive definite. Its eigenvalues are greater or equal to 0. And that will mean that we can take their square roots. And that's what we will do. So A transpose A will have a factorization. It's symmetric. It'll have like a Q lambda Q transpose. But I'm going to call it V lambda. No, yeah, lambda. I'll still call it lambda. V transpose. So these V's, what do we know about the eigenvectors? So these V's are eigenvectors of this guy. Square, symmetric, positive definite matrix. So we're in good shape. And what do we know about the eigenvalues of A transpose A? They are all positive. So the eigenvalues are, well, are equal to 0. And these guys are orthogonal. And these guys are greater or equal. And OK, so that's good. That's one of our, we'll depend a lot on that. But also, you've got to recognize that A A transpose is a different guy. A A transpose. So what's the shape of A A transpose? How big is that? Now I've got, what do I have? M by N times N by M. So this will be what size? M by M, different shape. But with the same eigenvalues. The same eigenvalues. So it's going to have some other eigenvectors. Of course, I'm going to call them U, because I'm going to go in over there. They'll be the same. Well, the same, yeah, I shouldn't. I have to say, when I say the same, I can't quite literally mean the very same, because this has got N eigenvalues, and this has got M eigenvalues. But the missing guys, the ones that are in one of them and not in the other, depending on the sizes, are 0's. So really, the heart of the thing, the non-zero eigenvalues are the same. Now, well, actually, I've pretty much revealed what the SVD is going to use. It's going to use the U's from here and the V's from here. But that's the story. You've got to see that story. OK, so fresh start on the singular value decomposition. What are we looking for? Well, as a factorization, so we're looking for, we want vectors V, so that when I multiply by V, so if it was an eigenvector, it would be Av equal lambda V. But now for A, it's rectangular. It hasn't got eigenvectors. So Av is sigma, the new singular value, times U. That's the first guy and the second guy and the rth guy. I'll stop at r, the rank. Oh, yeah. Is that what I want? Let me just see. Av is sigma U. Yeah, that's good. OK, so this is what takes the place of Ax equal lambda x. A times one set of singular vectors gives me a number times the other set of singular vectors. And why did I stop at r, the rank? Because after that, the sigmas are 0. So after that, I could stop at r. After that, the sigmas are 0, so after that, I could have some more guys, but they'll be in the null space 0 on down to Vn. So these are the important ones. So that's what I'm looking for. Let me say it now in words. I'm looking for a bunch of orthogonal vectors, V. So that when I multiply them by A, I get a bunch of orthogonal vectors, U. That is not so clearly possible, but it is possible. It does happen. I'm looking for one set of orthogonal vectors, V, in the input space, you could say, so that the Av's in the output space are also orthogonal. In our picture of the fundamental, the big picture of linear algebra, we have V's in this space and then stuff in the null space. And we have U's over here in the column space and some stuff in the null space over there. And the idea is that I have orthogonal V's here. And when I multiply by A, so multiply by A, then I get orthogonal U's over here, orthogonal to orthogonal. That's what makes the V's and the U's special. That's the property. And then when we write down, well, let me write down what that would mean. So I've just drawn a picture to go with those equations. That picture just goes with these equations. And let me just write down what it means. It means in matrix. So I've written it. Oh, yeah, I've written it here in vectors, one at a time. But of course, I'm going to put those vectors into the columns of a matrix. So A times V1 up to, let's say, Vr will equal, oh, yeah, it equals sigmas times U's. So this is what I'm after, is U1 up to Ur multiplied by sigma 1 along to sigma r. What I'm doing now is just to say I'm converting these individual singular vectors, each V going into a U, to putting them all together into a matrix. And of course, what I've written here is Av equals U sigma. Av equals U sigma. That's what that amounts to. Well, then I'm going to put a V transpose on this side. And I'm going to get to A equals U sigma V transpose, multiplying both sides there by V transpose. I'm kind of writing the same thing in different forms, matrix form, vector at a time form. And now we have to find them. Now I've used up boards saying what we're after, but now we've got to get there. So what are the V's and what are the U's? Well, the cool idea is to think of A transpose A. So you're with me what we're for. And now think about A transpose A. So if this is what I'm hoping for, what will A transpose A turn out to be? So big moment that's going to reveal what the V's are. So if I form A transpose A, so A transpose, I've got to transpose this guy. So A transpose is V sigma transpose U transpose, right? And then comes A, which is this, U sigma V transpose. So why did I do that? Why is it that A transpose A is the cool thing to look at to make the problem simpler? Well, what becomes simpler in that line just written? U transpose U is the identity, because I'm looking for orthogonal, in fact, orthonormal U's. So that's the identity. So this is V sigma transpose sigma V transpose. And I'll put parentheses around that, because that's a diagonal matrix. OK. What does that tell me? What does that tell all of us? A transpose A has this form. Now, we've seen that form before. We know that this is a symmetric matrix, symmetric and even positive definite. So what are the V's? The V's are the eigenvectors of A transpose A. This is the Q lambda Q transpose for that symmetric matrix. So we know the V's are the eigenvectors, V's the eigenvectors of A transpose A. And I guess we're also going to get the singular values. So the sigma transpose sigma, which will be the sigma squareds, are the eigenvalues of A transpose A. Good. Sort of by looking for the correct thing, U sigma V transpose, and then just using the U transpose U equal identity, we got it back to something we perfectly recognize. A transpose A has that form. So now we know what the V's are. And if I do it the other way, which what's the other way? Instead of A transpose A, the other way is? AUDIENCE 1, 2, 3. Look at AA transpose. And if I write all that down, the A is the U sigma V transpose, and the A transpose is the V sigma transpose U transpose. And again, this stuff goes away and leaves me with U sigma sigma transpose U transpose. So I know what the U's are, too. They're the eigenvectors of AA transpose. Isn't that a beautiful symmetry? You just A transpose A and AA transpose are two different guys now. So they have each has its own eigenvectors, and we use both. It's just right. And I just have to take the final step, and we've established the SVD. So the final step is to remember what I'm going for here. A times a V is supposed to be sigma times a U. See, what I have to deal with now is I haven't quite finished. It's just perfect as far as it goes, but it hasn't gone to the end yet. Because we could have double eigenvalues and triple eigenvalues and all those horrible possibilities. And if I have triple eigenvalues or double eigenvalues, then what's the deal with eigenvectors if I have double eigenvalues? Suppose a matrix has a, say, a symmetric matrix has a double eigenvalue. Let me just take an example. So symmetric matrix like, say, 1, 1, 5. Make it. Why not? What's the deal with eigenvectors for that matrix 1, 1, 5? So 5 has got an eigenvector. You can see what it is, 0, 0, 1. What about the eigenvectors that go with lambda equal 1 for that matrix? What's up? What would be eigenvectors for lambda equal 1? Unfortunately, there's a whole plane of them. Any vector of the form x, y, 0. Any vector in the xy plane would produce x, y, 0. So I have a whole plane of eigenvectors, and I've got to pick two that are orthogonal, which I can do. And then they have to be in the SVD. Those two orthogonal guys have to go to two orthogonal guys. In other words, there's a little bit of detail here. A little getting into this exactly what is. Well, actually, let me tell you the steps. So I use this to conclude that the V's, the singular vectors, should be eigenvalues. I concluded those guys from this step. Now, I'm not going to use this step so much. Of course, it's in the back of my mind, but I'm not using it. I'm going to get the U's from here. So U1 is A V1 over sigma 1. V UR is A VR over sigma R. You see what I'm doing here? I'm picking in a possible plane of things the one I want, the U's I want. So I've chosen the V's. I've chosen the sigmas. They are fixed for A transpose A. The eigenvectors are V's. The eigenvalues are sigma squared. And now, then, this is the U I want. Are you with me? So I had to give a I want to get these U's correct. And if I have a whole plane of possibilities, I've got to pick the right one. And now, finally, I have to show that it's the right one. So what is left to show? I should show that these U's are eigenvectors of AA transpose. And I should show that they're orthogonal. That's the key. I would like to show that these are orthogonal. And that's what goes in this picture. The V's, I've got orthogonal guys, because they're the eigenvectors of a symmetric matrix. Pick them orthogonal. But now, I'm multiplying by A. So I'm getting the U, which is AV over sigma for the basis vectors. And I have to show they're orthogonal. So this is like the final moment. Does everything come together right? And if I've picked the V's as the eigenvectors of A transpose A, and then I take these for the U's, are they orthogonal? So I would like to think that we can check that fact and that it will come out. Could you just help me through this one? I'll never ask for anything again. Just get the SVD for me. OK. So I would like to show that A, that U1, that's so let me put up what I'm doing. I'm trying to show that U1 transpose U2 is 0. They're orthogonal. So U1 is AV1 over sigma 1. That's transpose. That's U1. And U2 is AV2 over sigma 2. And I want to get 0. The whole conversation is ending right here. Why is that thing 0? The V's are orthogonal. We know the V's are orthogonal. They're orthogonal eigenvectors of A transpose A. Let me repeat that. The V's are orthogonal eigenvectors of A transpose A, which I know we can find them. Then I chose the U's to be this, and I want to get the answer 0. Are you ready to do it? We want to compute that and get 0. So what do I get? We just have to do it. So I can see that the denominator is that. So is it V1 transpose A transpose, right, times AV2? AV2, and I'm hoping to get 0. Do I get 0 here? You hope so. V1 is orthogonal to V2, but I've got A transpose A stuck in the middle there. So what happens here? How do I look at that? V2 is an eigenvector of A transpose A. Terrific. So this is V1 transpose, and this is the matrix times V2. So that's sigma 2 transpose V2, isn't it? It's the eigenvector with eigenvalue sigma 2 squared times V2, divided by sigma 1, sigma 2. So the A's are out of there now. So I've just got these numbers, sigma 2 squared. So that would be sigma 2 over sigma 1. I've accounted for these numbers here, times V1 transpose V2. And now what's up? They're orthonormal. We've got it. That's 0. That is 0 there. Yeah. So not only are the V's orthogonal to each other, but because they're eigenvectors of A transpose A, when I do this, I discover that the A V's are orthogonal to each other over in the column space. So orthogonal V's in the row space, orthogonal A V's over in the column space. That was discovered late, much long after eigenvectors. And it's an interesting history. And it just comes out right. And then it was discovered, but not much used for, oh, 100 years probably. And then people saw that it was exactly the right thing. And data matrices became important, which are large rectangular matrices. And we have not, oh, I better say a word, just a word here, about actually computing the V's and the sigmas and the U's. So how would you actually find them? What I most want to say is you would not go this A transpose A route. Why is it like a big mistake? If you have a matrix A, say, 5,000 by 10,000, why is it a mistake to actually use A transpose A in the computation? We used it heavily in the proof. And we could find another proof that wouldn't use it so much. But why would I not multiply these two together? It's very big, very expensive. It adds in a whole lot of round off. You have a matrix that's now, its vulnerability to round off errors is squared. That's called its condition number gets squared. And you just don't go there. So the actual computational methods are quite different. And we'll talk about those. But the A transpose A, because it's symmetric positive definite, made the proof so nice. That's what you've seen the nicest proof, I'd say, of the. And now I should think about the geometry. So what does A equal U sigma? Maybe I take another board, but it will fill it. But it's a good U sigma V transpose. So it's got three factors there. And I would like each factor is kind of a special matrix. U and V are orthogonal matrix. So I think of those as rotations. Sigma is a diagonal matrix. I think of it as stretching. So now I'm just going to draw the picture. So here's unit vectors. And the first thing, so if I multiply by x, this is the first thing that happens. So that rotates. Here's x's. Then V transpose x's, that's still a circle. Length didn't change for those when I multiply by an orthogonal matrix. But the vectors turn. It's a rotation. Could be a reflection, but let's keep it as a rotation. Now, what does sigma do? So I have this unit circle. I'm in 2D. So I'm drawing a picture of the vectors. These are the unit vectors in 2D, xy. They got turned by the orthogonal matrix. What does sigma do to that picture? It stretches, because sigma multiplies by sigma 1 in the first component, sigma 2 in the second. So it stretches these guys. And let's suppose this is number 1 and this is number 2. This is number 1 and this is number 2. So sigma 1, our convention is sigma 1, we always take sigma 1 greater or equal sigma 2, greater or equal whatever, greater or equal sigma rank. And they're all positive. And the rest are 0. So sigma 1 will be bigger than sigma 2. So I'm expecting this ellipse, circle goes to an ellipse when you stretch the, I didn't get it quite perfect, but not bad. So this would be sigma 2v2, sigma 1v1. And this would be sigma 2v2. And we now have an ellipse. So we started with x's in a circle. We rotated, we stretched, and now the final step is take these guys and multiply them by u. So this was the sigma v transpose x. And now I'm ready for the u part, which comes last, because it's at the left. And what's the picture now? What does u do to the ellipse? It rotates it. It's another orthogonal matrix. It rotates it somewhere, maybe there. And now we see the u's, u2 and u1. Maybe they're there. Maybe I, well, let me think about that. Basically, that's right. So this SVD is telling us something quite remarkable, that every linear transformation, every matrix multiplication factors into a rotation times a stretch times a different rotation, but possibly different. And actually, when would the u be the same as the v? Here's a good question. When is u the same as v? When are the two singular vectors just the same? AUDIENCE 1. A squared? PROFESSOR STRANGE. Because a would have to be square. And we want this to be the same as q lambda q transpose, if they're the same. So the u's would be the same as the v's when the matrix is symmetric. And actually, we need it to be positive definite. Why is that? Because our convention is these guys are greater or equal to 0. These guys, if it's going to be the same, then so for a positive definite symmetric matrix, the s that we started with is the same as the a on the next line. Yeah. The q is the u. The q transpose is the v transpose. The lambda is the sigma. So those are the good matrices, and they're the ones that you can't improve, basically. They're so good, you can't make a positive definite symmetric matrix better than it is. Well, maybe diagonalize it or something. OK. Now, I think I'd like one question here that helps me, anyway, to keep this figure straight. I want to count the parameters in this factorization. So I'm 2 by 2. I'm 2 by 2. So A has four numbers, A, B, C, D. Then I guess I feel that four numbers should appear on the right-hand side. Somehow, the u and the sigma and the v transpose should use up a total of four numbers. So we have a nice counting match between the left side that's got four numbers, A, B, C, D, and the right side that's got four numbers buried in there somewhere. So can we dig them out? How many numbers in sigma? That's pretty clear. Two. Sigma 1 and sigma 2, the two eigenvalues. How many numbers in this rotation? Rotation. So if I had a different color chalk, I would put 2 for the number of things accounted for by sigma. How many parameters does a 2 by 2 rotation require? 1. And what's a good word for that 1? That one parameter. It's like I have a cos theta, sine theta, minus sine theta, cos theta. There's a number theta. It's the angle it rotates. So that's one guy to tell the rotation angle, two guys to tell the stretchings, and one more to tell the rotation from you, adding up to four. So those match up with the four numbers, A, B, C, D, that we start with. Of course, it's a complicated relation between those four numbers and rotations and stretches, but it's 4 equals 4 anyway. And I guess if you did 3 by 3's, oh, 3 by 3's, what would happen then? So let me take 3. Do you want to care for 3 by 3's? It's sort of satisfying to get 4 equal 4. But now what do we get 3 by 3? We got how many numbers here? 9. So where are those 9 numbers? How many here? That's usually the easy 3. So what's your guess for the how many in a rotation? In a 3D rotation, you take a sphere and you rotate it. How many numbers to tell you what you did? 3. We hope 3. Yeah, it's going to be 3, 3, and 3 for the three-dimensional world that we live in. So people who do rotations for a living understand that a rotation in 3D. But how do you see this? AUDIENCE 1. Roll, pitch, and yaw. PROFESSOR STRANGE-WALSH. Sorry? Roll, pitch, and yaw. That sounds good. I mean, it's three words, and we've got it, right? OK, yeah, roll, pitch, and yaw. Yeah, I guess a pilot, hopefully, knows about those three. Yeah, yeah, yeah. Which is roll, when you sort of like forward and back? Does anybody, anybody? Roll, pitch, and yaw. Yeah, you guys must know. Pitch is the up and down one. Pitch is the up and down one, OK. Roll is like a barrel hole. Yes, sorry. And yaw is your side-side rotation. Oh, yaw, you stay in a plane, and you, OK, beautiful. Right, right. And that leads us to our four, four dimensions. What's your guess on 4D? Well, we could do the count again. If it was 4 by 4, we'd have 16 numbers there. And in the middle, we always have an easy time with that. That would be 4. So we've got 12 left to share out. So 6, somehow, 6 angles in four dimensions. Well, we'll leave it there. Yeah, yeah, yeah, OK. OK, so there's the SVD, but without an example. Examples, I would have to compute A transpose A and find it. So the text will do that, does it for a particular matrix. Oh, yeah, the text does it for a matrix 3, 4, 0, 5 that came out pretty well. A few facts we could learn, though. So if I multiply all the eigenvalues together, then for a matrix A, what do I get? I get the determinant. What if I multiply the singular values together? Well, again, I get the determinant. You can see it right away from the big formula. Take determinants. Take determinants. Well, assuming the matrix A is square, so it's got a determinant. Then I take determinants, the determinant of this product. I can take the separate determinants. That has determinant equal to 1. An orthogonal matrix, the determinant is 1. And similarly here, so the product of the sigmas is also the determinant. So the product of the sigmas is also the determinant. The product of the sigmas here will be 15. But you'll find that sigma 1 is smaller than lambda 1. So here are the eigenvalues. Lambda 1 less or equal to lambda 2, say. But the singular values are outside them. But they still multiply. Sigma 1 times sigma 2 will still be 15. And that's the same as lambda 1 times lambda 2. But overall, computing the examples of the SVD take more time because, well, yeah, you just compute A transpose A. And you've got the V's, and you're on your way. And you have to take the square roots of the eigenvalues. So that's the SVD as a piece of pure math. But of course, what we'll do next time, starting right away, is use SVD. And let me tell you even today the most important pieces of the SVD. So what do I mean by pieces of the SVD? I've got one more blackboard still to write on. So here we go. So let me write out A is the U's times the sigmas, sigmas 1 to R times the V's, V transpose, V1 transpose down to VR transpose. So those are across. Yeah, actually, what I've written here, so you could say there is a large, big economy. There is a smaller size SVD that has the real stuff that really counts. And then there's a larger SVD that has a whole lot of 0's. So this would be the smaller one. M by R. This would be R by R. And these would all be positive. And this would be R by N. So that's only using the R non-zeros. All these guys are greater than 0. Then the other one we could fill out to get a square orthogonal matrix, the sigmas, and square V's, V1 transpose to VN transpose. So what are the shapes now? This shape is M by M. It's a proper orthogonal matrix. This one also, N by N. So this guy has to be, this is the sigma now. So it has to be what size? M by N. That's the remaining space. So it starts with the sigmas. And then it's all 0's, accounting for null space stuff. Yeah, so you should really see that these two are possible. That all these 0's, when you multiply out, just give nothing. So that really the only thing that non-zero is in these bits. But there is a complete one. So what are these extra U's there in the null space? Of A transpose or A transpose A. Yeah, so two sizes. The large size and the small size. But then the things that count are all in there. So I was going to do one more thing. Let me see what it was. So this is section 1.8 of the notes. And you'll see examples there. And you'll see a second approach to the finding the U's and B's and sigmas. I can tell you what that is. But maybe just to do something nice at the end, let me tell you about another factorization of A that's famous in engineering. And it's famous in geometry. So this is NEA is U sigma B transpose. We've got that. Now, the other one that I'm thinking of, I'll tell you its name. It's called the polar decomposition of a matrix. And all I want you to see is that it's virtually here. So a polar means what's polar? And for a complex number, what's the polar form of a complex number? AUDIENCE 1. e to the i theta? GILBERT STRANGEVOCESTEIN It's e to the i theta times r. A real guy. So the real guy r will translate into a symmetric guy. And the e to the i theta will translate into? So what kind of a matrix reminds you of e to the i theta? AUDIENCE 1. Orthogonal. GILBERT STRANGEVOCESTEIN Orthogonal. Size 1. So orthogonal. So that's a very kind of nice. Every matrix factors into a symmetric matrix times an orthogonal matrix. And I have, of course, described these as the most important classes of matrices. And here we're saying every matrix is a S times a Q. And I'm also saying that I can get that quickly out of the SVD. So I just want to do it. So I want to find an S and find a Q out of this. So to get an S, so let me just start it. U sigma. But now I'm looking for an S. So what shall I put in now? I'd better put in, if I've got U sigma something and I want it to be symmetric, I should put in? AUDIENCE 1. U transpose would do it. But then if I put in U transpose, I've got to put in U. So now I've got U sigma. U transpose U is the identity. Now I've got to get V transpose. And have I got what the polar decomposition is asking for in this line? So yeah. I think, what have I got here? Where's the S in this? So you see, I took the SVD and I just put the identity in there. Just shifted things a little. And now where is the S that I can read off? First rate. That's an S. That's a symmetric matrix. And where's the Q? Well, I guess we can see where the Q has to be. It's here. So just by sticking U transpose U and putting the parentheses right, I recover that decomposition of a matrix, which in mechanical engineering language tells me that any strain can be, which is like stretching of an elastic thing, has a symmetric kind of a stretch and an internal twist. So that's good. Well, this was three, six, nine boards filled with matrices. Well, it is 18.065, so maybe that's all right. But the idea is to use them on a matrix of data. And I'll just tell you the key fact. The key fact, if I have a big matrix of data, A, and if I want to pull out of that matrix the important part. So that's what data science has to be doing. Out of a big matrix, some part of it is noise. Some part of it is signal. I'm looking for the most important part of the signal here. So I'm looking for the most important part of the matrix. In a way, the biggest numbers, but of course, I don't look at individual numbers. So what's the biggest part of the matrix? What are the principal components? Now we're really getting in. If this could be data and we want to do statistics, so we want to see what has high variance, what has low variance, we'll do these connections with statistics. But what's the important part of the matrix? Well, let me look at the U sigma V transpose here. Yeah, let me look at it. So what's the one most important part of that matrix? The rank 1, it's a rank 1 piece. So when I say a part, of course, it's going to be a matrix part. So the simple matrix building block is like a rank 1 matrix, a something, something transpose. And what should I pull out of that as being the most important rank 1 matrix that's in that product? So I'll erase the 1.8 while you think. What do I do to pick out the big deal, the thing that the data is telling me first? Well, these are orthonormal. No one is bigger than another one. These are orthonormal. No one is bigger than another one. But here I look here. Which is the most important number? Sigma 1. Sigma 1. So the part I pick out is this biggest number times its row times its column. So it's U1 sigma 1 V1 transpose is the top principal part of the matrix A. It's the leading part of the matrix A. It's the biggest rank 1 part of the matrix is there. So computing those three guys is the first step to understanding the data. So that's what's coming next is, and I guess tomorrow since. They moved MIT declared Tuesday to be Monday. They didn't change Wednesday. So I'll see you tomorrow for the principal components.