 OK. So at the end of last time, let me just restate the theorem that we proved, which was the Weierstrass M-test, which I'll state in briefer form now as the following. Suppose fj are a sequence of functions from some subset S of R, so that for all j, there exists non-negative number m sub j, such that two things hold. First off, these m sub j's dominate the f sub j's. f sub j of x is less than or equal to m sub j. And two, these m sub j's are summable. OK? Then the conclusion is the sequence of partial sums converges uniformly. So there exists some function f from S to R, so that the partial sums converge uniformly. So this is a sequence of functions built up by the fj's so by just taking the sum of the first j equals 1 to n. All right, and so for example, and I kind of went through this last time. So example, if fj of x equals our old friend, cosine of 160j x over 4 to the j, and let's say on R, so S is R, then a couple of things. fj of x for all x is less than or equal to 4 to the minus j, because cosine of whatever you put into it is always bounded by 1. And since this converges, this implies that this series, which I'm writing in this way, which you should think of as a limit of functions of the partial sums, which are functions, converges uniformly on R. OK? Not only that, this function defined by I take x, I stick it into here, is continuous. Because last time we proved that the uniform limit of continuous functions is continuous. This function here is the uniform limit of the partial sums, which is just finitely many cosines of things. So that's continuous. So this gives another proof of one of the things we did by hand when we spoke about differentiability, namely that this function that we considered was continuous. OK. So let's go back to power series, which was our original motivation. So I'm going to be saying that a series, so let me, again, make this perfectly clear. When I say, if you like, make this a definition or really this is just, I was using this terminology up there in that theorem when I was talking about this example afterwards. But when I say a series involving functions converges uniformly, I mean the partial sums converge uniformly. So it means there exists a function f such that all of these are from s to r as well, such that just so that there's no ambiguity. So if I have a sequence of functions f sub j and I form their series and I say that converges uniformly, that means there exists some function f so that the sequence of partial sums, these are now functions, converges uniformly to the function f. That's what power series are. They're expressions involving a free variable x, and therefore you can think of them as a series involving functions. So let me just state the following theorem about when we have uniform convergence. So let's suppose we have a power series with radius of convergence rho, which I will recall is defined to be the limit as j goes to infinity, aj 1 over j inverse. If this limit is 0, we say that the radius of convergence is infinity. So this is a finite number. This is a meaningful expression. If this is 0, then this is shorthand for saying the radius of convergence is infinity. So let's assume this is. Then as long as I stay strictly inside the radius of convergence, I have uniform convergence. This is the statement of the theorem. Then for all r and 0 rho, this power series, now thought of as a series of functions of x, converges uniformly on x0 minus r, x0 plus r. So the picture is we have some radius of convergence, x0, x0 plus rho, x0 minus rho. If we take any closed interval basically inside of that, strictly inside of that interval, then we have uniform convergence of the power series. So we'll prove this using the Weierstrass M-test. So let r be in 0 rho. Then for all j. And so the Weierstrass M-test is for j equals starting from 1 and going to infinity. But you don't have to, just like for series, we don't have to start at 1. It can start at 0. So for all j union 0, what do we have? And for all x in this interval where we want to show uniform convergence, we have that this function a sub j, x minus x0 to the j, an absolute value. What is this bounded by? This is bounded by absolute value of aj and then j. And because x is in this interval, its distance to x0 is less than or equal to r. So r to the j. So this will be my Mj that I'll use for the Weierstrass M-test. Let me put that in parentheses because I may not use that Mj. So these individual functions, just polynomials, are bounded by this number for each j. And now I want to see if the series involving these numbers converges. So let me apply the root test. And we have this is equal to, now this r to the j to the 1 over j just becomes r. And it can just come out of this limit. And I pick up a to the 1 over j. And this is equal to 1 over rho. So this is equal to, if you like, I can put r over rho as long as rho is less than infinity, meaning if the radius of convergence is less than infinity and 0. And what do we notice? Now r is less than rho. r is coming from the closed interval 0 up to the rho, not including rho. So this number, so this is always less than 1. But this number is always less than 1, too, as long as r is less than rho, which implies that the series sum from j equals 0 to infinity of aj r to the j converges. So now we have the sequence of functions, the polynomials that we use to build up our power series, each of them bounded by this number, these numbers, a sub j absolute value times r to the j. And these numbers are summable. The series converges. So by the Weierstrass M test, we can see that this implies that the power series converges uniformly on this interval. So as long as we stay strictly inside the radius of convergence, we have uniform convergence of the power series. And therefore, using the theorems that we proved before, we can differentiate and integrate, term by term, the power series, which follows immediately from this previous theorem and then what we proved last time, which I'll state now. Power series with radius of convergence rho positive. So let me write it this way. And the first is, for all c, inside x0 minus r, x0 plus r, the function given by the power series, which I'm not going to call this f or anything. I'm just going to refer to the power series directly. This is differentiable at c. And to compute the derivative, you can just do it term by term, meaning I can take the derivative inside. And I dx of j equals 0 to infinity, a sub j, evaluated at x equals c. This is equal to simply differentiating term by term. OK? And number two, for all a, b, for all a less than b, well, with x0 minus rho less than a less than b less than x0 plus rho. So here I'm sticking to strictly with inside the radius of convergence. There's the x0 plus rho, x0 minus rho. And so now I'm going to integrate over some interval inside. Then I can integrate term by term. The integral from a to b, sum from j equals 0 to infinity of a sub j, x minus x0 j dx equals, and I'll just write it this way, sum from j equals 0 to infinity of. OK? And in fact, just to really emphasize that we're interchanging limits here, let me instead write this equivalently as sum from j equals 0 to infinity of j d by dx of a sub j, x minus x0, j evaluated at x equals c. OK? So for power series, I can interchange the limits. OK? The limit being taken a derivative and the sum, I can interchange. And then I can also interchange the integration and the sum, as long as I stay within the radius of convergence. OK? And this statement here is a pointwise. OK, so let's see why one is the case. Two follows immediately from what we've proven. And so this previous theorem and the theorem we have about integration and uniform convergence. So what about one? So first off, we know we have uniform convergence of the series strictly inside of the radius of convergence. So how about we need to check to see if the formal derivative also has radius of convergence as well equal to rho. So I claim that the radius convergence of the derivatives, which equals, I can write it this way. No. So the derivative of this guy is j times a sub j times x minus x0 to the j minus 1. So just shifting indices, this is equal to times j plus 1 times x minus x0 to the j. So our claim is that this power series here has radius of convergence equal to rho, the original radius of convergence. So rho is the radius of convergence of, why did I come over here? I still had a board left. OK, sorry about that. The original power series has radius of convergence rho. What we need to show to prove part one is that the radius of convergence of the derivatives of taking the derivatives inside also has radius of convergence rho. Because then this would imply that this power series, which is the derivative of that one, converges uniformly on the same set. So then we have uniform convergence of the power series and uniform convergence of the derivative of the power series. So by the theorem we proved in the last lecture, we would have that the derivative of the power series equals the power series of the derivatives. So I'm just going to focus on this claim. And this, again, just follows from what we know about limits. And so we compute that if we take the limit as j goes to infinity, now these are the coefficients for the new power series. So aj plus 1, j plus 1, raised to the 1 over j, this is equal to limit j goes to infinity of a sub j plus 1, now raised to the 1 over j plus 1, j plus 1, 1 over j plus 1. Now all raised to j plus 1 over j. Now this was a special limit that we looked at last time. The limit as j goes to infinity, not last time, but way back when we were looking at sequences, the limit as j goes to infinity of this guy is 1. So this, in fact, equals limit as j goes to infinity. This goes to 1, this goes to 1, so you can actually say this converges to 1 as well. So this equals to the j plus 1 over j. OK? Now, strictly speaking, you would have to, OK, so this thing also converges to, what does it converge to? This, remember, is, so where do we have it? Do we still have it up here? So the radius, well, it's way over there, so let me recall that the radius of convergence rho, let me put an inverse here, this is equal to the limit as k goes to infinity of a sub k 1 over k. So this is, the exponent's converging to 1. This is converging to rho inverse again, so I get rho inverse to the 1. So therefore, this is supposed to be the radius 1 over the radius of convergence to the differentiated power series, which implies the differentiated power series j is 1 over this limit. 1 over that limit, which we computed is 1 over rho, which is the original radius of convergence. OK? So all this to say is that the radius of convergence of the differentiated power series, the formally differentiated power series, is the same as the radius of convergence of the original power series. OK? And therefore, you have uniform convergence of the derivatives wherever you have uniform convergence of the original power series. And therefore, by the theorem we proved last time, since both the derivatives and, so since the power series and the derivative of the power series converges uniformly on the same set, the power series, the infinitesimum, so this guy, is actually differentiable, and the derivative of the power series is the power series of the derivatives. OK? But you can iterate this, right? Because I now have this power series with radius of convergence equal to the radius of convergence of the original. And then I can take a derivative of that and show that has the same radius of convergence as the original. So let me just leave this as a remark and not state it as a theorem. So iterating can prove that I want the k-th derivative of the power series. So on, so let me say, all x in here, the k-th derivative of the power series is equal to the power series of differentiating term by term. So this holds for all k equals 1, 2, and so on. And so in particular, if I evaluate at x0, this tells me that k factorial a sub k is equal to the derivative of this function, which you get out when you stick x into this power series. And you can interpret this, although we never called them Taylor series, as a statement that every power series is the Taylor series of a function. So at least in this setting that we're looking at. We've answered pretty definitively, at least for the scope of this class, when we can interchange limits. We can do that as long as we have uniform convergence of the objects that we're interested in, be it the function, continuous functions, or function and its derivative. But there are more powerful statements out there that one can make, especially when it comes to integration. This is essentially why a different theory of integration was created, or one reason why. But hopefully, if I see some of you in 18.102, which is what I'm teaching next semester, we'll get into that further when we discuss Lebesgue integration. That was thought of because somehow Riemann integration is not complete, the same way that rational numbers are not complete. Riemann integration is not complete. Lebesgue integration is complete in a certain sense. I'm being very vague here for a reason. So I'm just giving you where you can go with this. What the next step is, at least in proving when can you interchange two limits, is really a topic that's fundamental to the study of Lebesgue integration. And you have much more powerful theorems there than you do here, which allows you to prove interesting results, and especially about Fourier series. In fact, if I had more time, we would apply, in fact, some of the stuff that we've done here. We could apply to the study of Fourier series. But maybe we'll do that in 18.102. OK, so I want to now prove the last theorem of the class, which is also due to the godfather. So when we had these power series, so these are defining functions, very special types of functions, what are called analytic. They are, by definition, essentially, the limit of polynomials. They're the limit of these finite sums of a sub j's times x minus x0 raised to the jth power. That's a polynomial. So for analytic functions, which are these functions equal to power series, they are the limits of polynomials. But that's a pretty small class of functions, analytic functions. But the Weierstrass proved this very interesting result that actually something like this is true for all continuous functions. So roughly speaking, Weierstrass proved that basically every continuous function is, in some sense, almost a polynomial. Just as we saw for these analytic functions, meaning defined by power series, they are very close to being, at least within their radius of convergence, a polynomial. In fact, this is true for all continuous functions that are not necessarily equal to a power series, but every continuous function is almost a polynomial. And in what sense do I mean that? So this is Weierstrass's approximation theorem, which states the following. If f is a continuous function on the unit interval, say, you can make it a b just by rescaling the variables. But I'll just state it for continuous functions on the unit interval. Then there exists a sequence of polynomials, p sub n of x, such that p sub n converges to f uniformly on 0, 1. And so in this sense, every continuous function is well approximated by polynomials. So every continuous function is close to being a polynomial. OK, so I need to first prove a couple of things that will be needed. So first off, I'm going to look at only a, so let me make some remarks before we go to the proof. So we're putting the proof on hold now. So let me just make a remark. I don't necessarily need to prove this statement for every f, just for certain f, and then for every f will follow from this special class. So we'll only consider the case f of 0 equals 0, and f of 1 equals 0. So f is 0 at the endpoints. Why we're doing this is so that we can extend f to a continuous function outside of the unit interval by just setting it equal to 0. And so let's suppose we've proven this special case, and we look at the general case. Then if I take any continuous function now, what do we know? That there exists a sequence of polynomials, P sub n, such that these polynomials, P sub n, converge uniformly to a small modification of f that results in. So this function here, if I look at it, it's a continuous function on 0, 1, because I'm just modifying it by constants and then times x. And then at f of 1, I get f of 1 minus f of 0 minus f of 1 minus f of 0, so I get 0. And at 0, I get f of 0 minus 0. f of 0 is 0 minus 0 times something I don't care equals 0. So there exist polynomials converging to this function now, if we've been able to prove the case just for f of 0 equals 0 and f of 1 equals 0. So uniformly. And therefore, the polynomials given by P sub n of x plus now x times f of 1 minus f of 0 plus. So this is still, if this was a polynomial, so is adding this to it, that's still a polynomial, converges to f tilde of x uniformly. And this, again, this is still a polynomial. So the whole point of this remark is that we only need to consider the case f of 0 equals 0 and f of 1 equals 0. And we're going to do that just so that we can extend f to a continuous function outside the interval. All right. All right. Now, the way we're going to build these polynomials that converge to f is by what's called an approximation to the identity. OK. Really, I guess you could think of it as an approximation to the delta function. And I'll explain that in just a minute. So for all n natural number, define C sub n. This will be the integral from minus 1 to 1 of 1 minus x squared raised to the n dx, 1 over. And then Q sub n of x, this is going to be equal to C sub n times 1 minus x squared raised to the n. Then a few simple consequences are, so first off, this is the integral of a function which is non-negative, but it's positive at plenty of places between minus 1 and 1. And you proved in the homework that this means that the integral has to be positive. Then the first is for all n natural number, for all x and 0, 1. So the first two of these observations are very clear. Qn of x is greater than, so minus 1 to 1. Qn of x is bigger than or equal to 0. So it's just C sub n, which is a positive number, times 1 minus x squared, x squared. So x is between minus 1 and 1, and therefore 1 minus x squared is always non-negative. Two is, this is also pretty clear, the integral from minus 1 to 1 of Q sub n of x dx equals 1. OK? Now, why is this clear? Because this is equal to the integral of 1 minus x squared raised to the n times C sub n. But C sub n is the inverse of that integral, so we should just pick up 1. Now the third and less trivial thing, which is important, is that for all delta and 0, 1, this function Q sub n, I mean this is just a polynomial, converges to 0 uniformly on the set delta is less than x, such that delta is less than or equal to the absolute value of x is less than or equal to 1. So in other words, here's minus 1, 1, 0, delta, minus delta. If I look in these regions, so in the union of those two intervals, then Q sub n, this polynomial, is converging to 0 uniformly as n goes to infinity on the union of these two intervals. OK? So what is the picture of what's going on? What do these Q sub n's look like? Here's minus 1, here's 1. So kind of the first one looks like some constant times 1 minus x squared. So there's the first one. And then as n keeps getting bigger and bigger, this is 0 at higher and higher order at 1 and minus 1 and getting pretty small near here. And in fact, according to 3, if I take any interval around 0 and look outside of it, as n goes to infinity, Q sub n is going to 0 uniformly. So what it should look like is maybe the next one looks like that. And then further still, like that. So that if I look inside over here or over here, Q sub n is going to 0 uniformly. So what you should think of is that the Q sub n's, as n goes to infinity, is something like a Dirac delta function, which is not exactly a function. So again, so let me reemphasize that. And this is in quotes. You should think of Q sub n acts like delta function centered at x equals 0. So that's in quotes because that is meaningless. But some of you who have taken physics know what properties are of a delta function. And the integral is 1. It's somehow 0 everywhere away except for the origin. And it's infinite there, but somehow has integral 1. And so OK. So these Q sub n's will show form an approximation to the identity in a certain sense. But let's prove these. Let's prove the only non-trivial one, which is 3. Number 1 and 2 are clear based on how they're defined. So let's first estimate how big is this. So this constant C sub n, we don't really know what it is explicitly. But let's compute at least a rough size of it. And then we'll use this to prove the third part. So we have for all n a natural number, the following inequality, nxn minus 1, 1, 1 minus x squared raised to the n. This is greater than or equal to 1 minus nx squared. Now, if it's not, so it shouldn't be like, you know, hit you in the face clear why this is true. But one way you can prove it is that if you look at the function g of x equals 1 minus x squared n. So first off, this inequality is even in x. Doesn't matter if x is negative or positive. So let's look at this function on 0, 1. Then what's the point? I look at g of 0, this is 0. And if I compute g prime of x, this is equal to n times 2x times what, times 1 minus 1 minus x squared to the n minus 1. And this is always less than or equal to 1, this thing in parentheses. So this thing is always bigger than or equal to 0 on 0, 1. So on 0, 1, this function is increasing. And its value at 0 is 0, so we get that, which is exactly what we wanted to prove. So now we compute the size of c sub n. So to do that, let me look at 1 over c sub n. If I want an upper bound on c sub n, I need to prove a lower bound on 1 over c sub n. So let's take 1 over c sub n and find a lower bound for it. This is the integral from minus 1 to 1 of 1 minus x squared raised to the n dx. Now, I can't remember if I made this a homework problem or not, but for even functions integrating over an interval even with respect to the origin, this is just twice. I mean, I'm sure you remember this from calculus. It's not hard to prove with what we know of the change of variables formula and so on, that this is equal to twice times the integral from 0 to 1 of 1 minus x squared raised to the n. OK. Now, this is bigger than or equal to 2 times if I integrate to a certain point. This certain point is chosen so that I get a result in the end that's pretty, essentially. Now, this is where I replace this by the smaller thing, which is easy to integrate. So this is greater than or equal to 2 integral 0 1 over root n 1 minus n x squared dx. And I leave it to you to verify that with this choice at the end point, what I get is 4 over 3 root n 1 over root n. And therefore, and this is bigger than 1 over root n. So I started with 1 over c sub n, and I showed that it was bigger than 1 over root n. And therefore, c sub n is less than 1 over, is c sub n is less than the square root of n. OK. Now, we'll use this to compute what we want. In fact, I mean, we really just needed to show that c sub n is bounded by some polynomial in n, but this will suffice. So we now want to show, let delta be positive. We now want to show that q sub n converges uniformly to 0 on that set where the absolute value of x is less than or equal to 1, is bigger than or equal to delta. Now, we note that the following sequence converges to 0, that square root of n times 1 minus delta squared raised to the n. So we should also put deltas in 0, 1. So this sequence here converges to 0 as n goes to infinity. Now, intuitively, why is this? This is because this is some number less than 1 raised to the nth power. Exponential always beats just a power of n. If you want to see exactly why this is, we could compute the limit as n goes to infinity of this sequence raised to 1 over n power. Then this is equal to the limit as n goes to infinity of n to the 1 over n 1 half 1 minus delta squared. And this converges to 1. We prove that. And so this equals 1 minus delta squared, which is less than 1. And we have this theorem from our section on sequences that says if this limit is less than 1, then the thing here converges to 0. Or you could interpret this as saying that the series with this as the individual terms converges. And therefore, the individual terms have to converge to 0, which implies limit as n goes to infinity equals 0. So now we have this. And once we have this, we'll have what we want. So we want to prove uniform convergence on that set. So let epsilon be positive. Then since this sequence converges to 0, there exists an m, a natural number, so that for all n bigger than or equal to m, square root of n times 1 minus delta squared raised to the n is less than epsilon. And for all n bigger than or equal to m, for all x, so that delta is between less than 1, we get that. q sub n, so it's non-negative, minus 0 in absolute value. That's just q sub n of x. So that's x raised to the n. This is less than or equal to square root of n. That's for this guy. 1 minus x squared. This is getting smaller as I get closer to 1. So it's biggest at delta. And how we chose m is this is less than epsilon. OK, so now we're ready for the proof of the Weierstrass approximation theorem. So suppose f is continuous function on 0, 1. f of 0 equals 0. f of 1 equals 0. So this polynomial here is very concentrated at 0. So first off, suppose f and it's continuous here. So you can check that if it's 0 there and I extend it to be 0 outside of 0, 1, this is still a continuous function. I'm just defining it this way because I want to write certain symbols in a little bit without specifying exactly where the bounds of the integration are. So we extend f by 0 outside of 0, 1. And this function f is continuous function on the real number line. OK. So we define, now we're going to define this polynomial, sequence of polynomials, p sub n. This is going to be equal to the integral from 0 to 1 of f of t times q sub n of t minus x dt. And just to remind you, this is equal to the integral from 0 to 1 of f of t times c sub n 1 minus x minus t squared raised to the n dt. OK, so this is just c sub n times this thing. If you expand everything out, it's just a, using the binomial theorem, this is equal to j equals, I'm just going to put a sum here, meaning a finite sum. Some numbers a sub j n times x to the j times t to the, just some finite numbers a j k times x to the j t to the k. All I'm saying is if you expand this out using the binomial theorem, you get this polynomial in x sub j t sub k. And then this is getting integrated against f of t dt. And so this is, in fact, a polynomial. So let's write this out and be precise just so that you're convinced that it's a polynomial. This is equal to the integral from 0 to 1 of f of t times c sub n. Now we use the binomial theorem. This is equal to j 0 to n n choose j. And then minus x minus t squared raised to the n minus j, or I could put j, to j. And then dt. I'm not sure if this is even helping or anything, but just so that you see this is actually a polynomial. f of t c sub n sum from j equals 0 to n. And now sum from k equals 0 to 2j n choose j minus 1 to the j. Now 2j choose k minus t to the k times x to the 2j minus k. And then dt. So I have all this junk integrated dt, and then this is x to the 2j minus k, so that just pops out. So this is a polynomial. I mean, this is a polynomial, but then when it integrates against f of t, this x to the 2j minus k comes out of the integral. And I get that times just all, this becomes a sum of terms with x to the 2j minus k outside this integral times f of t times integrated against minus 2 to the k. I think I said more than I needed to there, so the point is p sub n of x is a polynomial. Now we can write this slightly differently. So p sub n of x, this is equal to this as it was before. Now this we change variables and is equal to, so we do u substitution now where u is equal to x minus t, and then I'm going to change the dt. So what I did here was a change of variables by setting u is equal to x minus t, I guess you could, if you like, t minus x. And then I just recalled, and then I just called ut then again. So I'm eventually looking at these polynomials only in the interval 0, 1. OK, so this is what it looks like. But now this f of x plus t for t between minus x and 1 minus x, it's actually 0 outside of that. So I can extend the integration to minus 1 and 1. Again, with the understanding that I've extended f to be 0 outside of 0, 1. So since f y equals 0, or let me write it this way, since f of x plus t equals 0 for t not n minus x and 1 minus x. All right, so that was a lot of explaining for some stuff. But here, let me get to the point. What is p sub n really? So why should you expect this to converge to f? So this is a little discussion. So I said that, and this is for those who understood my comment about delta functions, q sub n is very concentrated at 0, at t equals 0. So this you should think of as being approximately like a delta function at t. If you don't know what a delta function is or never heard of it, then forget the rest of this remark. But then I'll say something. So q sub n kind of looks like a delta function. And therefore, minus 1 of x plus t, q n of t, dt should look more and more like f of x plus t delta of t, dt. And what we know about direct delta functions is that when you integrate them against a function, you just pick up the function evaluated at 0, which is. OK? So that's kind of why you expect it. I mean, if you look back at this picture of what the q n's are looking like, they're concentrating more and more at 0. So all of the contribution to this integral here, which defines these polynomials, is happening at t equals 0. At t equals 0, I just pick up f of x. OK? So that's why you should kind of expect these polynomials to converge back to the function. OK? I think I can finish on this board. So this is a picture you should have seared in your mind. This is what the q sub n's look like. They're concentrating all of their mass right at the origin. And therefore, I should just pick up f at the point x. Since this is concentrating at the origin, t equals 0, I should just pick up f of x plus t at t equals 0, which is f of x. OK? Why f of x plus t, not some constant times f of x plus t, that's because the integral of q sub n's is 1. OK. So now let's prove that the pn's converge to f uniformly. All right? OK. So let epsilon be positive. Since f is a continuous function on a closed and bounded interval, we know that it's uniformly continuous. And therefore, there exists delta positive such that if, let's write it this way, for all z, y, so that z minus y less than delta, we get that f of z minus f of y is. Actually, we don't need. OK, so I mean, we don't need so much about f, but we'll go with this anyways. We really just know. OK, never mind. I'll stop. So we know that f is uniformly continuous, so there exists a delta so that I have this. Yeah? So less than epsilon over 2. OK? So if z and y are within delta to each other, then f of z and f of y, no matter what z and y are, are within epsilon over 2 of each other. OK? Now, since f is continuous, it has a maximum on this interval. I should say since f is a continuous function, it has both a max and a min on this interval, and therefore, there exists some number c such that f of x is bounded by c for all x and 0, 1. All right, so now I have this delta coming from the uniform continuity of f. I have this c coming from the fact that it's bounded on this interval. And now I'm going to choose my m for the uniform convergence of the polynomials just depending on these pieces of input. Then as we showed, since square root of n 1 minus delta squared n converges to 0, this implies there exists an m, natural number, so that for all n bigger than or equal to m, I get that square root of n 1 minus delta squared raised to the n is less than epsilon over 8c. All right, I claim this m works, that for all n bigger than or equal to m, for all x and 0, 1, pn minus f is less than epsilon. And for all n bigger than or equal to m, for all x and 0, 1, if we look at p sub n of x minus f of x, we're going to use that this thing is an approximation to the identity, meaning it's essentially that it satisfies those three properties that I wrote a minute ago. So this is equal to the integral from minus 1 to 1 of f of x plus t times qn of t dt minus f of x. What's the integral from minus 1 to 1 of qn sub t? That's equal to 1. So f of x is at times, so this is all of analysis is writing 1 or 0 in a certain way. Not all, but so I can write this as f of x plus t, now minus f of x times qn of t dt. Again, because the integral of q sub n of t equals 1. So again, if you just expand this out, this is f of x. I'm integrating with respect to t. So I just pick up what I had before. Now by the triangle inequality for integrals, this is less than or equal to the integral from minus 1 to 1 of f of x plus t minus f of x times the absolute value of q sub n of t. But since q sub n of t is non-negative, that's just q sub n of t. And now I'm going to split this integral up into two parts. This is equal to a part where t is less than or equal to, so it should minus delta f of x plus t minus f of x q sub n of t dt, and then plus the other part, which I can write as. So it's a union of the two intervals, now away from delta and out to 1. So it's the sum of the integral over these two intervals, which I'm going to write as the integral over delta less than or equal to t is less than or equal to 1, and times q sub n of t. Now, t is between minus delta and delta, and therefore x plus t minus x, an absolute value, is less than delta. So in this integral here, I want to note that if t is in this interval here, x plus t minus x equals t is less than delta in this here. So since this guy minus this guy is less than delta, an absolute value, I can use this uniform continuity part to say that this is less than epsilon over 2 qn of t dt. Plus, now the absolute value of this guy is less than by the triangle inequality is less than or equal to the sum of the absolute values, which is less than or equal to 2c. So 1 and then 2c. Because that's bounded by c, that's bounded by c. So the absolute value of the difference is less than or equal to the sum of the absolute values, which is bounded by each of those bounded by c. Now, this is less than epsilon over 2 integral from, if I just integrate the whole thing from minus 1 to 1, that just gives me 1 plus 2c times qn of t. On this interval is, again, so I should put delta there, 2c c sub n 1 minus delta squared raised to the n dt. And so there's no t here. So this is equal to epsilon over 2, because this integral is 1 plus this c sub n, remember, is less than the square root of n times 1 minus delta squared n times the integral over this region dt, which I can make bigger by going from minus 1 to 1. It gives me 2. And this is equal to epsilon over 2 plus 4c square root of n 1 minus delta squared n. And this is less than epsilon over 2 plus epsilon over 2 equals epsilon. I think I have one minute to spare. So this was quite an experience teaching to an empty room. I hope you did get something out of this class. Unfortunately, I wasn't able to meet a lot of you. And that's one of the best parts about teaching and being able to see you grasp in real time what I'm talking about. So hopefully, this nightmare will end soon. And we'll get to see each other in the future.