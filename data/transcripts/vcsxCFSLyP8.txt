 We're about to embark on a new chapter in this course, where I want to tell you about Szemeredi's graph regularity lemma. Szemeredi's graph regularity lemma is a very powerful tool in modern graph theory developed back in the 70s. Today, I want to show you the statement and the proof of this graph regularity lemma. And next time, we'll see how to apply the lemma for graph theoretic applications. And we'll also use it to give a proof of Roth's theorem. The idea of Szemeredi's regularity lemma is that if you are given a very large graph, G, and it's a fairly robust theorem, so any large, dense graph, and here dense means, let's say, positive edge density, then it is possible to partition the vertex set of this graph, G, into a bounded number of pieces so that G looks random, like between most pairs of parts. So for instance, I might produce for you a partition of the vertex set into some number of parts. I'll draw five here. So you give me a graph, G. I manage to produce for you this vertex partition so that if I look at between a typical pair of parts, you'll see here maybe the edge density is close to 0.2. But otherwise, the bipartite graph looks like a random graph in some precise sense I will describe in a bit. And if you look at what the graph looks like between another pair of parts, maybe now it's a different edge density. Maybe it's around 0.4. And again, it looks like a random graph with that density. So in some sense, Szemeredi's regularity lemma is a universal structural description that allows you to approximate a graph by a bounded amount of information. So that's informally the idea. And you can already sense that this can be a very powerful tool. It doesn't matter what graph you input. You apply this lemma, and you get a approximate structural, or as later on, we'll see it's also in some sense an analytic description of the graph. So the first part of today's lecture will develop just a statement of this regularity lemma, show you what exactly do I mean by random-like. Well, first, let me give some definitions. I denote by the letter E if I input a pair of vertex sets, x and y. Here I might later on drop the subscript g if it's clear that I'm always talking about some graph g. So this is basically the number of edges between x and y. And I say basically because even though I will draw and depict everything as if x and y are disjoint sets, and that's the easiest case to think about, I'm also going to allow x and y to overlap, and also allow x and y to be the same set, in which case you should read the definition as to what this means. But it's fine to think of it as disjoint sets, so you're looking at a bipartite graph between x and y. We're also going to look at the edge density between x and y. And this is simply the number of edges divided by the product of the sizes of the sets. So what fraction of the possible pairs are actual edges? So from now on, I'll refer to this quantity as edge density. So now here's the definition of what random-like means for the purpose of Szemeredi's regularity lemma. So we'll define a notion of an epsilon regular pair to be as follows. So throughout, and later on I will omit even saying this, G will be some graph. And we're going to be looking at subsets of vertices of G. And we say that this pair of subsets of vertices is epsilon regular. Again, in G, but later on I will even drop saying in G, if it's clear which graph we're working with. So we say x and y is epsilon regular if for all subsets A of x, all B subsets of y that are not too small, so each at least an epsilon proportion of the sets that they live in, we find that the edge density between A and B differs from the edge density between x and y by no more than epsilon. Let me draw you a picture. I have sets A and B. So I have sets x and y in my graph G. And I want to say that the edges between x and y are epsilon regular. This random-like if the following holds, that whenever I pick a subset A in the left set and a subset B of the right set, the edge density between A and B is approximately the same as the overall edge density between x and y. So in particular, this bipartite graph, for instance, is not really dancing one part and really sparse in another part. Somehow the edges are evenly distributed in this precise manner. So that's the definition of epsilon regular. Yes, question? AUDIENCE 2 What is the epsilon for the size of A same as epsilon? YUFEI ZHAO The question is, here, why are we using the same epsilon here, here, and there? And that's a great question. So that's mostly out of convenience. So you could use different parameters. And they do play somewhat different roles. But at the end, we'll generally be looking at one type of epsilons. So it'll just make our life easier. You could extend the definition by having epsilon, comma, eta, if you like. But it will not be necessary for us, and mostly for simplification. Any more questions? All right. AUDIENCE APPLAUDS Now, if you have a pair x, y that is not epsilon regular, I just want to introduce a piece of terminology so you can read from the definition what it means to be not epsilon regular. And sometimes I will say epsilon irregular. But to be precise, I'll stick with not epsilon regular. Then we can exhibit this A and B that witnesses the irregularity. So if x, y is not epsilon regular, then their irregularity is to say it's witnessed by some pair A in x and B in y, satisfying, well, basically you read the definition, and such that the density between A and B differs quite a bit from the density between x and y. So when I say to exhibit or to witness irregularity, that's what I mean. Now, there's a bit of an unfortunate nomenclature in graph theory where previously we said d-regular graphs to mean that every vertex has degree d. And now we say epsilon regular to mean this. Sorry about that. These are both standard. So usually from context, it's clear which one is meant. So this is what it means for a single pair of vertex sets to be epsilon regular. But now I give you a graph, and I give you a partition of the vertex set. So what does it mean for that partition to be epsilon regular? Here's a second definition. So epsilon regular partition. So we say that a partition, and generally I will denote partitions by curly letters such as that, p. So the partition will divide a vertex set into a bunch of subsets. So we say that that partition is epsilon regular if the following is true. If I sum over all pairs of irregular, all pairs of vertex sets that are not epsilon regular, so over Vi, Vj, not epsilon regular, and sum over the product of their sizes, then what I would like is for the sum to be at most epsilon times the number of pairs of vertices in G. In other words, a small fraction of pairs of vertices, not necessarily edges, but just pairs of vertices, lie between pairs of vertex parts that are not epsilon regular. So for instance, if you do not have epsilon, if all of your pairs are epsilon regular, then the partition is epsilon regular. But I do allow a small number of blemishes. And that will be necessary. OK. Just to clarify a subtle point here, here I do allow in the summation i equals to j, although in practice, it doesn't really matter. You'll see that it's not really going to come up as an issue. And one of the reasons that it's not going to come up as an issue is usually when we apply this lemma, we're going to have a lot of parts. In fact, we can make sure that there is a minimum number of parts. And if all the parts are, none of the parts are too big, then having i equals to j contribute very little to that sum anyway. And in particular, if all the set sizes in this partition are roughly the same, so if they're all roughly 1 over k fraction of the entire vertex set, then that statement up there being an epsilon regular partition up to changing this epsilon is basically the same as saying that fewer than epsilon fraction of the pairs vi, vj are not epsilon regular. And here, if k is large enough, I can even let you make i and j different. It's not going to affect things after small changes in epsilon. So when it comes to, so for people who are seeing Szemeredi's regularity lemma for the first time, I think that's maybe all of you, or most of you, I don't want you to focus on the precise statements so much as the spirit of the lemma. Because if you get too nitty gritty with, oh, it's not the same as that epsilon, you get very confused very quickly. So I want you to focus on the spirit of this lemma. I will state everything precisely, but the idea is that most pairs are not epsilon regular. And don't worry too much about if you're allowed to take i equals to j or not. So now we're ready to state Szemeredi's regularity lemma. And it says that for every epsilon, there exists some constant m depending only on epsilon, such that every graph has an epsilon regular partition into at most m parts. You give me the epsilon, for example, 1%, and there exists some constant such that every graph has a 1% regular partition into a bounded number of parts. In particular, and this is very important, make sure you understand this part, that the number of parts does not depend on the size of the graph. Now, it's true that for some graphs, maybe you don't need very many parts. But the number of parts does not get substantially bigger, or does not exceed this bound, even if you look at graphs that have unbounded size. So it is really a universal theorem in the sense that it's independent of the size of the graph. Any questions about the statement of this theorem? Yes. AUDIENCE 2 So in the informal statement at the beginning, you said g is a large, dense graph. YUFEI ZHAO That's right. AUDIENCE 2 Is the dense condition appropriate anywhere in here? YUFEI ZHAO So the question is, why did I say that g is a large, dense graph? And that's a great question. And it's because if g had a sublinear number of edges, then I claim that all, if you take, if you look at the definition of epsilon regular pair, and your epsilon is a constant, and if your edge densities are sublinear, then all of these guys, they are little o of 1. They go to 0. So trivially, you will satisfy the epsilon regular condition. So if your graph is sparse, sparse in the sense of having subquadratic number of edges, then you trivially obtain epsilon regularity. And so the theorem is still true. It's just not meaningful. It's just not useful. But there are settings where having sparse graphs, and we'll come back to this later in the course, it's important to explore what happens to sparse graphs. Yeah? AUDIENCE 2 So that m is independent of g? YUFEI ZHAO. Yes, m is independent of g. m depends only on epsilon. AUDIENCE 2 What if m is really large, but there are not enough vertices in the graph? YUFEI ZHAO. OK, question is, what happens when m is very large, but there are not enough vertices in the graph? Well, if your m is a million, and your graph only has 1,000 vertices, what you can do is have every vertex be its own part. So every vertex is its own part, a singleton partition. And you can check that that partition satisfies the properties. Every pair is a single edge, and it's epsilon regular. Yeah? AUDIENCE 3 So in the partition, you're sort of like, all or nothing, either epsilon regular, like, maybe not. Do you get anything where, if you, like, say, make this more continuous, so, like, you allow for, sort of, like, you quantify how irregular it is, and then, like, can you, like, make this any binary? YUFEI ZHAO So my understanding of what you're asking is, in the definition up there, the sum is, we put the pair in the sum of this epsilon regular. Otherwise, don't put it. Is there some gradual way to put some measure of irregularity into that sum? And there are versions of regularity lemma that do that. But they're all, in spirit, more or less the same as that one there. Yeah? AUDIENCE 4 In formal definition, what does random like mean? YUFEI ZHAO So in the informal definition, what does random like mean? This is the formal definition of what random like means. So actually, later on in the course, one of the chapters is we'll explore what pseudorandom graphs are. So pseudorandom graph, in some sense, means graphs that are not random, but behave, in some sense, like random. So random like generally just means that in some aspect, in some property, it looks like a random object. And this is one way that something can look like random. So a random graph has this property. But random graphs also have many other properties that are not being exhibited in this definition. But this is one way that graph can look like random. So that's a great question. And we'll come back to that topic later in the course. All of these are great questions. So Szemeredi's regularity lemma, the first time you see it, it can look somewhat scary. But I want you to try to understand it more conceptually. So please do ask questions. Before diving into the proof, I want to make a few more remarks about the statement. So it is possible to, OK, so we will prove this version of the regularity lemma. But as I mentioned, it's the spirit of the regularity lemma that I care more about. And it's a very robust statement. You can add on extra decorations that somehow doesn't change the spirit. And the proof will be more or less the same, but for various applications will be slightly more useful. So in particular, it is possible to make the partition equitable. And equitable partition sometimes is also called an equipartition, meaning that it has such that all the ai's, all the vi's, have sizes differing by at most 1. So basically, all the parts have the same size, up to at most 1 because of the visibility. So let me state a version of regularity lemma for equitable partitions. So for every epsilon, an m, little m, not, there exists a big M such that every graph has an epsilon regular equitable partition of the vertex set into k parts where k is at least little m. So I can guarantee a minimum number of parts and at most some bounded number. Again, this bound may depend on your inputs epsilon and m0. So this is m0. But it does not depend on the graph itself. And you see, it's a slightly stronger conclusion and for many applications is more convenient to use this formulation. And I will comment on how you might modify the proof of that we'll see today into one where you can guarantee equitability. And you see that for this m, little m, not too small, for example, if it's somewhat larger than 1 over epsilon, when you look at the definition of epsilon regular partition, it suffices to check that at most epsilon k square, epsilon fraction of the pairs Vi, Bj, epsilon regular over i different from j. Again, up to changing epsilon, let's say, by a factor of 2. So all of these definitions are basically the same up to small changes in the parameters. Next time, we'll see how to apply the regularity lemma. And we will apply it in the first form. But you see, the second form guarantees you a somewhat stronger conclusion and sometimes more convenient to use. So for example, on the homework problems, if you wish to use a second form, then please go ahead. So just to make your life somewhat easier, but it essentially captures all the spirit of Szemeredi's regularity lemma. Any questions so far? I want to explain the idea of the proof of the regularity lemma. And this is a very important technique in this area called the energy increment argument. Here's the idea. We start with some partition. So for example, the trivial partition. And by that, I mean you only have one part. All the vertices are in one part. You're not doing anything to the vertex set. It's one gigantic part. Or if you're looking at some other variant, you can easily modify the proof. So for example, you can also look at an arbitrary partition into little m0 parts if you wish to have that as your starting point. So all I'm saying that this proof is fairly robust. And we're going to do some iterations. So as long as your partition is not epsilon regular, we will do something to the partition to move forward. And what we will do is look at each pair of parts in your partition that's not epsilon regular. Well, if they're not epsilon regular, then I can find a pair of subsets, which I don't know by the a's, that witnesses this non-regularity. So that witnesses the irregularity. And we start with some partition. So now let us refine the partition into a partition even more parts by simultaneously refine the partition using all of these aij's that we found in the step above. So you start with some partition. If it is not regular, I can chop up the various parts in some way. So I start with some partition over here. And what we're going to do is, well, let's say between these two, it's not epsilon regular. So I can find some pairs of vertex sets that exhibits the irregularity. I chop it up. And I can keep further chopping up the rest of the parts. If these two parts are not epsilon regular, then I chop it up like that. And I can keep on doing it. I originally have three parts. Now I have 12 parts. And this is a refined partition. And now I repeat until I am done. I am done when I obtain a partition that is epsilon regular. Now, the basic question when it comes to this strategy is, are you ever going to be done? When are you going to be done? And if this process goes on forever or goes on for a very long time, then you might have a lot of parts. But we want to guarantee that there is a bounded number of parts. So what we will show is that to show that you have a small number of parts, in other words, why does this process even stop? In particular, we want it to stop after a small number of steps, after a bounded number of steps. And to do this, we will define some notion called an energy of a partition. And this energy will increase. Well, so first of all, the energy is some quantity that we will define that lies between 0 and 1, some real number lying between 0 and 1. And each step, the energy goes up by some specific quantity. Therefore, because the energy cannot increase past 1, this iteration stops after a bounded number of steps. And once it's done, we end up with a epsilon regular partition. So that's the basic strategy. And what I want to show you is how to execute that strategy. Any questions so far? Yes? AUDIENCE 2 Just to clarify the algorithm a bit, so if some Vi is in two non-epsilon regular partitions, it's possible for Aij and Aik to overlap somehow, right? Do you just make that those three partitions? YUFEI ZHAO So if I understand correctly, you're worried about between different pairs, you might have interactions. So you are seeing the proof. But I think this is actually a very important and very somewhat subtle point, is that I do not refine at each step, I find a pair of witnessing sets. I find all of these witnessing sets all at the same time, and I refine everything all at once. AUDIENCE 2 So it's like if you do have overlap between two witnessing sets, that's OK? YUFEI ZHAO That is OK, because this step doesn't care. If you have two witnessing sets that overlap, that is OK. We'll see the proof. Yes? AUDIENCE 2 Can you just find one pair of witnessing sets for each Vi, Vj, and then a ranking mark? YUFEI ZHAO OK. Question is, do we find just one pair of witnessing sets, even though there could be more? And the answer is yes, we just need to find one. There could be lots. So if it's not epsilon regular, it might be very not epsilon regular. And in fact, being a witnessing set is a fairly robust notion. If you just take out a small number of vertices, it's still a witnessing set. Any more questions? Great. So let's take a quick break, and then we'll see the proof. Let's get started with the proof of Szemeredi's regularity lemma. And to do the proof, I want to develop this notion of energy, which you saw in the proof sketch. So what do I mean by energy? Well, first, if I define some quantities. If I have two vertex subsets, U and W, let me define this quantity Q, which is basically the edge density squared. But I normalize it somewhat according to how big U and W are. I'm going to use the letter N to denote the number of vertices in G. So this is some Q. And for partitions, if I have a pair of partitions, Pu of U into K parts and the partition Pw of W into L parts, I set this Q of Pu and Pw to be the quantity where I sum over basically all pairs, one part from U, one part from W, of this Q between Ui and Wj. So this is the density squared. And I'm taking some kind of weighted average of the squared density. So here is the weighted average. If you prefer to think about a special case where this partition is an equipartition, then it is really the average of the squared densities. It's a mean squared density. And finally, for a partition P of the vertex set of G into M parts, we define this Q of this partition P to be Q of P with itself according to the previous definition. Or in other words, I do this double sum, i from 1 to M, j from 1 to M, Q of Vi, Vj. And this is the quantity that I will call the energy of the partition. It is a mean squared density, some weighted mean, of the edge densities between pairs of parts in the partition. You might ask, why is it called an energy? So you might see from this formula here, it's some kind of a mean squared density. So it's some kind of a average of squares. So in particular, it's some kind of an L2 quantity. And there's a general phenomenon in mathematics, I think, borrowed from physical intuitions that you can pretty much call anything that's an L2 quantity an energy. And so that's, I think, where the name comes from. So this is the important object for our proof. And let's see how to execute a strategy, the energy increment argument outlined on the board over there. We want to show that you can refine a partition that is not epsilon regular in such a way that the energy goes up. And to do that, let me state a few lemmas regarding the energy of a partition under refinement. And the point of the next several lemmas is that the energy never decreases under refinement. And it sometimes increases if your graph is not if your partition is not epsilon regular. So the first lemma is that if you look at the energy between a pair of partitions, it is never less than the energy between the two vertex sets. So for instance, if you have u and w like that, and I partition them into Pu and Pw, and I measure the energy, just basically the squared density between u and v versus summing up the individual squared densities after the partition, the left side is always at least as great as the right side. So this is really a claim. It's a fairly simple claim about convexity. But let me set it up in a way that will help some of the later proofs. So let me define a random variable, which I'll call z, in the following way. Here's a process that I will use to define this random variable. I will select x, little x, to be a vertex uniformly chosen from u, from the left vertex set. And I will select a vertex y uniformly chosen from w. So x and y, they fall into some part in the partition. So suppose ui is the part where xi falls, and wi is the set in the partition where y falls. So ui is a member of this partition. wi is a member of the other partition of w. Then I define my random variable z to be the edge density between ui and wj. So it's a wj. So that's the definition. You pick x randomly, pick y randomly. Suppose x falls in ui. Suppose y falls in uj. Then z is the edge density between these two parts. So z is some random variable. Let's look at properties of this random variable. First, what is its expectation? It's a discrete random variable, and you can easily compute all of these quantities by just summing up according to how z is generated. So I look over all i and j. What's the probability that x falls in ui? It is the size of ui as a fraction of u. What's the probability that y falls in wj? It's the size of wj as a fraction of the size of w. And then z is this quantity here. So this is what I find to be the expectation of z. But you see, the density multiplied by the product of the vertex set sizes, that's just the number of edges between u and w. And you sum over all the ij's. So that, which is simply the edge density between u and w. So that's the expectation of the z variable. On the other hand, what's the second moment? In other words, what's the expectation of the square of z? Again, we do the same computation. So the first part is the same. The second part now becomes a d squared. And, well, look at how we define energy. This quantity here is basically the energy Q between the partition u and the partition of w, except there's normalization. That's not quite the same as the one we used before. So we will just put in that normalization. So now you compare the expectation of z versus the expectation of z squared. And we know by convexity that the expectation of z squared is at least as large as the expectation of z. That quantity squared. But if you plug in what values you get for these two guys, you derive the inequality claimed in lemma 1. You have to cancel some normalization factors, but that's easy to do. So that's the first lemma. The second one now, so the first one is just about a pair of parts. And then if I partition each part, what happens to the energy between this pair? And the second one is a direct corollary of the first one. Says that if you have a second partition, p prime, that refines p, then the energy of the second partition, the refinement, is never less than the energy of the first partition. And it is a direct consequence of the first lemma because we simply apply this lemma to every pair of parts in p. Between every pair of parts, the energy can never go down. So overall, the energy does not go down. And finally, so so far, we just said that the partitions can never make the energy go down. But in order to do this proof, we need to show that the energy sometimes goes up. And that's the point of the third lemma. So the third lemma tells us that you can get an energy boost. So this is the red bull lemma. You can get an energy boost if you are feeling irregular. So if uw is not epsilon regular, and this epsilon regularity is witnessed by u1 in u and w1 in w, then I claim that the energy obtained by chopping u into u1 and its complement against w1, against the complement of w1 in w. So here, again, u and w, I find a witnessing set for their irregularity. And now I partition left and right according to chop each part into two. So this energy between this partition into two on both sides is bigger than the original energy plus something where we can gain. And there's something where we can gain. Turns out to be at least epsilon raised to the power of 4 times size of u, size of w, divided by n squared. Let me prove it. Let's define z the same as in the previous proof, as in the proof of Lemma 1. In Lemma 1, we just use the fact that the L2 norm of z, the expectation of the square, is at least the square of the expectation. But actually, their difference has a name. It's called the variance. The variance of z is the difference between these two quantities. We know that it's always non-negative. So if you look at how we derived the expectation of z and expectation of z squared, you immediately see that this variance we can write as, up to a normalizing factor, the difference between this energy on one hand and the energy between u and w, namely the mean squared up to normalization. On the other hand, the different way to calculate the variance is that it is equal to the expectation of the deviation from the mean squared. So let's think about the deviation from the mean. I'm choosing a random vector on the left, u, and another random point, random vertex on the left, and a random point on the right. In the event that they both lie in the sets that witness the irregularity, so in the event where x falls here and y falls here, which occurs with this probability, see that this quantity here is equal to the density between u 1 and w 1 minus the density. And this expectation of z is just the density between u and w. So interpreting this expectation for what happens when x falls in u 1 and when y falls in w 1, ignoring all the other events, because the quantity is always non-negative everywhere. But now, from the definition of epsilon regularity, or rather the witnessing of epsilon irregularity, you see that this u 1 is at least an epsilon fraction of u. w 1 is at least an epsilon fraction of w. And this final quantity here is at least epsilon inside, so at least epsilon squared. So here, we're using all the different components of the definition of epsilon regular. Yes? AUDIENCE 1 What happens if we're dividing with more witnessing sets? YUFEI ZHAO So you're asking, what happens if we divide with more witnessing sets? So hold on to that thought. So right now, I'm just showing what happens if you have one witnessing set. Any more questions? So here, we have epsilon to the 4. And if you're putting the normalization, comparing these two interpretations, you find the inequality claimed by the lemma. So now, we are ready to show the key part of this iteration, showing that this iteration, I'll show you precisely how this iteration works, and show that you always get an energy boost in the overall partition. So I'll call the next one lemma 4. And it says that if you have a partition, this P of the vertex set of G into k parts, if this partition is not epsilon regular, then there exists a refinement, which we'll call Q, where every part B sub i is partitioned further into, at most, 2 to the k parts, and such that the partition of Q, so this energy of the new partition Q, increases substantially from the previous partition P, and will show that you can increase by at least epsilon to the fifth power. So some constant in epsilon. So if you look at the strategy up there, if you can do this every step, then that means that the number of iterations is bounded by 1 over epsilon to the fifth power. So to prove this lemma here, we will use the three lemmas up there and put them together. So for all the pairs i, j such that V sub i, V sub j not epsilon regular, so as outlined in the proof, in the outline up there, we will find this A superscript i, j in Vi and A superscript ji in Vj that witness the irregularity. So do this simultaneously for all pairs i, j where Vi, Vj is not epsilon regular. Now, what we're going to define Q as is the common refinement. So take all of, just as indicated in that picture up there, simultaneously take all of these A's and use them to refine P. So starting with P, starting with the partition you have, simultaneously cut everything up using all of these witnessing sets. Remember, you only have witnessing pairs for pairs that are not epsilon regular. If they're epsilon regular, you don't worry about them. All right, one of the claims in the lemma now, OK, so this is the Q that we'll end up with. So we'll show that this Q has that property. So one of the claims in the lemma is that every Vi is partitioned into, at most, 2 to the k parts. So I hope that part is clear, because how are we doing the refinement? We're taking Vi is divided into parts using these Aij's, 1j coming from each pair that is irregular with Vi. So I'm cutting up Vi using, at most, k sets. So 1 coming from each of the other possible, maybe fewer than k, that's fine, but at most, k sets is used to cut up each Vi. So you have, at most, 2 to the k parts once you cut everything up. All right, so the tricky part is to show that you get an energy boost. So let's do this. How do we show that you get an energy boost? We're going to put the top three lemmas together. First, so we want to analyze the energy of Q. So let's write it out. So the energy of Q is the sum over this energy of individual partitions of the Vi's. And by this P sub Vi, P sub Vj, I mean the partition of Bj given by Q. So what happens after you cut up Vi? So that's what I mean by P sub Bj. Or rather, I should call it Q sub Vi, Q sub Vj. By lemma 2, we find that, so let me separate them into two cases. The first case sums over ij such that Vi, Vj is epsilon regular. And by lemma 1, so here we're using lemma 1, we find that this quantity here cannot be less than Q of Vi, Vj. So take those two parts. Before and after the refinement by Q, the energy cannot go down. So I don't worry too much about pairs that are epsilon regular. But now let me look at pairs that are not epsilon regular. So what we will do now is, even though, so let's look at that picture up there. So let's focus on what I drew in red. So let's focus, say, between 1 and 2. So suppose the shaded part is the witnessing sets. The witnessing sets got cut up further by other witnessing sets. But I don't have to worry about them, because lemma 2, or lemma 1, really, tells me that I can do an inequality where I go down to just comparing the energy between this partition of two parts, this single witnessing set and its complement, versus what happens in this partner. So in other words, over here, the Q of this pair, I am saying that it is no less than if I just look at what happens if you only cut up these two sets using the red lines. All right. Let's go on. Applying lemma 3, the energy boost lemma, the first part stays the same. So this first part stays the same. And the second part, now, because I'm looking at witnessing sets for irregularity, I get this extra boost. So this goes back to one of the questions asked earlier, where in lemma 3, I don't have to now worry about what happens if you have further cuts, because I only need to worry about the case where I only have a single cut between the epsilon irregular pairs. So putting it together, we see that the previous line is at least if you sum over the Qs of all the pairs, plus this extra epsilon to the fourth term for all pairs that are not epsilon regular. I'm applying this monotonicity of energy for the types of pairs that are epsilon regular and energy boost for pairs that are not epsilon regular. And for the latter type, I obtain this boost. Now, remember what's the definition of an epsilon regular partition. Unfortunately, it's no longer on the board. But it says that this sum over here, well, if it is epsilon regular partition, then it is at most epsilon. So if it is not epsilon regular, we can lower bound it. And that's indeed what we will do. The first sum here is by definition Q of the partition P. And the second sum by the definition of epsilon regular is at least epsilon to the power 5. So here we're using the definition of epsilon regular partition, namely that a large fraction, so at least an epsilon fraction, basically of pairs of vertex sets are not epsilon regular, but in this weighted sense. And that finishes the proof of lemma 4 up there. Any questions so far? All right. So now we are ready to finish everything off and prove Szemeredi's regularity lemma. So let's prove Szemeredi's regularity lemma. Let's start with the trivial partition, meaning just one large part. And we are going to repeatedly apply lemma 4 whenever the partition at hand is not regular, whenever the current partition is not epsilon regular. All right. So let's look at its energy. The energy of this partition, so this is a weighted mean of the edge density squared. So it always lies between 0 and 1, just from the definition of energy. On the other hand, the lemma tells us that, so lemma 4 tells us that the energy increases by at least epsilon to the fifth power at each iteration. So this process cannot continue forever. So it must stop after, at most, epsilon to the minus fifth power number of steps. Steps. And when we stop, we must result in an epsilon regular partition, because otherwise, you can continue applying the lemma and push it even further. And that's it. So that proves Szemeredi's graph regularity lemma. Question? AUDIENCE 1 This gives you some really big value. YUFEI ZHAO OK, let's talk about bounds. So let's talk about how many parts does this produce. Also, we can figure it out. So we have some number of steps. Each step increases the number of parts by something. So if P has k parts, so then lemma 4 refines P into, at most, how many parts? So k times 2 to the k. k, and I have many iterations of this guy. So some of you are already laughing, because it's going to be a very large number. In fact, because it's going to be so large, it makes my calculations slightly more convenient. It really doesn't change the answer so much if I just bound k to the 2 to the k by 2 to the 2 to the k. So the final number of parts is this function iterated on itself, epsilon to the minus 5 times. So it's a power of 2 of height, at most, 2 to the epsilon to the 5. It's a finite number. So it depends only on epsilon and not on the size of your graph. And this is the most important thing. It does not depend on the size of your graph. It is quite large. In fact, even for reasonable values of epsilon, like 1% or even 10%, this number is astronomically large. And you might ask, is it really necessary? Because we did this proof, and it came out fairly elegantly, I would say, how the proof was set up. And it arrived at this finite bound. But maybe there's a better proof. Maybe you can work harder and obtain somewhat better bounds. So you can ask, is it possible that the truth is really somehow much smaller? And the answer turns out to be no. So there's a theorem by Tim Gowers, which says that, OK, so there exists some constant. The precise statement, again, it's not so important. But basically what I just said, you can't improve this bound given by this proof. So for every epsilon, let's say small enough, there exists a graph whose epsilon regular partition requires how many parts? So the number of parts, at least this tower of twos of height sum epsilon to the minus c. So really, it's a tower of exponentials of size, essentially polynomial in 1 over epsilon. So maybe you can squeeze the 5 to something less. Actually, we don't even know if that's the case. But certainly, you cannot do substantially better than what the proof gives. So Szemeredi's regularity lemma is an extremely powerful tool. And we'll see applications that are basically very difficult to prove. And for some of these applications, we don't really know other proofs except using Szemeredi's regularity lemma. But on the other hand, it gives terrible quantitative bounds. So there is a lot of interest in combinatorics where once you see a proof that requires Szemeredi's regularity lemma, that is first proved using this technique, to ask, can it be used using some other technique? In fact, Szemeredi himself has worked a lot in that direction, trying to get rid of the uses of his lemma. Any questions? AUDIENCE 1 How could you modify it for equipartitions? YUFEI ZHAO Questions, how can we modify it for equipartitions? So let's talk about that. It's a fantastic question. So look at this proof and see what can we do if we really want all the parts to have roughly the same size, let's say differing by at most 1. OK, let me. OK, so how to make the epsilon regular partition equitable? Any guesses? Any attempts on what we can do? I mean, basically, it's going to follow this proof. As I said, the spirit of Szemeredi's regularity lemma is what I've shown you. But the details and executions may vary somewhat depending on the specific purpose you have in mind. Yeah? AUDIENCE 2 It's like, we're personally adding things to the smaller part because we know that by the fact that it's not epsilon regular, that parts aren't used. YUFEI ZHAO So you're saying we're going to add something or to massage the partition to make it epsilon? AUDIENCE 2 Add vertices to the smaller part. YUFEI ZHAO Add vertices to the smaller parts of the partition. Now, when are you going to do that? AUDIENCE 2 When they're, like, say, do their assignments, then when they're not, we're going to do this. YUFEI ZHAO So you want to do this at every stage of the process? AUDIENCE 2 Yes. YUFEI ZHAO I like that idea. So here's what we're going to do. So we still run the same process. So we're going to have this p, which is the current partition. So I have current partition. And as before, we initially have it as either the trivial partition, if you like, or m arbitrary equitable parts. So start with something where you don't really care about anything except for the size. And you run basically the same proof, where if your p is not epsilon regular, then so we'll do what we've done before. So basically, exactly the same thing. We refine p using pairs witnessing irregularity. So same as the proof that we just did. And now we need to do something a little bit more to obtain equitability. And what we will do is right after, so each step in the iteration, right after we do this refinement, so after we cut up our graph where maybe some of the parts are really tiny, let's massage the partition somewhat to make them equitable. And to make our life a little bit easier, we can refine the partition somewhat further to chop it up into somewhat smaller resolution. And this part, you can really do it either arbitrarily or randomly. It doesn't some ways maybe slightly easier to execute, but it doesn't really matter how you do it. It's fairly robust. You refine it further. And well, basically, I want to make it equitable. Sometimes you can just do that by refining, but maybe if you have some really small parts, then you might need to move some vertices around. So I'll call that rebalancing. So move and merge some vertices, but only a very small number of vertices to make equitable. So you run this loop until you find that your partition is epsilon regular, then you're done. Whenever you run this loop, because we're doing the second step, your partition is always going to be equitable. But we now need to control the energy again to limit the number of steps. And the point here is that the first part still is exactly the same as before, but the energy goes up by at least epsilon to the minus 5. But the second part, the energy might go down, because we're no longer refining, just refining, because we're doing some rebalancing. But you can do it in such a way that the amount of rebalancing that you do is really small. You're not actually changing the energy by so much. So I'll just hand wave here and say that we can do this in such a way where the energy might go down, but only a little bit. So you're only changing a very small number of vertices, very small fraction of vertices. So if you change only an epsilon fraction of vertices, you don't expect the energy, which is something that comes out of summing pairs of vertex parts, to change by all that much. So putting these two together, you see that the energy still goes up by, let's say, at least half of epsilon over to the fifth power. And so then the rest of the proof runs the same as before. You finish in some bounded number of steps, and you result in an equitable partition. That's epsilon regular. So I don't want to belabor the details. I mean, here, there are some things to check, but it's, I think, fairly routine. One needs more of an exercise in technical details. But the thing that actually is somewhat important is there's a wrong way to do this. I just want to point that out. What's the wrong way to do this? It's that you apply regularity lemma, and you think, well, now I have something that's epsilon regular. Then I massage it to try to make it equitable at the end. So I don't look into the proof. I just look at a statement of Szemeredi's regularity lemma. I get something that's epsilon regular. I say, I'm just going to divide things up a little bit further. That doesn't work, because the property of being epsilon regular is actually not preserved under refinement. So look at the definition. You have something that's epsilon regular. You refine the partition. It might fail to be epsilon regular. So you really have to dig into the proof to get equitability. So just to repeat, a wrong way to try to get equitability is to apply regularity lemma and, at the end, try to massage it to get equitable. That doesn't work. Next time, I will show you how to apply Szemeredi's regularity lemma.