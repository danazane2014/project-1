 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. OK, so I thought I'd begin today with, as we're coming to the end of the sort of focus on linear algebra and moving on to a little probability, a little more optimization, and a lot of deep learning. So this was like my way of review to write down the big factorizations of a matrix. And so my idea, and I kind of enjoyed it, is checking that the number of free parameters, say in L and U or in Q and R or each of those, that the number of free parameters agrees with the number of parameters in A itself, like n squared usually. So A usually has n squared. And then can we replace A? After we've computed L and U, can we throw away A? Yes, because all the information is in L and U. And it fills that same n by n matrix. Well, that's kind of obvious, because L is a lower triangular and the diagonal, all ones, are not free parameters. And U is triangular, upper triangular, and it's diagonal, the pivots. Those are free parameters. But can I just write down the count? So I'll go through each of these just quickly after I've figured out how these are the building blocks. So how many free parameters are there in these two triangular matrices? Well, I think the answer is 1 half n n minus 1 and 1 half n n plus 1. That's a familiar number. That's the sum of, you recognize that as the sum of 1 plus 2 up to n. And you have one free in the upper triangular U. You've got one free parameter up in the corner, two in the next one. And as you're coming down, you end up with n on the main diagonal. And they add up to that. And you see that those two are different by n, which is what we want. OK. Diagonal, the answer is obviously n. How about the eigenvector matrix? This whole exercise is like something I've never seen in a textbook. But for me, it brings back all these key, really, the condensed course in linear algebra is on that top line. So how many free parameters in an eigenvector matrix? OK, and of course, you're sort of thinking, what's the rule for free parameters? So my answer is going to be for the number of free parameters. So this is an n by n matrix with the n eigenvectors in it. But there's a certain freedom there. And what is that? What freedom do we have in choosing the eigenvector matrix? Every eigenvector can be multiplied by a scalar. If x is an eigenvector, so is 2x, so is 3x. So we could make a convention that the first component was always 1. Maybe that wouldn't be the most intelligent convention in the world. But it would show that that top row of 1's were not to be counted. So I get n squared minus n for that. Oh, yeah, well, having done those two, let me look at this one. Does that come out a total of n squared? Yes, because the eigenvector x has n squared minus n by this reasoning, the little hokey reasoning that I just gave. And then there are n more for the eigenvalue matrix. And there's nothing left for the inverse, because it's determined by x. So do you see the count adding up to n squared for those? Now, I left open the orthogonal one. I think we kind of talked about that when we met it. And it's a little less obvious. But do you remember? So I'm talking about an n by n orthogonal matrix, Q. So how many free parameters in column 1? That column is what we always call Q1. Does it have n free parameters, or is there a condition that cuts that back? There is a condition, right? And what's the condition on the first column that removes one parameter? It's normalized. Its length is 1. So I only get n minus 1 from the first column. And now, if I move over to the second column, how many free parameters there? Again, it's a unit vector. But also, it is orthogonal to the first. So two parameters got used. Two rules got imposed, and two parameters got removed. So this is n minus 2, and then finally, whatever. So I think that that sum of these guys is exactly the same that we had up here. I think it's also 1 half n n minus 1, or 1 half n squared minus n. So not as many as you might think, because the matrix is size n squared. Now, can I use those? Because these are like the building blocks. Can I just check these? Let's see. I'll just go along the list. L times u. So L had this, and u had that. And when I add those, it adds up to n squared. The minus cancels the plus, and the half n squared twice gives me n squared. So good for that one. What about qr? Well, r is upper triangular, like so. And then q, oh yeah, we just got it right there. So for q times r, it's that plus that, again, adding to n squared. Good for that one. n squared for that one. This one we just did. n squared minus n in x. n on the diagonal, total n squared. What about this guy? What about the big, really fundamental one that I would normally write the matrix as S instead of A to remind us that the matrix here is symmetric? So I'm not expecting n squared for a symmetric matrix. Oh, I should have put that on my list. What's the count for a symmetric matrix? So because this is an S here. So I'm not expecting to get n squared. I'm only expecting to get the number of symmetric S. What's the number of free parameters that I start with that I hope will reappear in Q and lambda? So how many? What's the deal for a symmetric matrix? Let's see. I'm free to choose. Is it the same count as this? Yeah, because I'm free to choose the upper triangular part and the diagonal. But I'm not free to choose the lower. So I'd say that's a half n times n minus 1. And plus 1, sorry. Diagonal's in there. So do I get that total, half of n squared plus n, from these guys? Well, I probably do. The diagonal guy gives me n. This gives me n. And that's a Q, which is my other favorite number there. And when I add that to that, that becomes a plus sign. And I'm good. Yeah, you see how I enjoy doing this. But I'm near the end. But the last one is kind of not well-known. OK, Q times S. Do you remember that factorization? That's called the polar decomposition. It's an orthogonal times a symmetric. And it is often used in engineering as a way to decompose a displacement or strain matrix. Anyway, Q times S. And it actually is very, very close to the SVD. And I have friends who say, better to compute QS than the SVD. And then just move along. Anyway, Q times S. So Q is this guy. And S, what's S? Symmetric, that's this guy. So that's Q. Let me write that letter Q and S so I don't lose it. What do those add up to? N squared. Happy. OK, so finally, the SVD. Finally, the SVD. What's the count? Well, now I've got rectangular stuff in there. So I've got to, I'm ready for this one. And I have to think a little bit. And we may have done this. So let's suppose that M is less or equal N. Suppose that. Yeah, otherwise we would just transpose and look at the SVD. So let's say M less or equal N. So let's say it's got full rank. And what's the largest rank that the matrix can have? M, clearly. Full rank M. So the SVD will be M by M. Let's remember the U, the sigma, and the V transpose. This will be M by N. And this will be N by N for the full scale SVD. And if the rank is equal to M, then I really expect to get, I expect it to add up to the total. For A, the original A has MN, right? It's an M by N matrix. The matrix A is M by N with M less or equal N giving me these things. So it has MN parameters. So do we get M times N from this? I hope we do. I know how many we get from sigma. How many, what's the count for sigma? M. And what's the count for V? So that's an N by N. And what's the count for U? OK. They're orthogonal matrices. So I should be able to go up to that line. This was an M by M one. So is that a half N, N minus 1? Am I copying that correctly out of this circle there? That's an M by M orthogonal matrix. Oh, but I have to write M. That was foolish. OK. M. M. Yeah, because that matrix is of size M. So that's an M. And then I have that. And then I have whatever V transpose N by N. Oh, what's the deal in there? Do I want all of the 1 half N, N minus 1? Oh, god. I thought I'd got this straight. Let's see. No, I'm not. I could subtract this from this and find out what I should say. Well, students have been known to do this too. Let's see. Well, let's try to think anyway. So I have this N by N orthogonal matrix. So first, it could be any orthogonal matrix. Yeah. But is it only the first M columns that I really need? The rest I could just throw away. So let me try to imagine that it's just the first. Well, then I won't have any N in here. So maybe I better take a half N. Help. Oh, oh, yes, of course. I've got only M columns that matter. Everybody now understands that SVD. The rank is M. Don't forget that. Then the first R, the first M columns of V are important. Those are the singular vectors that go with non-zero singular values that really matter. And the rest really don't matter. So I'm going to just, I have to count how many. So sorry. V, the important part of V, has how many? Only M columns, but it's an N by N matrix. And those columns are orthogonal. So the answer is not MN for this guy. I have to go through this foolish reasoning again. I have N minus 1 plus N minus 2 plus so on plus N minus M. There were N minus 1 parameters in the first orthogonal vector or unit vector, N minus 2 in the second one up to N minus M in the third. And then V has some more columns that are coming really from a null space that are not important. I believe this is the right thing to do. I'm hoping you agree. And now I'm hoping even more that those add up to M times N. OK, I have a half N. Oh, I really have to total this thing. OK, this had M terms. So there's M of these N's. And then I have to subtract off 1 plus 2 plus 3 up to M. And so what am I subtracting off? What's that sum 1 plus 2 plus 3 stopping at M? It's one of these guys, 1 half. Is it 1 half M, M plus 1? Yeah, 1 half M, M plus 1. Sorry, 1 half M, M plus 1. I was supposed to enjoy this. And now I'm a little nervous. But OK, so I believe that that is that. OK, well, we have the MN. That's a good sign that we're shooting for. So does the rest of it add to nothing? Well, I guess, yeah, I guess it does. When I put these two together, I have 1 half M, M plus 1. And then I'm subtracting it away again. So I get MN. Hooray. Well, had to happen, or we wouldn't. Before I raise that board and consign that to history, should I pause a little more? Minute. This will be, like I'm hoping, a one-page appendix of the notes and the book, and you'll see it. But I do have one more count to do. And then I'm good with this review and ready to move onward to the topic of saddle points and ready to move onward after that. Well, I'll say a little bit about the next lab homework that I'm creating. And then our next topic will be like covariance matrices. A little statistics this week. And then we get a week off to digest it and then come back for gradient descent and deep learning and those things. OK. Everybody happy with that? So what's my final question? My final question is the SVD for any matrix of rank R. So it's an m by n matrix, but the rank is only R. It's a natural question. How many parameters are there in a rank R matrix? And we may even have touched on this question. And I have two ways to answer it. And one way is the SVD. And that'll be similar to what I just pushed up there. So if the rank is R, the SVD of this typical rank R matrix will be U sigma V transpose. But U, now this is like the condensed thing where I've thrown away stuff that's automatically 0. Because if the rank is only R, like if the rank was 1, suppose the rank was 1. Then I'd have one column times one sigma times one row. And I could do that count for R equal 1. Now I have R columns. So this is m by R. The sigma is diagonal, of course. So I'm going to get R numbers out of that. And this one is now R by n. In other words, maybe I should save this little bit here that was helpful. But now I've got m is reduced to R. So I believe that if I count these three, I'll get the right number of parameters for a rank R matrix. And that's not so obvious. Because the rank R matrices are not a, we don't have a subspace. If I add a rank R matrix to another rank R matrix, well, the rank could be as big as 2R and probably will be. So it's a little interesting to get your hands on matrices of rank R. Because they're kind of a thin, like a, well, a math person would call it a manifold, some kind of a surface within matrix space. Have you ever thought about matrix space? So that's a vector space. Because we can add matrices. We can multiply them by constants. We can take linear combinations. So we could call them vectors if we like. That would be a vector space of m by n matrices. What would be the dimension of that space? So the vector space of all 3 by 4 matrices. That has what dimension? 12. 12. Because you've got 12 numbers to choose. And it is a space because you can add. Now, if I say 3 by 4 matrices of rank 2, I don't have a space anymore. That word space is seriously preserved for meaning vector space, meaning I can take combinations. But if I take a rank 2 matrix plus a rank 2 matrix, I'm not. So it's sort of a surface within 12D. The 3 by 4 matrices of rank 2. And we're about to find the dimension of that surface. Does your mind sort of visualize a surface in 12 dimensions? Yeah, well, give it a shot anyway. But that surface could be 11 dimensional, so to speak. Like, meaning locally, it wouldn't have to be an 11 dimensional plane going through the origin. In fact, it wouldn't go through the origin because the origin won't have rank r. So it's some kind of a surface. And maybe it's got some different pieces. Probably some smart person knows what that surface looks like. But we're just going to find out something about its number of parameters, its local dimension. Well, I know that this answer is r. Because I've got r sigmas. And this one I'm pretty good at. But now it's r by n. So here r was m. But now down here, r is r. So I think it's rn minus 1 half. What's that? Is that an m? So now it's an r r plus 1, I think. I think. And what about the u? So u is going to be similar, except instead of the n here, we've got an m. So I think for u, we'll have m minus 1 plus m minus 2 plus. So let me write it here. m minus 1. So u, I'm talking about u here. It's got r columns. The first one has m minus 1, because I throw away 1 because it's a unit vector, up to m minus r. That's its rth column. And now, so what does that add up to? Well, I put all the m's together. So that's rm. Or let me say mr. And then I'm subtracting on 1 plus 2 plus 3 up to r. Now tell me again what that adds up to. 1 plus 2 plus 3, stop at r. That's what we had here. And we've got it for v. And we've got it again here. Minus 1 half r r plus 1. Are you OK with that? And now I just want to add them up. So I have mr and I have nr. And then I have two of these. So let me get it here. mr and nr. And now I have to look at. So mr, check. nr, check. Now I have two of these guys. So they combine into r squared plus r. And then I r squared. Yeah, minus r squared plus r. Sorry. They combine into minus r squared plus r. And then here is a r coming in with a plus. I think we have a minus r squared. And that is the right answer. Yeah. OK. So I took a bit longer than I intended. But this is a number that's sort of interesting. I mentioned saddle points sort of like separately from maxima and minima just because they are definitely not as easy to work with. You understand what I mean by saddle points. The matrices involved are not positive definite. Those would go with a maximum. They're not negative. They're not negative. They're not. Well, those would go with maxima and minima. But we're looking in between. So saddle points. OK. I'll get going on those. OK. I sort of realized that there are two main sources of saddle points. One of them is when I have problems that, let's say, I minimize. So this will be the constraint. The saddle points that come from the constraint. So Lagrange is going to be responsible for these saddle points. So we might have some minimum problem like minimize some positive definite thing. And of course, if we don't say any more, the minimum is 0, because otherwise it's positive. But we're going to put on constraints, Ax equal b. So this is the classical constrained optimization problem. Quadratic cost function, linear constraints. We could solve this exactly. But let's just see where saddle points are going to arise. So this S is positive definite. But now, how do we deal with that problem? Well, Lagrange said what to do. Lagrange said, look at the Lagrangian. OK. He introduced lambda. So this x is in n dimensions. That's an n by n matrix. But I have m constraints. So the matrix A is m by n. I have m constraints. And then I'm going to follow the rules and introduce m Lagrange multipliers. That's an m. And then the neat part of the Lagrange. And what is this? Well, I take the function. And then I introduce, remember, lambda is a vector now, not just a number. We had some application where it was just one constraint. But now I have m constraints. So I take lambda. So lambda transpose times Ax minus b. And the plus or the minus sign here is not important. I mean, you can choose it because that'll determine the sign of lambda. But either way, it's correct. OK. So we've introduced a function that now depends on x and also on lambda. And they multiply each other. And my point is that Lagrange says, take the derivatives with respect to x and lambda. So that's the cool thing that he's contributed. He says, if you only create my function, now you can take x derivative and lambda derivative. That'll give you n equations for the x from this one, from the x derivative, and m equations from the lambda derivative. It'll be n plus m. It'll determine the good x and the lambda. But I'm saying that's all true and all important. But I'm saying that the x and that pair x lambda will be a saddle point of this function. This function has saddle points, not a maximum. OK. Let's just take the derivatives and see what we get. So the derivatives with respect to x, d by dx. x is now a vector, so I really should say the gradient in the x direction. I get Sx. And here, the derivative with respect to x, that is A transpose lambda. Because this is the dot product of A transpose lambda with x. I put parentheses around it and followed the transpose rule. So that's the dot product of A transpose lambda with x. It's linear in x, so its derivative is just A transpose lambda. And that's 0. And now I take the other one, the lambda derivative. The lambda derivative, this doesn't depend on lambda. The lambda derivative is just Ax minus b. It brings back the constraints. So that's pretty simple. Now, it doesn't even require much thought, because you just know the constraints are coming back. And of course, b should be put over on this side, because it's a constant. So there we see, to a block, we see a very important class of problems. And the matrix we're seeing, we could write this in block matrix form, S minus A transpose. Oh, I'm going to change that to a plus, because I'm more of a plus person. A transpose and A. Yeah, when I took the derivative with respect to lambda, I didn't put the minus sign in here. And I didn't want to, so let's make it a plus. A, and then there's nothing there. And the x, and the lambda, and the 0, and the b. That is the model of a constrained minimum, a minimum problem with constraints. It's a model because the function here is quadratic, and the constraints are linear. It's in course six, it's everywhere, constantly appearing as the simplest model. And my point today is just that the solution x lambda, that total solution, the x together with a lambda, that that is a saddle point of the Lagrangian function L. It's a saddle point, not a minimum. It's sort of a minimum in the x direction, because this is positive definite. That makes that, as a function of x, it's going up. But somehow, the appearance of lambda makes this matrix indefinite. It starts positive definite, but it has this A transpose A, and that 0. It couldn't be positive. Actually, if I look at that matrix, I see it's not positive definite. Why do I say that immediately? When I look at that matrix, it's not a positive definite matrix, because when I see that 0 on the diagonal, that shoots positive definite. Couldn't be. Take as an example, S equal 3, 1, and 1, 0. Take that matrix. Just random. I made it 2 by 2 instead of size m plus n. Do you see? How do I know that the eigenvalues of that matrix, one is plus and one is minus? The determinant is negative. So that tells me right away that one is plus and one is minus. Thanks. Yes, yes, yes. The determinant is negative. And somehow, here, the determinant is similar calculation would produce A transpose A or something with the minus, because I'm going this way. Well, I could do better than that. But you saw the point, that that matrix, that simple example of this has eigenvalues of both signs. Let me just quickly say, and I'll put it in the notes or in that chapter. I guess all this is coming is still 3.2. That was originally 4.2, and you'll see it. So what do I want to say? I'd like to say that that example is pretty convincing to me, that these KKT matrices, if you talk to people in optimization, that's Karush, Kuhn, and Tucker, three famous guys. And these are the KKT conditions that they derived following Lagrange. And my point is, and this is a typical sort. So it's an indefinite matrix. I believe it has that if I do elimination, yeah, tell me this. This is a good way to look at it. Suppose I do elimination on this one or on this one? Well, suppose I do elimination there. What is the first pivot? 3, positive. So now let me turn down to here. What if I do elimination on this block matrix? Then I start up here, and that first pivot is positive again, right? This S is a positive definite matrix. Don't forget. In fact, the first n pivots will all be positive because the first n pivots, you're working away in this corner. And if you're only thinking about the first n, this corner is size n by n. Then you don't even see a. You're doing some subtractions, and I'll do those. But the pivots themselves are all coming from S, and S is positive definite. So we know that one of the tests for a positive definite matrix is all pivots are positive. So I think all n of the first pivots will be positive. And when we use them, let's just see what happens when we use them. So here is the KKT matrix that I start with. And what do I end up with? Well, really what I'm doing is I'm multiplying that block row by something and subtracting to kill that a. So these rows, well, near enough. Let me do block elimination. Block elimination is easier. I don't have to write down all little tiny numbers. So I just want to multiply this row by something. Tell me what. And subtract from this second row. Suppose there are numbers or letters. I guess they are letters. What do I multiply that first row by and subtract? Let's see. If these were just little tiny numbers, as like in 3, 1, 1, 0, what do I multiply that row by and subtract from this? I multiply by a over s. I do multiply by a over s, which puts an a there. Then I subtract. So here I'll multiply by a over s. But these are matrices. So I multiply by a s inverse. When I multiply by a s inverse times this s, I get a. And then I subtract, and I get the 0. And when I multiply by this guy and subtract, I get minus, because I'm subtracting, this thing minus a s inverse a transpose. That was block elimination, which just, in other words, is just you've learned about 2 by 2 matrices. 3x plus 4y equals 7 and stuff. Now I'm just doing it with blocks instead of single numbers. But you see, this produced those positive pivots. And what can you tell me about that matrix? What can you tell me about the signs or the eigenvalues or whatever of this matrix? Suppose s was the identity. What could you tell me about minus a a transpose? Minus a a transpose, and my voice should emphasize that minus. That matrix there is negative definite. So all the next set of m pivots that come from here will all be negative. So I get m, or rather n, n positive, and n negative pivots. And then I remember that the pivots actually have the same sign as the eigenvalues. That's just a beautiful fact. We know that for positive definite ones, the eigenvalues are all positive. The pivots are all positive. But it's even better than that, that if we have some mixture for the signs of the pivots, that tells us the signs of the eigenvalues. That's a really neat fact. So I'll just write that down. Plus and minus signs of pivots give us the plus and minus signs of the eigenvalues. So I've sneaked in a nice matrix here for symmetric matrices. This is symmetric matrices. OK, that's what I wanted to say about constraints and saddle points coming from there. And then I now want to say something about constraints, and rather not constraints now. I'm going to look at the second source of saddle points. So these will be saddles from this remarkable function that we know. So I now have a symmetric matrix S. Could be even positive definite. Usually it is here. Do you know what the name for R is? It's a ratio or a quotient. It's named after somebody starting with R. Who's that? Rayleigh, right? It's Rayleigh quotient. And what is the largest possible value of the Rayleigh quotient? We've seen this idea. It is the maximum value of that Rayleigh quotient, of that ratio, is lambda max. Lambda 1, the biggest one. And the x that does it is the eigenvector. So max is lambda 1 and at x equal q1, because q1 transpose Sq1 over q1 transpose q1. So I'm plugging in this winner. And Sq1 is lambda 1 q1. It's the first eigenvector. And so a lambda 1 comes out, so I get lambda 1. I know everything about that. And what I know is if I put in any x, what do I know? If I put in any x whatever and look at this number, what do I know about that number? It's smaller than lambda 1. Or it might hit lambda 1, but it's not bigger. That's why maxima are easy. You put in any vector, and you know what's happening. You know it's not above the max, obviously. And what about the min? That's equally simple, of course. It's at the bottom. So what would be the minimum of that, really, of that quotient? If I was looking for what eigenvector and eigenvalue will I find when I look at the bottom of this? I will find lambda n, the last guy, lambda min. And the winning x will be its eigenvector. And again, this stuff will equal lambda n. So that's easy. I know that if I put in any vector whatever, just choose any vector in n dimensions and compute r of x, what do I now also know about that r of that vector? It's greater than lambda n, below the max, above the min. But now, if I put in, now what about the other lambdas? Well, the point is that those are saddle points. The beautiful thing about this Rayleigh quotient is its derivative equals 0 right at the eigenvectors. And its value at the eigenvectors is the eigenvalue. So I have, you see what I'm saying? I have lambda 1 here, a max, lambda n here, a min. And in between, I have a bunch of other lambdas, which are saddle points. And if I put an x in to r of x and look to see what happens, I have no idea whether I'm on this side, the below it, or this side above, lambda i. So the saddle points are more difficult and take a little more patience. So that's the other source of saddle points. So these are, so let me just emphasize again what I'm saying, at lambda at x equal qk, I have some number, the r of x has some number of positive eigenvalues and some number of negative ones for the things above and below qk. OK, I've run out of time to follow up on the saddle point part of the details of this picture. I might, that will be in the notes, and I might come back to it at the very start of next time. But I will, before that, you will have the lab number 3. And then I think we should discuss it, because I haven't done this lab. It's intended to give you some feeling for overfitting and also intended to give you a little introduction to deep learning. And so I'll get it to you, and we can talk about it Wednesday. And again, it won't be due until the Wednesday after break. OK, thanks. So I'll see you Wednesday.