 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. It's because if I was not, this would be basically the last topic we would ever see. And this is arguably probably the most important topic in statistics, or at least that's probably the reason why most of you are taking this class. Because regression implies prediction. And prediction is what people are all after now. You don't need to understand what the model for the financial market is if you actually have a formula to predict what the stock prices are going to be tomorrow. And regression, in a way, allows us to do that. And we'll start with a very simple version of regression, which is linear regression, which is the most standard one. And then we'll move on to slightly more advanced notions, such as non-parametric regression. At least we're going to see the principles behind it. And I'll touch upon a little bit of high dimensional regression, which is what people are doing today. So the goal of regression is to try to predict one variable based on another variable. So here, the notation is very important. It's extremely standard. It goes from one. It goes everywhere, essentially. And essentially, you're trying to explain y as a function of x, which is the usual y equals f of x question, except that if you look at a calculus class, people tell you y equals f of x. And they give you a specific form for f. And then you do something. Here, we're just going to try to estimate what this link function is. And this is why we often call y the explained variable and x the explanatory variable. So we're statisticians. And we start with data. And what does our data look like? Well, it looks like a bunch of input output to this relationship. So we have a bunch of xi, yi. Those are pairs. And I can do a scatter plot of those guys. So each point here has a x-coordinate, which is xi, and a y-coordinate, which is yi. And here, I have a bunch of endpoints. And I just draw them like that. So the functions we're going to be interested in are often function of the form y equals a plus b times x. And that means that this function looks like this. So if I do x and y, this function looks exactly like a line. And clearly, those points are not on the line. And it will basically never happen that those points are on a line. There's a famous t-shirt from, I think, UC Berkeley's stats department that shows this picture and put a line in between them. We're going to see it. And it says, ah, statisticians, so many points, and you still manage to miss all of them. And so basically, essentially, we don't believe that this relationship y is equal to a plus bx is true, but maybe up to some noise. And that's where the statistics is going to come into play. There's going to be some random noise that's going to play out. And hopefully, the noise is going to be spread out evenly so that we can average it if we have enough points, average it out. And so this epsilon here is not necessarily due to randomness. But again, just like we did modeling in the first place, it essentially accounts for everything we don't understand about this relationship. So for example, so here I'm not going to be, so give me one second. So we'll see an example in a second. But the idea here is that if you have data, and if you believe that it's of the form a plus bx plus some noise, you're trying to find the line that will explain your data the best. In the terminology we've been using before, this would be the most likely line that explains the data. So we can see that it's slightly, we've just added another dimension to our statistical problem. We don't have just x's, but we have y's. And we're trying to find the most likely explanation of the relationship between y and x. All right, and so in practice, the way it's going to look like is that we're going to have basically two parameters to find, the slope b and the intercept a. And given data, the goal is going to be to try to find the best possible y. So what we're going to find is not exactly a and b, the ones that actually generate the data, but some estimators of those parameters, a hat and b hat, constructed from the data. So we'll see that more generally, but we're not going to go too much in the details of this. There's actually quite a bit that you can understand if you do what's called univariate regression, when x is actually a real valued random variable. So when this happens, this is called univariate regression. And when x is in Rp for p larger than or equal to 2, this is called multivariate regression. So here, we're just trying to explain y is a plus bx plus epsilon. And here, we're going to have something more complicated. We're going to have y, which is equal to a plus b1 x1 plus b2 x2 plus bp xp plus epsilon, where x is equal to the coordinates of x are given by x1 to xp and Rp. So it's still linear. They still add all the coordinates of x with a coefficient in front of them, but it's a bit more complicated than just one coefficient for one coordinate of x. So we'll come back to multivariate regression. There's, of course, you can write this as a x transpose b. So this entire thing here, this linear combination is of the form x transpose b, where b is the vector that has coordinates b1 to bp. Sorry, here it's in Rd. p is the natural notation. So our goal here in the univariate one is to try to write a model, make sense of this little twiddle here. Essentially, from a statistical modeling question, the question is going to be, what distributional assumptions do you want to put on epsilon? Are you going to say they're Gaussian or are you going to say they're binomial? Binomial? Are you going to say they're binomial or are you going to say they're Bernoulli? So that's going to be what we're going to make sense. And then we're going to try to find a method to estimate a and b. And then maybe we're going to try to do some inference about a and b, maybe test if a and b take certain values. If they're less than something, maybe find some confidence regions for a and b. So why would you want to do this? Well, I'm sure all of you have an application why, if I give you some x, you're trying to predict what y is. Machine learning is all about doing this, without maybe trying to even understand the physics behind this. They're saying, well, you give me a bag of words. I want to understand whether it's going to be a spam or not. You give me a bunch of economic indicators. I want you to tell me how much I should be selling my car for. You give me a bunch of measurements on some patient. I want you to predict how this person is going to respond to my drug, and things like this. And so often, we actually don't have much modeling intuition about what the relationship between x and y is. And this linear thing is basically the simplest function we can think of. Arguably, linear functions are the simplest functions that are not trivial. Otherwise, we would just say, well, let's just set x, y to be a constant, meaning it does not depend on x. But if you want it to depend on x, linear functions are basically as simple as it gets. Turns out, amazingly, this does the trick quite often. So for example, if you look at economics, you might want to assume that the demand is a linear function of the price. So if your price is 0, there's going to be a certain demand. And as the price increases, the demand is going to move. Do you think b is going to be positive or negative here? What? AUDIENCE MEMBER 1. Negative. AUDIENCE MEMBER 2. Typically, it's negative, unless we're talking about maybe luxury goods, where the more expensive, the more people actually want it. I mean, if we're talking about actual economic demand, it's probably definitely negative. It doesn't have to be clearly linear so that you can actually make it linear, transform it into some linear. So for example, you have this multiplicative relationship, right? PV equals nRT, which is the ideal gas law. If you want to actually write this relationship, if you want to predict what the pressure is going to be as a function of the volume and the temperature, and well, let's assume that n is the Avogadro constant, and let's assume that the radius is actually fixed, then you have basically, you take the log on each side, right? So you get PV equals nRT. So what that means is that log PV is equal to log nRT. So that means that log P plus log V is equal to log nR plus log T. So this here, we said that R is constant. So this is actually a constant. I'm going to call it A. And then that means that log P is equal to minus log V. Seriously? That log P is equal to A minus log V plus log T. OK? And so in particular, if I write B equal to negative 1 and C equal to plus 1, this gives me the formula that I have here. Now again, it might be the case that this is the ideal gas law. So in practice, if I start recording pressure and temperature and volume, those things, I might make measurement errors. It might be slightly different conditions in such a way that I'm not going to get exactly those. And I'm just going to put this little twiddle to account for the fact that the points that I'm going to be recording for log pressure, log volume, and log temperature are not going to be exactly on one line. OK, they're going to be close. Actually, in those physics experiments, usually they're very close. It's actually because the conditions are controlled under lab experiments. So it means that the noise is very small. But for other cases, like demand and price, this is not a law of physics. And so this might change. Even the linear structure is probably not clear. At some point, there's probably going to be some weird curvature happening. So this slide is just to tell you maybe you don't have, obviously, a linear relationship. But maybe you do if you start taking logs, exponentials, squares. You can sometimes take the product of two variables, things like this. So this is variable transformation. And it's mostly domain-specific. So we're not going to go into much details of this. Any questions? All right, so now I'm going to be given. So if we start thinking a little more about what those coefficients should be, well, remember, so just everybody is clear why I don't put the little i here? I don't put the little i because I'm just talking about a generic x and a generic y. But the observations are x1, y1. So typically, on the blackboard, I'm often going to write only xy, but the data really is x1, y1, all the way to xn, yn. So those are these points in this two-dimensional plot. But I think of those as being copies, independent copies of the pair xy. They have to contain the relationship. And so when I talk about distribution of those random variables, I talk about the distribution of xy, and that's the same. So the first thing you might want to ask is, well, if I have an infinite amount of data, what can I hope to get for a and b? If my sample size goes to infinity, then I should actually know exactly what the distribution of xy is. And so there should be an a and a b that captures this linear relationship between y and x. And so in particular, we're going to try to ask the population or theoretic values of a and b. And you can see that you can actually compute them explicitly. So let's just try to find how. So as I said, we have a bunch of points on this line, close to a line, and I'm trying to find the best fit. So this guy is not a good fit. This guy is not a good fit. And we know that this guy is a good fit somehow. So we need to mathematically formulate the fact that this line here is better than this line here, or better than this line here. So what we're trying to do is to create a function that has values that are smaller for this curve and larger for these two curves. And the way we do it is by measuring the fit. And the fit is essentially the aggregate distance of all the points to the curve. And there's many ways I can measure the distance to a curve. So if I want to find, so let's just open a parenthesis. If I have a point here, so we're going to do it for one point at a time. So if I have a point, there's many ways I can measure its distance to the curve. I can measure it like that. That is one distance to the curve. I can measure it like that by having a right angle here. That is one distance to the curve. Or I can measure it like that. That is another distance to the curve. There's many ways I can go for it. It turns out that one is actually going to be fairly convenient for us. And that's the one that says, let's look at the square of the value of x on the curve. So if this is the curve, y is equal to a plus bx. Now, I'm going to think of this point as a random point, capital X, capital Y. And so that means that it's going to be x1, y1, or x2, y2, et cetera. Now, I want to measure the distance. Can somebody tell me which of the three, the first one, the second one, or the third one, this formula, expectation of y minus a minus bx squared, which of the three is it representing? The second one where I have the right angle? OK. Everybody agrees with this? Anybody wants to vote for something else? Yeah? The third one, everybody agrees with the third one? So by default, everybody's on the first one? Yeah? AUDIENCE MEMBER 2. It's the vertical distance. PHILIPPE RIGOLLET. Yeah, it is the vertical distance, actually. And the reason is, if it was the one with the straight angle, with the right angle, it would actually be a very complicated mathematical formula. So let's just see why. And by why, I mean why. So this means that this is my x, and this is my y. So that means that this point is xy. So what I'm measuring is the difference between y minus a plus b times x. This is the thing I'm going to take the expectation of, the square, and then the expectation. So a plus b times x, if this is this line, this is this point. So that's this value here. This value here is a plus bx. So what I'm really measuring is the difference between y and m plus bx, which is this distance here. And since I like things like Pythagoras' theorem, I'm actually going to put a square here before I take the expectation. So now this is a random variable. This is this random variable. So I want a number. So I'm going to turn it into a deterministic number. And the way I do this is by taking expectation. And doing expectation, if you think expectation should be close to average, this is the same thing as saying I want that in average, the y's are close to the a plus bx. So we're doing it in expectation, but that's going to translate into doing it in average for all the points. So this is the thing I want to measure. So that's this vertical distance. Yeah. Oh, again. OK. Sorry, I'm just, this is my fault, actually. Maybe we should close those shades. OK, I cannot do just one at a time. Sorry. OK. So the, all right. So now that I do those vertical distances, I can ask, well, now I have this function, right? I have a function that takes two parameters, a and b, maps it to the expectation of y minus a plus bx squared. Sorry, the square is here. And I can ask, well, this is a function that measures the fit of the parameters a and b, right? This function should be small. The value of this function here, function of a and b that measures how close the point x, y is to the line a plus b times x, well, y is equal a plus b times x in the expectation. OK, agreed? This is what we just said. And you can think, again, if you're not comfortable with the reason why it's you get expectations, just think about having data points and taking the average value for this guy. So it's basically an aggregate distance of the points to the line. OK, everybody agrees this is a legitimate measure? If all my points were on the line, if my distribution, if y was actually equal to a plus bx for some a and b, then this function would be equal to 0 for the correct a and b, right? If they're far, well, it's going to depend on how much noise I'm getting. But it's still going to be minimized for the best one. So let's minimize this thing. So here, I don't make any, again, sorry, I don't make any assumption on the distribution of x or y. Here, I assume somehow that the variance of x is not equal to 0. Can somebody tell me why? Yeah? I have another question. On the slide, you have y minus a minus bx quantity squared expectation of that. And here, you've written the square of the expectation. No, here, I've actually written the expectation of the square. If I wanted to write the square of the expectation, I would just do this. So let's just make it clear. Right? Do you want me to put an extra set of parentheses? That's what you want me to do? Yeah. Yeah, sorry. I just confused it with something. OK. That's the one that makes sense. So the square of the expectation. Yeah. The expectation of the square, sorry. Yeah. Yeah. Dyslexia. All right. Any question? Yeah? Does this assume that the error is Gaussian? No. Or I mean, in the sense that if we knew that the error was e to the minus, followed some e to the minus x to the fourth distribution, wouldn't we want to minimize the expectation of the fourth power of y minus a equals bx in order to get the best fit? Why? Because let's just. So you know the answer to your question. So I just want you to use the words that. So why would you want to use the fourth power? Because we want a more strongly penalized deviation. We would expect very large deviations to be very rare, or more rare than the words that would have gotten. Yeah. So that would be the maximum likelihood estimator that you're describing to me. I can actually write the likelihood of a pair of numbers ab. And if I know this, that's actually what's going to come into it, because I know that the density is going to come into play when I talk about the error. But here, I'm just talking about a fit. This is a mechanical tool. I'm just saying, let's minimize the distance to the curve. Another thing I could have done is take the absolute value of this thing, for example. I just decided to take the square before I did it. So regardless of what I'm doing, I'm just taking the squares, because that's just going to be convenient for me to do my computations for now. But we don't have any statistical model at this point. I didn't say anything that y follows this, x follows this. I'm just doing minimal assumptions as we go. So the variance of x is not equal to 0. Can somebody tell me why? What would my clouded point look like if the variance of x was equal to 0? Yeah, they would all be at the same point. So it's going to be hard for me to start fitting a line. I mean, best case scenario, I have this x. It has variance 0. So this is the expectation of x. And all my points have the same expectation. And so yes, I could probably fit that line. But it wouldn't help very much for other x's. So I need a bit of variance so that things spread out a little bit. OK, I'm going to have to do this. I think it's just my, OK, let's hope I'm going to. All right, so I'm going to put a little bit of variance. And the other thing is here, I don't want to do much more. But I'm actually going to think of x as having mean 0. And the way I do this is as follows. Let's define x tilde, which is x minus the expectation of x. So definitely, the expectation of x tilde is what? 0. And so now I want to minimize in AB expectation of y minus A plus Bx squared. And the way I'm going to do this is by turning x into x tilde and putting the extra expectation of x into the A. So I'm going to write this as expectation of y minus A plus B expectation of x, which I'm going to call A tilde, and plus Bx tilde. And everybody agrees with this? So now I have two parameters, A tilde and B. And I'm going to pretend that now the role of x is split by x tilde, which is now a centered random variable. So I'm going to call this guy A tilde. But for my computations, I'm going to call it A. So how do I find the minimum of this thing? Derivative equal to 0, right? So here, it's a quadratic thing. It's going to be like that. I take the derivative, set it to 0. So I'm first going to take the derivative with respect to A and set it equal to 0. So that's equivalent to saying that the expectation of, well, here I'm going to pick up a 2. y minus A plus Bx tilde is equal to 0. And then I also have that the derivative with respect to B is equal to 0, which is equivalent to the expectation of, well, I have a negative sign somewhere, so let me put it here, minus 2x tilde y minus A plus Bx tilde. OK, see, that's why I don't want to put too many parentheses. OK? So I just took the derivative with respect to A, which is just basically the square. And then I have a negative 1 that comes out from inside. And then I take the derivative with respect to B. Since B has x tilde in factor, it comes out as well. So the minus 2's really won't matter for me. And so now I have two equations. The first equation, well, it's pretty simple. It's just telling me that the expectation of y minus A is equal to 0. So what I know is that A is equal to the expectation of y. And really, that was A tilde, which implies that the A I want is actually equal to the expectation of y minus B times the expectation of x. OK? Just because A tilde is A plus B times the expectation of x. So that's for my A. And then for my B, I use the second one. So the second one tells me that the expectation of x tilde y is equal to A plus B times the expectation of x tilde, which is 0. Right? OK? But this A is actually A tilde in this problem. So it's actually A plus B expectation of x. Now, this is the expectation of the product of two random variables. But x tilde is centered, right? It's x minus expectation of x. So this thing is actually equal to the covariance between x and y by definition of covariance. So now I have everything I need, right? How do I just? I'm sorry about that. So I have everything I need. Now, I have two equations with two unknowns. And all I have to do is to basically plug it in. So it's essentially telling me that the covariance of xy. So the first equation tells me that the covariance of xy is equal to A plus B expectation of x. But A is expectation of y minus B expectation of x. So it's, well, actually, maybe I should start with B. Oh, sorry, sorry, sorry. OK. I forgot one thing. Sorry. This is not true, right? I forgot this term. x tilde multiplies x tilde here. So what I'm left with is x tilde. It's minus B times the expectation of x tilde squared. So that's actually minus B times the variance of x tilde, because x tilde is already centered, which is actually the variance of x. So now I have that this thing is actually A plus B expectation of x minus B variance of x. And I also have that A is equal to expectation of y minus B expectation of x. So if I sum the two, those guys are going to cancel, those guys are going to cancel. And so what I'm going to be left with is covariance of xy is equal to expectation of x, expectation of y. And then I'm left with this term here, minus B times the variance of x. And so that tells me that B, why do I still have the variance there? So is the covariance really the expectation of x tilde times y minus expectation of y? Because y is not centered, right? Yeah. OK. So that's still the center. But it's x tilde centered, right? So you just need to have one that's centered for this to work. Oh, I see. I mean, you can check it. But basically, when you're going to have the product of the expectations, you only need one of the two in the product to be 0. So the product is 0. So OK, why do I keep my, so I get AA and then the B expectation. OK, so that's probably earlier that I made a mistake. So I get, so this was a tilde. I just need to be clear about the. So that tells me that a tilde, maybe it's not super fair of me to, yeah, OK, I think I know where I made a mistake. I should not have centered. I wanted to make my life easier. And I should not have done that. And the reason is a tilde depends on b. So when I take the derivative with respect to b, what I'm left with here, since a tilde depends on b, when I take the derivative of this guy, I actually don't get a tilde here. But I really get, so again, this was not, so that's the first one. Yeah, so this is actually x here, right? When I take the derivative with respect to b. And so now what I'm left with is that the expectation, so yeah, I'm basically left with nothing that helps. So I'm sorry about that. Let's start from the beginning, because this is not getting us anywhere. And a fix is not going to help. So let's just do it again. Sorry about that. So let's not center anything and just do it brute force, because we're going to bx squared. All right, partial with respect to a is equal 0 is equivalent, so my minus 2 is going to cancel, right? So I'm going to actually forget about this. So it's actually telling me that the expectation of y minus a plus bx is equal to 0, which is equivalent to a plus b expectation of x is equal to the expectation of y. Now, if I take the derivative with respect to b and set it equal to 0, this is telling me that the expectation of, well, it's the same thing, except that this time I'm going to pull out an x. This guy is equal to 0. This guy is not here. And so that implies that the expectation of xy is equal to a times the expectation of x plus b times the expectation of x squared. All right, so the first one is actually not giving me much. So I need to actually work with the two of those guys. So I'm going to take the first. So let me rewrite those two inequalities that I have. I have a plus b, e of x is equal to e of y. And then I have e of xy. And now what I do is that I multiply this guy. So I want to cancel one of these things. So I'm going to take this guy, and I'm going to multiply it by e of x and take the difference. So I do times e of x. And then I take the sum of those two, and then those two terms are going to cancel. So then that tells me that b times e of x squared plus the expectation of xy is equal to. So this guy is the one that canceled. Then I get this guy here, expectation of x times expectation of y plus the guy that remains here, which is b times the expectation of x squared. So here I have b expectation of x, the whole thing squared. And here I have b expectation of x squared. So if I pull this guy here, what do I get? b times the variance of x. So I'm going to move here. And this guy here, when I move this guy here, I get the expectation of x times y minus the expectation of x times the expectation of y. So this is actually telling me that the covariance of x and y is equal to b times the variance of x. And so then that tells me that b is equal to covariance of xy divided by the variance of x. And that's why I actually need the variance of x to be non-zero, because I couldn't do that otherwise. And because if it was, it would mean that b should be plus infinity, which is what the limit of this guy is when the variance goes to 0. Or negative infinity, I cannot sort them out. So I'm sorry about the mess, but that should be more clear. Then a, of course, you can write it by plugging in the value of b so you know it's only a function of your distribution. So what are the characteristics of the distribution? So distribution can have a bunch of things. It can have moments of order 4, of order 26. It can have heavy tails or light tails. But when you compute least squares, the only thing that matters are the variance of x, the expectation of the individual ones, and really what captures how y changes when you change x is captured in the covariance. The rest is really just normalization. It's just telling you, I want things to cross the y-axis at the right place. I want things to cross the x-axis at the right place. But the slope is really captured by how much more covariance you have relative to the variance of x. So this is essentially setting the scale for the x-axis. And this is telling you for a unit scale, this is the unit of y that you're changing. So we have explicit forms. And what I could do if I wanted to estimate those things is just say, well, again, we have expectations. The covariance is the expectation of xy minus the product of the expectations. I could replace expectations by averages and get an empirical covariance, just like we can replace the expectations for the variance and get a sample covariance. And this is basically what we're going to be doing. This is essentially what you want. The problem is that if you view it that way, you sort of prevent yourself from being able to solve the multivariate problem. Because it's only in the univariate problem that you have closed form solutions for your problem. But if you actually go to multivariate, this is not where you want to replace expectations by averages. You actually want to replace expectation by averages here. And once you do it here, then you can actually just solve the minimization problem. OK. So one thing that arises from this guy is that this is an interesting formula. All right? Think about it. If I have that y is a plus bx plus some noise now. Things are no longer on something. I have that y is equal to a plus bx plus some noise, which is usually denoted by epsilon. So that's the distribution, right? If I tell you the distribution of x, and I say y is a plus bx plus epsilon, I tell you the distribution of y. And if they mean that those two are independent, you have a distribution on y. So what happens is that I can actually always say, well, this is equivalent to saying that epsilon is equal to y minus a plus bx, right? I can always write this. I mean, it's the tautology. But here, for those guys, this is not for any guy, right? This is really for the best fit, a and b. Those ones that satisfy this gradient is equal to 0 thing, then what we had is that the expectation of epsilon was equal to expectation of y minus a plus b expectation of x, right? By linearity of the expectation, which was equal to 0. So for this best fit, we have 0. Now, the covariance between x and y, between, sorry, x and epsilon, is what? Well, it's the covariance between x and, well, epsilon was y minus a plus bx. OK, now, the covariance is bilinear. So what I have is that the covariance of, this is the covariance of x and times y, sorry, of x and y minus the covariance, well, minus a plus b covariance of x and x, which is the variance of x. Covariance of xy minus a plus b covariance of x. And covariance, OK, I didn't write it. So here I have covariance of xy is equal to b variance of x, right? OK, so why did I? Covariance of xy, yeah, that's because I cannot do that with the covariance. Yeah, I have those averages again. No, because this is centered, right? Sorry, this is centered. So this is actually equal to the expectation of x times y minus a plus bx, right? The covariance is equal to the product just because this right-hand side is actually centered. So this is the expectation of x times y minus the expectation of a times x, a times the expectation of x, plus b minus b times the expectation of x squared. Well, actually, maybe I should not really go too far. So this is actually the one that I need. But if I stop here, this is actually equal to 0, right? Those are the same equations. OK? Yeah? So we're just saying that if I actually believed that this best fit was the one that gave me the right parameters, what would that imply on the noise itself, on this epsilon? So here, we're actually just trying to find some necessary condition for the noise to hold, for the noise. And so those conditions are that first, the expectation is 0. That's what we got here. And then that the covariance between the noise and x has to be 0 as well. OK? So those are actually conditions that the noise must satisfy. But the noise was just not really defined as noise itself. We were just saying, OK, if we're going to put some assumptions on epsilon, what do we better have? So the first one is that it's centered, which is good, because otherwise, the noise would shift everything. So now, when you look at a linear regression model, typically, if you look, if you open a book, it doesn't start by saying, let the noise be the difference between y and what I actually want y to be. It says, let's y be a plus bx plus epsilon. So conversely, if we assume that this is the model that we have, then we're going to have to assume that epsilon is centered and that the covariance between x and epsilon is 0. Actually, often, we're going to assume much more. And one way to ensure that those two things are satisfied is to assume that x is independent of epsilon, for example. If you assume that x is independent of epsilon, of course, the covariance is going to be 0. Or we might assume that the conditional expectation of epsilon given x is equal to 0, then that implies that. Now, the fact that it's centered is one thing. So if we make this assumption, the only thing it's telling us is that those a, b's that come, we started from there. y is equal to a plus bx plus some epsilon for some a and some b. What it turns out is that those a's and b's are actually the ones that you would get by solving this expectation of square thing. So when you asked back when you were following, so when you asked, why don't we take the square, for example, or the power 4 or something like this, then here I'm saying, well, if I have y is equal to a plus bx, I don't actually need to put too much assumptions on epsilon. If epsilon is actually satisfying those two things, expectation is equal to 0 and the covariance with x is equal to 0, then the right a and b that I'm looking for are actually the ones that come with the square, not with power 4 or power 25. So those are actually pretty weak assumptions. If we want to do inference, we're going to have to assume slightly more. If we want to use t distributions at some point, for example, and we will, we're going to have to assume that epsilon has a Gaussian distribution. So if you want to start doing more statistics beyond just doing this least squares thing, which is minimizing the square quaternion, you're actually going to have to put more assumptions. But right now, we did not need them. We only need that epsilon as mean 0 and covariance 0 with x. OK, so let's start with, so that was basically probabilistic. We're trying to say, if I were to do probability and I were trying to model the relationship between two random variables, x and y, in the form y is a plus bx plus some noise, this is what would come out. Everything was expectations. There was no data involved. So now let's go to the data problem, which is now, I do not have, I do not know what these expectations are. In particular, I don't know what the covariance of x and y is. And I don't know what the expectation of x and the expectation of y are. So I have data to do that. So how am I going to do this? Well, I'm just going to say, well, if I want x1, y1, xn, yn, and I'm going to assume that they're iid. And I'm actually going to assume that they have some model. So I'm going to assume that I have that a, so that yi follows the same model. So epsilon i, our iid. And I want to say that expectation of epsilon i is 0 and covariance of xi epsilon i is equal to 0. So I'm going to put the same model on all the data. So you can see that a is not ai and b is not bi. It's the same. So as my data increases, I should be able to recover the correct things, as the size of my data increases. So this is what the statistical problem looks like. You're given the points. There is a true line from which this point was generated. There was this line. There was a true a, b that I used to draw this plot. And that was the line. So first, I picked an x, say, uniformly at random on this interval 0 to 2. I'd say that was this one. Then I said, well, I want y to be a plus bx. So it should be here. But then I'm going to add some noise epsilon to go way again back from this line. And that's actually me. Here, we actually got two points correct on this line. So there's basically two epsilons that were small enough that the dots actually look like they're on the line. Everybody's clear about what I'm drawing? So now, of course, if you're a statistician, you don't see this. You only see this. And you have to recover this guy. And it's going to look like this. You're going to have an estimated line, which is the red one, and the blue line, which is the true one, the one that actually generated the data. And your question is, well, this line corresponds to some parameters a hat and b hat. How can I make sure that those two lines, how far those two lines are? And one way to address this question is to say, how far is a from a hat, and how far is b from b hat? Another question, of course, that you may ask is, how do you find a hat and b hat? And as you can see, it's basically the same thing. Remember, what was a? So b was the covariance between x and y divided by the variance of x, which I can rewrite as the expectation of xy minus expectation of x times expectation of y divided by expectation of x squared minus expectation of x. The whole thing squared. OK? If you look at the expression for b hat, I basically replaced all the expectations by bars. So I said, well, this guy I'm going to estimate by an average. So that's the xy bar is 1 over n sum from i equal 1 to n of xi times yi. x bar, of course, is just the one that we're used to. And same for y bar. x squared bar, the one that's here, is the average of the squares. And x bar squared is the square of the average. OK? So you just basically replace this guy by x bar, this guy by y bar, this guy by x squared bar, and this guy by x bar in the square. OK, so that's basically one way to do it. Everywhere you see an expectation, you replace it by an average. That's the usual statistical hammer. You can actually be slightly more subtle about this. And as an exercise, I invite you to just to make sure that you know how to do this computation. It's going to be exactly the same kind of computations that we've done. But as an exercise, you can check that if you actually look at, say, well, what I wanted to minimize here, I had an expectation, right? And I said, let's minimize this thing. Well, let's replace this by an average first and now minimize. OK? So if I do this, it turns out I'm going to actually get the same result. The minimum of the average is basically when I replace the expectation by the average and then minimize, it's the same thing as first minimizing and then replacing expectation by average in this case. Again, this is a much more general principle because if you don't have a closed form for the minimum, for some, say, likelihood problems, well, you might not actually have a possibility to just look at what the formula looks like, see where the expectations show up, and then just plug in the averages instead. So this is the one you want to keep in mind. And again, as an exercise, so here and then you do expectation replaced by averages. And then that's the same answer. And I encourage you to solve the exercise. OK, everybody's clear that this is actually the same expression for a hat and b hat that we had before that we had for a and b when we replaced the expectations by averages? Here, by the way, I minimize the sum rather than the average. It's clear to everyone that this is the same thing, right? Yeah? So if instead of replacing it, you just go ahead and minimize the expectation and assume you've switched the derivative and the expectation, would you have the same thing? So we did switch the derivative and the expectation before you came, I think. All right, so indeed, the picture was the one that we said. So visually, this is what we're doing. We're looking among all the lines. For each line, we compute this distance. So if I give you another line, there would be another set of arrows. You look at their length, you square it, and then you sum it all. And you find the line that has the minimum sum of squared length of the arrows. And those are the arrows that we're looking at. But again, you could actually think of other distances. And you could actually get different solutions. So there's something called mean absolute deviation, which rather than minimizing this thing, is actually minimizing the sum from i equal 1 to n of the absolute value of y minus a plus bxi. And that's not something for which you're going to have a closed form, as you can imagine. You might have something that's sort of implicit. But you can actually still solve it numerically. And this is something that people also like to use, but way, way less than the least squares one. AUDIENCE 1. What did you just write? PHILIPPE RIGOLLET. What did I just what? AUDIENCE 1. What did you just write on the screen? PHILIPPE RIGOLLET. The sum of the absolute values of yi minus a plus bxi. So it's the same, except I don't have the square here. So arguably, predicting a demand based on price is a fairly naive problem. Typically, what we have is a bunch of data that we've collected. And we're hoping that together, they can help us do a better prediction. So maybe I don't have only the price, but maybe I have a bunch of other social indicators. Maybe I know the competition, the price of the competition. Maybe I know a bunch of other things that are actually relevant. And so I'm trying to find a way to combine a bunch of points, a bunch of measures. Let's say, for example, there's a nice example that I like, which is people were trying to measure something related to your body mass index. So basically, the volume of the density of your body. And the way you can do this is by just really weighing someone and also putting them in some cubic meter of water and see how much overflows. And then you have both the volume and the mass of this person. And you can start computing density. But as you can imagine, I would not personally like to go to a gym where the first thing they ask me is to just go in some bucket of water. And so people try to find ways to measure this based on other indicators that are much easier to measure. For example, I don't know, the length of my forearm and the circumference of my head and maybe my belly would probably be more appropriate here. And so they just try to find something that actually makes sense. And so there's actually a nice example where you can show that if you measure, I think one of the most significant was with the circumference of your wrist. This is actually a very good indicator of your body density. And it turns out that if you stuff all the bunch of things together, you might actually get a very good formula to explain things. So what we're going to do is rather than saying we have only one x to explain y's, let's say we have 20 x's that we're trying to combine to explain y. And again, just like assuming something of the form y is a plus b times x was the simplest thing we could do, here we're just going to assume that we have y is a plus b1 x1 plus b2 x2 plus b3 x3. And we can write it in a vector form by writing that yi is xi transpose b, which is now a vector, plus epsilon i. And here on the board, I'm going to have a hard time doing boldface, but all these things are vectors, except for y, which is a number. yi is a number. It's always the value of my y-axis. So even if my x-axis lives on this is x1 and this is x2, y is really just the real valued function. And so I'm going to get a bunch of points x1, y1, and I'm going to see how much they respond. So for example, my body density is y, and then all the x's are a bunch of other things. Agreed with that? So this is an equation that holds on the real line, but this guy here is in RP, and this guy is in RP. It's actually common to call b beta when it's a vector, and that's the usual linear regression notation. y is x beta plus epsilon. So x's are called explanatory variables. y is called explained variable or dependent variable or response variable. It has a bunch of names. You can use whatever you feel more comfortable with. It should actually be explicit, so that's all you care about. Now, what we typically do is that rather, so you notice here that there's actually no intercept. If I actually fold that back down to one dimension, there's actually a is equal to 0. If I go back to p is equal to 1, that would imply that yi is, well, say, beta times x plus epsilon i. And that's not good. I want to have an intercept. And the way I do this, rather than writing a plus this and just have an overload of notation, what I'm actually doing is that I fold back. I fold my intercept back into my x. And so if I measure 20 variables, I'm going to create a 21st variable, which is always equal to 1. So you should need to think of x as being 1 and then x1, xp. And now, sorry, xp minus 1, I guess. And now this is in rp. I'm always going to assume that the first one is 1. I can always do that. If I have a table of data, if my data is given to me in an Excel spreadsheet, and here I have the density that I measured on my data, and then maybe here I have the height, and here I have the wrist circumference, and I have all these things, all I have to do is to create another column here of 1's. I just put 1, 1, 1, 1, 1, 1. That's all I have to do to create this guy. Agreed? And now my x is going to be just one of those rows. So this is xi, this entire row, and this entry here is yi. So now, for my noise coefficients, I'm still going to ask for the same thing, except that here, the covariance is not between one random variable and another random variable. It's between a random vector and a random variable. How do I measure the covariance between a vector and a random variable? AUDIENCE 1. You can get a covariance vector. PHILIPPE RIGOLLET. Yeah, so basically, wow, OK. AUDIENCE 1. Maybe you can take a normal covariance vector. PHILIPPE RIGOLLET. Yeah, but I mean, the covariance vector is equal to 0 is the same thing as it's normal equal to 0. Yeah, this is basically thought of entry-wise. For each coordinate of x, I want that the covariance between epsilon and this coordinate of x is equal to 0. So I'm just asking this for all coordinates. Again, in most instances, we're going to think that epsilon is independent of x, and that's something we can understand without thinking about coordinates. All right, so yeah? AUDIENCE 1. PHILIPPE RIGOLLET. I'm sorry, can you repeat that question? I didn't hear. AUDIENCE 1. Is this a parameter, beta, a parameter? PHILIPPE RIGOLLET. Yeah, beta is the parameter we're looking for, right? Just like it was a and the pair a, b has become the whole vector beta now. AUDIENCE 1. And what's beta? PHILIPPE RIGOLLET. So there's, well, can you think of an intercept of a function that takes, I mean, there is one, actually. That's the one for which beta is, all the betas that don't correspond to the vector of all ones. So the intercept is really the weight that I put on this guy. That's the beta that's going to come to this guy, but we don't really talk about intercept. In two dimensions, if x lives in two dimensions, the way you want to think about this is you take a sheet of paper like that. So now I have points that live in three dimensions. So let's say one direction here is x1. This direction is x2. And this direction is y. And so what's going to happen is that I'm going to have my points that live in this three-dimensional space. And what I'm trying to do when I'm trying to do a linear model for those guys, when I assume a linear model, what I assume is that there's a plane in those three dimensions. So think of this guy as being going everywhere, and there's a plane close to which all my points should be. That's what's happening in two dimensions. If you see higher dimensions, then congratulations to you. But I can't. But you can definitely formalize that fairly easily mathematically and just talk about vectors. So now here, if I talk about the least squared error estimator or just the least squares estimator of beta, it's simply the same thing as before. Just like we said. So remember, you should think of beta as being both the pair AB generalized. So we said, oh, we wanted to minimize the expectation of y minus a plus bx squared. Now, so that's for p is equal to 1. Now, for p larger than or equal to 2, we're just going to write it as y minus x transpose beta squared. So I'm just trying to minimize this quantity. Of course, I don't have access to this. So what I'm going to do is I'm going to replace my expectation by an average. So here, I'm using the notation t because beta is the true one. And I don't want to just, so here I have a variable t that's just moving around. And so now I'm going to take the square of this thing. And when I minimize this overall t in our p, the arg min, the minimum is attained at beta hat, which is my estimator. So if I want to actually compute, yeah? AUDIENCE 2 On the last slide, do we require the expectation of the noise term to be 0? You mean the previous slide? So again, I'm just defining an estimator. Just like I would tell you, just take the estimator that has coordinates 4 everywhere. Something like in that time, if the noise term we want to satisfy the covariance of that guy, we also want them to satisfy expectation of each noise term to be 0? So yes. So the answer is yes. I was just trying to think if this was captured. So it is not captured in this guy because this is just telling me that the expectation of epsilon i minus expectation of epsilon i is equal to 0. OK, yeah. So yes, I need to have that epsilon i has mean 0. So let's assume that expectation of epsilon i is 0 for this problem. And we're going to need something about some sort of question about the variance being not equal to 0, right? But this is going to come up later. So let's think for one second about doing the same approach as we did before. Take the partial derivative with respect to the first coordinate of t, with respect to the second coordinate of t, with respect to the third coordinate of t, et cetera. So that's what we did before. We had two equations. And we reconciled them because it was fairly easy to solve, right? But in general, what's going to happen is we're going to have a system of equations. We're going to have a system of p equations, one for each of the coordinates of t. And we're going to have p unknowns, each coordinate of t. And so we're going to have the system to solve. Actually, it turns out it's going to be a linear system. But it's not going to be something that we're going to be able to solve coordinate by coordinate. It's going to be annoying to solve. You can guess what's going to happen, right? Here it involved the covariance between x and epsilon, right? That's what it involved to understand the correlation between x and y, to understand how the solution of this problem was. In this case, there's going to be not only the correlation, sorry, the covariance between x1 and y, x2 and y, x3, et cetera, all the way to xp and y. There's also going to be all the cross-covariances between xj and xk. And so this is going to be a nightmare to solve in this system. And what we do is that we go on to using a matrix notation so that when we take derivatives, we take derivative, we talk about gradients. And then we can invert matrices and solve linear systems in a somewhat formal manner by just saying that if I want to solve the system Ax equals b, rather than actually solving this for each coordinate of x individually, I just say that x is equal to A inverse times b. So that's really why we're going to the equation 1, because we have a formalism to write that x is the solution of this system. I'm not telling you that this is going to be easy to solve numerically, but at least I can write it. And so here's how it goes. I have a bunch of vectors. So what are my vectors? So I have x1. Oh, by the way, I didn't actually mention that. When I put the lower case, when I put the subscript, I'm talking about the observation. And when I put the superscript, I'm talking about the coordinates. So I have x1, which is equal to 1, x11, x1p, x2, which is 1, x21, x2p, all the way to xn, which is 1, xn1, xnp. So those are my n observed x's. And then I have another y1, y2, yn that comes paired with those guys. So the first thing is that I'm going to stack those guys into some vector that I'm going to call y. So maybe I should put an arrow for the purpose of the blackboard, and it's just y1 to yn. So this is a vector in Rn. Now, if I want to stack those guys together, I can either create a long vector of size n times p. But the problem is that I lose the role of who's a coordinate and who's at observation. And so it's actually nicer for me to just put those guys next to each other and create one new variable. And so the way I'm going to do this is rather than actually stacking those guys like that, I'm going to take their transpose and stack them as rows of a matrix. So I'm going to create a matrix, which here is denoted typically by I'm going to write x double bar. And here I'm going to actually just, since I'm taking those guys like this, the first column is going to be only ones. And then I'm going to have x11, x1p. And here I'm going to have xn1, xnp. So here, the number of rows is n, and the number of columns is p. One row per observation, one column per coordinate. And again, I make your life miserable, because this really should be p minus 1, because I already used the first one for this guy. I'm sorry about that. It's a bit painful. So usually, we don't even write what's in there, so we don't have to think about it. Those are just vectors of size p. So now that I create this thing, I can actually just basically stack up all my models. So Yi equals Xi transpose beta plus epsilon i for all i equal 1 to n. This transforms into, this is equivalent to saying that the vector y is equal to the matrix x times beta plus a matrix plus a vector epsilon, where epsilon is just epsilon 1 to epsilon n. So I have just this system, which I write as a matrix, which really just consists in stacking up all these equations next to each other. So now that I have this model, this is the usual least squares model. And here, when I want to write my least squares criterion in terms of matrices, my least squares criterion, remember, was sum from i equal 1 to n of Yi minus Xi transpose beta squared. Well, here, it's really just the sum of the square of the coefficients of the vector y minus x beta. So this is actually equal to the norm squared of y minus x beta squared. So that's just the square norm is, by definition, the sum of the square of the coordinates. And so now I can actually talk about minimizing a norm squared. And here, it's going to be easier for me to take derivatives. So we'll do that next time.