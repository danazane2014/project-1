 The following content is provided under a Creative Commons license. Your support will help MIT OpenCourseWare continue to offer high quality educational resources for free. To make a donation or to view additional materials from hundreds of MIT courses, visit MIT OpenCourseWare at ocw.mit.edu. So I've worked hard over the weekend, figured out what I was doing last time and what I'm doing this time. And improved the notes. So you'll get a new set of notes on the last lecture and on this one. And I kind of got a better picture of what we're doing. And that board is aiming to describe the large picture of what we're doing last time and this time. So last time was about changes in A inverse when A changed. This time is about changes in eigenvalues and changes in singular values when A changes. You can imagine this is a natural important situation. Matrices move. And therefore, their inverses change. Their eigenvalues change. Their singular values change. And you hope for a formula. Well, so we did have a formula for last time for the change in the inverse matrix. And I didn't get every u and v transpose in the right place in the video or in the first version of the notes. But I hope that they'll be the formula, that that Woodbury-Morrison formula will be correct this time. OK. So I won't go back over that part. But I realize also there's another question that we can answer when the change is very small, when the change in A is dA or delta A, a small change. And that's, of course, what calculus is about. So I have two sort of parallel topics here. What is the derivative when the change is infinitesimal? And what is the actual change when the change is finite size? OK. So now let me say what we can do and what we can't do. Oh, I'll start out by figuring out what the derivative is for the inverse. So that's like completing the last time for infinitesimal changes. Then I'll move on to changes in the eigenvalues and singular values. And there, you cannot expect an exact formula. We had a formula that was exact apart from any typos for this. And we'll find a formula for this. And we'll find a formula for that and for that. Well, that one will come from this one. So this will be a highlight today. How do the eigenvalues change when the matrix changes? But we won't be able to do parallel to this. We won't be able to. Oh, well, we will be able to do something for finite changes. That's important. Mathematics would have to keep hitting at that problem until it got somewhere. So I won't get an exact formula for the change. That's too much. But I'll get inequalities, how big that change could be. What can I say about it? So that's highly interesting. May I start with completing the last lecture? What is the derivative of the inverse? So I'm thinking here, so what's the setup? The setup is my matrix A depends on t. And it has an inverse. A inverse depends on t. And if I know this dependence, in other words, if I know dA dt, how much, how the matrix is depending on t, then I hope I could figure out what the derivative of A inverse is. We should be able to do this. So let me just start with the it's not hard. And it complements this one by doing the calculus case, the infinitesimal change. So I want to get to that. So I'm given, I can figure out the change in A. And my job is to find the change, the derivative of A inverse. So here's a handy identity. And I just put this here. So here's my useful identity. I want to, so as last time, I start with a finite change, because calculus always does that, right? Starts with a delta t, and then it goes to 0. So here I'm up at the full size change. So I think that this is equal to B inverse A minus B A inverse. And if it's true, it's a pretty cool formula. And look, it is true, because over on this right-hand side, I have B inverse times AA inverse. That's the identity. So that's my B inverse. And I have the minus. The B inverse B is the identity. There's A inverse. It's good, right? Well, I could actually learn from that the rank of this equals the rank of this. That's a point that I made from the big formula, but now we can see it from the easy formula. I'm assuming that B and A, everywhere here, I'm assuming that A and B are invertible matrices. So when I multiply by an invertible matrix, that does not change the rank. So those have the same rank. But I want to get further than that. I want to find this. OK. So how do I go? How do I go forward with that job to find the derivative of the inverse? Well, I'm going to let I'm going to call this a change in A inverse. And over here, I'll have B will be, yeah, OK. Let's see. Am I right? Yeah. So B inverse will be, this is really, this is A plus delta A inverse. And this is, well, that's A minus B. So that's really minus delta A. Right? The change is, from A to B is the change. Here I'm looking at the difference A minus B. So it's minus the change. And here I have A inverse. I haven't done anything except to introduce this delta for the and get B out of it and brought delta in. OK. Now I'm going to do calculus. So I'm thinking of B as being a little, there's a sort of a delta t. And I'm going to divide both sides by delta t. I have to do this if I want. And now I'll let delta t go to 0. So this is calculus appears. Finally, our, I won't say our enemy calculus, but there is a sort of competition between linear algebra and calculus for college mathematics. Calculus has had far, far too much time and attention. Like it gets three or four semesters of calculus for people who don't get any linear algebra. I'm glad this won't be on the video, but I'm afraid it will. So anyway, of course, calculus is fine in its place. OK. So here's its place. Now let delta t go to 0. So what does this equation become? Then everybody knows that the limit of delta t goes to 0. I replace deltas by the delta. So this delta A divided by delta t, that has a meaning. Each, the top has a meaning and the bottom has a meaning. But then in the limit, it's the ratio that has a meaning. So dA by itself, I don't attach a meaning to. That's infinitesimal. It's the limit. So that's why I wanted a delta over a delta, so I could do calculus. So what happens now as delta t goes to 0? And of course, as delta t goes to 0, that carries delta A to 0. So that becomes A inverse. And what does this approach as delta t goes to 0? dA dt with that minus sign. Oh, I've got to remember the minus sign. Minus sign is in here. So I'm bringing out the minus sign. Then this was A inverse, as we had. And that's dA dt. And that's A inverse. That's our formula, a nice formula, which sort of belongs in people's knowledge. You recognize that if A was a 1 by 1 matrix, we could call it x instead of A. If A was a 1 by 1 matrix x, then I'm seeing the formula for the derivative of 1 over x. A inverse, just a 1 by 1 case, is just 1 over x. So the derivative of 1 over, or maybe t, I should be saying. If A is just t, then the derivative of 1 over t with respect to t is minus 1 over t squared. The 1 by 1 case, we know. That's what calculus does. And now we're able to do the n by n case. So that's just good. And then it's sort of parallel to formulas like this, where this delta A is not gone to 0. It's full size, but low rank. That was the point. Actually, the formula would apply if the rank wasn't low, but the interest is in low rank. Are we good for this? That's really the completion of last time's lecture with derivatives. OK. OK, come back to here, to the new thing now. Lambdas. Let's focus on lambdas, eigenvalues. How does the eigenvalue change when the matrix changes? How does the eigenvalue change when the matrix changes? So I have two possibilities. One is small change when I'm doing calculus and I'm letting a delta t go to 0. The other is full size, order 1 change, where I will not be able to give you a formula for the new lambdas, but I'll be able to tell you important facts about them. So this is today's lecture now. You could say that's the completion of Friday's lecture. What about d lambda dt? It's a nice formula. And proof is fun, too. I was very happy about this proof. OK, so I guess calculus is showing up here on this middle board. So how do I start with the eigenvalues? Well, start with what I know. So these are facts, you could say, that I have to get the eigenvalues into it. And of course, eigenvalues have to come with eigenvectors. So I'll again use A of t. It'll be depending on t. And an eigenvector that depends on t is an eigenvalue that depends on t times an eigenvector that depends on t. Good? That's fact one that we plan to take the derivative of somehow. There's also a second fact that comes into play here. What's the deal on the eigenvalues of A transpose? They're the same. The eigenvalues of A transpose are the same as the eigenvalues of A. Are the eigenvectors the same? Not usually. Of course, if the matrix was symmetric, then A and A transpose are just the same thing. So they would have, so A transpose would have that eigenvalue. But generally, eigenvector. But generally, it has a different eigenvector. And really, to keep it sort of separate from this one, let's call it y. It'll have the same eigenvalue. I'm going to call it y. But I'm going to make it a row vector. Because A transpose is what? Instead of writing down A transpose, I'm going to stay with A, but put the eigenvalue on the left side. So here's the eigenvalue, eigenvector for A on the left. And it has the same eigenvalue times that eigenvector. But that eigenvector is a row eigenvector, of course. This is an equality between rows. A row times my matrix gives a row. So that's the eigenvalues of, and it has the same eigenvalue. So this is totally parallel to that. Totally parallel. And maybe sort of less, definitely less seen. But it's just the same thing for A transpose. Everybody sees it. If I transpose this equation, then I've got something that looks like that. But I'd rather have it this way. Now, one more thing about these, one more fact I need. There has to be some normalization. How long? What should be the length of the? Right now, x could have any length. y could have any length. And there's a natural normalization, which is y transpose times x equal to 1. So that normalizes the two. Doesn't tell me the length of x or the length of y, but it tells me the key thing, the length of both. So what I've got there is tracking along one eigenvalue and its pair of eigenvectors. And you're always welcome to think of the symmetric case when y and x are the same. And then I would call them q. Oh, well, I would call them q if it was a symmetric matrix. But I would have if it's a symmetric matrix, I would call the both eigenvectors would be called q. And this would be saying that q is a unit vector. So this is all stuff we know. And actually, maybe I should write it in matrix notation, just so, because it's important. And so that's for one eigenvector. This is for all of them at once. Everybody's with it? The x's are the columns of x. And I need to multiply. Lambda is the diagonal matrix of lambdas. And it has to sit on the right so that it will multiply those columns. So this is like all eigenvectors at once. What would this one be? This would be like y transpose A equals A. Oh, y transpose, yes, equals. And probably these are multiply. I don't want to put y. I feel wrong if I write y transpose here. Like here, the x was on the right and on the left. And I'll want, oh, yeah, y transpose. Yeah, OK. So what do I put? Lambda y transpose. And what do I put here? What does this translate to if this was for one eigenvector for all of them at once? It's just going to translate to y transpose x equal the identity. This is pretty basic stuff, but stuff somehow we don't always necessarily see. OK, those are the key facts. And now I plan to take the derivative. Take the derivative with respect to lambda. Oh, OK, I can derive one more fact. So this would be a formula. This is formula 1. Formula 1 just says, what do I get if I hit this on the left by y transpose? Can I do that? y transpose of t, A of t, x of t equals lambda of t. That's a number, so I can always bring that out in front of the inner product, the vector notation. Are you good for that? I'm pleading like everything I've done is totally OK. And now I have an improvement to make on this right-hand side, which is? So what is y transpose times x? It's 1. So let's remember that. It's 1. So in other words, I have got a formula for lambda of t. The eigenvalue, as time changes, the matrix changes. Its eigenvalues change according to this formula. Its eigenvectors change according to this formula. And its left eigenvectors change according to that formula. So everything here is above board. And now what's the point? The point is I'm going to find this, the derivative. So I'm going to take the derivative of that equation and see what I get. That'll be the formula for the derivative of an eigenvalue. And amazingly, it's not that widely known. Of course, it's classical, but it's not always part of courses. So as time varies, the matrix varies, A. And therefore, its eigenvalues vary. And its eigenvectors vary. So we're going to find d lambda dt. It's one level of difficulty more to find dx dt, the derivative of the eigenvector, or the second derivative of the eigenvalue. Those kind of come together, and I'm not going to go there. I'm just going to do the one great thing here, take the derivative of that equation. Shall I do it over there? So here we go. So I want to compute d lambda dt. And I'm using this formula for lambda there. So I've got three things that depend on t. And I'm taking the derivative of their product. So I'm going to use the product rule. I'll apply the product rule to that derivative. Takes the derivative of the first guy times A times x. Takes the derivative of the second guy times the second guy. And the third guy. y transpose of t A of t dx dt. OK. OK. We are one minute away from a great formula. And I'm really happy, if you allow me to say it, that the formula comes by just taking those facts we know, putting them together into this expression that we also know. And this is like lambda equals x inverse Ax. That's the diagonalizing thing. And then taking the derivative. So what do I get if I take that derivative? Well, this term I'm going to keep. I'm not going to play with that. Everybody's clear? That's a number. Here's a matrix. dA dt is a matrix. The derivative, I take the derivative of every entry in A. Here's its column vector. Its eigenvector. And here's a row vector. So row times matrix times column is a number, one by one. And actually, that's my answer. That's my answer. So I'm saying that these two terms cancel each other out. Those two terms add to 0. And this is the right answer for the derivative. That's a nice formula. So to find the derivative of an eigenvalue, you take the matrix is changing. You multiply by the eigenvector and by the left eigenvector. Gives you a number. And that's the d lambda dt. So why do those two guys add to 0? That's all that remains here. And then this topic is ended with this nice formula. OK. So I want to simplify that, simplify that, and show that they cancel each other. OK. So what is Ax? It's lambda x. So this guy is nothing but its lambda, that depends on time, of course, times dy dt, dy transpose dt. I'm just copying that. Ax is lambda x. Sorry, I didn't mean to make that look hard. OK with that? Ax is lambda x. And I am perfectly safe, because lambda is just a number to bring it out to the left. So it doesn't look like it's in the way. And what about this other term? So I have y transpose. Oh, y transpose A. What's that? What's y transpose A? That's the combination that I know. y transpose A. A, y is that left eigenvector. y transpose A brings out a lambda. So this also brings out a lambda times y transpose times dx dt. OK. I just used Ax equal lambda x there. That was really nothing. Now what do I do? I want this to be 0. Can you see it happening? It's a great pleasure to see it happening. So what do I have here? What's my first step now? Bring lambda outside. That's not 0. We don't know what that is. Bring lambda outside there times the whole thing. So for some wonderful reason, I believe that this number, which is a row times a column, a row times a column, two terms there, I believe they knock each other out. And that result is 0. And why? Why? Because I come back to the, this board has all that I know. And here's y transpose times x equal 1. And how does that help me? Because what I'm seeing in that square, in those brackets, is the derivative of y transpose x. So it's the derivative of 1, therefore 0. So this is the derivative of 1. It equals 0. Those two terms knock each other out and leave just the nice term that we're seeing. So the derivative of the eigenvalue, just to have one more look at it before we leave it. The derivative of the eigenvalue is this formula. It's the rate at which the matrix is changing times the eigenvectors on right and left. Sometimes they're called the right eigenvector and the left eigenvector at the time t. So A is, so we're not seeing in this d lambda dt, in other words, is not, I don't need to, I get a nice formula, which doesn't involve the derivative of the eigenvector. That's the beauty of it. If I want to go up to take the next step, I tried this weekend, but it's a mess. It would be to take the, so this is my formula then, d lambda dt equals this. And I can take the next derivative of that. And it will involve d second A dt squared, right? But it will also involve dx dt and dy dt. And in fact, the pseudo inverse even shows up. It's another step, and I'm not going that far, because we've got the best formula there. So now that has answered this question. And I could answer that question the same way. It would involve A transpose A and the singular vectors instead of involving A and the eigenvectors. Maybe that's a suitable exercise. I don't know. I haven't done it myself. What I want to do is this. Now say, what can we say about the change in the eigenvalue? And I'll just stay, first of all, with eigenvalues. When the change is like rank 1. This is a perfect example, when the change is rank 1. So what can we say about the eigenvalues? Let's take the top, the largest eigenvalue, or all of them, lambda j, all of them, of A plus A rank 1 matrix uv transpose. Oh, let me make it, let's do the nice case here, the nice case. Because if I allow a general matrix A, I have to worry about does it have enough eigenvectors? Can it diagonalize? All that stuff. Let's make it a symmetric matrix. And let's make the rank 1 change symmetric too. So the question is, what can I say about the eigenvalues after a rank 1 change? So again, this isn't calculus now, because the change that I'm making is fully, is a true vector and not a differential. And I'm not going to have an exact formula for the new eigenvalues, as I said. But what I am going to do is write down the beautiful facts that are known about that. And here they are. So first of all, the eigenvalues are in descending order. We use descending order for the singular values. Let's use them also for eigenvalues. So lambda 1 is greater or equal to lambda 2, greater or equal to lambda 3, and so on. Oh, give me an idea. What do you expect from that rank 1 change? So that change is rank 1. Can you tell me any more about that change, UU transpose? What kind of a matrix is UU transpose? It's rank 1, but we can say more. It is symmetric, of course. And it is positive semidefinite. Positive semidefinite. This is a positive change. UU transpose is the typical rank 1 positive semidefinite. It couldn't be positive definite, because it's only got rank 1. What's the eigenvector of that matrix? Why not here? We can do this in two seconds. So UU transpose, that's the matrix I'm asking you to think about. And it's a full n by n matrix, column times a row. Tell me an eigenvector of that matrix. Yes? U, if I multiply my matrix by U, I get, what do I get? I get some number times U. And what is that number lambda? That lambda happens to be U transpose U. And what is the, so that's different from UU transpose. This was a matrix. Everybody, this is 18.065 now. That's a number. And what can you tell me about that number? It is greater, well, even more, greater than 0. Greater, because this is a true vector. So this is greater than 0. It's the only eigenvalue. All the other eigenvalues of that rank 1 matrix are 0. But the one non-zero eigenvalue is over on the plus side. It's U transpose U. We all recognize that as the length of U squared. It's certainly positive. So we do have a positive semidefinite matrix. What would your guess be of the effect on the eigenvalues of A? So I'm coming back to my real problem, eigenvalues of S. Sorry, S. I have symmetric matrices. I'm saying symmetric. What is your guess, if I have a symmetric matrix and I add on UU transpose, what do you imagine that does to the eigenvalues? You're going to get it right. Just say it. What happens to the eigenvalues of S if I add on UU transpose? They will be more positive. They'll go up. This is a positive thing, like adding 17 to something. It moves up. So therefore, what I believe is, so now, I have two sets of eigenvalues now. One is the eigenvalues of S. The other is the different eigenvalues of S. So I can't call them both lambdas or I'm in trouble. So let me start. So do you have a favorite other Greek letter for the eigenvalues of S? Gamma. OK, gamma. As long as you say a Greek letter, I have some idea how to write. Zeta, it seems to me, like the world's toughest letter to write. And electrical engineers can coolly flush off a zeta. But I've never succeeded. So I'll write, what did you say? Gamma. J of the original. OK, so those are the eigenvalues of the original. These are the eigenvalues of the modified. And we're expecting the lambdas to be bigger than the gammas. It's just a qualitative statement. And it's true. Each lambda is bigger than the gamma. Sorry, yeah, yeah. Each lambda, by adding this stuff, the lambdas are bigger than. So I'll just write that. Lambdas are bigger than gammas. And that's a fundamental fact, which we could prove. But a little more is known. Of course, the question is, how much bigger? How much can they be way bigger? Well, I don't believe they could be bigger by more than that number myself. But there's better news than that. So the lambdas are bigger than the gammas. So lambda 1 is bigger than gamma 1. So this is the S plus UU transpose matrix. And these are the eigenvalues of the S matrix. Lambda 1 is bigger than gamma 1. But look what's happening in this line of text here. I'm saying that lambda 2 is smaller than gamma 1. Isn't that neat? The eigenvalues go up, but they don't just go anywhere. And that's called interlacing. So this is one of those wonderful theorems that makes your heart happy, that if I do a rank 1 change, and it's a positive change, then the eigenvalues increase. But they don't increase. The new eigenvalue is below. The new second eigenvalue doesn't pass up the old first eigenvalue. And the new third eigenvalue doesn't pass up the old second eigenvalue. So that's the interlacing theorem that's associated with the names of famous math guys. And it's just, of course, you have to say that's beautiful. While we're writing down such a theorem, make a guess of what the theorem would be if I do a rank 2 change. Suppose I do an S staying symmetric. And I do a rank 1 change. But then I also do a rank 2 change, say, W W transpose. So what's the deal here? What do I know about that? The change matrix, the delta S here, I know its rank is 2. I'm assuming U and W are not in the same direction. So that's a rank 2 matrix. And what can you tell me about the eigenvalues of that rank 2 matrix? So it's got n eigenvalues, because it's an n by n matrix. But how many non-zero eigenvalues has it got? 2, because its rank is 2. The rank tells you the number of non-zero eigenvalues when matrices are symmetric. It doesn't tell you enough. If matrices are unsymmetric, eigenvalues can be weird. So stay symmetric here. OK, so this has two non-zero eigenvalues. And can you tell me their sign? Is that matrix positive semidefinite? Yes, of course it is. Of course. So this was, and this was, and together it certainly is. So now I've added a rank 2 positive semidefinite matrix. And now just, I'm not going to rewrite this line, but what would you expect to be true? You would expect that the eigenvalues increase. But how big could lambda 3, or sorry, gamma, yeah. So gamma 2, let's follow gamma 2. So now, well, maybe I should use another. Do the Greeks have any other letters than lambda and gamma? They must have had some. Who? C? How about that? Who knows when I can write? Alpha, good, alpha, yes, alpha. So alpha is the eigenvalues of this rank 2 change. OK, now what am I going to be able to say? Can I say anything about the, well, of course, alpha 1 is bigger than lambda 1, which was bigger than, eigenvalues are going up, right? I'm adding positive definite or positive semidefinite stuff. There's no way eigenvalues can start going down on me. So alpha 1 is greater than or equal to the lambda 1, which had just a rank 1 change, which is greater equal to the mu, was it mu? Gamma, gamma 1, which is, and that's, yeah, and so on, OK. Now, let's see, is alpha 1, is gamma 1 bigger than alpha? What am I struggling to write down here? What could I say? Well, what can I say that reflects the fact that this lambda 2, sorry, it reflects, so gamma 1 went up, gamma 1 was bigger than lambda 2. That was the point here. Gamma 1 is bigger. So this was sort of easy, because I'm adding stuff. I expected the lambdas to go up. This is where the theorem is, that it didn't go up so far as to pass, or sorry, the lambda 2, which went up, didn't pass up gamma 1. Lambda 2 didn't pass up gamma 1. And now, let me write those words down. Now, the alpha 2, well, could alpha 2 pass up lambda 1? And what about alpha 3? So let me say what I believe. I think alpha 2, which is like 1 behind, but I'm adding rank 2. I think alpha 2 could pass up lambda 1. It could pass lambda 1, but alpha 3 can't. I believe that alpha 3 is smaller than lambda 1. So alpha 3 is smaller than gamma 1, the original guy. Yeah, yeah. Yeah. OK. Yeah. Anyway, I'll get it right in the notes. You know what question I'm asking, and for me, that's the important thing. Now, there is a little matter of why is this true. This is the good case. Let me give you another example of interlacing. Can I do that? It really comes from this. But let me give you another example that's just striking. So I have a symmetric matrix, N by N. Call it S. And then I throw away the last row and column. So in here is S, N minus 1. The big matrix was S, N. This one is of size N minus 1. So it's got sort of less degrees of freedom, because the last degree of freedom got removed. And what do you think about the eigenvalues of this? The N minus 1 eigenvalues of this and the N eigenvalues of that. They interlace. So the eigenvalues, so this has eigenvalue lambda 1. This would have an eigenvalue smaller than that. This would have an eigenvalue lambda 2. This would have an eigenvalue smaller than that, and so on. Just the same interlacing, and basically for the same reason. That when you, like it's, this reduction to size N minus 1 is like I'm saying, XN has to be 0 in the energy or any of those expressions. And the fact of making XN be 0 is like a 1 constraint, taking one degree of freedom away. It reduces the eigenvalues, but not by 2. OK, now I have one final mystery. And let me try to tell you what it worried me. Now, what is it that worried me? Yes, suppose this U, suppose this change, this U, this change that I'm making. Suppose it's actually the second eigenvector of S. So can I write this down? Suppose U is actually the second eigenvector of S. What do I mean by that? So I mean that S times U is lambda 2 times U. Now I look at, now I'm going to change it, S plus UU transpose. That's what I've been looking at, and that moves the eigenvalues up. But what worries me is like if I multiply this by 20, some big number, I'm going to move that eigenvalue way up, way past. I got worried about this inequality, right? When I add this, that same U is lambda 2 plus 20 U, right? SU is lambda 2 U, and this 20 is 20. U is a unit vector. So you see my worry? Here I'm doing a rank 1 change, but it's moved an eigenvalue way, way up. So how could this statement be true? So I've just figured out here what gamma 2, well, yeah. You see my question? I could leave it as a question to answer next time. Let me do that. And I'll put it on the online so you'll see it clearly. It looks like, and it happens, that this eigenvector now has eigenvalue lambda 2 plus 20. Why doesn't that blow away this statement? OK, I'll put that because it's sort of coming with minus 10 seconds to go in the class. So let's leave that. And a discussion of this for next time. But I'm happy with this lecture if you are. The last lecture, I got U's and V's mixed up, and it's not reliable. Here, I like the proof of d lambda dt, and we're started on this topic too. Good, thank you.